<html><head><title>NVIDIA GPUDirect Storage Overview Guide - NVIDIA Docs</title></head><body><body class="Page-body">
 <!-- Putting icons here, so we don't have to include in a bunch of -body hbs's -->
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <ul class="Navigation-items">
  <li class="Navigation-items-item">
   <a href="https://developer.nvidia.com/" target="_blank">
    NVIDIA Developer
   </a>
  </li>
  <li class="Navigation-items-item">
   <a href="https://developer.nvidia.com/blog/" target="_blank">
    Blog
   </a>
  </li>
  <li class="Navigation-items-item">
   <a href="https://forums.developer.nvidia.com/" target="_blank">
    Forums
   </a>
  </li>
  <li class="Navigation-items-item">
   <a class="Button" data-size="small" data-theme="secondary" href="https://docs.nvidia.com/login" rel="nofollow noopener" style="--button-border-radius: 0px">
    Join
   </a>
  </li>
 </ul>
 <a aria-label="home page" href="https://docs.nvidia.com/">
 </a>
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <ul class="Navigation-items">
  <li class="Navigation-items-item">
   <a data-cms-ai="0" href="https://developer.nvidia.com/" target="_blank">
    NVIDIA Developer
   </a>
  </li>
  <li class="Navigation-items-item">
   <a data-cms-ai="0" href="https://developer.nvidia.com/blog/" target="_blank">
    Blog
   </a>
  </li>
  <li class="Navigation-items-item">
   <a data-cms-ai="0" href="https://forums.developer.nvidia.com/" target="_blank">
    Forums
   </a>
  </li>
  <li class="Navigation-items-item">
   <a class="Button" data-cms-ai="0" data-size="small" data-theme="secondary" href="https://docs.nvidia.com/login" rel="nofollow noopener" style="--button-border-radius: 0px">
    Join
   </a>
  </li>
 </ul>
 <span class="label">
  Menu
 </span>
 <a class="Page-BackToTop" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html">
 </a>
 <h1 class="PageHeading-title">
  NVIDIA GPUDirect Storage Overview Guide
 </h1>
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/">
  NVIDIA Docs Hub
 </a>
 <span class="Link">
  NVIDIA GPUDirect Storage (GDS)
 </span>
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/index.html">
  NVIDIA GPUDirect Storage
 </a>
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html">
  NVIDIA GPUDirect Storage Overview Guide
 </a>
 <span class="TopicPage-version">
  NVIDIA GPUDirect Storage (GDS) (Latest Release)
 </span>
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/pdf/overview-guide.pdf">
  Download PDF
 </a>
 <h2>
  <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#abstract" id="abstract" name="abstract" shape="rect">
   NVIDIA Magnum IO GPUDirect Storage Overview Guide
  </a>
 </h2>
 <p>
  The NVIDIA® Magnum IO GPUDirect® Storage Overview Guide provides a high-level overview of GDS.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#overview-intro">
   1. Introduction
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="overview-intro" name="overview-intro" shape="rect">
  </a>
  GDS enables a direct data path for direct memory access (DMA) transfers between GPU memory and storage, which avoids a bounce buffer through the CPU. This direct path increases system bandwidth and decreases the latency and utilization load on the CPU.
 </p>
 <p>
  This guide provides a high-level overview of GPUDirect Storage (GDS), guidance to help you enable filesystems for GDS, and some insights about the features of a file system and how it relates to GDS. The guide also outlines the functionalities, considerations, and software architecture about GDS. This high-level introduction sets the stage for deeper technical information in the
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html" shape="rect">
   cuFile API Reference Guide
  </a>
  for GDS users who need to modify the kernel.
  <a class="Link" data-cms-ai="0" id="related-docs" name="related-docs" shape="rect">
  </a>
 </p>
 <h3 id="related-docs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#related-docs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#related-docs" id="related-docs" name="related-docs" shape="rect">
    1.1. Related Documents
   </a>
  </a>
 </h3>
 <p>
  The following documents and online resources provide support and provide additional context for the optimal use of and understanding of this specification: Refer to the following guides for more information about GDS:
  <a class="Link" data-cms-ai="0" id="related-docs__ul_sl5_qv1_hnb" name="related-docs__ul_sl5_qv1_hnb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html" shape="rect">
    GPUDirect Storage Design Guide
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html" shape="rect">
    cuFile API Reference Guide
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html" shape="rect">
    GPUDirect Storage Release Notes
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html" shape="rect">
    GPUDirect Storage Benchmarking and Configuration Guide
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html" shape="rect">
    GPUDirect Storage Best Practices Guide
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html" shape="rect">
    GPUDirect Storage Installation and Troubleshooting Guide
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html" shape="rect">
    GPUDirect Storage O_DIRECT Requirements Guide
   </a>
  </li>
 </ul>
 <p>
  To learn more about GDS, refer to the following blogs:
  <a class="Link" data-cms-ai="0" id="related-docs__ul_ccg_1ty_2nb" name="related-docs__ul_ccg_1ty_2nb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   <a class="xref" data-cms-ai="0" href="https://devblogs.nvidia.com/gpudirect-storage/" shape="rect">
    GPUDirect Storage: A Direct Path Between Storage and GPU Memory
   </a>
   .
  </li>
  <li>
   The
   <a class="xref" data-cms-ai="0" href="https://developer.nvidia.com/blog/tag/magnum-io/" shape="rect">
    Magnum IO
   </a>
   blog.
  </li>
 </ul>
 <h3 id="dev-benefits">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dev-benefits">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dev-benefits" id="dev-benefits" name="dev-benefits" shape="rect">
    1.2. Benefits for a Developer
   </a>
  </a>
 </h3>
 <p>
  GDS provides the following benefits for application developers:
 </p>
 <ul>
  <li>
   Enables a direct path between GPU memory and storage.
  </li>
  <li>
   Increases the bandwidth, reduces the latency, and reduces the load on CPUs and GPUs for data transferral.
  </li>
  <li>
   Reduces the performance impact and dependence on CPUs to process storage data transfer.
  </li>
  <li>
   Performance force multiplier on top of the compute advantage for computational pipelines that are fully migrated to the GPU so that the GPU, rather than the CPU, has the first and last touch of data that moves between storage and the GPU.
  </li>
  <li>
   Supports interoperability with other OS-based file access, which enables data to be transferred to and from the device by using traditional file IO, which is then accessed by a program that uses the cuFile APIs.
  </li>
 </ul>
 <p>
  The cuFile APIs and their implementations provide the following benefits:
 </p>
 <ul>
  <li>
   A family of APIs that provide CUDA® applications with the best-performing access to local or distributed file and block storage.
  </li>
  <li>
   When transferring to and from the GPU, increased performance relative to existing standard Linux file IO.
  </li>
  <li>
   Greater ease of use by removing the need for the careful expert management of memory allocation and data movement.
  </li>
  <li>
   A simpler API sequence that is relative to existing implicit file-GPU data movement methods, which require a more complex management of memory and data movement on and between the CPU and GPU.
  </li>
  <li>
   Generality across a variety of storage types that span various local and distributed file systems, block interfaces, and namespace systems, including standard Linux and third-party solutions.
  </li>
  <li>
   Primary API to do file I/O independent of memory type for GPU applications.
  </li>
 </ul>
 <p>
  The Stream subset of the cuFile APIs provide the following benefits:
 </p>
 <ul>
  <li>
   Asynchronous offloaded operations are ordered with respect to a CUDA stream.
   <ul>
    <li>
     IO after compute: The GPU kernel produces data before it is transferred to IO.
    </li>
    <li>
     Compute after IO: After the data transfer is complete, the GPU kernel can proceed.
    </li>
   </ul>
  </li>
  <li>
   Available concurrency across streams.
   <ul>
    <li>
     Using different CUDA streams allows the possibility of concurrent execution and the concurrent use of multiple DMA engines.
    </li>
   </ul>
  </li>
 </ul>
 <h3 id="intended-uses">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#intended-uses">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#intended-uses" id="intended-uses" name="intended-uses" shape="rect">
    1.3. Intended Uses
   </a>
  </a>
 </h3>
 <p>
  cuFile features can be used in the following ways:
 </p>
 <ul>
  <li>
   cuFile implementations boost throughput when IO between storage and GPU memory is a performance bottleneck.
   <p>
    This condition arises in cases where the compute pipeline has been migrated to the GPU from the CPU, so that the first and last agents to touch data, before or after transfers with storage, execute on the GPU.
   </p>
  </li>
  <li>
   cuFile APIs are currently explicit, and reading or writing between storage and buffers that completely fit into the available GPU physical memory.
  </li>
  <li>
   Rather than fine-grained random access, the cuFile APIs are a suitable match for coarse-grained streaming transfers.
  </li>
  <li>
   For fine-grained accesses, the underlying software overheads for making a kernel transition and going through the operating system can be amortized.
  </li>
 </ul>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#func-overview">
   2. Functional Overview
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="func-overview" name="func-overview" shape="rect">
  </a>
  This section provides a functional overview of GDS. It covers basic usage, generality, performance considerations, and a scope of the solution. This documentation applies to the cuFile APIs, which are issued from the CPU.
 </p>
 <h3 id="explicit-and-direct">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#explicit-and-direct">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#explicit-and-direct" id="explicit-and-direct" name="explicit-and-direct" shape="rect">
    2.1. Explicit and Direct
   </a>
  </a>
 </h3>
 <p>
  GDS is a performance-centric solution, so the performance of an end-to-end transfer is a function of latency overheads and the maximal achievable bandwidth.
 </p>
 <p>
  The following are some terms used in GDS:
 </p>
 Explicit programmatic request
 An explicit programmatic request that immediately invokes the transfer between the storage and the GPU memory is
 proactive
 .
 Implicit request
 An implicit request to storage, which is induced by a memory reference that causes a page miss from the GPU back to the CPU, and potentially the CPU to storage, is
 reactive
 .
 Note:
 <p>
  Reactive activity tends to induce more overhead. As a result of being explicit and proactive, GDS maximizes performance with its explicit cuFile APIs.
 </p>
 <p>
  Latency is lower when extra copies are avoided, and the highest bandwidth paths are taken. Without GDS, an extra copy through a bounce buffer in the CPU is necessary, which introduces latency and lowers effective bandwidth.
 </p>
 Note:
 <p>
  The latency improvements from GDS are most apparent with small transfers.
 </p>
 <p>
  With GDS, although there are exceptions, a zero-copy approach is possible. Additionally, when a copy through the CPU is no longer necessary, the data path does not include the CPU. On some systems, a direct path between local or remote storage that goes through a PCIe switch offers at least twice the peak bandwidth as compared to taking a data path through the CPU. Using cuFile APIs to access GDS technology enables explicit and direct transfers, which offers lower latency and higher bandwidth. For direct data transfers between GPU memory and storage, the file must be opened in
  <span>
   O_DIRECT
  </span>
  mode. If the file is not opened in this mode, contents might be buffered in the CPU system memory, which is incompatible with direct transfers.
 </p>
 Note:
 <p>
  Starting from CUDA Toolkit 12.2 (GDS version 1.7.x) even for files opened in non-O_DIRECT mode, the cuFile library takes the GDS driven O_DIRECT path for transfers between GPU memory and storage for page aligned buffers with aligned sizes and offsets.
 </p>
 <h3 id="explicit-and-direct__section_ilp_l2x_nnb">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#explicit-and-direct__section_ilp_l2x_nnb">
   Explicit Copy versus Using mmap
  </a>
 </h3>
 <p>
  The following code samples compare the code sequences of an explicit copy versus using mmap and incurring an implicit page fault where necessary. This code sample uses explicit copy:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>int fd = open(file_name,...)
void *sysmem_buf, *gpumem_buf;
sysmem_buf = malloc(buf_size);
cudaMalloc(gpumem_buf, buf_size);
pread(fd, sysmem_buf, buf_size);
cudaMemcpy(sysmem_buf, 
  gpumem_buf, buf_size, H2D); 
doit&lt;&lt;&lt;gpumem_buf, …&gt;&gt;&gt; 
// no faults;</p>
        </pre>
 <p>
  This code sample uses mmap:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>int fd = open(file_name, O_DIRECT,...)
void *mgd_mem_buf;
cudaMallocManaged(mgd_mem_buf, buf_size);
mmap(mgd_mem_buf, buf_size, …, fd, …)
doit&lt;&lt;&lt;mgd_mem_buf, …&gt;&gt;&gt; 
// fault on references to mgdmem_buf</p>
        </pre>
 <p>
  In the first sample,
  <span>
   pread
  </span>
  is used to move data from storage into a CPU bounce buffer,
  <span>
   sysmem_buf
  </span>
  , and
  <span>
   cudaMemcpy
  </span>
  is used to move that data to the GPU. In the second sample,
  <span>
   mmap
  </span>
  makes the managed memory backed by the file. The references to managed memory from the GPU that are not present in GPU memory will induce a fault back to the CPU and then to storage, which causes an implicit transfer.
 </p>
 <p>
  GDS enables DMA between agents (NICs or NVMe drives) near storage and GPU memory. Traditional POSIX read and write APIs only work with addresses of buffers that reside in CPU system memory. cuFile APIs, in contrast, operate on addresses of buffers that reside in GPU memory. So they look very similar, but have a few differences, as shown in Figure 2.
  <a class="Link" data-cms-ai="0" id="explicit-and-direct__section_bf2_t2x_nnb" name="explicit-and-direct__section_bf2_t2x_nnb" shape="rect">
  </a>
 </p>
 <h3 id="explicit-and-direct__section_bf2_t2x_nnb">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#explicit-and-direct__section_bf2_t2x_nnb">
   Comparing the POSIX APIs and the cuFile APIs
  </a>
 </h3>
 <p>
  The following code samples compare the POSIX APIs and cuFile APIs. POSIX pread and pwrite require buffers in CPU system memory and an extra copy, but cuFile read and write only requires file handle registration. This code sample uses the POSIX APIs:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>int fd = open(...)
void *sysmem_buf, *gpumem_buf;
sysmem_buf = malloc(buf_size);
cudaMalloc(gpumem_buf, buf_size);
pread(fd, sysmem_buf, buf_size);
cudaMemcpy(sysmem_buf, 
  gpumem_buf, buf_size, H2D); 
doit&lt;&lt;&lt;gpumem_buf, …&gt;&gt;&gt;</p>
        </pre>
 <p>
  This code sample uses the cuFile APIs:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>int fd = open(file_name, O_DIRECT,...)
CUFileHandle_t *fh; 
CUFileDescr_t desc; 
desc.type=CU_FILE_HANDLE_TYPE_OPAQUE_FD;
desc.handle.fd = fd;
cuFileHandleRegister(&amp;fh, &amp;desc);
void *gpumem_buf;
cudaMalloc(gpumem_buf, buf_size);
cuFileRead(&amp;fh, gpumem_buf, buf_size, …);
doit&lt;&lt;&lt;gpumem_buf, …&gt;&gt;&gt;</p>
        </pre>
 <p>
  The essential cuFile functionalities are:
 </p>
 <ul>
  <li>
   Explicit data transfers between storage and GPU memory, which closely mimic POSIX
   <span>
    pread
   </span>
   and
   <span>
    pwrite
   </span>
   .
  </li>
  <li>
   Performing IO in a CUDA stream, so that it is both async and ordered relative to the other commands in that same stream.
  </li>
 </ul>
 <p>
  The direct data path that GDS provides relies on the availability of file system drivers that are enabled with GDS. These drivers run on the CPU and implement the control path that sets up the direct data path.
 </p>
 <h3 id="perf-optimize">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#perf-optimize">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#perf-optimize" id="perf-optimize" name="perf-optimize" shape="rect">
    2.2. Performance Optimizations
   </a>
  </a>
 </h3>
 <p>
  After there is a viable path to explicitly and directly move data between storage and GPU memory, there are additional opportunities to improve performance.
 </p>
 <h3 id="imp-perf-enhance">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#imp-perf-enhance">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#imp-perf-enhance" id="imp-perf-enhance" name="imp-perf-enhance" shape="rect">
    2.2.1. Implementation Performance Enhancements
   </a>
  </a>
 </h3>
 <p>
  GDS provides a user interface that abstracts the implementation details. With the performance optimizations in that implementation, there are trade offs that are enhanced over time and are tuned to each platform and topology.
 </p>
 <p>
  The following graphic shows you a list of some of these performance optimizations:
  <a class="Link" data-cms-ai="0" id="imp-perf-enhance__fig_p2j_qy5_fnb" name="imp-perf-enhance__fig_p2j_qy5_fnb" shape="rect">
  </a>
 </p>
 <p>
  Figure 1. Performance Optimizations
 </p>
 <ul class="ul" id="imp-perf-enhance__ul_nrg_yzs_4mb">
  <li class="li">
   Path selection
   There might be multiple paths available between endpoints. In an NVIDIA DGX-2™ system, for example, GPU A and GPU B that are connected to CPU sockets CPU A and CPU B respectively may be connected via two paths.
   <a class="Link" data-cms-ai="0" id="imp-perf-enhance__ul_olx_zzs_4mb" name="imp-perf-enhance__ul_olx_zzs_4mb" shape="rect">
   </a>
   <ul>
    <li>
     GPU A
     --&gt;
     CPU A PCIe root port
     --&gt;
     CPU A to CPU B
     via the
     CPU interconnect
     --&gt;
     CPU B
     along another PCIe path to
     GPU B
     .
    </li>
    <li>
     GPU A
     --&gt;
     GPU B
     using NVLink.
    </li>
   </ul>
   <p>
    Similarly, a NIC that is attached to CPU A and to GPU A via PCIe by using an intervening switch has a choice of data paths to GPU B:
    <a class="Link" data-cms-ai="0" id="imp-perf-enhance__ul_dbh_41t_4mb" name="imp-perf-enhance__ul_dbh_41t_4mb" shape="rect">
    </a>
   </p>
   <ul>
    <li>
     The
     NIC
     --&gt;
     CPU A PCIe root port
     ,
     CPU A
     --&gt;
     CPU B
     via CPU interconnect, and CPU B along another
     PCIe path
     --&gt;
     GPU B
     .
    </li>
    <li>
     The
     NIC
     --&gt; a staging buffer in
     GPU A
     and
     NVLink
     --&gt;
     GPU B
     .
    </li>
   </ul>
  </li>
  <li class="li">
   Staging in intermediate buffers
   <p>
    Bulk data transfers are performed with DMA copy engines. Not all paths through a system are possible with a single-stage transfer, and sometimes a transfer is broken into multiple stages with a staging buffer along the way.
   </p>
   <p>
    In the NIC-GPU A-GPU B example in the graphic, a staging buffer in GPU A is required, and the DMA engine in GPU A or GPU B is used to transfer data between GPU A’s memory and GPU B’s memory.
   </p>
   <p>
    Data might be transferred through the CPUs along PCIe only or directly between GPUs over NVLink. Although DMA engines can reach across PCIe endpoints, paths that involve the NVLink may involve staging through a buffer (GPU A).
   </p>
  </li>
  <li class="li">
   Dynamic routing
   <p>
    Paths and staging. The two paths in the previous graphic are available between endpoints on the left half and the right half, the red PCIe path or the green NVLink path.
   </p>
  </li>
 </ul>
 <h3 id="concurrency-across-threads">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#concurrency-across-threads">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#concurrency-across-threads" id="concurrency-across-threads" name="concurrency-across-threads" shape="rect">
    2.2.2. Concurrency Across Threads
   </a>
  </a>
 </h3>
 Note:
 <p>
  All APIs are thread safe.
 </p>
 <p>
  Using GDS is a performance optimization. After the applications are functionally enabled to move data directly between storage and a GPU buffer by passing a pointer to the GPU buffer down through application layers, performance is the next concern. IO performance at the system level comes from concurrent transfers on multiple links and across multiple devices. Concurrent transfers for each 4 x 4 NVMe PCIe device is necessary to get full bandwidth from one x16 PCIe link. Since there are PCIe links to each GPU and to each NIC, many concurrent transfers are necessary to saturate the system. GDS does not boost concurrency, so this level of performance tuning is managed by the application.
 </p>
 <h3 id="asychrony">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#asychrony">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#asychrony" id="asychrony" name="asychrony" shape="rect">
    Asynchrony
   </a>
  </a>
 </h3>
 <p>
  Another form of concurrency, between the CPU and one or more GPUs, can be achieved in an application thread through asynchrony.
 </p>
 <p>
  In this process, work is submitted for deferred execution by the CPU, and the CPU can continue to submit more work to a GPU or complete the work on the CPU. This process adds support in CUDA for async IO, which can enable a graph of interdependent work that includes IO to be submitted for deferred execution.
 </p>
 <h3 id="batching">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#batching">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#batching" id="batching" name="batching" shape="rect">
    2.2.4. Batching
   </a>
  </a>
 </h3>
 <p>
  There is a fixed overhead involved with each submission from an application into the cuFile implementation. For usage models where many IO transactions get submitted simultaneously, batching reduces the overhead by amortizing that fixed overhead across the transactions in the batch, which improves performance.
 </p>
 <p>
  Batch APIs are asynchronous in nature. Refer to the
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html" shape="rect">
   cuFile API Reference Guide
  </a>
  for more information.
 </p>
 <h3 id="streams">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#streams">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#streams" id="streams" name="streams" shape="rect">
    2.2.5. Use of CUDA Streams in cuFile
   </a>
  </a>
 </h3>
 <p>
  Provides a mechanism to perform asynchronous submission and asynchronous execution of I/Os following CUDA stream semantics.
 </p>
 <p>
  The data size, offsets in buffer and file may dynamically change within the valid range based on the execution of the previous CUDA kernel or function.
 </p>
 <h3 id="comp-and-gen">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#comp-and-gen">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#comp-and-gen" id="comp-and-gen" name="comp-and-gen" shape="rect">
    2.3. Compatibility and Generality
   </a>
  </a>
 </h3>
 <p>
  Although the purpose of GDS is to avoid using a bounce buffer in CPU system memory, the ability to fall back to this approach allows the cuFile APIs to be used ubiquitously even under suboptimal circumstances. A compatibility mode is available for unsupported configurations that maps IO operations to a fallback path.
 </p>
 <p>
  This path stages through CPU system memory for systems where one or more of the following conditions is true:
  <a class="Link" data-cms-ai="0" id="comp-and-gen__ul_zzl_pft_4mb" name="comp-and-gen__ul_zzl_pft_4mb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   Explicit configuration control by using the user version of the
   <span>
    cufile.json
   </span>
   file.
   <p>
    Refer to the
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html" shape="rect">
     cuFile API Reference Guide
    </a>
    for more information.
   </p>
  </li>
  <li>
   The lack of availability of the
   <span>
    nvidia-fs.ko
   </span>
   kernel driver, for example, because it was not installed on the host machine, where a container with an application that uses cuFile, is running.
  </li>
  <li>
   The lack of availability of relevant GDS-enabled filesystems on the selected file mounts, for example, because one of several used system mounts does not support GDS.
  </li>
  <li>
   File-system-specific conditions, such as when
   <span>
    O_DIRECT
   </span>
   cannot be applied.
   <p>
    Vendors, middleware developers, and users who are doing a low-level analysis of file systems should review the
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html" shape="rect">
     GPUDirect Storage O_DIRECT Requirements Guide
    </a>
    for more information.
   </p>
  </li>
 </ul>
 <p>
  Refer to
  <span>
   cuFileHandleRegister
  </span>
  in the
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html" shape="rect">
   cuFile API Reference Guide
  </a>
  for more information. Performance on GPU-based applications that transfer between the storage and GPU memory in compatibility mode is at least the same or better than current CPU-based APIs when GDS is not used. Testing for the CPU path is limited to POSIX-based APIs and qualified platforms and filesystems that do not include GDS. Even when transfers are possible with GDS, a direct transfer is not always possible. Here is a sampling of cases that are handled seamlessly by the cuFile APIs:
  <a class="Link" data-cms-ai="0" id="comp-and-gen__ul_sz3_5ft_4mb" name="comp-and-gen__ul_sz3_5ft_4mb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   The buffer is not aligned, such as the following:
   <a class="Link" data-cms-ai="0" id="comp-and-gen__ul_kk2_wft_4mb" name="comp-and-gen__ul_kk2_wft_4mb" shape="rect">
   </a>
   <ul>
    <li>
     The offsets of the file are not cufile block size aligned.
    </li>
    <li>
     The GPU memory buffer address is not cufile block size aligned.
    </li>
    <li>
     The IO request size is not a multiple of cufile block size.
    </li>
    <li>
     The requested IO size is too small, and the filesystem cannot support RDMA.
    </li>
   </ul>
  </li>
  <li>
   The size of the transfer exceeds the size of the GPU BAR1 aperture.
  </li>
  <li>
   The optimal transfer path between the GPU memory buffer and storage involves an intermediate staging buffer, for example, to use NVLink.
  </li>
 </ul>
 <p>
  The compatibility mode and the seamless handling of cases that require extra steps broaden the generality of GDS and makes it easier to use.
 </p>
 <h3 id="monitoring">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#monitoring">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#monitoring" id="monitoring" name="monitoring" shape="rect">
    2.4. Monitoring
   </a>
  </a>
 </h3>
 <p>
  This section provides information about the monitoring facilities that are available to track functional and performance issues in GDS.
 </p>
 <p>
  GDS supports the following monitoring facilities for tracking functional and performance issues:
  <a class="Link" data-cms-ai="0" id="monitoring__ul_r44_sgt_4mb" name="monitoring__ul_r44_sgt_4mb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   <span>
    Ftrace
   </span>
   <p>
    Exported symbols for GDS functions can be traced using
    <span>
     Ftrace
    </span>
    . You can also use static tracepoints in the
    <span>
     libcufile.so
    </span>
    library, but the tracepoints are not yet supported for
    <span>
     nvidia-fs.ko
    </span>
    . Refer to the
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html" shape="rect">
     GPUDirect Storage Troubleshooting Guide
    </a>
    for more information.
   </p>
  </li>
  <li>
   <span>
    Logging
   </span>
   <p>
    Error conditions and debugging outputs can be generated in a log file. This information is useful for conditions that affect many of the APIs but need only be reported once or affect APIs with no return value to report errors. The
    <span>
     cufile.json
    </span>
    file is used to select at least reporting level, such as
    <span>
     ERROR
    </span>
    ,
    <span>
     WARN
    </span>
    ,
    <span>
     INFO
    </span>
    ,
    <span>
     DEBUG
    </span>
    , and
    <span>
     TRACE
    </span>
    .
   </p>
  </li>
  <li>
   <span>
    Profiling
   </span>
   <p>
    GDS can be configured to collect a variety of statistics.
   </p>
  </li>
 </ul>
 <p>
  These facilities, and the limitations of third-party tools support, are described in greater detail in the
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html" shape="rect">
   GPUDirect Storage Troubleshooting Guide
  </a>
  .
 </p>
 <h3 id="solution-scope">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#solution-scope">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#solution-scope" id="solution-scope" name="solution-scope" shape="rect">
    2.5. Scope of the Solutions in GDS
   </a>
  </a>
 </h3>
 <p>
  Here is some information about the solutions that are available in GDS.
 </p>
 <p>
  GDS has added new APIs with functionality that is not supported by today’s operating systems, including direct transfers to GPU buffers, asynchrony, and batching. These APIs offer a performance boost, with a platform-tuned and topology-tuned selection of paths and staging, which add enduring value. The implementations under cuFile APIs overcome limitations in current operating systems. Some of those limitations are transient and may be removed in future versions of operating systems. Although these solutions are not currently available and may require time for adoption, other GDS-enabled solutions are needed today. Here are the solutions currently available in GDS:
 </p>
 <ul>
  <li>
   Third-party vendor solutions for distributed file systems.
  </li>
  <li>
   Long-term support through open source, upstreamed Linux that future GDS implementations will seamlessly use.
  </li>
  <li>
   Local filesystem support by using modified storage drivers.
  </li>
  <li>
   The overall cuFile architecture involves a combination of components, some from NVIDIA and some from third parties.
  </li>
  <li>
   Here is a list of the NVIDIA-originated content:
   <ul>
    <li>
     User-level cuFile library,
     <span>
      libcufile.so
     </span>
     , which implements the following in the closed source code:
     <ul>
      <li>
       cuFile Driver APIs:
       <ul>
        <li>
         <span>
          cuFileDriver{Open, Close}
         </span>
        </li>
        <li>
         <span>
          cuFileDriver{GetProperties, Set*}
         </span>
        </li>
       </ul>
      </li>
      <li>
       cuFile synchronous IO APIs:
       <ul>
        <li>
         <span>
          cuFileHandle{Register, Deregister}
         </span>
        </li>
        <li>
         <span>
          cuFileBuf(Register, Deregister}
         </span>
        </li>
        <li>
         <span>
          cuFile{Read, Write}
         </span>
        </li>
        <li>
         &lt;li class="li"&gt;&lt;/li&gt;
        </li>
       </ul>
      </li>
      <li>
       Stream subset of the cuFile APIs:
       <ul>
        <li>
         <span>
          cuFile{Read, Write}Async, cuFileStreamRegister, cuFileStreamDeregister
         </span>
        </li>
       </ul>
      </li>
      <li>
       cuFileBatch APIs:
       <ul>
        <li>
         <span>
          cuFileBatchIO{SetUp, Submit, GetStatus, Cancel, Destroy}
         </span>
        </li>
        <li>
         Calls to VFS components in standard Linux whether the filesystem is standard Linux for all kernel-based file systems and raw device files.
        </li>
       </ul>
      </li>
      <li>
       <span>
        nvidia-fs.ko
       </span>
       , the kernel-level driver:
       <ul>
        <li>
         Implements callbacks from modified Linux kernel modules or from proprietary filesystems that enable direct DMA to GPU memory.
        </li>
        <li>
         Licensed under GPLv2.
         <p>
          Likewise, any kernel third-party kernel components that call the nvidia-fs APIs should expect to be subject to GPLv2.
         </p>
        </li>
       </ul>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li>
   Third-party content
   <ul>
    <li>
     Proprietary code stacks that replace portions of the Linux filesystem and block system, and so on.
    </li>
   </ul>
  </li>
 </ul>
 <h3 id="dynamic-routing-overview">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dynamic-routing-overview">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dynamic-routing-overview" id="dynamic-routing-overview" name="dynamic-routing-overview" shape="rect">
    2.6. Dynamic Routing
   </a>
  </a>
 </h3>
 <p>
  GDS Dynamic Routing is a feature for choosing the optimal path for cuFileReads and cuFileWrites to and from files on network-based file systems such as DDN-EXAScaler, VAST-NFS, and WekaFS. For hardware platforms, where GPUs do not share the same Root Port with the Storage NICs, peer to peer transactions (p2p) may have higher latency and are inefficient compared to p2p traffic under PCIe switches.
 </p>
 <p>
  With this feature, based on platform configuration, the cuFile library tries to efficiently route the I/O to and from GPU without paying the penalty of cross-root port p2p traffic. For example, if the storage NIC shares a common PCIe root port with another allowed GPU (say GPU1) and the target GPU (say GPU0) is across the CPU root complex, cuFile library can use a bounce buffer on GPU1 to perform the p2p transaction to GPU1 and copy the data to target GPU0. The presence of NVLINKs across GPUs can further accelerate the subsequent device to device (GPU1-&gt;GPU0) I/O transfers using NVLINK instead of PCIe. For each mount/volume, cuFile library pre-computes the best GPUs having the smallest PCI-distance with the available storage NICs for routing the I/O. During reads and writes cuFile checks if the target GPU shares a common PCIe switch and does not need traffic to cross the CPU root complex. If the path is already optimal, then dynamic routing does not apply, otherwise cuFile library selects a candidate GPU for intermediate bounce buffer and performs a device to device copy to the target GPU buffer.
 </p>
 Note:
 <p>
  There is a possibility that there might be multiple candidate GPUs for the staging the intermediate buffer and may not be equidistant from all the Storage NICs, in that cuFile relies on the underlying file-system driver to pick the best storage NIC for the candidate GPU based on the nvidia-fs callback interface to choose the best NIC based on the GPU buffer.
 </p>
 <h3 id="cufile-config-dynamic-routing">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#cufile-config-dynamic-routing">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#cufile-config-dynamic-routing" id="cufile-config-dynamic-routing" name="cufile-config-dynamic-routing" shape="rect">
    2.6.1. cuFile Configuration for Dynamic Routing
   </a>
  </a>
 </h3>
 <p>
  The
  <span>
   “properties.rdma_dynamic_routing”
  </span>
  enables/disables dynamic routing feature for distributed file systems (Lustre, WekaFS, NFS). By default, this feature is turned off.
 </p>
 <p>
  In platforms where I/O transfer to a GPU will cause cross RootPort PCie transfers, enabling this feature might help improve overall BW provided there exists a GPU(s) with Root Port common to that of the storage NIC(s). If this feature is enabled, please provide the ip addresses used by the mount either in file-system specific section for mount_table or in the
  <span>
   rdma_dev_addr_list
  </span>
  property in properties section
 </p>
 <p>
  <span>
   “rdma_dynamic_routing": false,
  </span>
 </p>
 <p>
  The
  <span>
   “properties.rdma_dynamic_routing order”
  </span>
  expresses routing rules. The routing policy is selected based on the order in which they are specified if they are applicable. By default the routing order is as shown below. If the first policy is not applicable, fall back to the next and so on.
 </p>
 <ul>
  <li>
   policy GPU_MEM_NVLINKS: use GPU memory with NVLink to transfer data between GPUs
  </li>
  <li>
   policy GPU_MEM: use GPU memory with PCIe to transfer data between GPUs
  </li>
  <li>
   policy SYS_MEM: use system memory with PCIe to transfer data to GPU
  </li>
  <li>
   policy P2P: use P2P PCIe to transfer across between NIC and GPU
  </li>
  <li>
   <span>
    “rdma_dynamic_routing_order": [ “GPU_MEM_NVLINKS”, “GPU_MEM”, “SYS_MEM”, “P2P” ]
   </span>
  </li>
 </ul>
 The table below summarizes the use cases where dynamic routing can apply and describes each routing policy.
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 1. Dynamic Routing Policies
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d561e976" rowspan="1" valign="top" width="20%">
    Use Case
   </th>
   <th align="left" class="entry" colspan="1" id="d561e979" rowspan="1" valign="top" width="20%">
    GPU_MEM_NVLINKS
   </th>
   <th align="left" class="entry" colspan="1" id="d561e982" rowspan="1" valign="top" width="20%">
    GPU_MEM
   </th>
   <th align="left" class="entry" colspan="1" id="d561e985" rowspan="1" valign="top" width="20%">
    SYS_MEM
   </th>
   <th align="left" class="entry" colspan="1" id="d561e988" rowspan="1" valign="top" width="20%">
    P2P
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e976" rowspan="1" valign="top" width="20%">
    <p>
     NIC and GPU do not share a common parent PCIe switch
    </p>
    <p>
     Dynamic Routing will be used if enabled
    </p>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e979" rowspan="1" valign="top" width="20%">
    <p>
     GPUs in the system have NVLinks
    </p>
    <p>
     Use GPU memory with NVLink to transfer data between GPUs. This policy applies only:
    </p>
    <p>
     a) if there is another GPU which is in the same PCIe tree as the NIC
    </p>
    <p>
     b) there exists an NVLink between the pair of GPUs
    </p>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e982" rowspan="1" valign="top" width="20%">
    <p>
     GPUs in the system do not have NVLinks
    </p>
    <p>
     Use GPU memory with PCIe to transfer data between GPUs. This policy applies only if there is a GPU which is in the same PCIe tree as the NIC.
    </p>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e985" rowspan="1" valign="top" width="20%">
    <p>
     No GPUs share a common Root-Port with the storage NIC(s)
    </p>
    <p>
     Use pinned system memory and PCIe to transfer data to GPU
    </p>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e988" rowspan="1" valign="top" width="20%">
    <p>
     This is default mode without dynamic routing.
    </p>
    <p>
     Use P2P PCIe to transfer between NIC and GPU. The PCIe traffic will be across RootPorts
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e976" rowspan="1" valign="top" width="20%">
    <p>
     NIC and GPU reside in the same PCIe Tree
    </p>
    <p>
     Dynamic Routing will not be used even if enabled
    </p>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e979" rowspan="1" valign="top" width="20%">
    P2P/Compatible Mode
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e982" rowspan="1" valign="top" width="20%">
    P2P/Compatible Mode
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e985" rowspan="1" valign="top" width="20%">
    P2P/Compatible Mode
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e988" rowspan="1" valign="top" width="20%">
    P2P/Compatible Mode
   </td>
  </tr>
 </table>
 <p>
  A sample routing order could be:
 </p>
 <p>
  <span>
   “rdma_dynamic_routing_order": [ “GPU_MEM_NVLINKS”, “SYS_MEM”]
  </span>
 </p>
 <p>
  A use case could be if there exists a pair of GPUs but without NVLinks, the routing policy shall fallback to using system memory for I/O transfer.
 </p>
 <p>
  Another sample routing order could be:
 </p>
 <p>
  <span>
   “rdma_dynamic_routing_order": [ “SYS_MEM”]
  </span>
 </p>
 <p>
  If such an order is specified, then even if there exists a pair of GPUs with NVLinks, the routing policy will use system memory for I/O transfer.
 </p>
 <h3 id="cufile-config-for-dfs-mount">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#cufile-config-for-dfs-mount">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#cufile-config-for-dfs-mount" id="cufile-config-for-dfs-mount" name="cufile-config-for-dfs-mount" shape="rect">
    2.6.2. cuFile Configuration for DFS Mount
   </a>
  </a>
 </h3>
 The user can specify to the library the list of Storage NICs used by the distributed filesystem mount through the json property
 <span>
  rdma_dev_addr_list
 </span>
 . This information is used by the library to compute the best GPUs which can be used for routing the IO. The device addresses list can be specified in a hierarchical format.
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 2. Dynamic Routing Policies
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d561e1132" rowspan="1" valign="top" width="50%">
    Property Name
   </th>
   <th align="left" class="entry" colspan="1" id="d561e1135" rowspan="1" valign="top" width="50%">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e1132" rowspan="1" valign="top" width="50%">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>property.rdma_dev_addr_list : {}</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1135" rowspan="1" valign="top" width="50%">
    <p>
     Applies to a Single File System, Single Mount.
    </p>
    <p>
     This is the global config which is used only if the per file system or the per-mount IP address list is empty.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e1132" rowspan="1" valign="top" width="50%">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>fs.lustre.rdma_dev_addr_list : {}</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1135" rowspan="1" valign="top" width="50%">
    <p>
     Applies to all Lustre file system mounts.
    </p>
    <p>
     This is the per-file system IP address list. This overrides the global IP address list for a file belonging to this file-system. Please note that if this key is set, the user should not configure the per mount_table and it is treated as a config error.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e1132" rowspan="1" valign="top" width="50%">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>fs.lustre.mount_table : {
   /mnt/001 : { rdma_dev_addr_list : []
}</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1135" rowspan="1" valign="top" width="50%">
    Applies to multiple Lustre mounts.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e1132" rowspan="1" valign="top" width="50%">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>/mnt/002 : {
   rdma_dev_addr_list : [] } 
} </p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1135" rowspan="1" valign="top" width="50%">
    <p>
     This is the per mount IP address list. This overrides the per-file system IP address list. This setting has the highest priority in terms of the address list configuration and overrides all above settings for that mount.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e1132" rowspan="1" valign="top" width="50%">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>fs.nfs.rdma_dev_addr_list : {}</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1135" rowspan="1" valign="top" width="50%">
    Applies to all NFS mounts.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e1132" rowspan="1" valign="top" width="50%">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>fs.nfs.mount_table : { 
   /mnt/003 : { rdma_dev_addr_list : [] }
   /mnt/004 : { rdma_dev_addr_list : [] }
}</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1135" rowspan="1" valign="top" width="50%">
    Applies to NFS shares.
   </td>
  </tr>
 </table>
 <p>
  A Sample RDMA Configuration
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>{
   "lustre": {
      // IO threshold for read/write (param should be 4K aligned)) equal to or below which 
      // cuFile will use posix read/write
      "posix_gds_min_kb" : 0,
      "rdma_dev_addr_list" : [],
      "mount_table" : {
         "/lustre/ai200/client1" : {
            "rdma_dev_addr_list" : ["172.172.1.40"]
         },
         "/lustre/ai200/client2" : {
            "rdma_dev_addr_list" : ["172.172.2.40"]
         }
      }
   },
   "nfs": {
      "rdma_dev_addr_list" : [],
      "mount_table" : {
         "/mnt/nfs/ib0/data/0" : {
            "rdma_dev_addr_list" : ["192.168.0.12"]
         },
         "/mnt/nfs/ib1/data/0" : {
            "rdma_dev_addr_list" : ["192.168.1.12"]
         },
         "/mnt/nfs/ib2/data/0" : {
            "rdma_dev_addr_list" : ["192.168.2.12"]
         },
         "/mnt/nfs/ib3/data/0" : {
            "rdma_dev_addr_list" : ["192.168.3.12"]
         }
      },
      "weka": {
         // enable/disable RDMA write
         "rdma_write_support" : false
      },
   }
}</p>
        </pre>
 <p>
  Limitation:
 </p>
 <p>
  The library cannot distinguish multiple mount points which use the same network shared file system share specified through the config; and considers all such IP addresses as a unique mount entry. So it is advised to provide configuration only for unique shares in the configuration.
 </p>
 <h3 id="dynamic-routing-cufile-config-validation">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dynamic-routing-cufile-config-validation">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dynamic-routing-cufile-config-validation" id="dynamic-routing-cufile-config-validation" name="dynamic-routing-cufile-config-validation" shape="rect">
    2.6.3. cuFile Configuration Validation for Dynamic Routing
   </a>
  </a>
 </h3>
 <p>
  Once the cuFile configuration is enabled for dynamic routing with the required IP address configuration, users can use
  <span>
   gdscheck
  </span>
  to validate the same.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ ./gdscheck -p
...
properties.rdma_dynamic_routing : 1
properties.rdma_dynamic_routing_order : GPU_MEM_NVLINKS GPU_MEM SYS_MEM P2P
fs.lustre.mount_table :
/lustre/ai200/client1 dev_id 64768 : 172.172.1.40
/lustre/ai200/client2 dev_id 64769 : 172.172.2.40
fs.weka.rdma_write_support: 0
fs.nfs.mount_table :
/mnt/nfs/ib0/data/0 dev_id 58 : 192.168.0.12 
/mnt/nfs/ib1/data/0 dev_id 59 : 192.168.1.12 
/mnt/nfs/ib2/data/0 dev_id 60 : 192.168.2.12 
/mnt/nfs/ib3/data/0 dev_id 61 : 192.168.3.12
 ...</p>
        </pre>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#software-arch">
   3. Software Architecture
  </a>
 </h2>
 <p>
  GDS enables a DMA engine near storage (NVMe or NIC) to push (or pull) data directly into (and out of) GPU memory. cuFile APIs are passed parameters for one file, a file offset, a size to transfer, and a GPU virtual address to which the parameters can read or write. Although the resulting aggregate transfer is one contiguous virtual address range, several smaller transfers may occur in the implementation. The filesystem breaks the contiguous virtual address range into what might become multiple transfers that might span multiple devices. An example is RAID-0 and potentially multiple pages with non-contiguous physical address ranges. The resulting set of physical address ranges is called a scatter-gather list.
 </p>
 <p>
  Existing operating systems attempting to program DMA engines cannot process GPU virtual addresses without help. The GDS-enabled kernel drivers use callbacks to the GDS kernel module,
  <span>
   nvidia-fs.ko
  </span>
  . These callbacks provide the GPU virtual addresses needed in the final scatter-gather list used to program the DMA engine.
  <a class="Link" data-cms-ai="0" id="software-comp" name="software-comp" shape="rect">
  </a>
 </p>
 <h3 id="software-comp">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#software-comp">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#software-comp" id="software-comp" name="software-comp" shape="rect">
    3.1. Software Components
   </a>
  </a>
 </h3>
 <p>
  The following layers exist in the GDS software stack:
  <a class="Link" data-cms-ai="0" id="software-comp__ul_mh1_x3t_4mb" name="software-comp__ul_mh1_x3t_4mb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   The application, which includes cufile.h and which makes cuFile API calls from the CPU.
  </li>
  <li>
   The GDS user-level library,
   <span>
    libcufile.so
   </span>
   .
  </li>
  <li>
   The Linux virtual filesystem, VFS.
  </li>
  <li>
   Linux or vendor kernel storage drivers.
  </li>
  <li>
   The GDS kernel-level library,
   <span>
    nvidia-fs.ko
   </span>
   .
  </li>
 </ul>
 <h3 id="prim-comp">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#prim-comp">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#prim-comp" id="prim-comp" name="prim-comp" shape="rect">
    3.2. Primary Components
   </a>
  </a>
 </h3>
 <p>
  The primary components in the GDS software architecture are:
 </p>
 <ul>
  <li>
   (
   From NVIDIA
   )
   <span>
    libcufile.so
   </span>
   , which is the user-level cuFile library:
   <ul>
    <li>
     Implements the cuFile API, which is the application-facing API for GDS.
     <p>
      <span>
       cuFileRead
      </span>
      is shown in the architecture overview graphic in
      <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#software-comp" shape="rect">
       Software Components
      </a>
      .
     </p>
    </li>
    <li>
     There are two alternatives to implement the cuFile API:
     <ul>
      <li>
       Use the
       <span>
        nvidia-fs.ko
       </span>
       kernel driver.
       <p>
        All filesystems that use VFS use this path.
       </p>
      </li>
      <li>
       The cuFile user library implements an alternative implementation that does the following:
       <ul>
        <li>
         Uses its non-page cache buffering in the CPU system memory.
        </li>
        <li>
         Uses the standard POSIX call implementations.
        </li>
        <li>
         Does not need to use the NVFS kernel driver.
         <p>
          This is a compatibility mode that does not enjoy the GDS benefits.
         </p>
        </li>
       </ul>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li>
   (
   Not from NVIDIA
   ) Non-block-based or distributed filesystems:
   <ul>
    <li>
     These filesystems might be the standard Linux virtual filesystem (VFS), for example an NFS driver or a third-party proprietary system.
     <p>
      The selection of control paths is based on how filesystems are mounted:
     </p>
     <p>
      &lt;file path&gt;
      --&gt;
      &lt;mount point&gt;
      --&gt;
      &lt;filesystem selection&gt;
     </p>
    </li>
    <li>
     In some cases, NVIDIA provides patches to these, or alternate, implementations, for example, to kernel modules for NVMe and NVMe-oF.
    </li>
   </ul>
  </li>
  <li>
   (
   From NVIDIA
   ) Kernel-level
   <span>
    nvidia-fs.ko
   </span>
   driver:
   <ul>
    <li>
     Handles IOCTLs from the cuFile user library.
    </li>
    <li>
     Implements DMA callbacks to check and translate GPU virtual addresses to physical addresses. These callbacks are called from storage drivers.
    </li>
    <li>
     Manages the mechanisms and buffering that enable DMA from the device.
    </li>
   </ul>
   Note:
   <p>
    The Linux kernel core is completely unmodified.
   </p>
  </li>
 </ul>
 <h3 id="gds-workflows">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#gds-workflows">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#gds-workflows" id="gds-workflows" name="gds-workflows" shape="rect">
    3.2.1. Workflows for GDS Functionality
   </a>
  </a>
 </h3>
 <p>
  The two workflows that are associated with GDS functionality are illustrated in the following graphic:
 </p>
 <p>
  Figure 2. Workflows for GDS Functionality
 </p>
 <p>
  For more information about these workflows, see
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-1" shape="rect" title="Use the following steps to complete Workflow 1.">
   Workflow 1
  </a>
  and
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-2" shape="rect" title="This section provides information about the second workflow that relates to reads and writes with user-space RDMA using ib_verbs.">
   Workflow 2
  </a>
  .
 </p>
 <h3 id="workflow-1">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-1">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-1" id="workflow-1" name="workflow-1" shape="rect">
    3.2.2. Workflow 1
   </a>
  </a>
 </h3>
 <p>
  Use the following steps to complete Workflow 1.
 </p>
 <p>
  The first workflow pertains to
  <span>
   cuFileRead
  </span>
  and
  <span>
   cuFileWrite
  </span>
  usage. The GPU virtual addresses are represented by proxy CPU system memory addresses. The proxy CPU system memory addresses are passed through the Linux IO stack and are converted to device-specific DMA bus addresses.
 </p>
 Note:
 <p>
  None of the following steps are used on a standard
  <span>
   pread
  </span>
  or
  <span>
   pwrite
  </span>
  POSIX call.
 </p>
 <ol>
  <li>
   <span>
    App
   </span>
   to
   <span>
    libcufile.so
   </span>
   .
   <ol>
    <li>
     GPU applications or GPU-enabled frameworks link to the cuFile library
    </li>
    <li>
     The applications or frameworks call the cuFile Driver and IO APIs, such as
     <span>
      cuFileRead
     </span>
     and
     <span>
      cuFileWrite
     </span>
     .
    </li>
   </ol>
   <p>
    The alignment is handled at this level, and there might be some performance impact, so that buffers do not need to be aligned, such as to 4KB pages or 512KB storage offsets and chunk sizes.
   </p>
  </li>
  <li>
   <span>
    libcufile
   </span>
   .
   <span>
    libcufile
   </span>
   makes decisions about which mode to use based on the filesystem, the configuration, and the hardware support to select between compatibility mode and GDS, and whether to use internal GPU buffers for efficiency.
  </li>
  <li>
   <span>
    libcufile.so
   </span>
   to
   <span>
    nvidia-fs.ko
   </span>
   .
   <ol>
    <li>
     The cuFile library,
     <span>
      libcufile.so
     </span>
     , services those calls and makes appropriate IOCTL calls to the
     <span>
      nvidia-fs.ko
     </span>
     driver.
    </li>
    <li>
     The library interacts with the CUDA user-mode driver library,
     <span>
      libcuda.so
     </span>
     , as necessary for the stream subset of the cuFile APIs.
    </li>
   </ol>
  </li>
  <li>
   <span>
    nvidia-fs.ko
   </span>
   to
   <span>
    VFS
   </span>
   .
   <ol>
    <li>
     The kernel driver iterates through the set of necessary IO operations and passes in the IO completion callback, in
     <span>
      kiocb-&gt;common.ki_complete
     </span>
     with the callback function value
     <span>
      nvfs_io_complete
     </span>
     that will be used in step 7. Those calls are to the VFS, which calls the appropriate lower layers, such as the standard Linux block system (ext4/XFS and NVMe) or another vendor distributed filesystem such as EXAScaler.
    </li>
   </ol>
  </li>
  <li>
   Storage kernel drivers to
   <span>
    nvidia-fs.ko
   </span>
   : Callback APIs are registered via the
   <span>
    cuFileDriverOpen
   </span>
   initialization, as described in Filesystems Interoperability in the GDS External Architecture Spec. With this design, drivers need only need to handle GPU addresses through the substeps below. GPU memory addresses are available in a separate map, outside the Linux page map, so the
   <span>
    nvidia-fs.ko
   </span>
   APIs are used to complete the following tasks:
   <ul class="ul">
    <li class="li">
     Check whether the DMA target address is on the GPU (
     <span>
      nvfs_is_gpu_page
     </span>
     ) and needs to be handled differently.
    </li>
    <li class="li">
     Query the list of GPU DMA target addresses by using
     <span>
      nvfs_dma_map_sg*
     </span>
     , which are used instead of the CPU system memory address that is passed through the VFS.
    </li>
   </ul>
  </li>
  <li>
   Storage kernel drivers to DMA/RDMA engines
   : After the appropriate GPU memory addresses are obtained, the underlying DMA engines at (for example, NVMe drivers) or near (for example, NIC) storage can be programmed to move data directly between storage (for example, NVMe or storage controller or NIC) and GPU memory. The special proxy addresses in CPU system memory are not accessed by the DMA engines.
  </li>
  <li>
   DMA/RDMA engines to storage kernel driver
   : Completion of each block transfer is signaled back to the storage driver layer.
  </li>
 </ol>
 <p>
  The completion of each iteration is signaled back to the
  <span>
   nvidia-fs.ko
  </span>
  driver by using the callback that was registered in step 4.
 </p>
 <h3 id="workflow-2">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-2">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-2" id="workflow-2" name="workflow-2" shape="rect">
    3.2.3. Workflow 2
   </a>
  </a>
 </h3>
 <p>
  This section provides information about the second workflow that relates to reads and writes with user-space RDMA using
  <span>
   ib_verbs
  </span>
  .
 </p>
 <ol>
  <li>
   App to
   <span>
    libcufile.so
   </span>
   : GPU applications or GPU-enabled frameworks link to the cuFile library and call the cuFile Driver and IO APIs. The alignment is handled at this level, though perhaps with some performance impact, so that buffers do not need to be aligned, such as 4KB pages or 512KB storage offsets and chunk sizes.
  </li>
  <li>
   <span>
    libcufile_rdma.so
   </span>
   : obtain RDMA info (keys, GID, LID, and so on) to libcufile.
  </li>
  <li>
   <span>
    libcufile.so
    to vendor library
   </span>
   : libcufile calls the appropriate vendor library callback functions to communicate the Rkeys directly in userspace or through the
   <span>
    nvidia-fs
   </span>
   kernel callbacks depending on the vendor driver implementation.
  </li>
 </ol>
 <h3 id="align-linux-initiatives">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#align-linux-initiatives">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#align-linux-initiatives" id="align-linux-initiatives" name="align-linux-initiatives" shape="rect">
    Aligning with Other Linux Initiatives
   </a>
  </a>
 </h3>
 <p>
  There are efforts in the Linux community to add native support for DMA among peer devices, which can include NICs and GPUs. After this support is upstreamed, it will take time for all users to adopt the new Linux versions via distributions. Until then, NVIDIA will work with third-party vendors to enable GDS.
 </p>
 <p>
  The cuFile APIs and their implementation is the mechanism by which CUDA adds support for file IO. The cuFile APIs cover explicit transfers between CPU and GPU storage and memory. The APIs also add support for asynchrony and batching, which are not available in POSIX IO. The cuFile APIs will remain relevant after the functionalities mentioned earlier are added to Linux. Only the underlying implementations will change, but not the existing cuFile APIs.
 </p>
 <p>
  NVIDIA’s initial implementations for cuFile focus on distributed filesystems and systems where appropriate drivers have been installed to enable a direct transfer between storage and GPU memory without using a bounce buffer in the CPU. For compatibility and broader applicability, later implementations may support extensions for local storage and implicit transfers.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#deployment">
   4. Deployment
  </a>
 </h2>
 <p>
  Effective use of GDS entails having a good understanding of how GDS is deployed, its dependencies, and its limitations and constraints.
  <a class="Link" data-cms-ai="0" id="deploy-software-comp" name="deploy-software-comp" shape="rect">
  </a>
 </p>
 <h3 id="deploy-software-comp">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#deploy-software-comp">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#deploy-software-comp" id="deploy-software-comp" name="deploy-software-comp" shape="rect">
    4.1. Software Components for Deployment
   </a>
  </a>
 </h3>
 <p>
  Here is some information about the software components that are required to deploy GDS.
 </p>
 <p>
  cuFile APIs are a supplement to the CUDA driver and runtime APIs and are distributed and installed with the CUDA toolkit.
 </p>
 <p>
  Applications access cuFile functionality by including cuFile.h and linking against the
  <span>
   libcufile.so
  </span>
  library. The stream subset of the cuFile APIs, a CUDA stream parameter is needed, which takes different forms for runtime and driver APIs. The cudaFile and cuFile prefixes are used for those two cases, respectively. The conversion from runtime to drive APIs can be done in header files.
 </p>
 <p>
  Beyond
  <span>
   libcufile.so
  </span>
  , there are no linker dependencies that are required to use the cuFile API, but a runtime dynamic dependency on libcuda.so exists. No link dependency on other CUDA Toolkit libraries, CUDA Runtime libraries, or any other components of the display driver currently exist. However, an eventual runtime dependency on the CUDA Runtime library should be anticipated for applications that are using the cudaFile* APIs after they are added to CUDA Runtime. This is step is consistent with the application using any other cuda* API and use of the CUDA Runtime in deployment is covered in the CUDA deployment documentation at NVIDIA Developer Documentation.
 </p>
 <p>
  In addition to
  <span>
   libcuda.so
  </span>
  , cuFile has dependencies on external third-party libraries.
 </p>
 The following table provides information about the third-party libraries and CUDA library levels:
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" id="deploy-software-comp__table_gv3_lqt_4mb" rules="all" summary="">
  Table 3. Third-Party Libraries and CUDA Library Levels
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d561e1850" rowspan="1" valign="top">
    Level
   </th>
   <th align="left" class="entry" colspan="1" id="d561e1853" rowspan="1" valign="top">
    APIs, Types, and Enum Style
   </th>
   <th align="left" class="entry" colspan="1" id="d561e1856" rowspan="1" valign="top">
    Dependencies
   </th>
   <th align="left" class="entry" colspan="1" id="d561e1859" rowspan="1" valign="top">
    Packaged Together
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e1850" rowspan="1" valign="top">
    cuFile user library
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1853" rowspan="1" valign="top">
    Matches the following CUDA driver conventions:
    <a class="Link" data-cms-ai="0" id="deploy-software-comp__ul_lg1_rqt_4mb" name="deploy-software-comp__ul_lg1_rqt_4mb" shape="rect">
    </a>
    <ul>
     <li>
      cuFile APIs
     </li>
     <li>
      <span>
       cuFile_ enum
      </span>
      vals and defines
     </li>
     <li>
      <span>
       CU_FILE_ errors
      </span>
     </li>
    </ul>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1856" rowspan="1" valign="top">
    <a class="Link" data-cms-ai="0" id="deploy-software-comp__ul_lnc_xqt_4mb" name="deploy-software-comp__ul_lnc_xqt_4mb" shape="rect">
    </a>
    <ul>
     <li>
      Provides
      <span>
       libcufile.so
      </span>
      until perhaps it gets merged into
      <span>
       libcuda.so
      </span>
      .
     </li>
     <li>
      Provides
      <span>
       cuFile.h
      </span>
      until perhaps it gets merged into
      <span>
       cuda.h
      </span>
      .
     </li>
     <li>
      External library dependencies:
      <span>
       libmount-dev libnuma-dev
      </span>
     </li>
    </ul>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1859" rowspan="1" valign="top">
    Shipped separately from
    <span>
     libcufile.so
    </span>
    .
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e1850" rowspan="1" valign="top">
    CUDA runtime + toolkit
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1853" rowspan="1" valign="top">
    Compatibility in cufile.h for streams APIs’ usage of
    <span>
     cudaStream_t
    </span>
    .
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1856" rowspan="1" valign="top">
    None
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1859" rowspan="1" valign="top">
    <span>
     cufile.h
    </span>
    remains distinct from
    <span>
     cuda.h
    </span>
    and
    <span>
     cuda_runtime.h
    </span>
    .
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e1850" rowspan="1" valign="top">
    <span>
     nvidia-fs kernel
    </span>
    driver
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1853" rowspan="1" valign="top">
    <span>
     nvfs_ prefix
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1856" rowspan="1" valign="top">
    <ul>
     <li>
      <p>
       Provides
       <span>
        nvidia-fs.ko
       </span>
       GPLv2.
      </p>
     </li>
    </ul>
    <ul>
     <li>
      cuFile uses kernel facilities that are in Linux kernel version 4.15.0.x and later.
     </li>
     <li>
      cuFile has a dependency on MLNX_OFED versions (5.1 and later) for support for RDMA-based filesystems.
     </li>
     <li>
      GPUDirect partners may have dependencies on host-channel adapters that are Mellanox Connect X-5 or later.
     </li>
    </ul>
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e1859" rowspan="1" valign="top">
    Separately shippable with respect to the NVIDIA driver (until perhaps it gets merged), but it might become co-installed.
   </td>
  </tr>
 </table>
 Note:
 <p>
  GDS has no internal dependencies on other libraries:
  <a class="Link" data-cms-ai="0" id="deploy-software-comp__ul_a1r_yst_4mb" name="deploy-software-comp__ul_a1r_yst_4mb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   cuFile libraries and drivers do not modify CUDA.
  </li>
  <li>
   The streams subset of the cuFile APIs use the CUDA user driver (
   <span>
    libcuda.so
   </span>
   ) and CUDA runtime (
   <span>
    libcudart.so
   </span>
   ).
   <p>
    The only APIs used by those drivers are public APIs.
   </p>
  </li>
 </ul>
 <h3 id="using-gds-containers">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#using-gds-containers">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#using-gds-containers" id="using-gds-containers" name="using-gds-containers" shape="rect">
    4.2. Using GPUDirect Storage in Containers
   </a>
  </a>
 </h3>
 <p>
  GDS has user-level and kernel-level components. Containers include only user-level code and rely on kernel-level components having been installed on the host machine. Applications can be developed with GDS’s header files and user-level library and distributed in containers. When the appropriate drivers and vendor-enable kernel software are not installed or properly configured, GDS’s compatibility mode enables the cuFile APIs to continue to maintain functional operation with minimized performance impact.
 </p>
 <p>
  See
  <a class="xref" data-cms-ai="0" href="https://github.com/NVIDIA/MagnumIO/tree/main/gds/docker" shape="rect">
   https://github.com/NVIDIA/MagnumIO/tree/main/gds/docker
  </a>
  for an example of GDS used in a container.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#notices-header">
   Notices
  </a>
 </h2>
 <h3>
  Notice
 </h3>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-1" name="notice__notice-para-1" shape="rect">
  </a>
  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-2" name="notice__notice-para-2" shape="rect">
  </a>
  NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-3" name="notice__notice-para-3" shape="rect">
  </a>
  Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-4" name="notice__notice-para-4" shape="rect">
  </a>
  NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-5" name="notice__notice-para-5" shape="rect">
  </a>
  NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-6" name="notice__notice-para-6" shape="rect">
  </a>
  NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-7" name="notice__notice-para-7" shape="rect">
  </a>
  No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-8" name="notice__notice-para-8" shape="rect">
  </a>
  Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-9" name="notice__notice-para-9" shape="rect">
  </a>
  THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.
 </p>
 <h3>
  OpenCL
 </h3>
 <p>
  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.
 </p>
 <h3>
  Trademarks
 </h3>
 <p>
  NVIDIA, the NVIDIA logo, DGX, DGX-1, DGX-2, DGX-A100, Tesla, and Quadro are trademarks and/or registered trademarks of NVIDIA Corporation in the United States and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.
 </p>
 <span class="Page-copyright-text">
  © 2020-2024 NVIDIA Corporation and affiliates. All rights reserved.
 </span>
 <span class="Page-copyright-update">
  Last updated on Jun 14, 2024.
 </span>
 Topics
 <ul class="Book-items">
  <li class="Book-items-item">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/index.html">
    NVIDIA GPUDirect Storage
   </a>
  </li>
  <li class="Book-items-item">
   <span class="Link">
    NVIDIA GPUDirect Storage
   </span>
   <ul class="Chapter-chapters">
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html">
      NVIDIA GPUDirect Storage Design Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#design-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#data-transfer-gpu-storage">
        2. Data Transfer Issues for GPU and Storage
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#gds-storage-benefits">
        3. GPUDirect Storage Benefits
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#app-sustain">
        4. Application Suitability
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#transfers-to-from-gpu">
          4.1. Transfers To and From the GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#io-bottleneck">
          4.2. Understanding IO Bottlenecks
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#explicit">
          4.3. Explicit GDS APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#pinned">
          4.4. Pinned Memory for DMA Transfers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#cufile-apis">
          4.5. cuFile APIs
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#plat-perf-stable">
        5. Platform Performance Suitability
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#bandw-from-storage">
          5.1. Bandwidth from Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#path-storage-gpu">
          5.2. Paths from Storage to GPUs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#gpu-bar1-size">
          5.3. GPU BAR1 Size
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#call-to-action">
        6. Call to Action
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html">
      NVIDIA GPUDirect Storage Overview Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#overview-intro">
        1. Introduction
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#related-docs">
          1.1. Related Documents
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dev-benefits">
          1.2. Benefits for a Developer
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#intended-uses">
          1.3. Intended Uses
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#func-overview">
        2. Functional Overview
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#explicit-and-direct">
          2.1. Explicit and Direct
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#perf-optimize">
          2.2. Performance Optimizations
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#imp-perf-enhance">
            2.2.1. Implementation Performance Enhancements
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#concurrency-across-threads">
            2.2.2. Concurrency Across Threads
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#asychrony">
            2.2.3. Asynchrony
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#batching">
            2.2.4. Batching
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#streams">
            2.2.5. Use of CUDA Streams in cuFile
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#comp-and-gen">
          2.3. Compatibility and Generality
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#monitoring">
          2.4. Monitoring
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#solution-scope">
          2.5. Scope of the Solutions in GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dynamic-routing-overview">
          2.6. Dynamic Routing
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#cufile-config-dynamic-routing">
            2.6.1. cuFile Configuration for Dynamic Routing
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#cufile-config-for-dfs-mount">
            2.6.2. cuFile Configuration for DFS Mount
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dynamic-routing-cufile-config-validation">
            2.6.3. cuFile Configuration Validation for Dynamic Routing
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#software-arch">
        3. Software Architecture
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#software-comp">
          3.1. Software Components
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#prim-comp">
          3.2. Primary Components
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#gds-workflows">
            3.2.1. Workflows for GDS Functionality
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-1">
            3.2.2. Workflow 1
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-2">
            3.2.3. Workflow 2
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#align-linux-initiatives">
          3.3. Aligning with Other Linux Initiatives
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#deployment">
        4. Deployment
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#deploy-software-comp">
          4.1. Software Components for Deployment
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#using-gds-containers">
          4.2. Using GPUDirect Storage in Containers
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html">
      cuFile API Reference Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#introduction">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#usage">
        2. Usage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#dynamic-interactions">
          2.1. Dynamic Interactions
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#driver-file-buffer">
          2.2. Driver, File, and Buffer Management
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-compatibility-mode">
          2.3. cuFile Compatibility Mode
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-api-specification">
        3. cuFile API Specification
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#data-types">
          3.1. Data Types
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#declarations-and-definitions">
            3.1.1. Declarations and Definitions
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#typedefs">
            3.1.2. Typedefs
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#enumerations">
            3.1.3. Enumerations
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-driver-api">
          3.2. cuFile Driver APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-io-api">
          3.3. cuFile Synchronous IO APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-file-handle-api">
          3.4. cuFile File Handle APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-buffer-api">
          3.5. cuFile Buffer APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-stream-api">
          3.6. cuFile Stream APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-batch-api">
          3.7. cuFile Batch APIs
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-api-functional-specification">
        4. cuFile API Functional Specification
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriver-api-functional-specification">
          4.1. cuFileDriver API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriveropen">
            4.1.1. cuFileDriverOpen
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriverclose">
            4.1.2. cuFileDriverClose
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledrivergetproperties">
            4.1.3. cuFileDriverGetProperties
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetpollmode">
            4.1.4. cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetmaxdirectiosize">
            4.1.5. cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetmaxcachesize">
            4.1.6. cuFileDriverSetMaxCacheSize(size_t max_cache_size)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetmaxpinnedmemsize">
            4.1.7. cuFileDriverSetMaxPinnedMemSize(size_t max_pinned_memory_size)
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-io-api-functional-specification">
          4.2. cuFile IO API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilehandleregister">
            4.2.1. cuFileHandleRegister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#unique_1423451733">
            4.2.2. cuFileHandleDeregister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufileread">
            4.2.3. cuFileRead
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilewrite">
            4.2.4. cuFileWrite
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-memory-mgmt-functional-specification">
          4.3. cuFile Memory Management Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebufregister">
            4.3.1. cuFileBufRegister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebufderegister">
            4.3.2. cuFileBufDeregister
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-stream-api-functional-specification">
          4.4. cuFile Stream API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilestreamregister">
            4.4.1. cuFileStreamRegister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilestreamderegister">
            4.4.2. cuFileStreamDeregister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilereadasync">
            4.4.3. cuFileReadAsync
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilewriteasync">
            4.4.4. cuFileWriteAsync
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-batch-api-functional-specification">
          4.5. cuFile Batch API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiosetup">
            4.5.1. cuFileBatchIOSetUp
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiosubmit">
            4.5.2. cuFileBatchIOSubmit
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiogetstatus">
            4.5.3. cuFileBatchIOGetStatus
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiocancel">
            4.5.4. cuFileBatchIOCancel
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiodestroy">
            4.5.5. cuFileBatchIODestroy
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#sample-program">
        5. Sample Program with cuFile APIs
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#batch-api-known-limitations">
        6. Known Limitations of cuFile Batch APIs
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html">
      NVIDIA GPUDirect Storage Release Notes
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#rn-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#new-features">
        2. New Features and Changes
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#mofed-fs-req">
        3. MLNX_OFED and Filesystem Requirements
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#support-matrix">
        4. Support Matrix
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#gds-enabled-libraries">
        5. GDS Enabled Libraries/Frameworks
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#included-packages">
        6. Included Packages
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#updates-bug-fixes">
        7. Minor Updates and Bug Fixes
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#known-issues">
        8. Known Issues
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#known-limitations">
        9. Known Limitations
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html">
      Getting Started with NVIDIA GPUDirect Storage
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#gs-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#sys-admin">
        2. If you are a system administrator or a performance engineer
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#developer">
        3. If you are a developer
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#oem-odm-csp">
        4. If you are OEM, ODM, CSP
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#troubleshoting-issues">
        5. Troubleshooting GDS issues
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="Book-items-item">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/index.html#understanding-gpudirect-storage">
    Understanding GPUDirect Storage
   </a>
   <ul class="Chapter-chapters">
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html">
      NVIDIA GPUDirect Storage Best Practices Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#gds-bp-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#settings">
        2. Software Settings
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#system-settings">
          2.1. System Settings
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cuda-context-in-gpu-kernels">
          2.2. Use of CUDA Context in GPU Kernels and Storage IO
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-config">
          2.3. cuFile Configuration Settings
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#api-usage">
        3. API Usage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-driveropen">
          3.1. cuFileDriverOpen
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-handle-register">
          3.2. cuFileHandleRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-bufregister-fileread-filewrite">
          3.3. cuFileBufRegister, cuFileRead, cuFileWrite, cuFileBatchIOSubmit, cuFileBatchIOGetStatus, cuFileReadAsync, cuFileWriteAsync, and cuFileStreamRegister
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-1">
            3.3.1. IO Pattern 1
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-2">
            3.3.2. IO Pattern 2
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-3">
            3.3.3. IO Pattern 3
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-4">
            3.3.4. IO Pattern 4
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-5">
            3.3.5. IO Pattern 5
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-6">
            3.3.6. IO Pattern 6
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-7">
            3.3.7. IO Pattern 7
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-handle-deregister">
          3.4. cuFileHandleDeregister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-buf-deregister">
          3.5. cuFileBufDeregister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-stream-register">
          3.6. cuFileStreamRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#unique_880314822">
          3.7. cuFileStreamDeregister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-driver-close">
          3.8. cuFileDriverClose
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html">
      NVIDIA GPUDirect Storage Benchmarking and Configuration Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#introduction">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#about-this-guide">
        2. About this Guide
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-gds">
        3. Benchmarking GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#determining-pcie-device-affinity">
          3.1. Determining PCIe Device Affinity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-configuration-parameters">
          3.2. GPUDirect Storage Configuration Parameters
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#system-parameters">
            3.2.1. System Parameters
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-parameters">
            3.2.2. GPUDirect Storage Parameters
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-tools">
          3.3. GPUDirect Storage Benchmarking Tools
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gdsio">
            3.3.1. gdsio Utility
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-stats">
            3.3.2. gds-stats Tool
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-direct-attached-storage-das">
        4. GPUDirect Storage Benchmarking on Direct Attached Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-perf-dgx2">
          4.1. GPUDirect Storage Performance on DGX-2 System
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-perf-dgx-a100">
          4.2. GPUDirect Storage Performance on a DGX A100 System
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-nas">
        5. GPUDirect Storage Benchmarking on Network Attached Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-nfs">
          5.1. GPUDirect Storage Benchmarking on NFS
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#summary">
        6. Summary
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-and-performance">
        A. Benchmarking and Performance
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#the-language-of-performance">
          A.1. The Language of Performance
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-storage-performance">
          A.2. Benchmarking Storage Performance
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html">
      NVIDIA GPUDirect Storage Installation and Troubleshooting Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-install">
        2. Installing GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-prereqs">
          2.1. Before You Install GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-installing">
          2.2. Installing GDS
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#config-file-system-settings">
            2.2.1. Configuring File System Settings for GDS
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-suc-install">
            2.2.2. Verifying a Successful GDS Installation
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-lib-tools">
          2.3. Installed GDS Libraries and Tools
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#uninstall-gds">
          2.4. Uninstalling GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#environment-variables">
          2.5. Environment Variables Used by GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#json-config-params">
          2.6. JSON Config Parameters Used by GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-config">
          2.7. GDS Configuration File Changes to Support Dynamic Routing
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-installed-version">
          2.8. Determining Which Version of GDS is Installed
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#experimental-repos-dgx">
          2.9. Experimental Repos for Network Install of GDS Packages for DGX Systems
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-errors">
        3. API Errors
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-1">
          3.1. CU_FILE_DRIVER_NOT_INITIALIZED
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-2">
          3.2. CU_FILE_DEVICE_NOT_SUPPORTED
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-3">
          3.3. CU_FILE_IO_NOT_SUPPORTED
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-4">
          3.4. CU_FILE_CUDA_MEMORY_TYPE_INVALID
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-troubleshooting">
        4. Basic Troubleshooting
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#log-files-gds-lib">
          4.1. Log Files for the GDS Library
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-diff-log-file-app">
          4.2. Enabling a Different cufile.log File for Each Application
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-trace-lib-calls">
          4.3. Enabling Tracing GDS Library API Calls
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-5">
          4.4. cuFileHandleRegister Error
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-cufile-errors">
          4.5. Troubleshooting Applications that Return cuFile Errors
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#error-no-activity-gds-stats">
          4.6. cuFile-* Errors with No Activity in GPUDirect Storage Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cuda-error-35">
          4.7. CUDA Runtime and Driver Mismatch with Error Code 35
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cuda-api-errors">
          4.8. CUDA API Errors when Running the cuFile-* APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#find-driver-stats">
          4.9. Finding GDS Driver Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-io-activity-gds-driver">
          4.10. Tracking IO Activity that Goes Through the GDS Driver
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#read-write-bandwidth-latency-nos">
          4.11. Read/Write Bandwidth and Latency Numbers in GDS Stats
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-regis-deregis-gpu-buffers">
          4.12. Tracking Registration and Deregistration of GPU Buffers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enabling-rdma-logging">
          4.13. Enabling RDMA-specific Logging for Userspace File Systems
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#system-not-ready">
          4.14. CUDA_ERROR_SYSTEM_NOT_READY After Installation
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#adding-udev-rules">
          4.15. Adding udev Rules for RAID Volumes
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#incomplete-write-on-nvme-drives">
          4.16. When You Observe “Incomplete write” on NVME Drives
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-async-io-failing">
          4.17. CUFILE async I/O is failing
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#advanced-troubleshooting">
        5. Advanced Troubleshooting
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#hung-cufile-no-response">
          5.1. Resolving Hung cuFile* APIs with No Response
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#kernel-panic-stack-traces">
          5.2. Sending Relevant Data to Customer Support
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-failure-stack-trace-warning">
          5.3. Resolving an IO Failure with EIO and Stack Trace Warning
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#control-gpu-bar-usage">
          5.4. Controlling GPU BAR Memory Usage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-how-much-cache">
          5.5. Determining the Amount of Cache to Set Aside
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-bar-mem-usage">
          5.6. Monitoring BAR Memory Usage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enomem-error-code">
          5.7. Resolving an ENOMEM Error Code
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-comp-mode">
          5.8. GDS and Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-comp-mode">
          5.9. Enabling Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-enable-comp-mode">
          5.10. Tracking the IO After Enabling Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#bypass-gds">
          5.11. Bypassing GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-not-working-mount">
          5.12. GDS Does Not Work for a Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-posix-io-same-file">
          5.13. Simultaneously Running the GPUDirect Storage IO and POSIX IO on the Same File
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-data-verif-tests">
          5.14. Running Data Verification Tests Using GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html">
          NVIDIA GPUDirect Storage Installation and Troubleshooting Guide
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-perf">
        6. Troubleshooting Performance
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#perf-benchmark-examples">
          6.1. Running Performance Benchmarks with GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-internal-cache">
          6.2. Tracking Whether GPUDirect Storage is Using an Internal Cache
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-cross-pcie-root-complex">
          6.3. Tracking when IO Crosses the PCIe Root Complex and Impacts Performance
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-cpu-usage">
          6.4. Using GPUDirect Statistics to Monitor CPU Activity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monit-perf-tracing">
          6.5. Monitoring Performance and Tracing with cuFile-* APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#ex-linux-tools">
          6.6. Example: Using Linux Tracing Tools
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trace-cufile-apis">
          6.7. Tracing the cuFile-* APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-improving-performance">
          6.8. Improving Performance using Dynamic Routing
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trouble-io-activity">
        7. Troubleshooting IO Activity
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#manage-coherency-page-cache">
          7.1. Managing Coherency of Data in the Page Cache and on Disk
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-fs-lnet">
        8. EXAScaler Filesystem LNet Troubleshooting
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-client-mod-version">
          8.1. Determining the EXAScaler Filesystem Client Module Version
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#lnet-network-setup">
          8.2. Checking the LNet Network Setup on a Client
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-peer-health">
          8.3. Checking the Health of the Peers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#multi-rail-support">
          8.4. Checking for Multi-Rail Support
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-peer-affinity">
          8.5. Checking GDS Peer Affinity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-lnet-errors">
          8.6. Checking for LNet-Level Errors
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#lnet-nids-deg-timeouts">
          8.7. Resolving LNet NIDs Health Degradation from Timeouts
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#multi-osts-peer-sel">
          8.8. Configuring LNet Networks with Multiple OSTs for Optimal Peer Selection
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-fs-perf">
        9. Understanding EXAScaler Filesystem Performance
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#osc-tune-perf-param">
          9.1. osc Tuning Performance Parameters
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#misc-comms">
          9.2. Miscellaneous Commands for osc, mdc, and stripesize
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#num-config-disks">
          9.3. Getting the Number of Configured Object-Based Disks
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#addl-stats-exascaler-fs">
          9.4. Getting Additional Statistics related to the EXAScaler Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#get-md-stats">
          9.5. Getting Metadata Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-exist-mount-lustre">
          9.6. Checking for an Existing Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-fs-cluster">
          9.7. Unmounting an EXAScaler Filesystem Cluster
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#sum-exacaler-fs-stats">
          9.8. Getting a Summary of EXAScaler Filesystem Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-poll-mode">
          9.9. Using GPUDirect Storage in Poll Mode
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trouble-faq-wekafs">
        10. Troubleshooting and FAQ for the WekaIO Filesystem
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#download-wekaio-cp">
          10.1. Downloading the WekaIO Client Package
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#wekaio-version-gds">
          10.2. Determining Whether the WekaIO Version is Ready for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-wekaio-fs-cluster">
          10.3. Mounting a WekaIO File System Cluster
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#resolve-fail-mount">
          10.4. Resolving a Failing Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#resolve-wekaio-usage">
          10.5. Resolving 100% Usage for WekaIO for Two Cores
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-existing-mount-wekafs">
          10.6. Checking for an Existing Mount in the Weka File System
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-summ-wekaio">
          10.7. Checking for a Summary of the WekaIO Filesystem Status
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#summ-wekaio-stats">
          10.8. Displaying the Summary of the WekaIO Filesystem Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#wekaio-writes-posix">
          10.9. Why WekaIO Writes Go Through POSIX
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-nvdiafsko-support">
          10.10. Checking for nvidia-fs.ko Support for Memory Peer Direct
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-mem-peer-stats">
          10.11. Checking Memory Peer Direct Stats
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#rel-nvidia-fs-stats">
          10.12. Checking for Relevant nvidia-fs Statistics for the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-wekaio-fs-test">
          10.13. Conducting a Basic WekaIO Filesystem Test
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-wekaio-fs-cluster">
          10.14. Unmounting a WekaIO File System Cluster
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-installed-libs-wekaio-fs">
          10.15. Verify the Installed Libraries for the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-config-file-changes">
          10.16. GDS Configuration File Changes to Support the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#relevant-userspace-stats-wekaio-fs">
          10.17. Check for Relevant User-Space Statistics for the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-wekafs-support">
          10.18. Check for WekaFS Support
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#spectrum-scale-intro">
        11. Enabling IBM Spectrum Scale Support with GDS
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#spectrum-scale-limitations-with-gds">
          11.1. IBM Spectrum Scale Limitations with GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-for-peerdirect-support">
          11.2. Checking nvidia-fs.ko Support for Mellanox PeerDirect
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verifying-libraries-for-spectrum-scale">
          11.3. Verifying Installed Libraries for IBM Spectrum Scale
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-peerdirect-stats">
          11.4. Checking PeerDirect Stats
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-for-relevant-stats">
          11.5. Checking for Relevant nvidia-fs Stats with IBM Spectrum Scale
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-user-space-stats">
          11.6. GDS User Space Stats for IBM Spectrum Scale for Each Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-configs-to-support-spectrum-scale">
          11.7. GDS Configuration to Support IBM Spectrum Scale
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#scenarios-compat-mode">
          11.8. Scenarios for Falling Back to Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-limitations-with-spectrum-scale">
          11.9. GDS Limitations with IBM Spectrum Scale
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-intro">
        12. NetApp E-series BeeGFS with GDS Solution Deployment
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-package-requirements">
          12.1. Netapp BeeGFS/GPUDirect Storage and Package Requirements
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-client-config-for-gds">
          12.2. BeeGFS Client Configuration for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-gpu-hca-topology">
          12.3. GPU/HCA Topology on the Client - DGX-A100 and OSS servers Client Server
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-setup">
          12.4. Verify the Setup
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-management-node">
            12.4.1. List the Management Node
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-metadata-nodes">
            12.4.2. List the Metadata Nodes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-storage-nodes">
            12.4.3. List the Storage Nodes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-client-nodes">
            12.4.4. List the Client Nodes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-display-client-connections">
            12.4.5. Display Client Connections
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-connectivity-svcs">
            12.4.6. Verify Connectivity to the Different Services
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-storage-pools">
            12.4.7. List Storage Pools
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-display-free-space-inodes">
            12.4.8. Display the Free Space and inodes on the Storage and Metadata Targets
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-testing">
          12.5. Testing
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-integration">
            12.5.1. Verifying Integration is Working
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-conducting-basic-fs-test">
            12.5.2. Conducting a Basic NetApp BeeGFS Filesystem Test
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-set-up-vast-data">
        13. Setting Up and Troubleshooting VAST Data (NFSoRDMA+MultiPath)
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-mofed-vast">
          13.1. Installing MLNX_OFED and VAST NFSoRDMA+Multipath Packages
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#client-software-reqs">
            13.1.1. Client Software Requirements
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-vast">
            13.1.2. Install the VAST Multipath Package
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#networking-setup">
          13.2. Set Up the Networking
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#vast-network-config">
            13.2.1. VAST Network Configuration
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cllent-network-config">
            13.2.2. Client Network Configuration
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-network-connect">
            13.2.3. Verify Network Connectivity
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-vast-nfs">
          13.3. Mount VAST NFS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#debug-monitor">
          13.4. Debugging and Monitoring VAST Data
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nvme-nvmeof-support">
        14. Troubleshooting and FAQ for NVMe and NVMeOF Support
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mofed-req-install">
          14.1. MLNX_OFED Requirements and Installation
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-nvme-support-gds">
          14.2. Determining Whether the NVMe device is Supported for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-raid-level">
          14.3. RAID Support in GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-local-fs">
          14.4. Mounting a Local Filesystem for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-exist-mount">
          14.5. Check for an Existing EXT4 Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-io-stats-block-devmt">
          14.6. Check for IO Statistics with Block Device Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#raid-group-config-gpu-aff">
          14.7. RAID Group Configuration for GPU Affinity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-ext4-fs-test">
          14.8. Conduct a Basic EXT4 Filesystem Test
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-ext4-fs">
          14.9. Unmount a EXT4 Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#udev-name-conv-block-device">
          14.10. Udev Device Naming for a Block Device
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#batch-io-performance">
          14.11. BATCH I/O Performance
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#display-gds-driver-stats">
        15. Displaying GDS NVIDIA FS Driver Statistics
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nvidia-fs-stats2">
          15.1. nvidia-fs Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#analyze-per-gpu-stats">
          15.2. Analyze Statistics for each GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#reset-nvidia-fs-stats">
          15.3. Resetting the nvidia-fs Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#review-peer-aff-stats">
          15.4. Checking Peer Affinity Stats for a Kernel Filesystem and Storage Drivers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#peer-affinity-usage-kernel-fs-sd">
          15.5. Checking the Peer Affinity Usage for a Kernel File System and Storage Drivers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gpu-peer-dist-table">
          15.6. Display the GPU-to-Peer Distance Table
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gdsio-tool">
          15.7. The GDSIO Tool
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tab-fields">
          15.8. Tabulated Fields
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gdscheck">
          15.9. The GDSCHECK Tool
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nfs-suppport-gds">
          15.10. NFS Support with GPUDirect Storage
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-nfs-server-rdma-mofed_5-1">
            15.10.1. Install Linux NFS server with RDMA Support on MLNX_OFED 5.3 or Later
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-gds-supp-nfs-client">
            15.10.2. Install GPUDirect Storage Support for the NFS Client
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nfs-gds-stas-debug">
          15.11. NFS GPUDirect Storage Statistics and Debugging
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-io-behavior">
          15.12. GPUDirect Storage IO Behavior
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#read-write-atomiticity-cons">
            15.12.1. Read/Write Atomicity Consistency with GPUDirect Storage Direct IO
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#write-file-in-o-append-mode">
            15.12.2. Write with File a Opened in O_APPEND Mode (cuFileWrite)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gpu-nic-peer-aff">
            15.12.3. GPU to NIC Peer Affinity
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#comp-mode-unreg-buffers">
            15.12.4. Compatible Mode with Unregistered Buffers
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unaligned-writes-non-reg-buff">
            15.12.5. Unaligned writes with Non-Registered Buffers
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#process-hang-nfs">
            15.12.6. Process Hang with NFS
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tool-limit-cuda-9">
            15.12.7. Tools Support Limitations for CUDA 9 and Earlier
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-stats">
          15.13. GDS Statistics for Dynamic Routing
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-peer-affinity">
            15.13.1. Peer Affinity Dynamic Routing
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-cufile-log">
            15.13.2. cuFile Log Related to Dynamic Routing
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-lib-tracing">
        16. GDS Library Tracing
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#display-tracepoints">
          16.1. Example: Display Tracepoints
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tracepoint-arguments">
            16.1.1. Example: Tracepoint Arguments
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-io-issue-cuapis">
          16.2. Example: Track the IO Activity of a Process that Issues cuFileRead/ cuFileWrite
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#disp-io-patters-through-gds">
          16.3. Example: Display the IO Pattern of all the IOs that Go Through GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-pattern-process">
          16.4. Understand the IO Pattern of a Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-pattern-file-desc">
          16.5. IO Pattern of a Process with the File Descriptor on Different GPUs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#iops-bandwidth-process-gpu">
          16.6. Determine the IOPS and Bandwidth for a Process in a GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#freq-reads-cufileread-api">
          16.7. Display the Frequency of Reads by Processes that Issue cuFileRead
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#freq-reads-cufileread-0.1-ms">
          16.8. Display the Frequency of Reads when cuFileRead Takes More than 0.1 ms
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#disp-latency-cufileread">
          16.9. Displaying the Latency of cuFileRead for Each Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-process-cufilebufregister">
          16.10. Example: Tracking the Processes that Issue cuFileBufRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#proc-constant-cufilebufregister">
          16.11. Example: Tracking Whether the Process is Constant when Invoking cuFileBufRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-io-bb">
          16.12. Example: Monitoring IOs that are Going Through the Bounce Buffer
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trace-cufileread-cufilewrite-issues">
          16.13. Example: Tracing cuFileRead and cuFileWrite Failures, Print, Error Codes, and Time of Failure
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#per-process-user-stats">
          16.14. Example: User-Space Statistics for Each GDS Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-stats-tool">
          16.15. Example: Viewing GDS User-Level Statistics for a Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#sample-ul-stats-per-process">
          16.16. Example: Displaying Sample User-Level Statistics for each GDS Process
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-space-counters">
        17. User-Space Counters in GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dist-io-util-gpu">
          17.1. Distribution of IO Usage in Each GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-user-space-stats">
          17.2. User-space Statistics for Dynamic Routing
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#user-space-rdma-counters">
        18. User-Space RDMA Counters in GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-io-counters">
          18.1. cuFile RDMA IO Counters (PER_GPU RDMA STATS)
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-mem-reg-counters">
          18.2. cuFile RDMA Memory Registration Counters (RDMA MRSTATS)
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cheat-sheet">
        19. Cheat Sheet for Diagnosing Problems
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html">
      NVIDIA GPUDirect Storage O_DIRECT Requirements Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#intro">
        1. Introduction
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#rel-docs">
          1.1. Related Documents
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#gds-req">
        2. GPUDirect Storage Requirements
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#summ-basic-req">
          2.1. Summary of Basic Requirements
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#client-server">
          2.2. Client and Server
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#odirect-not-a-fit">
          2.3. Cases Where O_DIRECT is Not a Fit
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#buffered-io">
            2.3.1. Buffered IO
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#inline-files">
            2.3.2. Inline Files
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#block-alloc-writes">
            2.3.3. Block Allocation For Writes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#examine-transform-data">
            2.3.4. Examining or Transforming User Data
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#summary">
            2.3.5. Summary
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
     </ul>
    </li>
   </ul>
  </li>
 </ul>
 <ul class="FooterNavigation-items" data-column-count="3">
  <li class="FooterNavigation-items-item">
   <span>
    Corporate Info
   </span>
   <ul class="FooterNavigationItem-items">
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://www.nvidia.com/en-us/" target="_blank">
      <span class="NavigationLink-text">
       NVIDIA.com Home
      </span>
     </a>
    </li>
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/" target="_blank">
      <span class="NavigationLink-text">
       About NVIDIA
      </span>
     </a>
    </li>
   </ul>
  </li>
  <li class="FooterNavigation-items-item">
   <span>
    ‎NVIDIA Developer
   </span>
   <ul class="FooterNavigationItem-items">
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://developer.nvidia.com/" target="_blank">
      <span class="NavigationLink-text">
       Developer Home
      </span>
     </a>
    </li>
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://blogs.nvidia.com/" target="_blank">
      <span class="NavigationLink-text">
       Blog
      </span>
     </a>
    </li>
   </ul>
  </li>
  <li class="FooterNavigation-items-item">
   <span>
    Resources
   </span>
   <ul class="FooterNavigationItem-items">
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://www.nvidia.com/en-us/contact/" target="_blank">
      <span class="NavigationLink-text">
       Contact Us
      </span>
     </a>
    </li>
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://developer.nvidia.com/developer-program" target="_blank">
      <span class="NavigationLink-text">
       Developer Program
      </span>
     </a>
    </li>
   </ul>
  </li>
 </ul>
 <p>
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" rel="noopener" target="_blank">
   Privacy Policy
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" rel="noopener" target="_blank">
   Manage My Privacy
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/preferences/start/" rel="noopener" target="_blank">
   Do Not Sell or Share My Data
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" rel="noopener" target="_blank">
   Terms of Service
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" rel="noopener" target="_blank">
   Accessibility
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" rel="noopener" target="_blank">
   Corporate Policies
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/product-security/" rel="noopener" target="_blank">
   Product Security
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/contact/" rel="noopener" target="_blank">
   Contact
  </a>
 </p>
 <p>
  Copyright © 2024 NVIDIA Corporation
 </p>
</body>
</body></html>