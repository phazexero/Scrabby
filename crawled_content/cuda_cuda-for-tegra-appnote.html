<html><head><title>CUDA for Tegra</title></head><body><body class="wy-body-for-nav">
 <a href="https://docs.nvidia.com/cuda/contents.html">
 </a>
 <ul class="current">
  <li class="toctree-l1 current">
   <a class="current reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote">
    1. CUDA for Tegra
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#overview">
    2. Overview
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#memory-management">
    3. Memory Management
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#i-o-coherency">
      3.1. I/O Coherency
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#estimating-total-allocatable-device-memory-on-an-integrated-gpu-device">
      3.2. Estimating Total Allocatable Device Memory on an Integrated GPU Device
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#porting-considerations">
    4. Porting Considerations
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#memory-selection">
      4.1. Memory Selection
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#id1">
      4.2. Pinned Memory
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#effective-usage-of-unified-memory-on-tegra">
      4.3. Effective Usage of Unified Memory on Tegra
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#gpu-selection">
      4.4. GPU Selection
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#synchronization-mechanism-selection">
      4.5. Synchronization Mechanism Selection
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-features-not-supported-on-tegra">
      4.6. CUDA Features Not Supported on Tegra
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#egl-interoperability">
    5. EGL Interoperability
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglstream">
      5.1. EGLStream
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglstream-flow">
        5.1.1. EGLStream Flow
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-as-producer">
        5.1.2. CUDA as Producer
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-as-consumer">
        5.1.3. CUDA as Consumer
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#implicit-synchronization">
        5.1.4. Implicit Synchronization
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#data-transfer-between-producer-and-consumer">
        5.1.5. Data Transfer Between Producer and Consumer
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglstream-pipeline">
        5.1.6. EGLStream Pipeline
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglimage">
      5.2. EGLImage
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-interop-with-eglimage">
        5.2.1. CUDA interop with EGLImage
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglsync">
      5.3. EGLSync
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-interop-with-eglsync">
        5.3.1. CUDA Interop with EGLSync
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#creating-eglsync-from-a-cuda-event">
        5.3.2. Creating EGLSync from a CUDA Event
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#creating-a-cuda-event-from-eglsync">
        5.3.3. Creating a CUDA Event from EGLSync
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-upgradable-package-for-jetson">
    6. CUDA Upgradable Package for Jetson
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#installing-the-cuda-upgrade-package">
      6.1. Installing the CUDA Upgrade Package
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#prerequisite">
        6.1.1. Prerequisite
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#from-network-repositories-or-local-installers">
        6.1.2. From Network Repositories or Local Installers
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#deployment-considerations-for-cuda-upgrade-package">
      6.2. Deployment Considerations for CUDA Upgrade Package
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#use-the-right-upgrade-package">
        6.2.1. Use the Right Upgrade Package
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#feature-exceptions">
        6.2.2. Feature Exceptions
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#check-for-compatibility-support">
        6.2.3. Check for Compatibility Support
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cudla">
    7. cuDLA
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#developer-guide">
      7.1. Developer Guide
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#device-model">
        7.1.1. Device Model
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#loading-and-querying-modules">
        7.1.2. Loading and Querying Modules
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#memory-model">
        7.1.3. Memory Model
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#task-execution-and-synchronization-model">
        7.1.4. Task Execution and Synchronization Model
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#task-execution">
          7.1.4.1. Task Execution
         </a>
         <ul>
          <li class="toctree-l5">
           <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#multithreaded-user-submission">
            7.1.4.1.1. Multithreaded User Submission
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#synchronization">
          7.1.4.2. Synchronization
         </a>
         <ul>
          <li class="toctree-l5">
           <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#registering-an-external-semaphore">
            7.1.4.2.1. Registering an external semaphore:
           </a>
          </li>
          <li class="toctree-l5">
           <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#events-setup-for-cudlasubmittask">
            7.1.4.2.2. Events setup for cudlaSubmitTask()
           </a>
          </li>
          <li class="toctree-l5">
           <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#waiting-on-the-signal-event">
            7.1.4.2.3. Waiting on the signal event
           </a>
          </li>
          <li class="toctree-l5">
           <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#supported-synchronization-primitives-in-cudla">
            7.1.4.2.4. Supported Synchronization Primitives in cuDLA
           </a>
          </li>
          <li class="toctree-l5">
           <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#setting-nvscisyncattrkey-requiredeterministicfences-key-in-nvscisyncattrlist">
            7.1.4.2.5. Setting NvSciSyncAttrKey_RequireDeterministicFences key in NvSciSyncAttrList
           </a>
          </li>
          <li class="toctree-l5">
           <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#timestamp-support-for-nvscifence">
            7.1.4.2.6. Timestamp Support for NvSciFence
           </a>
          </li>
          <li class="toctree-l5">
           <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#requesting-timestamp-support-for-nvscisync-object">
            7.1.4.2.7. Requesting Timestamp Support for NvSciSync Object
           </a>
          </li>
          <li class="toctree-l5">
           <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#extracting-timestamp-value-from-fence">
            7.1.4.2.8. Extracting Timestamp Value from Fence
           </a>
          </li>
         </ul>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#fault-diagnostics">
          7.1.4.3. Fault Diagnostics
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#noop-submission">
          7.1.4.4. NOOP Submission
         </a>
        </li>
       </ul>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#error-reporting-model">
        7.1.5. Error Reporting Model
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#migrating-from-nvmediadla-to-cudla">
      7.2. Migrating from NvMediaDla to cuDLA
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#profiling-a-cudla-app">
      7.3. Profiling a cuDLA App
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cudla-release-notes">
      7.4. cuDLA Release Notes
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#notices">
    8. Notices
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#notice">
      8.1. Notice
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#opencl">
      8.2. OpenCL
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#trademarks">
      8.3. Trademarks
     </a>
    </li>
   </ul>
  </li>
 </ul>
 <a href="https://docs.nvidia.com/cuda/contents.html">
  CUDA for Tegra
 </a>
 <ul class="wy-breadcrumbs">
  <li>
   <a class="icon icon-home" href="https://docs.nvidia.com/index.html">
   </a>
   Â»
  </li>
  <li>
   <span class="section-number">
    1.
   </span>
   CUDA for Tegra
  </li>
  <li class="wy-breadcrumbs-aside">
   <span>
    v12.5 |
   </span>
   <a class="reference external" href="https://docs.nvidia.com/pdf/CUDA-For-Tegra-AppNote.pdf">
    PDF
   </a>
   <span>
    |
   </span>
   <a class="reference external" href="https://developer.nvidia.com/cuda-toolkit-archive">
    Archive
   </a>
   <span>
    Â
   </span>
  </li>
 </ul>
 <h1>
  <span class="section-number">
   1.
  </span>
  CUDA for Tegra
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-for-tegra" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  This application note provides an overview of NVIDIAÂ® TegraÂ® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the TegraÂ® integrated GPU (iGPU). It also discusses EGL interoperability.
 </p>
 <h1>
  <span class="section-number">
   2.
  </span>
  Overview
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#overview" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  This document provides an overview of NVIDIAÂ® TegraÂ® memory architecture and considerations for porting code from a discrete GPU (dGPU) attached to an x86 system to the TegraÂ® integrated GPU (iGPU). It also discusses EGL interoperability.
 </p>
 <p>
  This guide is for developers who are already familiar with programming in CUDAÂ®, and C/C++, and who want to develop applications for the TegraÂ® SoC.
 </p>
 <p>
  Performance guidelines, best practices, terminology, and general information provided in the
  CUDA C++ Programming Guide
  and the
  CUDA C++ Best Practices Guide
  are applicable to all CUDA-capable GPU architectures, including TegraÂ® devices.
 </p>
 <p>
  The
  CUDA C++ Programming Guide
  and the
  CUDA C Best Practices Guide
  are available at the following web sites:
 </p>
 <p>
  CUDA C++ Programming Guide:
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">
   https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html
  </a>
 </p>
 <p>
  CUDA C++ Best Practices Guide:
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html">
   https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html
  </a>
 </p>
 <h1>
  <span class="section-number">
   3.
  </span>
  Memory Management
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#memory-management" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  In TegraÂ® devices, both the CPU (Host) and the iGPU share SoC DRAM memory. A dGPU with separate DRAM memory can be connected to the Tegra device over PCIe or NVLink. It is currently supported only on the NVIDIA DRIVE platform.
 </p>
 <p>
  An overview of a dGPU-connected TegraÂ® memory system is shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/index.html#memory-management__fig-dGPU-connected-tegra-memory-system">
   Figure 1
  </a>
  .
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/_images/dGPU-connected-tegra-memory-system.png">
 </a>
 <p>
  <span class="caption-text">
   dGPU-connected Tegra Memory System
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#memory-management-fig-dgpu-connected-tegra-memory-system" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  In Tegra, device memory, host memory, and unified memory are allocated on the same physical SoC DRAM. On a dGPU, device memory is allocated on the dGPU DRAM. The caching behavior in a Tegra system is different from that of an x86 system with a dGPU. The caching and accessing behavior of different memory types in a Tegra system is shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/index.html#memory-management__table_memory_types">
   Table 1
  </a>
  .
 </p>
 <table class="docutils align-default" id="id5">
  <span class="caption-text">
   Table 1. Characteristics of Different Memory Types in a Tegra System
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#id5" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <td>
    <p>
     Memory Type
    </p>
   </td>
   <td>
    <p>
     CPU
    </p>
   </td>
   <td>
    <p>
     iGPU
    </p>
   </td>
   <td>
    <p>
     Tegra-connected dGPU
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Device memory
    </p>
   </td>
   <td>
    <p>
     Not directly accessible
    </p>
   </td>
   <td>
    <p>
     Cached
    </p>
   </td>
   <td>
    <p>
     Cached
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Pageable host memory
    </p>
   </td>
   <td>
    <p>
     Cached
    </p>
   </td>
   <td>
    <p>
     Not directly accessible
    </p>
   </td>
   <td>
    <p>
     Not directly accessible
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Pinned host memory
    </p>
   </td>
   <td>
    <p>
     Uncached where compute capability is less than 7.2.
    </p>
    <p>
     Cached where compute capability is greater than or equal to 7.2.
    </p>
   </td>
   <td>
    <p>
     Uncached
    </p>
   </td>
   <td>
    <p>
     Uncached
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Unified memory
    </p>
   </td>
   <td>
    <p>
     Cached
    </p>
   </td>
   <td>
    <p>
     Cached
    </p>
   </td>
   <td>
    <p>
     Not supported
    </p>
   </td>
  </tr>
 </table>
 <p>
  On Tegra, because device memory, host memory, and unified memory are allocated on the same physical SoC DRAM, duplicate memory allocations and data transfers can be avoided.
 </p>
 <h2>
  <span class="section-number">
   3.1.
  </span>
  I/O Coherency
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#i-o-coherency" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  I/O coherency (also known as one-way coherency) is a feature with which an I/O device such as a GPU can read the latest updates in CPU caches. It removes the need to perform CPU cache management operations when the same physical memory is shared between CPU and GPU. The GPU cache management operations still need to be performed because the coherency is one way. Please note that the CUDA driver internally performs the GPU cache management operations when managed memory or interop memory is used.
 </p>
 <p>
  I/O coherency is supported on Tegra devices starting with Xavier SOC. Applications should realize benefits from this HW feature without needing to make changes to the applicationâs code (see point 2 below).
 </p>
 <p>
  The following functionalities depend on I/O coherency support:
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    <span class="pre">
     cudaHostRegister()
    </span>
    /
    <span class="pre">
     cuMemHostRegister()
    </span>
    is supported only on platforms which are I/O coherent. The host register support can be queried using the device attribute
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1gg49e2f8c2c0bd6fe264f2fc970912e5cd6ea4a004a336c3c95b6ff06ec6269e29">
     cudaDevAttrHostRegisterSupported
    </a>
    / CU_DEVICE_ATTRIBUTE_HOST_REGISTER_SUPPORTED.
   </p>
  </li>
  <li>
   <p>
    CPU cache for pinned memory allocated using
    <span class="pre">
     cudaMallocHost()
    </span>
    /
    <span class="pre">
     cuMemHostAlloc()
    </span>
    /
    <span class="pre">
     cuMemAllocHost()
    </span>
    is enabled only on platforms which are I/O coherent.
   </p>
  </li>
 </ol>
 <h2>
  <span class="section-number">
   3.2.
  </span>
  Estimating Total Allocatable Device Memory on an Integrated GPU Device
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#estimating-total-allocatable-device-memory-on-an-integrated-gpu-device" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The
  <span class="pre">
   cudaMemGetInfo()
  </span>
  API returns the snapshot of free and total amount of memory available for allocation for the GPU. The free memory could change if any other client allocate memory.
 </p>
 <p>
  The discrete GPU has the dedicated DRAM called VIDMEM which is separate from CPU memory. The snapshot of free memory in discrete GPU is returned by the
  <span class="pre">
   cudaMemGetInfo
  </span>
  API.
 </p>
 <p>
  The integrated GPU, on Tegra SoC, shares the DRAM with CPU and other the Tegra engines. The CPU can control the contents of DRAM and free DRAM memory by moving the contents of DMAR to SWAP area or vice versa. The
  <span class="pre">
   cudaMemGetInfo
  </span>
  API currently does not account for SWAP memory area. The
  <span class="pre">
   cudaMemGetInfo
  </span>
  API may return a smaller size than the actually allocatable memory since the CPU may be able to free up some DRAM region by moving pages to the SWAP area. In order to estimate the amount of allocatable device memory, CUDA application developers should consider following:
 </p>
 <p>
  On Linux and Android platforms:
  Device allocatable memory on Linux and Android depends mainly on the total and free sizes of swap space and main memory. The following points can help users to estimate the total amount of device allocatable memory in various situations:
 </p>
 <ul class="simple">
  <li>
   <p>
    Host allocated memory = Total used physical memory â Device allocated memory
   </p>
  </li>
  <li>
   <p>
    If (Host allocated memory &lt; Free Swap Space) then Device allocatable memory = Total Physical Memory â already allocated device memory
   </p>
  </li>
  <li>
   <p>
    If (Host allocated memory &gt; Free Swap Space) then Device allocatable memory = Total Physical Memory â (Host allocated memory - Free swap space)
   </p>
  </li>
 </ul>
 <p>
  Here,
 </p>
 <ul class="simple">
  <li>
   <p>
    Device allocated memory is memory already allocated on the device. It can be obtained from the
    <span class="pre">
     NvMapMemUsed
    </span>
    field in
    <span class="pre">
     /proc/meminfo
    </span>
    or from the
    <span class="pre">
     total
    </span>
    field of
    <span class="pre">
     /sys/kernel/debug/nvmap/iovmm/clients
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    Total used physical memory can be obtained using the
    <span class="pre">
     free
    </span>
    <span class="pre">
     -m
    </span>
    command. The
    <span class="pre">
     used
    </span>
    field in row
    <span class="pre">
     Mem
    </span>
    represents this information.
   </p>
  </li>
  <li>
   <p>
    Total Physical memory is obtained from the
    <span class="pre">
     MemTotal
    </span>
    field in
    <span class="pre">
     /proc/meminfo
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    Free swap space can be find by using the
    <span class="pre">
     free
    </span>
    <span class="pre">
     -m
    </span>
    command. The
    <span class="pre">
     free
    </span>
    field in the
    <span class="pre">
     Swap
    </span>
    row represents this information.
   </p>
  </li>
  <li>
   <p>
    If the
    <span class="pre">
     free
    </span>
    command is not available, the same information can be obtained from
    <span class="pre">
     /proc/meminfo
    </span>
    as:
   </p>
   <ul>
    <li>
     <p>
      Total Used physical memory =
      <span class="pre">
       MemTotal
      </span>
      â
      <span class="pre">
       MemFree
      </span>
     </p>
    </li>
    <li>
     <p>
      Free swap space =
      <span class="pre">
       SwapFree
      </span>
     </p>
    </li>
   </ul>
  </li>
 </ul>
 <p>
  On QNX platforms:
  QNX does not use swap space, hence,
  <span class="pre">
   cudaMemGetInfo.free
  </span>
  will be a fair estimate of allocatable device memory as there is no swap space to move memory pages to swap area.
 </p>
 <h1>
  <span class="section-number">
   4.
  </span>
  Porting Considerations
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#porting-considerations" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  CUDA applications originally developed for dGPUs attached to x86 systems may require modifications to perform efficiently on Tegra systems. This section describes the considerations for porting such applications to a Tegra system, such as selecting an appropriate memory buffer type (pinned memory, unified memory, and others) and selecting between iGPU and dGPU, to achieve efficient performance for the application.
 </p>
 <h2>
  <span class="section-number">
   4.1.
  </span>
  Memory Selection
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#memory-selection" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA applications can use various kinds of memory buffers, such as device memory, pageable host memory, pinned memory, and unified memory. Even though these memory buffer types are allocated on the same physical device, each has different accessing and caching behaviors, as shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/index.html#memory-management__table_memory_types">
   Table 1
  </a>
  . It is important to select the most appropriate memory buffer type for efficient application execution.
 </p>
 <p class="title sectiontitle rubric" id="device-memory">
  Device Memory
 </p>
 <p>
  Use device memory for buffers whose accessibility is limited to the iGPU. For example, in an application with multiple kernels, there may be buffers that are used only by the intermediate kernels of the application as input or output. These buffers are accessed only by the iGPU. Such buffers should be allocated with device memory.
 </p>
 <p class="title sectiontitle rubric" id="pageable-host-memory">
  Pageable Host Memory
 </p>
 <p>
  Use pageable host memory for buffers whose accessibility is limited to the CPU.
 </p>
 <p class="title sectiontitle rubric" id="pinned-memory">
  Pinned Memory
 </p>
 <p>
  TegraÂ® systems with different compute capabilities exhibit different behavior in terms of I/O coherency. For example, TegraÂ® systems with compute capability greater than or equal to 7.2 are I/O coherent and others are not I/O coherent. On TegraÂ® systems with I/O coherency, the CPU access time of pinned memory is as good as pageable host memory because it is cached on the CPU. However, on TegraÂ® systems without I/O coherency, the CPU access time of pinned memory is higher, because it is not cached on the CPU.
 </p>
 <p>
  Pinned memory is recommended for small buffers because the caching effect is negligible for such buffers and also because pinned memory does not involve any additional overhead, unlike Unified Memory. With no additional overhead, pinned memory is also preferable for large buffers if the access pattern is not cache friendly on iGPU. For large buffers, when the buffer is accessed only once on iGPU in a coalescing manner, performance on iGPU can be as good as unified memory on iGPU.
 </p>
 <p class="title sectiontitle rubric" id="unified-memory">
  Unified Memory
 </p>
 <p>
  Unified memory is cached on the iGPU and the CPU. On TegraÂ®, using unified memory in applications requires additional coherency and cache maintenance operations during the kernel launch, synchronization and prefetching hint calls. This coherency maintenance overhead is slightly higher on a TegraÂ® system with compute capability less than 7.2 as they lack I/O coherency.
 </p>
 <p>
  On TegraÂ® devices with I/O coherency (with a compute capability of 7.2 or greater) where unified memory is cached on both CPU and iGPU, for large buffers which are frequently accessed by the iGPU and the CPU and
  the accesses on iGPU are repetitive
  , unified memory is preferable since repetitive accesses can offset the cache maintenance cost. On TegraÂ® devices without I/O coherency (with a compute capability of less than 7.2), for large buffers which are frequently accessed by the CPU and the iGPU and
  the accesses on iGPU are not repetitive
  , unified memory is still preferable over pinned memory because pinned memory is not cached on both CPU and iGPU. That way, the application can take advantage of unified memory caching on the CPU.
 </p>
 <p>
  Pinned memory or unified memory can be used to reduce the data transfer overhead between CPU and iGPU as both memories are directly accessible from the CPU and the iGPU. In an application, input and output buffers that must be accessible on both the host and the iGPU can be allocated using either unified memory or pinned memory.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The unified memory model requires the driver and system software to manage coherence on the current Tegra SOC. Software managed coherence is by nature non-deterministic and not recommended in a safe context. Zero-copy memory (pinned memory) is preferable in these applications.
 </p>
 <p>
  Evaluate the impact of unified memory overheads, pinned memory cache misses, and device memory data transfers in applications to determine the correct memory selection.
 </p>
 <h2>
  <span class="section-number">
   4.2.
  </span>
  Pinned Memory
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#id1" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This section provides guidelines for porting applications that use pinned memory allocations in x86 systems with dGPUs to TegraÂ®. CUDA applications developed for a dGPU attached to x86 system use pinned memory to reduce data transfer time and to overlap data transfers with kernel execution time. For specific information on this topic, see âData Transfer Between Host and Deviceâ and âAsynchronous and Overlapping Transfers with Computationâ at the following websites.
 </p>
 <p>
  âData Transfer Between Host and Deviceâ:
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#data-transfer-between-host-and-device">
   https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#data-transfer-between-host-and-device
  </a>
 </p>
 <p>
  âAsynchronous and Overlapping Transfers with Computationâ:
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation">
   https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation
  </a>
 </p>
 <p>
  On TegraÂ® systems with no I/O coherency, repetitive access of pinned memory degrades application performance, because pinned memory is not cached on the CPU in such systems.
 </p>
 <p>
  A sample application is shown below in which a set of filters and operations (k1, k2, and k3) are applied to an image. Pinned memory is allocated to reduce data transfer time on an x86 system with a dGPU, increasing the overall application speed. However, targeting a TegraÂ® device with the same code causes a drastic increase in the execution time of the
  <span class="pre">
   readImage()
  </span>
  function because it repeatedly accesses an uncached buffer. This increases the overall application time. If the time taken by
  <span class="pre">
   readImage()
  </span>
  is significantly higher compared to kernels execution time, it is recommended to use unified memory to reduce the
  <span class="pre">
   readImage()
  </span>
  time. Otherwise, evaluate the application with pinned memory and unified memory by removing unnecessary data transfer calls to decide best suited memory.
 </p>
 <pre><span class="c1">// Sample code for an x86 system with a discrete GPU</span>
<span class="kt">int</span><span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
<span class="kt">int</span><span class="o">*</span><span class="n">h_a</span><span class="p">,</span><span class="o">*</span><span class="n">d_a</span><span class="p">,</span><span class="o">*</span><span class="n">d_b</span><span class="p">,</span><span class="o">*</span><span class="n">d_c</span><span class="p">,</span><span class="o">*</span><span class="n">d_d</span><span class="p">,</span><span class="o">*</span><span class="n">h_d</span><span class="p">;</span>
<span class="kt">int</span><span class="n">height</span><span class="o">=</span><span class="mi">1024</span><span class="p">;</span>
<span class="kt">int</span><span class="n">width</span><span class="o">=</span><span class="mi">1024</span><span class="p">;</span>
<span class="kt">size_t</span><span class="n">sizeOfImage</span><span class="o">=</span><span class="n">width</span><span class="o">*</span><span class="n">height</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span><span class="c1">// 4MB image</span>

<span class="c1">//Pinned memory allocated to reduce data transfer time</span>
<span class="n">cudaMallocHost</span><span class="p">(</span><span class="n">h_a</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">);</span>
<span class="n">cudaMallocHost</span><span class="p">(</span><span class="n">h_d</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">);</span>

<span class="c1">//Allocate buffers on GPU</span>
<span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_a</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">);</span>
<span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_b</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">);</span>
<span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_c</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">);</span>
<span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_d</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">);</span>

<span class="c1">//CPU reads Image;</span>
<span class="n">readImage</span><span class="p">(</span><span class="n">h_a</span><span class="p">);</span><span class="c1">// Intialize the h_a buffer</span>

<span class="c1">// Transfer image to GPU</span>
<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span><span class="n">h_a</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">,</span><span class="n">cudaMemcpyHostToDevice</span><span class="p">);</span>

<span class="c1">// Data transfer is fast as we used pinned memory</span>
<span class="c1">// ----- CUDA Application pipeline start ----</span>
<span class="n">k1</span><span class="o">&lt;&lt;&lt;</span><span class="p">..</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_a</span><span class="p">,</span><span class="n">d_b</span><span class="p">)</span><span class="c1">// Apply filter 1</span>
<span class="n">k2</span><span class="o">&lt;&lt;&lt;</span><span class="p">..</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_b</span><span class="p">,</span><span class="n">d_c</span><span class="p">)</span><span class="c1">// Apply filter 2</span>
<span class="n">k3</span><span class="o">&lt;&lt;&lt;</span><span class="p">..</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_c</span><span class="p">,</span><span class="n">d_d</span><span class="p">)</span><span class="c1">// Some operation on image data</span>
<span class="c1">// ----- CUDA Application pipeline end ----</span>

<span class="c1">// Transfer processed image to CPU</span>
<span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">h_d</span><span class="p">,</span><span class="n">d_d</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">,</span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">);</span>
<span class="c1">// Data transfer is fast as we used pinned memory</span>

<span class="c1">// Use processed Image i.e h_d in later computations on CPU.</span>
<span class="n">UseImageonCPU</span><span class="p">(</span><span class="n">h_d</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// Porting the code on Tegra</span>
<span class="kt">int</span><span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
<span class="kt">int</span><span class="o">*</span><span class="n">h_a</span><span class="p">,</span><span class="o">*</span><span class="n">d_b</span><span class="p">,</span><span class="o">*</span><span class="n">d_c</span><span class="p">,</span><span class="o">*</span><span class="n">h_d</span><span class="p">;</span>
<span class="kt">int</span><span class="n">height</span><span class="o">=</span><span class="mi">1024</span><span class="p">;</span>
<span class="kt">int</span><span class="n">width</span><span class="o">=</span><span class="mi">1024</span><span class="p">;</span>
<span class="kt">size_t</span><span class="n">sizeOfImage</span><span class="o">=</span><span class="n">width</span><span class="o">*</span><span class="n">height</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span><span class="c1">// 4MB image</span>

<span class="c1">//Unified memory allocated for input and output</span>
<span class="c1">//buffer of application pipeline</span>
<span class="n">cudaMallocManaged</span><span class="p">(</span><span class="n">h_a</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">,</span><span class="n">cudaMemAttachHost</span><span class="p">);</span>
<span class="n">cudaMallocManaged</span><span class="p">(</span><span class="n">h_d</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">);</span>

<span class="c1">//Intermediate buffers not needed on CPU side.</span>
<span class="c1">//So allocate them on device memory</span>
<span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_b</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">);</span>
<span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_c</span><span class="p">,</span><span class="n">sizeOfImage</span><span class="p">);</span>

<span class="c1">//CPU reads Image;</span>
<span class="n">readImage</span><span class="p">(</span><span class="n">h_a</span><span class="p">);</span><span class="c1">// Intialize the h_a buffer</span>
<span class="c1">// ----- CUDA Application pipeline start ----</span>
<span class="c1">// Prefetch input image data to GPU</span>
<span class="n">cudaStreamAttachMemAsync</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span><span class="n">h_a</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">cudaMemAttachGlobal</span><span class="p">);</span>
<span class="n">k1</span><span class="o">&lt;&lt;&lt;</span><span class="p">..</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">h_a</span><span class="p">,</span><span class="n">d_b</span><span class="p">)</span>
<span class="n">k2</span><span class="o">&lt;&lt;&lt;</span><span class="p">..</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_b</span><span class="p">,</span><span class="n">d_c</span><span class="p">)</span>
<span class="n">k3</span><span class="o">&lt;&lt;&lt;</span><span class="p">..</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_c</span><span class="p">,</span><span class="n">h_d</span><span class="p">)</span>
<span class="c1">// Prefetch output image data to CPU</span>
<span class="n">cudaStreamAttachMemAsync</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span><span class="n">h_d</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">cudaMemAttachHost</span><span class="p">);</span>
<span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="nb">NULL</span><span class="p">);</span>
<span class="c1">// ----- CUDA Application pipeline end ----</span>

<span class="c1">// Use processed Image i.e h_d on CPU side.</span>
<span class="n">UseImageonCPU</span><span class="p">(</span><span class="n">h_d</span><span class="p">);</span>
<span class="p">}</span>
</pre>
 <p class="title sectiontitle rubric" id="the-cudahostregister-function">
  The
  <span class="pre">
   cudaHostRegister()
  </span>
  function
 </p>
 <p>
  The
  <span class="pre">
   cudaHostRegister()
  </span>
  function is not supported on TegraÂ® devices with compute capability less than 7.2, because those devices do not have I/O coherency. Use other pinned memory allocation functions such as
  <span class="pre">
   cudaMallocHost()
  </span>
  and
  <span class="pre">
   cudaHostAlloc()
  </span>
  if
  <span class="pre">
   cudaHostRegister()
  </span>
  is not supported on the device.
 </p>
 <p class="title sectiontitle rubric" id="gnu-atomic-operations-on-pinned-memory">
  GNU Atomic operations on pinned memory
 </p>
 <p>
  The GNU atomic operations on uncached memory is not supported on TegraÂ® CPU. As pinned memory is not cached on TegraÂ® devices with compute capability less than 7.2, GNU atomic operations is not supported on pinned memory.
 </p>
 <h2>
  <span class="section-number">
   4.3.
  </span>
  Effective Usage of Unified Memory on Tegra
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#effective-usage-of-unified-memory-on-tegra" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Using unified memory in applications requires additional coherency and cache maintenance operations at kernel launch, synchronization, and prefetching hint calls. These operations are performed synchronously with other GPU work which can cause unpredictable latencies in the application.
 </p>
 <p>
  The performance of unified memory on TegraÂ® can be improved by providing data prefetching hints. The driver can use these prefetching hints to optimize the coherence operations. To prefetch the data, the
  <span class="pre">
   cudaStreamAttachMemAsync()
  </span>
  function can be used, in addition to the techniques described in the âCoherency and Concurrencyâ section of the
  CUDA C Programming Guide
  at the following link:
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-coherency-hd">
   https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-coherency-hd
  </a>
 </p>
 <p>
  to prefetch the data. The prefetching behavior of unified memory, as triggered by the changing states of the attachment flag, is shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/index.html#effective-usage-unified-memory__table_unified_memory_prefetching">
   Table 2
  </a>
  .
 </p>
 <table class="docutils align-default" id="id6">
  <span class="caption-text">
   Table 2. Unified Memory Prefetching Behavior per Changing Attachment Flag States
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#id6" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <td>
    <p>
     Previous Flag
    </p>
   </td>
   <td>
    <p>
     Current Flag
    </p>
   </td>
   <td>
    <p>
     Prefetching Behavior
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cudaMemAttachGlobal/cudaMemAttachSingle
    </p>
   </td>
   <td>
    <p>
     cudaMemAttachHost
    </p>
   </td>
   <td>
    <p>
     Causes prefetch to CPU
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     cudaMemAttachHost
    </p>
   </td>
   <td>
    <p>
     cudaMemAttachGlobal/
    </p>
    <p>
     cudaMemAttachSingle
    </p>
   </td>
   <td>
    <p>
     Causes prefetch to GPU
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cudaMemAttachGlobal
    </p>
   </td>
   <td>
    <p>
     cudaMemAttachSingle
    </p>
   </td>
   <td>
    <p>
     No prefetch to GPU
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     cudaMemAttachSingle
    </p>
   </td>
   <td>
    <p>
     cudaMemAttachGlobal
    </p>
   </td>
   <td>
    <p>
     No prefetch to GPU
    </p>
   </td>
  </tr>
 </table>
 <p>
  The following example shows usage of
  <span class="pre">
   cudaStreamAttachMemAsync()
  </span>
  to prefetch data.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  However, not supported on TegraÂ® devices are the data prefetching techniques that use
  <span class="pre">
   cudaMemPrefetchAsync()
  </span>
  as described in the âPerformance Tuningâ section of the
  CUDA C++ Programming Guide
  at the following web site:
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-performance-tuning">
   https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-performance-tuning
  </a>
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  There are limitations in QNX system software which prevent implementation of all UVM optimizations. Because of this, using
  <span class="pre">
   cudaStreamAttachMemAsync()
  </span>
  to prefetch hints on QNX does not benefit performance.
 </p>
 <pre><span class="n">__global__</span><span class="kt">void</span><span class="n">matrixMul</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="n">p</span><span class="p">,</span><span class="kt">int</span><span class="o">*</span><span class="n">q</span><span class="p">,</span><span class="kt">int</span><span class="o">*</span><span class="n">r</span><span class="p">,</span><span class="kt">int</span><span class="n">hp</span><span class="p">,</span><span class="kt">int</span><span class="n">hq</span><span class="p">,</span><span class="kt">int</span><span class="n">wp</span><span class="p">,</span><span class="kt">int</span><span class="n">wq</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// Matrix multiplication kernel code</span>
<span class="p">}</span>
<span class="kt">void</span><span class="n">MatrixMul</span><span class="p">(</span><span class="kt">int</span><span class="n">hp</span><span class="p">,</span><span class="kt">int</span><span class="n">hq</span><span class="p">,</span><span class="kt">int</span><span class="n">wp</span><span class="p">,</span><span class="kt">int</span><span class="n">wq</span><span class="p">)</span>
<span class="p">{</span>
<span class="kt">int</span><span class="o">*</span><span class="n">p</span><span class="p">,</span><span class="o">*</span><span class="n">q</span><span class="p">,</span><span class="o">*</span><span class="n">r</span><span class="p">;</span>
<span class="kt">int</span><span class="n">i</span><span class="p">;</span>
<span class="kt">size_t</span><span class="n">sizeP</span><span class="o">=</span><span class="n">hp</span><span class="o">*</span><span class="n">wp</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>
<span class="kt">size_t</span><span class="n">sizeQ</span><span class="o">=</span><span class="n">hq</span><span class="o">*</span><span class="n">wq</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>
<span class="kt">size_t</span><span class="n">sizeR</span><span class="o">=</span><span class="n">hp</span><span class="o">*</span><span class="n">wq</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span>

<span class="c1">//Attach buffers 'p' and 'q' to CPU and buffer 'r' to GPU</span>
<span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">p</span><span class="p">,</span><span class="n">sizeP</span><span class="p">,</span><span class="n">cudaMemAttachHost</span><span class="p">);</span>
<span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">q</span><span class="p">,</span><span class="n">sizeQ</span><span class="p">,</span><span class="n">cudaMemAttachHost</span><span class="p">);</span>
<span class="n">cudaMallocManaged</span><span class="p">(</span><span class="o">&amp;</span><span class="n">r</span><span class="p">,</span><span class="n">sizeR</span><span class="p">);</span>
<span class="c1">//Intialize with random values</span>
<span class="n">randFill</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">hp</span><span class="p">,</span><span class="n">wp</span><span class="p">,</span><span class="n">hq</span><span class="p">,</span><span class="n">wq</span><span class="p">);</span>

<span class="c1">// Prefetch p,q to GPU as they are needed in computation</span>
<span class="n">cudaStreamAttachMemAsync</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span><span class="n">p</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">cudaMemAttachGlobal</span><span class="p">);</span>
<span class="n">cudaStreamAttachMemAsync</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">cudaMemAttachGlobal</span><span class="p">);</span>
<span class="n">matrixMul</span><span class="o">&lt;&lt;&lt;</span><span class="p">....</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">p</span><span class="p">,</span><span class="n">q</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">hp</span><span class="p">,</span><span class="n">hq</span><span class="p">,</span><span class="n">wp</span><span class="p">,</span><span class="n">wq</span><span class="p">);</span>

<span class="c1">// Prefetch 'r' to CPU as only 'r' is needed</span>
<span class="n">cudaStreamAttachMemAsync</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">cudaMemAttachHost</span><span class="p">);</span>
<span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="nb">NULL</span><span class="p">);</span>

<span class="c1">// Print buffer 'r' values</span>
<span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">hp</span><span class="o">*</span><span class="n">wq</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="n">printf</span><span class="p">(</span><span class="s">"%d "</span><span class="p">,</span><span class="n">r</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="p">}</span>
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  An additional
  <span class="pre">
   cudaStreamSynchronize(NULL)
  </span>
  call can be added after the
  <span class="pre">
   matrixMul
  </span>
  kernel code to avoid callback threads that cause unpredictability in a
  <span class="pre">
   cudaStreamAttachMemAsync()
  </span>
  call.
 </p>
 <h2>
  <span class="section-number">
   4.4.
  </span>
  GPU Selection
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#gpu-selection" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  On a Tegra system with a dGPU, deciding whether a CUDA application runs on the iGPU or the dGPU can have implications for the performance of the application. Some of the factors that need to be considered while making such a decision are kernel execution time, data transfer time, data locality, and latency. For example, to run an application on a dGPU, data must be transferred between the SoC and the dGPU. This data transfer can be avoided if the application runs on an iGPU.
 </p>
 <h2>
  <span class="section-number">
   4.5.
  </span>
  Synchronization Mechanism Selection
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#synchronization-mechanism-selection" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The
  <span class="pre">
   cudaSetDeviceFlags
  </span>
  API is used to control the synchronization behaviour of CPU thread. Until CUDA 10.1, by default, the synchronization mechanism on iGPU uses
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g130ddae663f1873258fee5a6e0808b71">
   cudaDeviceBlockingSync
  </a>
  flag, which blocks the CPU thread on a synchronization primitive when waiting for the device to finish work. The
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1g130ddae663f1873258fee5a6e0808b71">
   cudaDeviceBlockingSync
  </a>
  flag is suited for platforms with power constraints. But on platforms which requires low latency,
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html#group__CUDART__TYPES_1gf01347c3dafebf07e1a0b4321a030a63">
   cudaDeviceScheduleSpin
  </a>
  flag needs to set manually. Since CUDA 10.1, for each platform, the default synchronization flag is determined based on what is optimized for that platform. More information about the synchronization flags is given at
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__DEVICE.html#group__CUDART__DEVICE_1g69e73c7dda3fc05306ae7c811a690fac">
   cudaSetDeviceFlags
  </a>
  API documentation.
 </p>
 <h2>
  <span class="section-number">
   4.6.
  </span>
  CUDA Features Not Supported on Tegra
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-features-not-supported-on-tegra" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  All core features of CUDA are supported on Tegra platforms. The exceptions are listed below.
 </p>
 <ul class="simple">
  <li>
   <p>
    The
    <span class="pre">
     cudaHostRegister()
    </span>
    function is not supported on QNX systems. This is due to the limitations on QNX OS. It is supported in Linux systems with compute capability greater than or equal to 7.2.
   </p>
  </li>
  <li>
   <p>
    System wide atomics are not supported on Tegra devices with compute capability less than 7.2.
   </p>
  </li>
  <li>
   <p>
    Unified memory is not supported on dGPU attached to Tegra.
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     cudaMemPrefetchAsync()
    </span>
    function is not supported since unified memory with concurrent access is not yet supported on iGPU.
   </p>
  </li>
  <li>
   <p>
    NVIDIA management library (NVML) library is not supported on Tegra. However, as an alternative to monitor the resource utilization,
    <span class="pre">
     tegrastats
    </span>
    can be used.
   </p>
  </li>
  <li>
   <p>
    Since CUDA 11.5, only events-sharing IPC APIs are supported on L4T and embedded Linux Tegra devices with compute capability 7.x and higher. The memory-sharing IPC APIs are still not supported on Tegra platforms. EGLStream, NvSci, or the
    <span class="pre">
     cuMemExportToShareableHandle()
    </span>
    /
    <span class="pre">
     cuMemImportFromShareableHandle()
    </span>
    APIs can be used to communicate between CUDA contexts in two processes.
   </p>
  </li>
  <li>
   <p>
    Remote direct memory access (RDMA) is supported only on Tegra devices running L4T or embedded-linux.
   </p>
  </li>
  <li>
   <p>
    JIT compilation might require a considerable amount of CPU and bandwidth resources, potentially interfering with other workloads in the system. Thus, JIT compilations such as PTX-JIT and NVRTC JIT are not recommended for deterministic embedded applications and can be bypassed completely by compiling for specific GPU targets. For example: If you are compiling for SM version 87, use this nvcc flag
    <span class="pre">
     --generate-code
    </span>
    <span class="pre">
     arch=compute_87,code=sm_87
    </span>
    to create a CUDA binary for this device. This avoids JIT compilation during the first run and improves the runtime performance.
JIT compilation is not supported on Tegra devices in the safe context.
   </p>
  </li>
  <li>
   <p>
    Multi process service (MPS) is not supported on Tegra.
   </p>
  </li>
  <li>
   <p>
    Peer to peer (P2P) communication calls are not supported on Tegra.
   </p>
  </li>
  <li>
   <p>
    The cuSOLVER library is not supported on Tegra systems running QNX.
   </p>
  </li>
  <li>
   <p>
    The nvGRAPH library is not supported.
   </p>
  </li>
  <li>
   <p>
    <a class="reference external" href="https://docs.nvidia.com/cuda/cub/index.html">
     CUB
    </a>
    is experimental on Tegra products.
   </p>
  </li>
 </ul>
 <p>
  More information on some of these features can be found at the following web sites:
 </p>
 <p>
  IPC:
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#interprocess-communication">
   https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#interprocess-communication
  </a>
 </p>
 <p>
  NVSCI:
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#nvidia-softwarcommunication-interface-interoperability-nvsci">
   https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#nvidia-softwarcommunication-interface-interoperability-nvsci
  </a>
 </p>
 <p>
  RDMA:
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/gpudirect-rdma/index.html">
   https://docs.nvidia.com/cuda/gpudirect-rdma/index.html
  </a>
 </p>
 <p>
  MPS:
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf">
   https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf
  </a>
 </p>
 <p>
  P2P:
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#peer-to-peer-memory-access">
   https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#peer-to-peer-memory-access
  </a>
 </p>
 <h1>
  <span class="section-number">
   5.
  </span>
  EGL Interoperability
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#egl-interoperability" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  An interop is an efficient mechanism to share resources between two APIs. To share data with multiple APIs, an API must implement an individual interop for each.
 </p>
 <p>
  EGL provides interop extensions that allow it to function as a hub connecting APIs, removing the need for multiple interops, and encapsulating the shared resource. An API must implement these extensions to interoperate with any other API via EGL. The CUDA supported EGL interops are EGLStream, EGLImage, and EGLSync.
 </p>
 <p>
  EGL interop extensions allow applications to switch between APIs without the need to rewrite code. For example, an EGLStream-based application in which NvMedia is the producer and CUDA is the consumer can be modified to use OpenGL as the consumer without modifying the producer code.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  On the DRIVE OS platform, NVSCI is provided as an alternative to EGL interoperability for safety critical applications. Please see
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#nvidia-softwarcommunication-interface-interoperability-nvsci">
   NVSCI
  </a>
  for more details.
 </p>
 <h2>
  <span class="section-number">
   5.1.
  </span>
  EGLStream
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglstream" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  EGLStream interoperability facilitates efficient transfer of a sequence of frames from one API to another API, allowing use of multiple TegraÂ® engines such as CPU, GPU, ISP, and others.
 </p>
 <p>
  Consider an application where a camera captures images continuously, shares them with CUDA for processing, and then later renders those images using OpenGL. In this application, the image frames are shared across NvMedia, CUDA and OpenGL. The absence of EGLStream interoperability would require the application to include multiple interops and redundant data transfers between APIs. EGLStream has one producer and one consumer.
 </p>
 <p>
  EGLStream offers the following benefits:
 </p>
 <ul class="simple">
  <li>
   <p>
    Efficient transfer of frames between a producer and a consumer.
   </p>
  </li>
  <li>
   <p>
    Implicit synchronization handling.
   </p>
  </li>
  <li>
   <p>
    Cross-process support.
   </p>
  </li>
  <li>
   <p>
    dGPU and iGPU support.
   </p>
  </li>
  <li>
   <p>
    Linux, QNX, and Android operating system support.
   </p>
  </li>
 </ul>
 <h3>
  <span class="section-number">
   5.1.1.
  </span>
  EGLStream Flow
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglstream-flow" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The EGLStream flow has the following steps:
 </p>
 <ol class="arabic">
  <li>
   <p>
    Initialize producer and consumer APIs
   </p>
  </li>
  <li>
   <p>
    Create an EGLStream and connect the consumer and the producer.
   </p>
   <p class="admonition-title">
    Note
   </p>
   <p>
    EGLStream is created using
    <span class="pre">
     eglCreateStreamKHR()
    </span>
    and destroyed using
    <span class="pre">
     eglDestroyStreamKHR()
    </span>
    .
   </p>
   <p>
    The consumer should always connect to EGLStream before the producer.
   </p>
   <p>
    For more information see the EGLStream specification at the following web site:
    <a class="reference external" href="https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_stream.txt">
     https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_stream.txt
    </a>
   </p>
  </li>
  <li>
   <p>
    Allocate memory used for EGL frames.
   </p>
  </li>
  <li>
   <p>
    The producer populates an EGL frame and presents it to EGLStream.
   </p>
  </li>
  <li>
   <p>
    The consumer acquires the frame from EGLStream and releases it back to EGLStream after processing.
   </p>
  </li>
  <li>
   <p>
    The producer collects the consumer-released frame from EGLStream.
   </p>
  </li>
  <li>
   <p>
    The producer presents the same frame, or a new frame to EGLStream.
   </p>
  </li>
  <li>
   <p>
    Steps 4-7 are repeated until completion of the task, with an old frame or a new frame.
   </p>
  </li>
  <li>
   <p>
    The consumer and the producer disconnect from EGLStream.
   </p>
  </li>
  <li>
   <p>
    Deallocate the memory used for EGL frames.
   </p>
  </li>
  <li>
   <p>
    De-initialize the producer and consumer APIs.
   </p>
  </li>
 </ol>
 <p>
  EGLStream application flow is shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/index.html#eglstream-flow__fig-eglstream-flow">
   Figure 2
  </a>
  .
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/_images/eglstream-flow.png">
 </a>
 <p>
  <span class="caption-text">
   EGLStream Flow
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglstream-flow-fig-eglstream-flow" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  CUDA producer and consumer functions are listed in
  <a class="reference external" href="https://docs.nvidia.com/cuda/index.html#eglstream-flow__table_producer_consumer_functions">
   Table 3
  </a>
  .
 </p>
 <table class="docutils align-default" id="id7">
  <span class="caption-text">
   Table 3. CUDA Producer and Consumer Functions
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#id7" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <td>
    <p>
     Role
    </p>
   </td>
   <td>
    <p>
     Functionality
    </p>
   </td>
   <td>
    <p>
     API
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Producer
    </p>
   </td>
   <td>
    <p>
     To connect a producer to EGLStream
    </p>
   </td>
   <td>
    <p>
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EGL.html#group__CUDA__EGL_1g5d181803d994a06f1bf9b05f52757bef">
      cuEGLStreamProducerConnect
     </a>
     ()
    </p>
    <p>
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EGL.html#group__CUDA__EGL_1g5d181803d994a06f1bf9b05f52757bef">
      cudaEGLStreamProducerConnect
     </a>
     ()
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     To present frame to EGLStream
    </p>
   </td>
   <td>
    <p>
     cuEGLStreamProducerPresentFrame()
    </p>
    <p>
     cudaEGLStreamProducerPresentFrame()
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Obtain released frames
    </p>
   </td>
   <td>
    <p>
     cuEGLStreamProducerReturnFrame()
    </p>
    <p>
     cudaEGLStreamProducerReturnFrame()
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     To disconnect from EGLStream
    </p>
   </td>
   <td>
    <p>
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EGL.html#group__CUDA__EGL_1gbdc9664bfb17dd3fa1e0a3ca68a8cafd">
      cuEGLStreamProducerDisconnect
     </a>
     ()
    </p>
    <p>
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EGL.html#group__CUDA__EGL_1gbdc9664bfb17dd3fa1e0a3ca68a8cafd">
      cudaEGLStreamProducerDisconnect
     </a>
     ()
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Consumer
    </p>
   </td>
   <td>
    <p>
     To connect a consumer to EGLStream
    </p>
   </td>
   <td>
    <p>
     cuEGLStreamConsumerConnect()
    </p>
    <p>
     cuEGLStreamConsumeConnectWithFlags()
    </p>
    <p>
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EGL.html#group__CUDART__EGL_1g7993b0e3802420547e3f403549be65a1">
      cudaEGLStreamConsumerConnect
     </a>
     ()
    </p>
    <p>
     cudaEGLStreamConsumerConnectWithFlags()
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     To acquire frame from EGLStream
    </p>
   </td>
   <td>
    <p>
     cuEGLStreamConsumerAcquireFrame()
    </p>
    <p>
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EGL.html#group__CUDART__EGL_1g83dd1bfea48c093d3f0b247754970f58">
      cudaEGLStreamConsumerAcquireFrame
     </a>
     ()
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     To release the consumed frame
    </p>
   </td>
   <td>
    <p>
     cuEGLStreamConsumerReleaseFrame()
    </p>
    <p>
     cudaEGLStreamConsumerReleaseFrame()
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     To disconnect from EGLStream
    </p>
   </td>
   <td>
    <p>
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EGL.html#group__CUDA__EGL_1g3ab15cff9be3b25447714101ecda6a61">
      cuEGLStreamConsumerDisconnect
     </a>
     ()
    </p>
    <p>
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EGL.html#group__CUDART__EGL_1gb2ef252e72ad2419506f3cf305753c6a">
      cudaEGLStreamConsumerDisconnect
     </a>
     ()
    </p>
   </td>
  </tr>
 </table>
 <h3>
  <span class="section-number">
   5.1.2.
  </span>
  CUDA as Producer
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-as-producer" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  When CUDA is the producer, the supported consumers are CUDA, NvMedia and OpenGL. API functions to be used when CUDA is the producer are listed in
  <a class="reference external" href="https://docs.nvidia.com/cuda/index.html#eglstream-flow__table_producer_consumer_functions">
   Table 3
  </a>
  . Except for connecting and disconnecting from EGLStream, all API calls are non-blocking.
 </p>
 <p>
  The following producer side steps are shown in the example code that follows:
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    Prepare a frame (lines 3-19).
   </p>
  </li>
  <li>
   <p>
    Connect the producer to EGLStream (line 21).
   </p>
  </li>
  <li>
   <p>
    Populate the frame and present to EGLStream (lines 23-25).
   </p>
  </li>
  <li>
   <p>
    Get the released frame back from EGLStream (Line 27).
   </p>
  </li>
  <li>
   <p>
    Disconnect the consumer after completion of the task. (Line 31).
   </p>
  </li>
 </ol>
 <pre>void ProducerThread(EGLStreamKHR eglStream) {
 //Prepares frame
 cudaEglFrame* cudaEgl = (cudaEglFrame *)malloc(sizeof(cudaEglFrame));
 cudaEgl-&gt;planeDesc[0].width = WIDTH;
 cudaEgl-&gt;planeDesc[0].depth = 0;
 cudaEgl-&gt;planeDesc[0].height = HEIGHT;
 cudaEgl-&gt;planeDesc[0].numChannels = 4;
 cudaEgl-&gt;planeDesc[0].pitch = WIDTH * cudaEgl-&gt;planeDesc[0].numChannels;
 cudaEgl-&gt;frameType = cudaEglFrameTypePitch;
 cudaEgl-&gt;planeCount = 1;
 cudaEgl-&gt;eglColorFormat = cudaEglColorFormatARGB;
 cudaEgl-&gt;planeDesc[0].channelDesc.f=cudaChannelFormatKindUnsigned
 cudaEgl-&gt;planeDesc[0].channelDesc.w = 8;
 cudaEgl-&gt;planeDesc[0].channelDesc.x = 8;
 cudaEgl-&gt;planeDesc[0].channelDesc.y = 8;
 cudaEgl-&gt;planeDesc[0].channelDesc.z = 8;
 size_t numElem = cudaEgl-&gt;planeDesc[0].pitch * cudaEgl-&gt;planeDesc[0].height;
 // Buffer allocated by producer
 cudaMalloc(&amp;(cudaEgl-&gt;pPitch[0].ptr), numElem);
 //CUDA producer connects to EGLStream
 cudaEGLStreamProducerConnect(&amp;conn, eglStream, WIDTH, HEIGHT))
 // Sets all elements in the buffer to 1
 K1&lt;&lt;&lt;...&gt;&gt;&gt;(cudaEgl-&gt;pPitch[0].ptr, 1, numElem);
 // Present frame to EGLStream
 cudaEGLStreamProducerPresentFrame(&amp;conn, *cudaEgl, NULL);

 cudaEGLStreamProducerReturnFrame(&amp;conn, cudaEgl, eglStream);
 .
 .
 //clean up
 cudaEGLStreamProducerDisconnect(&amp;conn);

 .
}
</pre>
 <p>
  A frame is represented as a
  <span class="pre">
   cudaEglFramestructure
  </span>
  . The
  <span class="pre">
   frameType
  </span>
  parameter in
  <span class="pre">
   cudaEglFrame
  </span>
  indicates the memory layout of the frame. The supported memory layouts are CUDA Array and device pointer. Any mismatch in the width and height values of frame with the values specified in
  <span class="pre">
   cudaEGLStreamProducerConnect()
  </span>
  leads to undefined behavior. In the sample, the CUDA producer is sending a single frame, but it can send multiple frames over a loop. CUDA cannot present more than 64 active frames to EGLStream.
 </p>
 <p>
  The
  <span class="pre">
   cudaEGLStreamProducerReturnFrame()
  </span>
  call waits until it receives the released frame from the consumer. Once the CUDA producer presents the first frame to EGLstream, at least one frame is always available for consumer acquisition until the producer disconnects. This prevents the removal of the last frame from EGLStream, which would block
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__EGL.html#group__CUDA__EGL_1g70c84d9d01f343fc07cd632f9cfc3a06">
   cudaEGLStreamProducerReturnFrame
  </a>
  ().
 </p>
 <p>
  Use the
  <span class="pre">
   EGL_NV_stream_reset
  </span>
  extension to set EGLStream attribute
  <span class="pre">
   EGL_SUPPORT_REUSE_NV
  </span>
  to false to allow the last frame to be removed from EGLStream. This allows removing or returning the last frame from EGLStream.
 </p>
 <h3>
  <span class="section-number">
   5.1.3.
  </span>
  CUDA as Consumer
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-as-consumer" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  When CUDA is the consumer, the supported producers are CUDA, OpenGL, NvMedia, Argus, and Camera. API functions to be used when CUDA is the consumer are listed in Table 3. Except for connecting and disconnecting from EGLStream, all API calls are non-blocking.
 </p>
 <p>
  The following consumer side steps are shown in the sample code that follows:
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    Connect consumer to EGLStream (line 5).
   </p>
  </li>
  <li>
   <p>
    Acquire frame from EGLStream (lines 8-10).
   </p>
  </li>
  <li>
   <p>
    Process the frame on consumer (line 16).
   </p>
  </li>
  <li>
   <p>
    Release frame back to EGLStream (line 19).
   </p>
  </li>
  <li>
   <p>
    Disconnect the consumer after completion of the task (line 22).
   </p>
  </li>
 </ol>
 <pre>void ConsumerThread(EGLStreamKHR eglStream) {
.
.
//Connect consumer to EGLStream
cudaEGLStreamConsumerConnect(&amp;conn, eglStream);
// consumer acquires a frame
unsigned int timeout = 16000;
cudaEGLStreamConsumerAcquireFrame(&amp; conn, &amp;cudaResource, eglStream, timeout);
//consumer gets a cuda object pointer
cudaGraphicsResourceGetMappedEglFrame(&amp;cudaEgl, cudaResource, 0, 0);
size_t numElem = cudaEgl-&gt;planeDesc[0].pitch * cudaEgl-&gt;planeDesc[0].height;
.
.
int checkIfOne = 1;
// Checks if each value in the buffer is 1, if any value is not 1, it sets checkIfOne = 0.
K2&lt;&lt;&lt;...&gt;&gt;&gt;(cudaEgl-&gt;pPitch[0].ptr, 1, numElem, checkIfOne);
.
.
cudaEGLStreamConsumerReleaseFrame(&amp;conn, cudaResource, &amp;eglStream);
.
.
cudaEGLStreamConsumerDisconnect(&amp;conn);
.
}
</pre>
 <p>
  In the sample code, the CUDA consumer receives a single frame, but it can also receive multiple frames over a loop. If a CUDA consumer fails to receive a new frame in the specified time limit using
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EGL.html#group__CUDART__EGL_1g83dd1bfea48c093d3f0b247754970f58">
   cudaEGLStreamConsumerAcquireFrame()
  </a>
  , it reacquires the previous frame from EGLStream. The time limit is indicated by the timeout parameter.
 </p>
 <p>
  The application can use
  <span class="pre">
   eglQueryStreamKHR()
  </span>
  to query for the availability of new frames using. If the consumer uses already released frames, it results in undefined behavior. The consumer behavior is defined only for read operations. Behavior is undefined when the consumer writes to a frame.
 </p>
 <p>
  If the CUDA context is destroyed while connected to EGLStream, the stream is placed in the
  <span class="pre">
   EGL_STREAM_STATE_DISCONNECTED_KHR
  </span>
  state and the connection handle is invalidated.
 </p>
 <h3>
  <span class="section-number">
   5.1.4.
  </span>
  Implicit Synchronization
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#implicit-synchronization" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  EGLStream provides implicit synchronization in an application. For example, in the previous code samples, both the producer and consumer threads are running in parallel and the K1 and K2 kernel processes access the same frame, but K2 execution in the consumer thread is guaranteed to occur only after kernel K1 in the producer thread finishes. The
  <span class="pre">
   cudaEGLStreamConsumerAcquireFrame()
  </span>
  function waits on the GPU side until K1 finishes and ensures synchronization between producer and consumer. The variable
  <span class="pre">
   checkIfOne
  </span>
  is never set to 0 inside the K2 kernel in the consumer thread.
 </p>
 <p>
  Similarly,
  <span class="pre">
   cudaEGLStreamProducerReturnFrame()
  </span>
  in the producer thread is guaranteed to get the frame only after K2 finishes and the consumer releases the frame. These non-blocking calls allow the CPU to do other computation in between, as synchronization is taken care of on the GPU side.
 </p>
 <p>
  The
  <span class="pre">
   EGLStreams_CUDA_Interop
  </span>
  CUDA sample code shows the usage of EGLStream in detail.
 </p>
 <h3>
  <span class="section-number">
   5.1.5.
  </span>
  Data Transfer Between Producer and Consumer
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#data-transfer-between-producer-and-consumer" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Data transfer between producer and consumer is avoided when they are present on the same device. In a TegraÂ® platform that includes a dGPU however, such as is in NVIDIA DRIVEâ¢ PX 2, the producer and consumer can be present on different devices. In that case, an additional memory copy is required internally to move the frame between TegraÂ® SoC DRAM and dGPU DRAM. EGLStream allows producer and consumer to run on any GPU without code modification.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  On systems where a TegraÂ® device is connected to a dGPU, if a producer frame uses CUDA array, both producer and consumer should be on the same GPU. But if a producer frame uses CUDA device pointers, the consumer can be present on any GPU.
 </p>
 <h3>
  <span class="section-number">
   5.1.6.
  </span>
  EGLStream Pipeline
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglstream-pipeline" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  An application can use multiple EGL streams in a pipeline to pass the frames from one API to another. For an application where NvMedia sends a frame to CUDA for computation, CUDA sends the same frame to OpenGL for rendering after the computation.
 </p>
 <p>
  The EGLStream pipeline is illustrated in
  <a class="reference external" href="https://docs.nvidia.com/cuda/index.html#eglstream-pipeline__fig-eglstream-pipeline">
   Figure 3
  </a>
  .
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/_images/eglstream-pipeline.png">
 </a>
 <p>
  <span class="caption-text">
   EGLStream Pipeline
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglstream-pipeline-fig-eglstream-pipeline" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  NvMedia and CUDA connect as producer and consumer respectively to one EGLStream. CUDA and OpenGL connect as producer and consumer respectively to another EGLStream.
 </p>
 <p>
  Using multiple EGLStreams in pipeline fashion gives the flexibility to send frames across multiple APIs without allocating additional memory or requiring explicit data transfers. Sending a frame across the above EGLStream pipeline involves the following steps.
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    NvMedia sends a frame to CUDA for processing.
   </p>
  </li>
  <li>
   <p>
    CUDA uses the frame for computation and sends to OpenGL for rendering.
   </p>
  </li>
  <li>
   <p>
    OpenGL consumes the frame and releases it back to CUDA.
   </p>
  </li>
  <li>
   <p>
    CUDA releases the frame back to NvMedia.
   </p>
  </li>
 </ol>
 <p>
  The above steps can be performed in a loop to facilitate the transfer of multiple frames in the EGLStream pipeline.
 </p>
 <h2>
  <span class="section-number">
   5.2.
  </span>
  EGLImage
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglimage" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  An EGLImage interop allows an EGL client API to share image data with other EGL client APIs. For example, an application can use an EGLImage interop to share an OpenGL texture with CUDA without allocating any additional memory. A single EGLImage object can be shared across multiple client APIs for modification.
 </p>
 <p>
  An EGLImage interop does not provide implicit synchronization. Applications must maintain synchronization to avoid race conditions.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  An EGLImage is created using
  <span class="pre">
   eglCreateImageKHR()
  </span>
  and destroyed using
  <span class="pre">
   eglDestroyImageKHR()
  </span>
  .
 </p>
 <p>
  For more information see the EGLImage specification at the following web site:
 </p>
 <p>
  <a class="reference external" href="https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_image_base.txt">
   https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_image_base.txt
  </a>
 </p>
 <h3>
  <span class="section-number">
   5.2.1.
  </span>
  CUDA interop with EGLImage
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-interop-with-eglimage" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  CUDA supports interoperation with EGLImage, allowing CUDA to read or modify the data of an EGLImage. An EGLImage can be a single or multi-planar resource. In CUDA, a single-planar EGLImage object is represented as a CUDA array or device pointer. Similarly, a multi-planar EGLImage object is represented as an array of device pointers or CUDA arrays. EGLImage is supported on TegraÂ® devices running the Linux, QNX, or Android operating systems.
 </p>
 <p>
  Use the
  <span class="pre">
   cudaGraphicsEGLRegisterImage()
  </span>
  API to register an EGLImage object with CUDA. Registering an EGLImage with CUDA creates a graphics resource object. An application can use
  <span class="pre">
   cudaGraphicsResourceGetMappedEglFrame()
  </span>
  to get a frame from the graphics resource object. In CUDA, a frame is represented as a
  <span class="pre">
   cudaEglFrame
  </span>
  structure. The
  <span class="pre">
   frameType
  </span>
  parameter in cudaEglFrame indicates if the frame is a CUDA device pointer or a CUDA array. For a single planar graphics resource, an application can directly obtain a device pointer or CUDA array using
  <span class="pre">
   cudaGraphicsResourceGetMappedPointer()
  </span>
  or
  <span class="pre">
   cudaGraphicsSubResourceGetMappedArray()
  </span>
  respectively. A CUDA array can be bound to a texture or surface reference to access inside a kernel. Also, a multi-dimensional CUDA array can be read and written via
  <span class="pre">
   cudaMemcpy3D()
  </span>
  .
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  An EGLImage cannot be created from a CUDA object. The
  <span class="pre">
   cudaGraphicsEGLRegisterImage()
  </span>
  function is only supported on TegraÂ® devices. Also,
  <span class="pre">
   cudaGraphicsEGLRegisterImage()
  </span>
  expects only the â0â flag as other API flags are for future use.
 </p>
 <p>
  The following sample code shows EGLImage interoperability. In the code, an EGLImage object
  <span class="pre">
   eglImage
  </span>
  is created using OpenGL texture. The
  <span class="pre">
   eglImage
  </span>
  object is mapped as a CUDA array
  <span class="pre">
   pArray
  </span>
  in CUDA. The
  <span class="pre">
   pArray
  </span>
  array is bound to a surface object to allow modification of the OpenGL texture in the changeTexture. The function
  <span class="pre">
   checkBuf()
  </span>
  checks if the texture is updated with new values.
 </p>
 <pre>int width = 256;
int height = 256;
int main()
{
 .
 .
 unsigned char *hostSurf;
 unsigned char *pSurf;
 CUarray pArray;
 unsigned int bufferSize = WIDTH * HEIGHT * 4;
 pSurf= (unsigned char *)malloc(bufferSize); hostSurf = (unsigned char *)malloc(bufferSize);
 // Initialize the buffer
 for(int y = 0; y &lt; HEIGHT; y++)
 {
    for(int x = 0; x &lt; WIDTH; x++)
    {
    pSurf[(y*WIDTH + x) * 4 ] = 0; pSurf[(y*WIDTH + x) * 4 + 1] = 0;
    pSurf[(y*WIDTH + x) * 4 + 2] = 0; pSurf[(y*WIDTH + x) * 4 + 3] = 0;
    }
 }

 // NOP call to error-check the above glut calls
 GL_SAFE_CALL({});

 //Init texture
 GL_SAFE_CALL(glGenTextures(1, &amp;tex));
 GL_SAFE_CALL(glBindTexture(GL_TEXTURE_2D, tex));
 GL_SAFE_CALL(glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, WIDTH, HEIGHT, 0, GL_RGBA, GL_UNSIGNED_BYTE, pSurf));

 EGLDisplay eglDisplayHandle = eglGetCurrentDisplay();
 EGLContext eglCtx = eglGetCurrentContext();

 // Create the EGL_Image
 EGLint eglImgAttrs[] = { EGL_IMAGE_PRESERVED_KHR, EGL_FALSE, EGL_NONE, EGL_NONE };
 EGLImageKHR eglImage = eglCreateImageKHR(eglDisplayHandle, eglCtx, EGL_GL_TEXTURE_2D_KHR, (EGLClientBuffer)(intptr_t)tex, eglImgAttrs);
 glFinish();
 glTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, WIDTH, HEIGHT, GL_RGBA, GL_UNSIGNED_BYTE, pSurf);
 glFinish();

 // Register buffer with CUDA
cuGraphicsEGLRegisterImage(&amp;pResource, eglImage,0);

 //Get CUDA array from graphics resource object
 cuGraphicsSubResourceGetMappedArray( &amp;pArray, pResource, 0, 0);

 cuCtxSynchronize();

 //Create a CUDA surface object from pArray
 CUresult status = CUDA_SUCCESS;
 CUDA_RESOURCE_DESC wdsc;
 memset(&amp;wdsc, 0, sizeof(wdsc));
 wdsc.resType = CU_RESOURCE_TYPE_ARRAY; wdsc.res.array.hArray = pArray;
 CUsurfObject writeSurface;
 cuSurfObjectCreate(&amp;writeSurface, &amp;wdsc);

 dim3 blockSize(32,32);
 dim3 gridSize(width/blockSize.x,height/blockSize.y);
 // Modifies the OpenGL texture using CUDA surface object
 changeTexture&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(writeSurface, width, height);
 cuCtxSynchronize();

 CUDA_MEMCPY3D cpdesc;
 memset(&amp;cpdesc, 0, sizeof(cpdesc));
 cpdesc.srcXInBytes = cpdesc.srcY = cpdesc.srcZ = cpdesc.srcLOD = 0;
 cpdesc.dstXInBytes = cpdesc.dstY = cpdesc.dstZ = cpdesc.dstLOD = 0;
 cpdesc.srcMemoryType = CU_MEMORYTYPE_ARRAY; cpdesc.dstMemoryType = CU_MEMORYTYPE_HOST;
 cpdesc.srcArray = pArray; cpdesc.dstHost = (void *)hostSurf;
 cpdesc.WidthInBytes = WIDTH * 4; cpdesc.Height = HEIGHT; cpdesc.Depth = 1;

 //Copy CUDA surface object values to hostSurf
 cuMemcpy3D(&amp;cpdesc);

 cuCtxSynchronize();

 unsigned char* temp = (unsigned char*)(malloc(bufferSize * sizeof(unsigned char)));
 // Get the modified texture values as
 GL_SAFE_CALL(glGetTexImage(GL_TEXTURE_2D, 0, GL_RGBA, GL_UNSIGNED_BYTE,(void*)temp));
 glFinish();
 // Check if the OpenGL texture got modified values
 checkbuf(temp,hostSurf);

 // Clean up CUDA
 cuGraphicsUnregisterResource(pResource);
 cuSurfObjectDestroy(writeSurface);
 .
 .
}
__global__ void changeTexture(cudaSurfaceObject_t arr, unsigned int width, unsigned int height){
 unsigned int x = threadIdx.x + blockIdx.x * blockDim.x;
 unsigned int y = threadIdx.y + blockIdx.y * blockDim.y;
 uchar4 data = make_uchar4(1, 2, 3, 4);
 surf2Dwrite(data, arr, x * 4, y);
}
void checkbuf(unsigned char *ref, unsigned char *hostSurf) {
 for(int y = 0; y &lt; height*width*4; y++){
 if (ref[y] != hostSurf[y])
 printf("mis match at %d\n",y);
 }
}
</pre>
 <p>
  Because EGLImage does not provide implicit synchronization, the above sample application uses
  <span class="pre">
   glFinish()
  </span>
  and
  <span class="pre">
   cudaThreadSynchronize()
  </span>
  calls to achieve synchronization. Both calls block the CPU thread. To avoid blocking the CPU thread, use EGLSync to provide synchronization. An example using EGLImage and EGLSync is shown in the following section.
 </p>
 <h2>
  <span class="section-number">
   5.3.
  </span>
  EGLSync
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#eglsync" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  EGLSync is a cross-API synchronization primitive. It allows an EGL client API to share its synchronization object with other EGL client APIs. For example, applications can use an EGLSync interop to share the OpenGL synchronization object with CUDA.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  An EGLSync object is created using
  <span class="pre">
   eglCreateSyncKHR()
  </span>
  and destroyed using
  <span class="pre">
   eglDestroySyncKHR()
  </span>
  .
 </p>
 <p>
  For more information see the EGLSync specification at the following web site:
 </p>
 <p>
  <a class="reference external" href="https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_fence_sync.txt">
   https://www.khronos.org/registry/EGL/extensions/KHR/EGL_KHR_fence_sync.txt
  </a>
 </p>
 <h3>
  <span class="section-number">
   5.3.1.
  </span>
  CUDA Interop with EGLSync
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-interop-with-eglsync" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  In an imaging application, where two clients run on a GPU and share a resource, the absence of a cross-API GPU synchronization object forces the clients to use CPU-side synchronization to avoid race conditions. The CUDA interop with EGLSync allows the application to exchange synchronization objects between CUDA and other client APIs directly. This avoids the need for CPU-side synchronization and allows CPU to complete other tasks. In CUDA, an EGLSync object is mapped as a CUDA event.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Currently CUDA interop with EGLSync is supported only on TegraÂ® devices.
 </p>
 <h3>
  <span class="section-number">
   5.3.2.
  </span>
  Creating EGLSync from a CUDA Event
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#creating-eglsync-from-a-cuda-event" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Creating an EGLSync object from a CUDA event is shown in the following sample code. Note that EGLSync object creation from a CUDA event should happen immediately after the CUDA event is recorded.
 </p>
 <pre><span class="n">EGLDisplay</span><span class="n">dpy</span><span class="o">=</span><span class="n">eglGetCurrentDisplay</span><span class="p">();</span>
<span class="c1">// Create CUDA event</span>
<span class="n">cudaEvent_t</span><span class="n">event</span><span class="p">;</span>
<span class="n">cudaStream_t</span><span class="o">*</span><span class="n">stream</span><span class="p">;</span>
<span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">event</span><span class="p">);</span>
<span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">);</span>
<span class="c1">// Record the event with cuda event</span>
<span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">event</span><span class="p">,</span><span class="n">stream</span><span class="p">);</span>
<span class="k">const</span><span class="n">EGLAttrib</span><span class="n">attribs</span><span class="p">[]</span><span class="o">=</span><span class="p">{</span>
<span class="n">EGL_CUDA_EVENT_HANDLE_NV</span><span class="p">,</span><span class="p">(</span><span class="n">EGLAttrib</span><span class="p">)</span><span class="n">event</span><span class="p">,</span>
<span class="n">EGL_NONE</span>
<span class="p">};</span>
<span class="c1">//Create EGLSync from the cuda event</span>
<span class="n">eglsync</span><span class="o">=</span><span class="n">eglCreateSync</span><span class="p">(</span><span class="n">dpy</span><span class="p">,</span><span class="n">EGL_NV_CUDA_EVENT_NV</span><span class="p">,</span><span class="n">attribs</span><span class="p">);</span>
<span class="c1">//Wait on the sync</span>
<span class="n">eglWaitSyncKHR</span><span class="p">(...);</span>
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Initialize a CUDA event before creating an EGLSync object from it to avoid undefined behavior.
 </p>
 <h3>
  <span class="section-number">
   5.3.3.
  </span>
  Creating a CUDA Event from EGLSync
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#creating-a-cuda-event-from-eglsync" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Creating a CUDA event from an EGLSync object is shown in the following sample code.
 </p>
 <pre><span class="n">EGLSync</span><span class="n">eglsync</span><span class="p">;</span>
<span class="n">EGLDisplay</span><span class="n">dpy</span><span class="o">=</span><span class="n">eglGetCurrentDisplay</span><span class="p">();</span>
<span class="c1">// Create an eglSync object from openGL fense sync object</span>
<span class="n">eglsync</span><span class="o">=</span><span class="n">eglCreateSyncKHR</span><span class="p">(</span><span class="n">dpy</span><span class="p">,</span><span class="n">EGL_SYNC_FENCE_KHR</span><span class="p">,</span><span class="nb">NULL</span><span class="p">);</span>
<span class="n">cudaEvent_t</span><span class="n">event</span><span class="p">;</span>
<span class="n">cudaStream_t</span><span class="o">*</span><span class="n">stream</span><span class="p">;</span>
<span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">);</span>
<span class="c1">// Create CUDA event from eglSync</span>
<span class="n">cudaEventCreateFromEGLSync</span><span class="p">(</span><span class="o">&amp;</span><span class="n">event</span><span class="p">,</span><span class="n">eglSync</span><span class="p">,</span><span class="n">cudaEventDefault</span><span class="p">);</span>
<span class="c1">// Wait on the cuda event. It waits on GPU till OpenGL finishes its</span>
<span class="c1">// task</span>
<span class="n">cudaStreamWaitEvent</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span><span class="n">event</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The
  <span class="pre">
   cudaEventRecord()
  </span>
  and
  <span class="pre">
   cudaEventElapsedTime()
  </span>
  functions are not supported for events created from an EGLSync object.
 </p>
 <p>
  The same example given in the EGLImage section is re-written below to illustrate the usage of an EGLSync interop. In the sample code, the CPU blocking calls such as
  <span class="pre">
   glFinish()
  </span>
  and
  <span class="pre">
   cudaThreadSynchronize()
  </span>
  are replaced with EGLSync interop calls.
 </p>
 <pre>int width = 256;
int height = 256;
int main()
{
 .
 .
 unsigned char *hostSurf;
 unsigned char *pSurf;
 cudaArray_t pArray;
 unsigned int bufferSize = WIDTH * HEIGHT * 4;
 pSurf= (unsigned char *)malloc(bufferSize); hostSurf = (unsigned char *)malloc(bufferSize);
 // Intialize the buffer
 for(int y = 0; y &lt; bufferSize; y++)
 pSurf[y] = 0;

 //Init texture
 GL_SAFE_CALL(glGenTextures(1, &amp;tex));
 GL_SAFE_CALL(glBindTexture(GL_TEXTURE_2D, tex));
 GL_SAFE_CALL(glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA, WIDTH, HEIGHT, 0, GL_RGBA, GL_UNSIGNED_BYTE, pSurf));
 EGLDisplay eglDisplayHandle = eglGetCurrentDisplay();
 EGLContext eglCtx = eglGetCurrentContext();

 cudaEvent_t cuda_event;
 cudaEventCreateWithFlags(cuda_event, cudaEventDisableTiming);
 EGLAttribKHR eglattrib[] = { EGL_CUDA_EVENT_HANDLE_NV, (EGLAttrib) cuda_event, EGL_NONE};
 cudaStream_t* stream;
 cudaStreamCreateWithFlags(&amp;stream,cudaStreamDefault);

 EGLSyncKHR eglsync1, eglsync2;
 cudaEvent_t egl_event;

 // Create the EGL_Image
 EGLint eglImgAttrs[] = { EGL_IMAGE_PRESERVED_KHR, EGL_FALSE, EGL_NONE, EGL_NONE };
 EGLImageKHR eglImage = eglCreateImageKHR(eglDisplayHandle, eglCtx, EGL_GL_TEXTURE_2D_KHR, (EGLClientBuffer)(intptr_t)tex, eglImgAttrs);

 glTexSubImage2D(GL_TEXTURE_2D, 0, 0, 0, WIDTH, HEIGHT, GL_RGBA, GL_UNSIGNED_BYTE, pSurf);
 //Creates an EGLSync object from GL Sync object to track
 //finishing of copy.
 eglsync1 = eglCreateSyncKHR(eglDisplayHandle, EGL_SYNC_FENCE_KHR, NULL);

 //Create CUDA event object from EGLSync obejct
 cuEventCreateFromEGLSync(&amp;egl_event, eglsync1, cudaEventDefault);

 //Waiting on GPU to finish GL copy
 cuStreamWaitEvent(stream, egl_event, 0);

 // Register buffer with CUDA
 cudaGraphicsEGLRegisterImage(&amp;pResource, eglImage, cudaGraphicsRegisterFlagsNone);
 //Get CUDA array from graphics resource object
 cudaGraphicsSubResourceGetMappedArray( &amp;pArray, pResource, 0, 0);
 .
 .
 //Create a CUDA surface object from pArray
 struct cudaResourceDesc resDesc;
 memset(&amp;resDesc, 0, sizeof(resDesc));
 resDesc.resType = cudaResourceTypeArray; resDesc.res.array.array = pArray;
 cudaSurfaceObject_t inputSurfObj = 0;
 cudaCreateSurfaceObject(&amp;inputSurfObj, &amp;resDesc);

 dim3 blockSize(32,32);
 dim3 gridSize(width/blockSize.x,height/blockSize.y);
 // Modifies the CUDA array using CUDA surface object
 changeTexture&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(inputSurfObj, width, height);
 cuEventRecord(cuda_event, stream);
 //Create EGLsync object from CUDA event cuda_event
 eglsync2 = eglCreateSync64KHR(dpy, EGL_SYNC_CUDA_EVENT_NV, eglattrib);
 //waits till kernel to finish
 eglWaitSyncKHR(eglDisplayHandle, eglsync2, 0);
 .
 //Copy modified pArray values to hostSurf
 .
 unsigned char* temp = (unsigned char*)(malloc(bufferSize * sizeof(unsigned char)));
 // Get the modified texture values
 GL_SAFE_CALL(glGetTexImage(GL_TEXTURE_2D, 0, GL_RGBA, GL_UNSIGNED_BYTE,(void*)temp));
 .
 .
 // This function check if the OpenGL texture got modified values
 checkbuf(temp,hostSurf);

 // Clean up CUDA
 cudaGraphicsUnregisterResource(pResource);
 cudaDestroySurfaceObject(inputSurfObj);
 eglDestroySyncKHR(eglDisplayHandle, eglsync1);
 eglDestroySyncKHR(eglDisplayHandle, eglsync2);
 cudaEventDestroy(egl_event);
 cudaEventDestroy(cuda_event);
 .
 .
}
</pre>
 <h1>
  <span class="section-number">
   6.
  </span>
  CUDA Upgradable Package for Jetson
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cuda-upgradable-package-for-jetson" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  CUDA introduced an upgrade path starting with JetPack SDK 5.0 which provides an option to update the CUDA driver and the CUDA toolkit to the latest version.
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/_images/upgradable-package-for-Jetson.jpg">
 </a>
 <h2>
  <span class="section-number">
   6.1.
  </span>
  Installing the CUDA Upgrade Package
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#installing-the-cuda-upgrade-package" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <h3>
  <span class="section-number">
   6.1.1.
  </span>
  Prerequisite
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#prerequisite" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The Jetson device must be installed with a compatible NVIDIA JetPack version. Refer to
  <a class="reference external" href="https://docs.nvidia.com/cuda/index.html#use-the-right-upgrade-package">
   Use the Right Upgrade Package
  </a>
  for more info.
 </p>
 <h3>
  <span class="section-number">
   6.1.2.
  </span>
  From Network Repositories or Local Installers
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#from-network-repositories-or-local-installers" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The
  <a class="reference external" href="https://developer.nvidia.com/cuda-downloads">
   CUDA Downloads
  </a>
  page provides step-by-step instructions on how to download and use the local installer or CUDA network repositories to install the latest Toolkit. The CUDA upgrade package gets downloaded and installed along with the corresponding CUDA toolkit for Linux-aarch64-jetson devices.
 </p>
 <p>
  For use cases where applications are built on the host and require just the CUDA upgrade package to be installed independently on the target, the corresponding Debians can be found in
  <a class="reference external" href="https://developer.download.nvidia.com/compute/cuda/repos/">
   CUDA Repos
  </a>
  . Taking 11.8 for example, this can be installed by running the command:
 </p>
 <pre>$ sudo apt-get install -y cuda-compat-11-8
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  This is the recommended path for CUDA upgrade for devices that have disk space (secondary storage) limitations.
 </p>
 <p>
  The installed upgrade package is available in the versioned toolkit location. For example, for 11.8 it is located in
  <span class="pre">
   /usr/local/cuda-11.8/
  </span>
  .
 </p>
 <p>
  The upgrade package consists of the following files:
 </p>
 <ul class="simple">
  <li>
   <p>
    <span class="pre">
     libcuda.so.*
    </span>
    - the CUDA Driver
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     libnvidia-nvvm.so.*
    </span>
    - Just In Time - Link Time Optimization (CUDA 11.8 and later only)
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     libnvidia-ptxjitcompiler.so.*
    </span>
    - the JIT (just-in-time) compiler for PTX files
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     nvidia-cuda-mps-control
    </span>
    - CUDA MPS control executable
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     nvidia-cuda-mps-server
    </span>
    - CUDA MPS server executable
   </p>
  </li>
 </ul>
 <p>
  These files together implement the CUDA 11.8 driver interfaces.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  This package only provides the files, and does not configure the system.
 </p>
 <p>
  Example
 </p>
 <p>
  The following commands show how the CUDA Upgrade package can be installed and used to run the applications.
 </p>
 <pre>$ sudo apt-get -y install cuda
Reading package lists...
Building dependency tree...
Reading state information...
The following additional packages will be installed:
  cuda-11-8 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compat-11-8
  ...&lt;snip&gt;...
The following NEW packages will be installed:
  cuda cuda-11-8 cuda-cccl-11-8 cuda-command-line-tools-11-8 cuda-compat-11-8
  ...&lt;snip&gt;...
0 upgraded, 48 newly installed, 0 to remove and 38 not upgraded.
Need to get 15.7 MB/1,294 MB of archives.
After this operation, 4,375 MB of additional disk space will be used.
Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/arm64  cuda-compat-11-8 11.8.31339915-1 [15.8 MB]
Fetched 15.7 MB in 12s (1,338 kB/s)
Selecting previously unselected package cuda-compat-11-8.
(Reading database ...
  ...&lt;snip&gt;...
(Reading database ... 100%
(Reading database ... 148682 files and directories currently installed.)
Preparing to unpack .../00-cuda-compat-11-8_11.8.30682616-1_arm64.deb ...
Unpacking cuda-compat-11-8 (11.8.30682616-1) ...
  ...&lt;snip&gt;...
Unpacking cuda-11-8 (11.8.0-1) ...
Selecting previously unselected package cuda.
Preparing to unpack .../47-cuda_11.8.0-1_arm64.deb ...
Unpacking cuda (11.8.0-1) ...
Setting up cuda-toolkit-config-common (11.8.56-1) ...
Setting up cuda-nvml-dev-11-8 (11.8.56-1) ...
Setting up cuda-compat-11-8 (11.8.30682616-1) ...
  ...&lt;snip&gt;...


$ ls -l /usr/local/cuda-11.8/compat
total 55300
lrwxrwxrwx 1 root root       12 Jan  6 19:14 libcuda.so -&gt; libcuda.so.1
lrwxrwxrwx 1 root root       14 Jan  6 19:14 libcuda.so.1 -&gt; libcuda.so.1.1
-rw-r--r-- 1 root root 21702832 Jan  6 19:14 libcuda.so.1.1
lrwxrwxrwx 1 root root       19 Jan  6 19:14 libnvidia-nvvm.so -&gt; libnvidia-nvvm.so.4
lrwxrwxrwx 1 root root       23 Jan  6 19:14 libnvidia-nvvm.so.4 -&gt; libnvidia-nvvm.so.4.0.0
-rw-r--r-- 1 root root 24255256 Jan  6 19:14 libnvidia-nvvm.so.4.0.0
-rw-r--r-- 1 root root 10665608 Jan  6 19:14 libnvidia-ptxjitcompiler.so
lrwxrwxrwx 1 root root       27 Jan  6 19:14 libnvidia-ptxjitcompiler.so.1 -&gt; libnvidia-ptxjitcompiler.so

$ export PATH=/usr/local/cuda-11.8/bin:$PATH
$ export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
</pre>
 <p>
  The user can set
  <span class="pre">
   LD_LIBRARY_PATH
  </span>
  to include the libraries installed by the upgrade package before running the CUDA 11.8 application:
 </p>
 <pre>$ LD_LIBRARY_PATH=/usr/local/cuda-11.8/compat:$LD_LIBRARY_PATH ~/Samples/1_Utilities/deviceQuery

CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 1 CUDA Capable device(s)

Device 0: "Orin"
  CUDA Driver Version / Runtime Version          11.8 / 11.8
  CUDA Capability Major/Minor version number:    8.7
      ...&lt;snip&gt;...
deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 11.8, CUDA Runtime Version = 11.8, NumDevs = 1
Result = PASS
</pre>
 <p>
  Only a single CUDA upgrade package can be installed at any point in time on a given system. While installing a new CUDA upgrade package, the previous version of the installed upgrade package will be removed and replaced with the new one. The default drivers (originally installed with the NVIDIA JetPack and part of the L4T BSP) will be retained by the installer. The application has the ability to use either the default version of CUDA (originally installed with NVIDIA JetPack) or the one installed by the upgrade package. The
  <span class="pre">
   LD_LIBRARY_PATH
  </span>
  variable can be used to choose the required version.
 </p>
 <p>
  In addition to
  <span class="pre">
   LD_LIBRARY_PATH
  </span>
  , CUDA MPS users must set the
  <span class="pre">
   PATH
  </span>
  variable in order to use the
  <span class="pre">
   nvidia-cuda-mps-*
  </span>
  executables installed by the upgrade package before starting MPS and running the CUDA applications that use MPS. The MPS executables installed with the upgrade package are only compatible with the CUDA driver installed with the same upgrade package, and vice versa, which can be checked with the version info.
 </p>
 <p>
  Installation of the upgrade package will fail if it is not compatible with the NVIDIA JetPack version.
 </p>
 <h2>
  <span class="section-number">
   6.2.
  </span>
  Deployment Considerations for CUDA Upgrade Package
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#deployment-considerations-for-cuda-upgrade-package" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <h3>
  <span class="section-number">
   6.2.1.
  </span>
  Use the Right Upgrade Package
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#use-the-right-upgrade-package" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The CUDA upgrade package is named after the highest toolkit that it can support. For example, if you are on the NVIDIA JetPack SDK 5.0 (11.4) driver but require 11.8 application support, install the CUDA upgrade package for 11.8.
 </p>
 <p>
  Each CUDA release will support upgrades only for a specific set of NVIDIA JetPack releases. The table below shows the NVIDIA JetPack SDK version supported by each CUDA release.
 </p>
 <table class="table-no-stripes docutils align-default">
  <tr class="row-odd">
   <th class="head">
    <p>
     JetPack SDK
    </p>
   </th>
   <th class="head">
    <p>
     CUDA 11.4
    </p>
   </th>
   <th class="head">
    <p>
     CUDA 11.8
    </p>
   </th>
   <th class="head">
    <p>
     CUDA 12.0
    </p>
   </th>
   <th class="head">
    <p>
     CUDA 12.1
    </p>
   </th>
   <th class="head">
    <p>
     CUDA 12.2
    </p>
   </th>
   <th class="head">
    <p>
     CUDA 12.3 onwards
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     5.x
    </p>
   </td>
   <td>
    <p>
     default
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
   <td>
    <p>
     X
    </p>
   </td>
  </tr>
 </table>
 <table class="table-no-stripes docutils align-default">
  <tr class="row-odd">
   <th class="head">
    <p>
     JetPack SDK
    </p>
   </th>
   <th class="head">
    <p>
     CUDA 12.2
    </p>
   </th>
   <th class="head">
    <p>
     CUDA 12.3
    </p>
   </th>
   <th class="head">
    <p>
     CUDA 12.4
    </p>
   </th>
   <th class="head">
    <p>
     CUDA 12.5
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     6.x
    </p>
   </td>
   <td>
    <p>
     default
    </p>
   </td>
   <td>
    <p>
     X
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
  </tr>
 </table>
 <p>
  The following table shows the CUDA UMD and CUDA Toolkit version compatibility on NVIDIA JetPack 5.x release:
 </p>
 <table class="table-no-stripes docutils align-default">
  <tr class="row-odd">
   <td rowspan="2">
    <p>
     CUDA UMD
    </p>
   </td>
   <td colspan="5">
    <p>
     CUDA Toolkit
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     11.4 (default - part of NVIDIA JetPack)
    </p>
   </td>
   <td>
    <p>
     11.8
    </p>
   </td>
   <td>
    <p>
     12.0
    </p>
   </td>
   <td>
    <p>
     12.1
    </p>
   </td>
   <td>
    <p>
     12.2
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     11.4 (default â part of NVIDIA JetPack)
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-across-minor-releases">
      Minor Version Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     X
    </p>
   </td>
   <td>
    <p>
     X
    </p>
   </td>
   <td>
    <p>
     X
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     11.8 (through Upgrade Package)
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
   <td>
    <p>
     X
    </p>
   </td>
   <td>
    <p>
     X
    </p>
   </td>
   <td>
    <p>
     X
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     12.0 (through Upgrade Package)
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-across-minor-releases">
      Minor Version Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-across-minor-releases">
      Minor Version Compatibility
     </a>
     )
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     12.1 (through Upgrade Package)
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-across-minor-releases">
      Minor Version Compatibility
     </a>
     )
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     12.2 (through Upgrade Package)
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
  </tr>
 </table>
 <p>
  The following table shows the CUDA UMD and CUDA Toolkit version compatibility on NVIDIA JetPack 6.x release:
 </p>
 <table class="table-no-stripes docutils align-default">
  <tr class="row-odd">
   <td rowspan="2">
    <p>
     CUDA UMD
    </p>
   </td>
   <td colspan="3">
    <p>
     CUDA Toolkit
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     12.2 (default - part of NVIDIA JetPack)
    </p>
   </td>
   <td>
    <p>
     12.4
    </p>
   </td>
   <td>
    <p>
     12.5
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     12.2 (default â part of NVIDIA JetPack)
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-across-minor-releases">
      Minor Version Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-across-minor-releases">
      Minor Version Compatibility
     </a>
     )
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     12.4 (through Upgrade Package)
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-across-minor-releases">
      Minor Version Compatibility
     </a>
     )
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     12.5 (through Upgrade Package)
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C (
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      Binary Compatibility
     </a>
     )
    </p>
   </td>
   <td>
    <p>
     C
    </p>
   </td>
  </tr>
 </table>
 <p>
  C - Compatible
 </p>
 <p>
  X â Not compatible
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  CUDA upgrade packages on NVIDIA JetPack SDK 5.x are available from CUDA 11.8 onwards.
 </p>
 <h3>
  <span class="section-number">
   6.2.2.
  </span>
  Feature Exceptions
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#feature-exceptions" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  CUDA upgrade package only updates the CUDA driver interfaces while leaving the rest of the NVIDIA JetPack SDK components as is. If a new feature in the latest CUDA driver needs an updated NVIDIA JetPack SDK component/interface, it might not work and error out when used.
 </p>
 <h3>
  <span class="section-number">
   6.2.3.
  </span>
  Check for Compatibility Support
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#check-for-compatibility-support" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  In addition to the CUDA driver and certain compiler components, there are other drivers in the NVIDIA JetPack that remain at the default version. The CUDA upgrade path is for CUDA only.
 </p>
 <p>
  A well-written application should use following error codes to determine if CUDA Upgrade is supported. System administrators should be aware of these error codes to determine if there are errors in the deployment.
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    <span class="pre">
     CUDA_ERROR_SYSTEM_DRIVER_MISMATCH
    </span>
    <span class="pre">
     =
    </span>
    <span class="pre">
     803
    </span>
    . This error indicates that there is a mismatch between the versions of the upgraded CUDA driver and the already installed drivers on the system.
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     CUDA_ERROR_COMPAT_NOT_SUPPORTED_ON_DEVICE
    </span>
    <span class="pre">
     =
    </span>
    <span class="pre">
     804
    </span>
    . This error indicates that the system was updated to run with the CUDA upgrade package but the visible hardware detected by CUDA does not support this configuration.
   </p>
  </li>
 </ol>
 <h1>
  <span class="section-number">
   7.
  </span>
  cuDLA
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cudla" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  DLA (Deep Learning Accelerator) is a fixed function accelerator present on the NVIDIA Tegra SoC and is used for inference applications. The DLA HW has superior performance/W and can natively run many of the layers in modern neural networks, thus making it an attractive value proposition for embedded AI applications. Programming the DLA typically consists of an offline and online step: in the offline step, an input network is parsed and compiled by the DLA compiler into a loadable and in the online step, that loadable is executed by the DLA HW to generate an inference result. The SW stack that is currently provided by NVIDIA to perform the online or execution step consists of NvMediaDla and the DLA runtime/KMD. Together, these APIs enable the user to submit a DLA task to the DLA HW for inferencing purposes. The main functional paths are illustrated in the figure below.
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/_images/cudla-functional-paths.png">
 </a>
 <p>
  <span class="caption-text">
   DLA SW stack
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cudla-intro-fig-cudla" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  It follows from the model above that users wishing to use GPU and DLA together in an application would have to use interop mechanisms such as EGLStreams/NvSci to share buffers as well as synchronization primitives between the GPU and DLA. These interop mechanisms usually involve many steps for each buffer that is being shared and have limited ability to fine-tune the scheduling of tasks between the GPU and DLA. cuDLA is an extension of the CUDA programming model that integrates DLA (Deep Learning Accelerator) with CUDA thereby making it possible to submit DLA tasks using CUDA programming constructs such as streams and graphs. Managing shared buffers as well as synchronizing the tasks between GPU and DLA is transparently handled by cuDLA, freeing up the programmer to focus on the high-level usecase.
 </p>
 <h2>
  <span class="section-number">
   7.1.
  </span>
  Developer Guide
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#developer-guide" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This section describes the key principles involved in programming the DLA HW using cuDLA APIs. The cuDLA interfaces expose mechanisms to initialize devices, manage memory and submit DLA tasks. As such, this section discusses how the cuDLA APIs can be used for these usecases. The detailed specification of these APIs is described in the API specification and should be referred while writing a cuDLA application.
 </p>
 <p>
  Since cuDLA is an extension of CUDA, it is designed to work in conjunction with CUDA APIs that perform CUDA functions such as GPU management, context management etc. Therefore, the current state of the application in terms of which GPU is selected and the current active context (and its lifecycle) are all important considerations while evaluating the behavior of a cuDLA API.
 </p>
 <h3>
  <span class="section-number">
   7.1.1.
  </span>
  Device Model
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#device-model" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  To perform any DLA operation, it is necessary that an application first create a cuDLA device handle. The
  <span class="pre">
   cudlaCreateDevice()
  </span>
  API creates a logical instance of a cuDLA device wherein the selected DLA HW instance is coupled with the current active GPU selected via CUDA. For example, the following code snippet would create a logical instance consisting of the current GPU (set via
  <span class="pre">
   cudaSetDevice()
  </span>
  ) and DLA HW 0. Currently, cuDLA supports only iGPU on Tegra and an attempt to create a device handle by setting the current GPU as a dGPU would result in a device creation error during
  <span class="pre">
   cudlaCreateDevice()
  </span>
  .
 </p>
 <pre><span class="n">cudlaDevHandle</span><span class="n">devHandle</span><span class="p">;</span>
<span class="n">cudlaStatus</span><span class="n">ret</span><span class="p">;</span>
<span class="n">ret</span><span class="o">=</span><span class="n">cudlaCreateDevice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="o">&amp;</span><span class="n">devHandle</span><span class="p">,</span><span class="n">CUDLA_CUDA_DLA</span><span class="p">);</span>
</pre>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/_images/cuDLA-create-device.png">
 </a>
 <p>
  <span class="caption-text">
   Device model
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cudla-device-model-fig-cudla-create-device" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  The user can create any number of such logical instances using
  <span class="pre">
   cudlaCreateDevice()
  </span>
  using any combination of GPU and DLA HW instances (subject to system resource availability):
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/_images/cuDLA-create-device-multiple.png">
 </a>
 <p>
  <span class="caption-text">
   Device model - multiple instances
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cudla-device-model-fig-cudla-create-device-multiple" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  In addition,
  <span class="pre">
   cudlaCreateDevice()
  </span>
  supports an alternative flag during device creation - CUDLA_STANDALONE. This flag can be used by applications when they wish to create a cuDLA device in standalone mode i.e without coupling it with a GPU device. All device submissions can be accomplished using cuDLA in standalone mode as well but in this mode there is no support for CUDA interactions. Consequently, in what follows, two modes of execution are considered while describing a particular API or a particular usecase: the hybrid mode and the standalone mode. The API spec has complete details about which API is supported in which mode.
 </p>
 <h3>
  <span class="section-number">
   7.1.2.
  </span>
  Loading and Querying Modules
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#loading-and-querying-modules" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The cuDLA device handle needs an appropriate loadable to be associated with it before any DLA task submission occurs. The loadable is usually created offline using TensorRT. The loadable has information about the number of input and output tensors as well as their respective metadata and can be queried by the application to retrieve this information. A typical application flow after a successful cuDLA device initialization would look like this (interspersed with some debug logs):
 </p>
 <pre><span class="n">DPRINTF</span><span class="p">(</span><span class="s">"Device created successfully</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>

<span class="c1">// Load the loadable from 'loadableData' in which the loadable binary has</span>
<span class="c1">// been copied from the location of the loadable - disk or otherwise.</span>
<span class="n">err</span><span class="o">=</span><span class="n">cudlaModuleLoadFromMemory</span><span class="p">(</span><span class="n">devHandle</span><span class="p">,</span><span class="n">loadableData</span><span class="p">,</span><span class="n">file_size</span><span class="p">,</span><span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="n">moduleHandle</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="c1">// Get tensor attributes.</span>
<span class="kt">uint32_t</span><span class="n">numInputTensors</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>
<span class="kt">uint32_t</span><span class="n">numOutputTensors</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>
<span class="n">cudlaModuleAttribute</span><span class="n">attribute</span><span class="p">;</span>

<span class="n">err</span><span class="o">=</span><span class="n">cudlaModuleGetAttributes</span><span class="p">(</span><span class="n">moduleHandle</span><span class="p">,</span><span class="n">CUDLA_NUM_INPUT_TENSORS</span><span class="p">,</span><span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="n">attribute</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>
<span class="n">numInputTensors</span><span class="o">=</span><span class="n">attribute</span><span class="p">.</span><span class="n">numInputTensors</span><span class="p">;</span>
<span class="n">DPRINTF</span><span class="p">(</span><span class="s">"numInputTensors = %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">numInputTensors</span><span class="p">);</span>

<span class="n">err</span><span class="o">=</span><span class="n">cudlaModuleGetAttributes</span><span class="p">(</span><span class="n">moduleHandle</span><span class="p">,</span><span class="n">CUDLA_NUM_OUTPUT_TENSORS</span><span class="p">,</span><span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="n">attribute</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>
<span class="n">numOutputTensors</span><span class="o">=</span><span class="n">attribute</span><span class="p">.</span><span class="n">numOutputTensors</span><span class="p">;</span>
<span class="n">DPRINTF</span><span class="p">(</span><span class="s">"numOutputTensors = %d</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">numOutputTensors</span><span class="p">);</span>

<span class="n">cudlaModuleTensorDescriptor</span><span class="o">*</span><span class="n">inputTensorDesc</span><span class="o">=</span>
<span class="p">(</span><span class="n">cudlaModuleTensorDescriptor</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="n">cudlaModuleTensorDescriptor</span><span class="p">)</span>
<span class="o">*</span><span class="n">numInputTensors</span><span class="p">);</span>
<span class="n">cudlaModuleTensorDescriptor</span><span class="o">*</span><span class="n">outputTensorDesc</span><span class="o">=</span>
<span class="p">(</span><span class="n">cudlaModuleTensorDescriptor</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="n">cudlaModuleTensorDescriptor</span><span class="p">)</span>
<span class="o">*</span><span class="n">numOutputTensors</span><span class="p">);</span>

<span class="k">if</span><span class="p">((</span><span class="n">inputTensorDesc</span><span class="o">==</span><span class="nb">NULL</span><span class="p">)</span><span class="o">||</span><span class="p">(</span><span class="n">outputTensorDesc</span><span class="o">==</span><span class="nb">NULL</span><span class="p">))</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">attribute</span><span class="p">.</span><span class="n">inputTensorDesc</span><span class="o">=</span><span class="n">inputTensorDesc</span><span class="p">;</span>
<span class="n">err</span><span class="o">=</span><span class="n">cudlaModuleGetAttributes</span><span class="p">(</span><span class="n">moduleHandle</span><span class="p">,</span>
<span class="n">CUDLA_INPUT_TENSOR_DESCRIPTORS</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="n">attribute</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">attribute</span><span class="p">.</span><span class="n">outputTensorDesc</span><span class="o">=</span><span class="n">outputTensorDesc</span><span class="p">;</span>
<span class="n">err</span><span class="o">=</span><span class="n">cudlaModuleGetAttributes</span><span class="p">(</span><span class="n">moduleHandle</span><span class="p">,</span>
<span class="n">CUDLA_OUTPUT_TENSOR_DESCRIPTORS</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="n">attribute</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>
</pre>
 <p>
  Applications can use the retrieved tensor descriptors to setup their data buffers in terms of size and formats. Detailed information about the contents of the tensor descriptors is present in the API specification section under
  <span class="pre">
   cudlaModuleGetAttributes()
  </span>
  .
 </p>
 <h3>
  <span class="section-number">
   7.1.3.
  </span>
  Memory Model
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#memory-model" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The GPU and DLA have different MMUs that manage the VA to PA conversion while performing their respective functions. The figure below shows an example where the GMMU performs a translation for GPU VAs and the SMMU performs a similar function for the VAs arriving from the DLA.
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/_images/cuDLA-memory-model.png">
 </a>
 <p>
  <span class="caption-text">
   Virtual address to physical address conversion
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cudla-memory-model-fig-cudla-memory-model" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  In hybrid mode, before a CUDA pointer can be accessed by the DLA, it is necessary that the CUDA pointer be
  registered
  with the DLA. This registration step creates an entry in the SMMU and returns the corresponding VA for use in task submissions. The following code snippet shows an example registration for a device handle created with the flag
  <span class="pre">
   CUDLA_CUDA_DLA
  </span>
  :
 </p>
 <pre><span class="c1">// Allocate memory on GPU.</span>
<span class="kt">void</span><span class="o">*</span><span class="n">buffer</span><span class="p">;</span>
<span class="kt">uint32_t</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">;</span>

<span class="n">result</span><span class="o">=</span><span class="n">cudaMalloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">inputBufferGPU</span><span class="p">,</span><span class="n">size</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">result</span><span class="o">!=</span><span class="n">cudaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="c1">// Register the CUDA-allocated buffers.</span>
<span class="kt">uint64_t</span><span class="o">*</span><span class="n">bufferRegisteredPtr</span><span class="o">=</span><span class="nb">NULL</span><span class="p">;</span>

<span class="n">err</span><span class="o">=</span><span class="n">cudlaMemRegister</span><span class="p">(</span><span class="n">devHandle</span><span class="p">,</span>
<span class="p">(</span><span class="kt">uint64_t</span><span class="o">*</span><span class="p">)</span><span class="n">inputBufferGPU</span><span class="p">,</span>
<span class="n">size</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">bufferRegisteredPtr</span><span class="p">,</span>
<span class="mi">0</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>
</pre>
 <p>
  In standalone mode, cuDLA functions without the underlying CUDA device. Consequently, in this mode, the memory allocations performed by the application (which need to be subsequently registered) need to come from outside CUDA. On Tegra systems, cuDLA supports registration of NvSciBuf allocations via the
  <span class="pre">
   cudlaImportExternalMemory()
  </span>
  API as the following code snippet shows:
 </p>
 <pre><span class="c1">// Allocate the NvSciBuf object.</span>
<span class="n">NvSciBufObj</span><span class="n">inputBufObj</span><span class="p">;</span>
<span class="n">sciError</span><span class="o">=</span><span class="n">NvSciBufObjAlloc</span><span class="p">(</span><span class="n">reconciledInputAttrList</span><span class="p">,</span><span class="o">&amp;</span><span class="n">inputBufObj</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="kt">uint64_t</span><span class="o">*</span><span class="n">inputBufObjRegPtr</span><span class="o">=</span><span class="nb">NULL</span><span class="p">;</span>

<span class="c1">// importing external memory</span>
<span class="n">cudlaExternalMemoryHandleDesc</span><span class="n">memDesc</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="n">memset</span><span class="p">(</span><span class="o">&amp;</span><span class="n">memDesc</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="k">sizeof</span><span class="p">(</span><span class="n">memDesc</span><span class="p">));</span>
<span class="n">memDesc</span><span class="p">.</span><span class="n">extBufObject</span><span class="o">=</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">inputBufObj</span><span class="p">;</span>
<span class="n">memDesc</span><span class="p">.</span><span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">;</span>
<span class="n">err</span><span class="o">=</span><span class="n">cudlaImportExternalMemory</span><span class="p">(</span><span class="n">devHandle</span><span class="p">,</span><span class="o">&amp;</span><span class="n">memDesc</span><span class="p">,</span><span class="o">&amp;</span><span class="n">inputBufObjRegPtr</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>
</pre>
 <h3>
  <span class="section-number">
   7.1.4.
  </span>
  Task Execution and Synchronization Model
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#task-execution-and-synchronization-model" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <h4>
  <span class="section-number">
   7.1.4.1.
  </span>
  Task Execution
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#task-execution" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Submitting a DLA task for execution is similar to submitting a CUDA kernel to the GPU. cuDLA natively supports CUDA streams and works seamlessly with the stream semantics to ensure that all tasks intended for the DLA are executed by the DLA HW only after the previous tasks on the stream have completed execution. This enables applications to setup complex processing workflows between the GPU and the DLA using familiar stream semantics without having to manage memory coherency and execution dependencies between GPU and DLA. A visual illustration of the execution model is shown in the following figure. DLA tasks can be interspersed with GPU tasks in a given stream or multiple streams and
  <span class="pre">
   cudlaSubmitTask()
  </span>
  handles all the memory/execution dependencies.
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/_images/cuDLA-execution-model.png">
 </a>
 <p>
  <span class="caption-text">
   cuDLA task execution model
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cudla-task-execution-fig-cudla-execution-model" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  The submit task API needs the input and output tensors in the form of the addresses registered with the DLA (using
  <span class="pre">
   cudlaMemRegister()
  </span>
  ). An application can pre-register all the required pointers with cuDLA and then use the registered pointers during
  <span class="pre">
   cudlaSubmitTask()
  </span>
  . This API, in turn, ensures that the results of the previous operations on the underlying memory corresponding to the registered pointers is visible to the DLA before it begins execution of the current task. A typical application code consisting of CUDA and cuDLA operations is shown in the snippet below:
 </p>
 <pre><span class="n">DPRINTF</span><span class="p">(</span><span class="s">"ALL MEMORY REGISTERED SUCCESSFULLY</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>

<span class="c1">// Copy data from CPU buffers to GPU buffers.</span>
<span class="n">result</span><span class="o">=</span><span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">inputBufferGPU</span><span class="p">,</span><span class="n">inputBuffer</span><span class="p">,</span><span class="n">inputTensorDesc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">size</span><span class="p">,</span><span class="n">cudaMemcpyHostToDevice</span><span class="p">,</span><span class="n">stream</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">result</span><span class="o">!=</span><span class="n">cudaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">result</span><span class="o">=</span><span class="n">cudaMemsetAsync</span><span class="p">(</span><span class="n">outputBufferGPU</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">outputTensorDesc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">size</span><span class="p">,</span><span class="n">stream</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">result</span><span class="o">!=</span><span class="n">cudaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="c1">// Enqueue a cuDLA task.</span>
<span class="n">cudlaTask</span><span class="n">task</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">moduleHandle</span><span class="o">=</span><span class="n">moduleHandle</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">outputTensor</span><span class="o">=</span><span class="o">&amp;</span><span class="n">outputBufferRegisteredPtr</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">numOutputTensors</span><span class="o">=</span><span class="mi">1</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">numInputTensors</span><span class="o">=</span><span class="mi">1</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">inputTensor</span><span class="o">=</span><span class="o">&amp;</span><span class="n">inputBufferRegisteredPtr</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">waitEvents</span><span class="o">=</span><span class="nb">NULL</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">signalEvents</span><span class="o">=</span><span class="nb">NULL</span><span class="p">;</span>
<span class="n">err</span><span class="o">=</span><span class="n">cudlaSubmitTask</span><span class="p">(</span><span class="n">devHandle</span><span class="p">,</span><span class="o">&amp;</span><span class="n">task</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">stream</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>
<span class="n">DPRINTF</span><span class="p">(</span><span class="s">"SUBMIT IS DONE !!!</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>

<span class="n">result</span><span class="o">=</span><span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">outputBuffer</span><span class="p">,</span><span class="n">outputBufferGPU</span><span class="p">,</span><span class="n">outputTensorDesc</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">size</span><span class="p">,</span><span class="n">cudaMemcpyDeviceToHost</span><span class="p">,</span><span class="n">stream</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">result</span><span class="o">!=</span><span class="n">cudaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>
</pre>
 <p>
  In standalone mode, the stream parameter in
  <span class="pre">
   cudlaSubmitTask()
  </span>
  must be specified as NULL as cuDLA is operating independently of CUDA. In this case, the tasks submitted to the DLA are executed in FIFO order.
 </p>
 <h5>
  <span class="section-number">
   7.1.4.1.1.
  </span>
  Multithreaded User Submission
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#multithreaded-user-submission" title="Permalink to this headline">
   ï
  </a>
 </h5>
 <p>
  Users can specify the
  <span class="pre">
   CUDLA_SUBMIT_SKIP_LOCK_ACQUIRE
  </span>
  flag during submission to a particular device handle if they are sure that submission to this particular device handle occurs only in this thread and that there is no shared data at the application level between this device handle and any other device handle which might be used in a parallel thread for submission. This flag facilitates some optimizations in the submission path which might lead to better submission times from the application point of view.
 </p>
 <h4>
  <span class="section-number">
   7.1.4.2.
  </span>
  Synchronization
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#synchronization" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Synchronization of tasks in hybrid mode does not need a different API. Since DLA tasks are submitted to CUDA streams, it is sufficient to wait on the stream to complete its work in order to ensure that all DLA tasks submitted on that stream are completed. In this regard DLA task synchronization is compatible with any of the different synchronization mechanisms available in CUDA â Event, Stream, Device â and the entire CUDA machinery is available for applications to setup different flows and usecases.
 </p>
 <p>
  In standalone mode, however, the synchronization mechanisms are different given that cuDLA operates independently of CUDA. In this mode, the cudlaTask structure has a provision to specify wait and signal events that cuDLA must wait on and signal respectively as part of
  <span class="pre">
   cudlaSubmitTask()
  </span>
  . Each submitted task will wait for all its wait events to be signaled before beginning execution and will provide a signal event (if one is requested for during
  <span class="pre">
   cudlaSubmitTask()
  </span>
  ) that the application (or any other entity) can wait on to ensure that the submitted task has completed execution. In cuDLA 1.0, only NvSciSync fences are supported as part of wait events. Furthermore, only NvSciSync objects can be registered and signaled as part of signal events and the fence corresponding to the signaled event is returned as part of
  <span class="pre">
   cudlaSubmitTask()
  </span>
  .
 </p>
 <p>
  Like all memory operations, the underlying backing store for the events (in this case the NvSciSync object) must be registered with cuDLA before using it in a task submission. The code snippet below shows an example flow where the application creates an input and output NvSciSync object and registers them, creates fences corresponding to them, marks the corresponding fences as wait/signal as part of
  <span class="pre">
   cudlaSubmitTask()
  </span>
  and then signals the input fence and waits on the output fence.
 </p>
 <h5>
  <span class="section-number">
   7.1.4.2.1.
  </span>
  Registering an external semaphore:
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#registering-an-external-semaphore" title="Permalink to this headline">
   ï
  </a>
 </h5>
 <pre><span class="n">sciError</span><span class="o">=</span><span class="n">NvSciSyncObjAlloc</span><span class="p">(</span><span class="n">nvSciSyncReconciledListObj1</span><span class="p">,</span><span class="o">&amp;</span><span class="n">syncObj1</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">sciError</span><span class="o">=</span><span class="n">NvSciSyncObjAlloc</span><span class="p">(</span><span class="n">nvSciSyncReconciledListObj2</span><span class="p">,</span><span class="o">&amp;</span><span class="n">syncObj2</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="c1">// importing external semaphore</span>
<span class="kt">uint64_t</span><span class="o">*</span><span class="n">nvSciSyncObjRegPtr1</span><span class="o">=</span><span class="nb">NULL</span><span class="p">;</span>
<span class="kt">uint64_t</span><span class="o">*</span><span class="n">nvSciSyncObjRegPtr2</span><span class="o">=</span><span class="nb">NULL</span><span class="p">;</span>
<span class="n">cudlaExternalSemaphoreHandleDesc</span><span class="n">semaMemDesc</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="n">memset</span><span class="p">(</span><span class="o">&amp;</span><span class="n">semaMemDesc</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="k">sizeof</span><span class="p">(</span><span class="n">semaMemDesc</span><span class="p">));</span>
<span class="n">semaMemDesc</span><span class="p">.</span><span class="n">extSyncObject</span><span class="o">=</span><span class="n">syncObj1</span><span class="p">;</span>
<span class="n">err</span><span class="o">=</span><span class="n">cudlaImportExternalSemaphore</span><span class="p">(</span><span class="n">devHandle</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">semaMemDesc</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">nvSciSyncObjRegPtr1</span><span class="p">,</span>
<span class="mi">0</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">memset</span><span class="p">(</span><span class="o">&amp;</span><span class="n">semaMemDesc</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="k">sizeof</span><span class="p">(</span><span class="n">semaMemDesc</span><span class="p">));</span>
<span class="n">semaMemDesc</span><span class="p">.</span><span class="n">extSyncObject</span><span class="o">=</span><span class="n">syncObj2</span><span class="p">;</span>
<span class="n">err</span><span class="o">=</span><span class="n">cudlaImportExternalSemaphore</span><span class="p">(</span><span class="n">devHandle</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">semaMemDesc</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">nvSciSyncObjRegPtr2</span><span class="p">,</span>
<span class="mi">0</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">DPRINTF</span><span class="p">(</span><span class="s">"ALL EXTERNAL SEMAPHORES REGISTERED SUCCESSFULLY</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
</pre>
 <h5>
  <span class="section-number">
   7.1.4.2.2.
  </span>
  Events setup for cudlaSubmitTask()
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#events-setup-for-cudlasubmittask" title="Permalink to this headline">
   ï
  </a>
 </h5>
 <pre><span class="c1">// Wait events</span>
<span class="n">NvSciSyncFence</span><span class="n">preFence</span><span class="o">=</span><span class="n">NvSciSyncFenceInitializer</span><span class="p">;</span>
<span class="n">sciError</span><span class="o">=</span><span class="n">NvSciSyncObjGenerateFence</span><span class="p">(</span><span class="n">syncObj1</span><span class="p">,</span><span class="o">&amp;</span><span class="n">preFence</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">cudlaWaitEvents</span><span class="o">*</span><span class="n">waitEvents</span><span class="p">;</span>
<span class="n">waitEvents</span><span class="o">=</span><span class="p">(</span><span class="n">cudlaWaitEvents</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="n">cudlaWaitEvents</span><span class="p">));</span>
<span class="k">if</span><span class="p">(</span><span class="n">waitEvents</span><span class="o">==</span><span class="nb">NULL</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">waitEvents</span><span class="o">-&gt;</span><span class="n">numEvents</span><span class="o">=</span><span class="mi">1</span><span class="p">;</span>
<span class="n">CudlaFence</span><span class="o">*</span><span class="n">preFences</span><span class="o">=</span><span class="p">(</span><span class="n">CudlaFence</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">waitEvents</span><span class="o">-&gt;</span><span class="n">numEvents</span><span class="o">*</span>
<span class="k">sizeof</span><span class="p">(</span><span class="n">CudlaFence</span><span class="p">));</span>
<span class="k">if</span><span class="p">(</span><span class="n">preFences</span><span class="o">==</span><span class="nb">NULL</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">preFences</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">fence</span><span class="o">=</span><span class="o">&amp;</span><span class="n">preFence</span><span class="p">;</span>
<span class="n">preFences</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">type</span><span class="o">=</span><span class="n">CUDLA_NVSCISYNC_FENCE</span><span class="p">;</span>
<span class="n">waitEvents</span><span class="o">-&gt;</span><span class="n">preFences</span><span class="o">=</span><span class="n">preFences</span><span class="p">;</span>

<span class="c1">// Signal Events</span>
<span class="n">cudlaSignalEvents</span><span class="o">*</span><span class="n">signalEvents</span><span class="p">;</span>
<span class="n">signalEvents</span><span class="o">=</span><span class="p">(</span><span class="n">cudlaSignalEvents</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="n">cudlaSignalEvents</span><span class="p">));</span>
<span class="k">if</span><span class="p">(</span><span class="n">signalEvents</span><span class="o">==</span><span class="nb">NULL</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">signalEvents</span><span class="o">-&gt;</span><span class="n">numEvents</span><span class="o">=</span><span class="mi">1</span><span class="p">;</span>
<span class="kt">uint64_t</span><span class="o">**</span><span class="n">devPtrs</span><span class="o">=</span><span class="p">(</span><span class="kt">uint64_t</span><span class="o">**</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">signalEvents</span><span class="o">-&gt;</span><span class="n">numEvents</span><span class="o">*</span>
<span class="k">sizeof</span><span class="p">(</span><span class="kt">uint64_t</span><span class="o">*</span><span class="p">));</span>
<span class="k">if</span><span class="p">(</span><span class="n">devPtrs</span><span class="o">==</span><span class="nb">NULL</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">devPtrs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="n">nvSciSyncObjRegPtr2</span><span class="p">;</span>
<span class="n">signalEvents</span><span class="o">-&gt;</span><span class="n">devPtrs</span><span class="o">=</span><span class="n">devPtrs</span><span class="p">;</span>

<span class="n">signalEvents</span><span class="o">-&gt;</span><span class="n">eofFences</span><span class="o">=</span><span class="p">(</span><span class="n">CudlaFence</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">signalEvents</span><span class="o">-&gt;</span><span class="n">numEvents</span><span class="o">*</span>
<span class="k">sizeof</span><span class="p">(</span><span class="n">CudlaFence</span><span class="p">));</span>
<span class="k">if</span><span class="p">(</span><span class="n">signalEvents</span><span class="o">-&gt;</span><span class="n">eofFences</span><span class="o">==</span><span class="nb">NULL</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>

<span class="n">NvSciSyncFence</span><span class="n">eofFence</span><span class="o">=</span><span class="n">NvSciSyncFenceInitializer</span><span class="p">;</span>
<span class="n">signalEvents</span><span class="o">-&gt;</span><span class="n">eofFences</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">fence</span><span class="o">=</span><span class="o">&amp;</span><span class="n">eofFence</span><span class="p">;</span>
<span class="n">signalEvents</span><span class="o">-&gt;</span><span class="n">eofFences</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">type</span><span class="o">=</span><span class="n">CUDLA_NVSCISYNC_FENCE</span><span class="p">;</span>

<span class="c1">// Enqueue a cuDLA task.</span>
<span class="n">cudlaTask</span><span class="n">task</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">moduleHandle</span><span class="o">=</span><span class="n">moduleHandle</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">outputTensor</span><span class="o">=</span><span class="o">&amp;</span><span class="n">outputBufObjRegPtr</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">numOutputTensors</span><span class="o">=</span><span class="mi">1</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">numInputTensors</span><span class="o">=</span><span class="mi">1</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">inputTensor</span><span class="o">=</span><span class="o">&amp;</span><span class="n">inputBufObjRegPtr</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">waitEvents</span><span class="o">=</span><span class="n">waitEvents</span><span class="p">;</span>
<span class="n">task</span><span class="p">.</span><span class="n">signalEvents</span><span class="o">=</span><span class="n">signalEvents</span><span class="p">;</span>
<span class="n">err</span><span class="o">=</span><span class="n">cudlaSubmitTask</span><span class="p">(</span><span class="n">devHandle</span><span class="p">,</span><span class="o">&amp;</span><span class="n">task</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="nb">NULL</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>
<span class="n">DPRINTF</span><span class="p">(</span><span class="s">"SUBMIT IS DONE !!!</span><span class="se">\n</span><span class="s">"</span><span class="p">);</span>
</pre>
 <h5>
  <span class="section-number">
   7.1.4.2.3.
  </span>
  Waiting on the signal event
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#waiting-on-the-signal-event" title="Permalink to this headline">
   ï
  </a>
 </h5>
 <pre><span class="c1">// Signal wait events.</span>
<span class="c1">// For illustration purposes only. In practice, this signal will be done by another</span>
<span class="c1">// entity or driver that provides the data input for this particular submitted task.</span>
<span class="n">NvSciSyncObjSignal</span><span class="p">(</span><span class="n">syncObj1</span><span class="p">);</span>

<span class="c1">// Wait for operations to finish.</span>
<span class="c1">// For illustration purposes only. In practice, this wait will be done by</span>
<span class="c1">// another entity or driver that is waiting for the output of the submitted task.</span>
<span class="n">sciError</span><span class="o">=</span><span class="n">NvSciSyncFenceWait</span><span class="p">(</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">NvSciSyncFence</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">signalEvents</span><span class="o">-&gt;</span><span class="n">eofFences</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">fence</span><span class="p">),</span>
<span class="n">nvSciCtx</span><span class="p">,</span><span class="mi">-1</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span>
<span class="p">{</span>
<span class="c1">// handle error</span>
<span class="p">}</span>
</pre>
 <h5>
  <span class="section-number">
   7.1.4.2.4.
  </span>
  Supported Synchronization Primitives in cuDLA
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#supported-synchronization-primitives-in-cudla" title="Permalink to this headline">
   ï
  </a>
 </h5>
 <p>
  cuDLA supports two types of NvSciSync object primitives. These are sync point and deterministic semaphores. By default, cuDLA prioritizes sync point primitive over deterministic semaphore primitive and sets these priorities in the NvSciSync attribute list when requested by the application using
  <span class="pre">
   cudlaGetNvSciSyncAttributes()
  </span>
  .
 </p>
 <p>
  For Deterministic semaphore, the NvSciSync attribute list used to create the NvSciSync object must have the value of
  <span class="pre">
   NvSciSyncAttrKey_RequireDeterministicFences
  </span>
  key set to true. Deterministic fences allow users to enqueue a wait over the semaphore object even before corresponding signal is enqueued. For such semaphore object, cuDLA guarantees that each signal operation will increment the fence value by â1â. Users are expected to keep track of signals enqueued on the semaphore object and insert waits accordingly.
 </p>
 <h5>
  <span class="section-number">
   7.1.4.2.5.
  </span>
  Setting NvSciSyncAttrKey_RequireDeterministicFences key in NvSciSyncAttrList
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#setting-nvscisyncattrkey-requiredeterministicfences-key-in-nvscisyncattrlist" title="Permalink to this headline">
   ï
  </a>
 </h5>
 <pre><span class="c1">// Set NvSciSyncAttrKey_RequireDeterministicFences key to true in</span>
<span class="c1">// NvScisyncAtrrList that is used to create NvSciSync object with</span>
<span class="c1">// Deterministic Semaphore primitive.</span>
<span class="n">NvSciSyncAccessPerm</span><span class="n">cpuPerm</span><span class="o">=</span><span class="n">NvSciSyncAccessPerm_SignalOnly</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">attrKey</span><span class="o">=</span><span class="n">NvSciSyncAttrKey_RequiredPerm</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">value</span><span class="o">=</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">cpuPerm</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">len</span><span class="o">=</span><span class="k">sizeof</span><span class="p">(</span><span class="n">cpuPerm</span><span class="p">);</span>

<span class="kt">bool</span><span class="n">detFenceReq</span><span class="o">=</span><span class="nb">true</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">attrKey</span><span class="o">=</span><span class="n">NvSciSyncAttrKey_RequireDeterministicFences</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">value</span><span class="o">=</span><span class="p">(</span><span class="k">const</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">detFenceReq</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">len</span><span class="o">=</span><span class="k">sizeof</span><span class="p">(</span><span class="n">detFenceReq</span><span class="p">);</span>

<span class="k">return</span><span class="n">NvSciSyncAttrListSetAttrs</span><span class="p">(</span><span class="n">list</span><span class="p">,</span><span class="n">keyValue</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span>
</pre>
 <h5>
  <span class="section-number">
   7.1.4.2.6.
  </span>
  Timestamp Support for NvSciFence
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#timestamp-support-for-nvscifence" title="Permalink to this headline">
   ï
  </a>
 </h5>
 <p>
  cuDLA supports the timestamp feature of NvSci in cuDLA standalone mode.
 </p>
 <p>
  Timestamp support enables users to get the time at which a particular fence has been signaled. This time value is the snapshot of the DLA clock in microseconds.
 </p>
 <p>
  cuDLA users can request timestamp support by setting the value of the
  <span class="pre">
   NvSciSyncAttrKey_WaiterRequireTimestamps
  </span>
  key as true while filling up the NvSci waiter attribute list.
 </p>
 <p>
  The users can use this timestamp along with SOF(Start Of Frame) fence and EOF(End OF Frame) fence to get a snapshot of DLA clock just before start of task &amp; after task completion respectively. This enables users to calculate time taken by DLA to execute the submitted task.
 </p>
 <h5>
  <span class="section-number">
   7.1.4.2.7.
  </span>
  Requesting Timestamp Support for NvSciSync Object
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#requesting-timestamp-support-for-nvscisync-object" title="Permalink to this headline">
   ï
  </a>
 </h5>
 <pre><span class="n">sciError</span><span class="nf">fillCpuWaiterAttrList</span><span class="p">(</span><span class="n">NvSciSyncAttrList</span><span class="n">list</span><span class="p">)</span>
<span class="p">{</span>
<span class="kt">bool</span><span class="n">cpuWaiter</span><span class="o">=</span><span class="nb">true</span><span class="p">;</span>
<span class="n">NvSciSyncAttrKeyValuePair</span><span class="n">keyValue</span><span class="p">[</span><span class="mi">3</span><span class="p">];</span>
<span class="n">memset</span><span class="p">(</span><span class="n">keyValue</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="k">sizeof</span><span class="p">(</span><span class="n">keyValue</span><span class="p">));</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">attrKey</span><span class="o">=</span><span class="n">NvSciSyncAttrKey_NeedCpuAccess</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">value</span><span class="o">=</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">cpuWaiter</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">len</span><span class="o">=</span><span class="k">sizeof</span><span class="p">(</span><span class="n">cpuWaiter</span><span class="p">);</span>

<span class="n">NvSciSyncAccessPerm</span><span class="n">cpuPerm</span><span class="o">=</span><span class="n">NvSciSyncAccessPerm_WaitOnly</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">attrKey</span><span class="o">=</span><span class="n">NvSciSyncAttrKey_RequiredPerm</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">value</span><span class="o">=</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">cpuPerm</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">len</span><span class="o">=</span><span class="k">sizeof</span><span class="p">(</span><span class="n">cpuPerm</span><span class="p">);</span>

<span class="kt">bool</span><span class="n">cpuRequiresTimeStamp</span><span class="o">=</span><span class="nb">true</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">attrKey</span><span class="o">=</span><span class="n">NvSciSyncAttrKey_WaiterRequireTimestamps</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">value</span><span class="o">=</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">cpuRequiresTimeStamp</span><span class="p">;</span>
<span class="n">keyValue</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">len</span><span class="o">=</span><span class="k">sizeof</span><span class="p">(</span><span class="n">cpuRequiresTimeStamp</span><span class="p">);</span>

<span class="k">return</span><span class="n">NvSciSyncAttrListSetAttrs</span><span class="p">(</span><span class="n">list</span><span class="p">,</span><span class="n">keyValue</span><span class="p">,</span><span class="mi">3</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">NvSciSyncCpuWaitContext</span><span class="n">nvSciCtx</span><span class="p">;</span>
<span class="n">NvSciSyncModule</span><span class="n">syncModule</span><span class="p">;</span>
<span class="n">NvSciSyncAttrList</span><span class="n">waiterAttrListObj</span><span class="o">=</span><span class="k">nullptr</span><span class="p">;</span>
<span class="n">NvSciSyncAttrList</span><span class="n">signalerAttrListObj</span><span class="o">=</span><span class="k">nullptr</span><span class="p">;</span>
<span class="n">NvSciSyncAttrList</span><span class="n">syncAttrListObj</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
<span class="n">NvSciSyncAttrList</span><span class="n">nvSciSyncConflictListObj</span><span class="p">;</span>
<span class="n">NvSciSyncAttrList</span><span class="n">nvSciSyncReconciledListObj</span><span class="p">;</span>

<span class="n">sciError</span><span class="o">=</span><span class="n">NvSciSyncModuleOpen</span><span class="p">(</span><span class="o">&amp;</span><span class="n">syncModule</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span><span class="p">{</span>
<span class="c1">//handle error</span>
<span class="p">}</span>

<span class="n">sciError</span><span class="o">=</span><span class="n">NvSciSyncAttrListCreate</span><span class="p">(</span><span class="n">syncModule</span><span class="p">,</span><span class="o">&amp;</span><span class="n">signalerAttrListObj</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span><span class="p">{</span>
<span class="c1">//handle error</span>
<span class="p">}</span>

<span class="n">sciError</span><span class="o">=</span><span class="n">NvSciSyncAttrListCreate</span><span class="p">(</span><span class="n">syncModule</span><span class="p">,</span><span class="o">&amp;</span><span class="n">waiterAttrListObj</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span><span class="p">{</span>
<span class="c1">//handle error</span>
<span class="p">}</span>

<span class="n">err</span><span class="o">=</span><span class="n">cudlaGetNvSciSyncAttributes</span><span class="p">(</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">uint64_t</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">signalerAttrListObj</span><span class="p">),</span>
<span class="n">CUDLA_NVSCISYNC_ATTR_SIGNAL</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">err</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span><span class="p">{</span>
<span class="c1">//handle error</span>
<span class="p">}</span>

<span class="n">sciError</span><span class="o">=</span><span class="n">fillCpuWaiterAttrList</span><span class="p">(</span><span class="n">waiterAttrListObj</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span><span class="p">{</span>
<span class="c1">//handle error</span>
<span class="p">}</span>

<span class="n">syncAttrListObj</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="n">signalerAttrListObj</span><span class="p">;</span>
<span class="n">syncAttrListObj</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="n">waiterAttrListObj</span><span class="p">;</span>
<span class="n">sciError</span><span class="o">=</span><span class="n">NvSciSyncAttrListReconcile</span><span class="p">(</span><span class="n">syncAttrListObj</span><span class="p">,</span>
<span class="mi">2</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">nvSciSyncReconciledListObj</span><span class="p">,</span>
<span class="o">&amp;</span><span class="n">nvSciSyncConflictListObj3</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span><span class="p">{</span>
<span class="c1">//handle error</span>
<span class="p">}</span>

<span class="n">sciError</span><span class="o">=</span><span class="n">NvSciSyncObjAlloc</span><span class="p">(</span><span class="n">nvSciSyncReconciledListObj</span><span class="p">,</span><span class="o">&amp;</span><span class="n">syncObj</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span><span class="p">{</span>
<span class="c1">//handle error</span>
<span class="p">}</span>

<span class="n">sciError</span><span class="o">=</span><span class="n">NvSciSyncCpuWaitContextAlloc</span><span class="p">(</span><span class="n">syncModule</span><span class="p">,</span><span class="o">&amp;</span><span class="n">nvSciCtx</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span><span class="p">{</span>
<span class="c1">//handle error</span>
<span class="p">}</span>
</pre>
 <h5>
  <span class="section-number">
   7.1.4.2.8.
  </span>
  Extracting Timestamp Value from Fence
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#extracting-timestamp-value-from-fence" title="Permalink to this headline">
   ï
  </a>
 </h5>
 <p>
  Refer to these sections for more information:
 </p>
 <ul class="simple">
  <li>
   <p>
    <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#registering-an-external-semaphore">
     <span class="std std-ref">
      Registering an external semaphore:
     </span>
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#events-setup-for-cudlasubmittask">
     <span class="std std-ref">
      Events setup for cudlaSubmitTask()
     </span>
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#waiting-on-the-signal-event">
     <span class="std std-ref">
      Waiting on the signal event
     </span>
    </a>
   </p>
  </li>
 </ul>
 <pre><span class="c1">// To extract Timestamp of the fence</span>
<span class="c1">// Timestamp will be valid only after fence is signaled</span>
<span class="c1">// hence Fence must be waited up on before extracting timestamp value</span>

<span class="kt">uint64_t</span><span class="n">eofTimestampUS</span><span class="o">=</span><span class="mi">0UL</span><span class="p">;</span>
<span class="n">sciError</span><span class="o">=</span><span class="n">NvSciSyncFenceGetTimestamp</span><span class="p">(</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">NvSciSyncFence</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">signalEvents</span><span class="o">-&gt;</span><span class="n">eofFences</span><span class="p">.</span><span class="n">fence</span><span class="p">),</span><span class="o">&amp;</span><span class="p">(</span><span class="n">eofTimestampUS</span><span class="p">));</span>
<span class="k">if</span><span class="p">((</span><span class="n">sciError</span><span class="o">!=</span><span class="n">NvSciError_Success</span><span class="p">)</span><span class="o">||</span><span class="p">(</span><span class="n">eofTimestampUS</span><span class="o">==</span><span class="mi">0UL</span><span class="p">))</span><span class="p">{</span>
<span class="c1">//handle error</span>
<span class="p">}</span>
</pre>
 <h4>
  <span class="section-number">
   7.1.4.3.
  </span>
  Fault Diagnostics
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#fault-diagnostics" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  To perform fault diagnostics for DLA HW, users should specify the
  <span class="pre">
   CUDLA_MODULE_ENABLE_FAULT_DIAGNOSTICS
  </span>
  flag to load the module and
  <span class="pre">
   CUDLA_SUBMIT_DIAGNOSTICS_TASK
  </span>
  during task submission. This task can be used to probe the state of DLA HW. With this flag set, in standalone mode the user is not allowed to do event only submissions, where tensor information is NULL and only events (wait/signal or both) are present in task. This is because the task always runs on an internally loaded diagnostic module. This diagnostic module does not expect any input tensors and so does not require input tensor memory. However the user is expected to query the number of output tensors, allocate the output tensor memory, and pass the same while using the submit task.
 </p>
 <h4>
  <span class="section-number">
   7.1.4.4.
  </span>
  NOOP Submission
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#noop-submission" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Users can mark certain tasks as noop tasks while calling
  <span class="pre">
   cudlaSubmitTask()
  </span>
  .
 </p>
 <p>
  This is done by passing
  <span class="pre">
   CUDLA_SUBMIT_NOOP
  </span>
  in the
  <span class="pre">
   flags
  </span>
  parameter of
  <span class="pre">
   cudlaSubmitTask()
  </span>
  . A noop submission implies that all the other submission semantics are maintained. Specifically, the task is submitted to DLA, wait/signal events are considered before and after and stream semantics are respected. The key difference is that the task is skipped by the DLA for execution. This is supported in both hybrid and standalone modes.
 </p>
 <h3>
  <span class="section-number">
   7.1.5.
  </span>
  Error Reporting Model
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#error-reporting-model" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The asynchronous nature of task execution results in two kinds of errors that can get reported via cuDLA APIs:
 </p>
 <ul class="simple">
  <li>
   <p>
    Synchronous errors
   </p>
  </li>
  <li>
   <p>
    Asynchronous errors
   </p>
  </li>
 </ul>
 <p>
  Synchronous errors are those that are reported by the cuDLA APIs as part of their return code when they are invoked in an application. Asynchronous errors are those that are detected later compared to sequential program execution. The typical scenario here is that each task submitted to the DLA HW executes after a particular duration of time. As a result, if there are errors in the task execution, they cannot be reported as part of the task submission APIs. Depending on the timing of the errors, they are reported during a subsequent cuDLA API call or after a synchronization operation. HW execution errors reported as part of cuDLA APIs are straightforward to handle at the application level. However, if there is a no cuDLA API call currently executing or about to execute in the application, then the application needs to perform extra steps to handle asynchronous errors.
 </p>
 <p>
  In hybrid mode, DLA HW errors can get reported via CUDA synchronization operations. As mentioned in the device model section, cuDLA logically associates DLA with a GPU for the purposes of execution. Therefore, any DLA HW errors are propagated via CUDA to the user. The user needs to check for DLA-specific errors from CUDA synchronization operations and then check the cuDLA device handle for the exact error using
  <span class="pre">
   cudlaGetLastError()
  </span>
  . If there are multiple cuDLA device handles in the application and each of them have submitted some tasks to cuDLA in hybrid mode, then each and every device handle much be checked for errors. The underlying model here is to use CUDA to detect DLA HW errors and then use
  <span class="pre">
   cudlaGetLastError()
  </span>
  on the relevant handle to report the exact error. The code snippet below shows an example:
 </p>
 <pre><span class="n">result</span><span class="o">=</span><span class="n">cudaStreamSynchronize</span><span class="p">(</span><span class="n">stream</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">result</span><span class="o">!=</span><span class="n">cudaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="n">DPRINTF</span><span class="p">(</span><span class="s">"Error in synchronizing stream = %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">cudaGetErrorName</span><span class="p">(</span><span class="n">result</span><span class="p">));</span>

<span class="k">if</span><span class="p">(</span><span class="n">result</span><span class="o">==</span><span class="n">cudaErrorExternalDevice</span><span class="p">)</span>
<span class="p">{</span>
<span class="n">cudlaStatus</span><span class="n">hwStatus</span><span class="o">=</span><span class="n">cudlaGetLastError</span><span class="p">(</span><span class="n">devHandle</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">hwStatus</span><span class="o">!=</span><span class="n">cudlaSuccess</span><span class="p">)</span>
<span class="p">{</span>
<span class="n">DPRINTF</span><span class="p">(</span><span class="s">"Asynchronous error in HW = %u</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">hwStatus</span><span class="p">);</span>
<span class="p">}</span>
<span class="p">}</span>
<span class="p">}</span>
</pre>
 <p>
  This error reporting model is compatible with CUDA Driver APIs as well and therefore if the application uses CUDA Driver APIs for synchronization, similar error codes and error handling flow is applicable.
 </p>
 <p>
  In standalone mode, the model is similar with the exception that there is no corresponding mechanism to detect errors as part of synchronization operations. In this mode, the only option that an application has to wait on the submitted tasks is to wait on the NvSciSync fence returned by the latest submission. As of this writing, NvSciSync does not support reporting DLA HW errors and therefore an application is expected to wait for the fence and then query
  <span class="pre">
   cudlaGetLastError()
  </span>
  for any errors during execution.
 </p>
 <h2>
  <span class="section-number">
   7.2.
  </span>
  Migrating from NvMediaDla to cuDLA
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#migrating-from-nvmediadla-to-cudla" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  NvMediaDla and cuDLA have different programming models with some degree of overlap in the functionality exposed by the respective APIs. The following table provides a mapping from the NvMediaDla API to the equivalent cuDLA API or functionality. This is intended to be used as a reference when migrating an NvMediaDla app to a cuDLA app.
 </p>
 <table class="table-no-stripes docutils align-default">
  <tr class="row-odd">
   <th class="head">
    <p>
     NvMediaDla
    </p>
   </th>
   <th class="head">
    <p>
     cuDLA
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaGetVersion()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaGetVersion()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaPingById()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not required as ping is done inside
     <span class="pre">
      cudlaCreateDevice
     </span>
     and only upon successful ping does device handle creation succeed.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaCreate()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaCreateDevice()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaDestroy()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaDestroyDevice()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaGetUMDVersion()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not available
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaGetNumEngines()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaDeviceGetCount()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaGetMaxOutstandingTasks()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not available
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaInit()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaCreateDevice
     </span>
     (but specifying number of input tasks is not available)
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaGetInstanceId()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not available
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaGetNumTasks()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not available
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaLoadableCreate()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not required as declaring a variable of type
     <span class="pre">
      cudlaModule
     </span>
     is sufficient alongwith
     <span class="pre">
      cudlaModuleLoadFromMemory()
     </span>
     .
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaLoadableDestroy()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not required as cuDLA modules are declared as variables of type
     <span class="pre">
      cudlaModule
     </span>
     .
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaAppendLoadable()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not required as this is done inside
     <span class="pre">
      cudlaModuleLoadFromMemory()
     </span>
     .
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaSetCurrentLoadable()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not required as this is done inside
     <span class="pre">
      cudlaModuleLoadFromMemory()
     </span>
     .
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaGetNumOfInputTensors()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaModuleGetAttributes()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaGetInputTensorDescriptor()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaModuleGetAttributes()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaGetNumOfOutputTensors()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaModuleGetAttributes()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaGetOutputTensorDescriptor()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaModuleGetAttributes()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaDataRegister()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaMemRegister()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaDataUnregister()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaMemUnregister()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaLoadLoadable()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaModuleLoadFromMemory()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaRemoveLoadable()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaModuleUnload()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaSubmit()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaSubmitTask()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaNvSciSyncGetVersion()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not available
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaFillNvSciSyncAttrList()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaGetNvSciSyncAttributes()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaRegisterNvSciSyncObj()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaImportExternalSemaphore()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaUnregisterNvSciSyncObj()
     </span>
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      cudlaMemUnregister()
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaSetNvSciSyncObjforEOF()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not required as
     <span class="pre">
      cudlaTask
     </span>
     structure has the required capability to specify this.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaInsertPreNvSciSyncFence()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not required as
     <span class="pre">
      cudlaTask
     </span>
     structure has the required capability to specify this.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      NvMediaDlaGetEOFNvSciSyncFence()
     </span>
    </p>
   </td>
   <td>
    <p>
     Not required as
     <span class="pre">
      cudlaTask
     </span>
     structure has the required capability to retrieve this.
    </p>
   </td>
  </tr>
 </table>
 <h2>
  <span class="section-number">
   7.3.
  </span>
  Profiling a cuDLA App
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#profiling-a-cudla-app" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  cuDLA APIs can be profiled using NVIDIA Nsight Systems. The following command can be used to generate traces for cuDLA APIs. These traces can be viewed in Nsight.
 </p>
 <pre>$ nsys profile --trace nvtx -e CUDLA_NVTX_LEVEL=1 --output &lt;file&gt; &lt;cudla_App&gt;
</pre>
 <h2>
  <span class="section-number">
   7.4.
  </span>
  cuDLA Release Notes
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#cudla-release-notes" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Known Issues in cuDLA 1.2.1:
 </p>
 <ul class="simple">
  <li>
   <p>
    In hybrid mode, cuDLA internally allocates memory with CUDA using the primary context. As a result, before destroying/resetting a CUDA primary context, it is mandatory that all cuDLA device initializations are destroyed.
   </p>
  </li>
  <li>
   <p>
    Before destroying a cuDLA device handle, it is important to ensure that all tasks submitted previously to the device are completed. Failure to do so can lead to application crashes as the internal memory allocations would still be in use.
   </p>
  </li>
  <li>
   <p>
    NvSciBuf buffer allocations made by the application must adhere to DLA alignment constraints.
   </p>
  </li>
  <li>
   <p>
    It is the applicationâs responsibility to ensure that there are no duplicate fences specified as part of wait events while submitting tasks.
   </p>
  </li>
  <li>
   <p>
    In general, any synchronous or asynchronous error returned by cuDLA APIs must be treated as a non-recoverable error. In this case, the application is expected to restart and initialize cuDLA again in order to submit DLA tasks. The exception to this rule is
    <span class="pre">
     cudlaErrorMemoryRegistered
    </span>
    which is returned by cuDLA when the application tries to register a particular memory again without unregistering.
   </p>
  </li>
  <li>
   <p>
    cuDLA does not support UVM between CUDA and DLA.
   </p>
  </li>
  <li>
   <p>
    cuDLA does not support CUDA Graph.
   </p>
  </li>
  <li>
   <p>
    cuDLA does not support per-thread default stream.
   </p>
  </li>
  <li>
   <p>
    cuDLA does not support CNP (DLA functions cannot be used with CNP).
   </p>
  </li>
  <li>
   <p>
    cuDLA does not support block linear memory.
   </p>
  </li>
  <li>
   <p>
    cuDLA does not support CUDA VMM APIs at the present moment.
   </p>
  </li>
  <li>
   <p>
    cuDLA does not support dGPU.
   </p>
  </li>
  <li>
   <p>
    Under certain conditions, DLA FW can hang for certain tasks. This can result in the application hanging in both hybrid as well as standalone mode. Applications are expected to detect these scenarios and respond accordingly.
   </p>
  </li>
 </ul>
 <h1>
  <span class="section-number">
   8.
  </span>
  Notices
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#notices" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   8.1.
  </span>
  Notice
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#notice" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
 </p>
 <p>
  NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
 </p>
 <p>
  Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
 </p>
 <p>
  NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
 </p>
 <p>
  NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
 </p>
 <p>
  NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
 </p>
 <p>
  No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
 </p>
 <p>
  Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
 </p>
 <p>
  THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.
 </p>
 <h2>
  <span class="section-number">
   8.2.
  </span>
  OpenCL
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#opencl" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.
 </p>
 <h2>
  <span class="section-number">
   8.3.
  </span>
  Trademarks
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote#trademarks" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.
 </p>
 <p class="notices">
  <a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">
   Privacy Policy
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">
   Manage My Privacy
  </a>
  |
  <a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">
   Do Not Sell or Share My Data
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">
   Terms of Service
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">
   Accessibility
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">
   Corporate Policies
  </a>
  |
  <a href="https://www.nvidia.com/en-us/product-security/" target="_blank">
   Product Security
  </a>
  |
  <a href="https://www.nvidia.com/en-us/contact/" target="_blank">
   Contact
  </a>
 </p>
 <p>
  Copyright Â© 2018-2024, NVIDIA Corporation &amp; affiliates. All rights reserved.
 </p>
 <p>
  <span class="lastupdated">
   Last updated on Jul 1, 2024.
  </span>
 </p>
</body>
</body></html>