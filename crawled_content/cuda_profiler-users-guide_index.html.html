<html><head><title>Profiler</title></head><body><body class="wy-body-for-nav">
 <a href="https://docs.nvidia.com/cuda/profiler-users-guide/contents.html">
 </a>
 <ul class="current">
  <li class="toctree-l1 current">
   <a class="current reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html">
    1. Preparing An Application For Profiling
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#focused-profiling">
      1.1. Focused Profiling
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#marking-regions-of-cpu-activity">
      1.2. Marking Regions of CPU Activity
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#naming-cpu-and-cuda-resources">
      1.3. Naming CPU and CUDA Resources
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#flush-profile-data">
      1.4. Flush Profile Data
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiling-cuda-fortran-applications">
      1.5. Profiling CUDA Fortran Applications
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual-profiler">
    2. âVisual Profiler
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#getting-started">
      2.1. Getting Started
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#setting-up-java-runtime-environment">
        2.1.1. Setting up Java Runtime Environment
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#installing-jre">
        2.1.2. Installing JRE
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#modify-your-application-for-profiling">
        2.1.3. Modify Your Application For Profiling
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#creating-a-session">
        2.1.4. Creating a Session
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#analyzing-your-application">
        2.1.5. Analyzing Your Application
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#exploring-the-timeline">
        2.1.6. Exploring the Timeline
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#looking-at-the-details">
        2.1.7. Looking at the Details
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#improve-loading-of-large-profiles">
        2.1.8. Improve Loading of Large Profiles
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#sessions">
      2.2. Sessions
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#executable-session">
        2.2.1. Executable Session
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-session">
        2.2.2. Import Session
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-single-process-nvprof-session">
          2.2.2.1. Import Single-Process
          <span class="pre">
           nvprof
          </span>
          Session
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-multi-process-nvprof-session">
          2.2.2.2. Import Multi-Process
          <span class="pre">
           nvprof
          </span>
          Session
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-command-line-profiler-session">
          2.2.2.3. Import Command-Line Profiler Session
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#application-requirements">
      2.3. Application Requirements
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual-profiler-views">
      2.4. Visual Profiler Views
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
        2.4.1. Timeline View
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-controls">
          2.4.1.1. Timeline Controls
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#navigating-the-timeline">
          2.4.1.2. Navigating the Timeline
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-refreshing">
          2.4.1.3. Timeline Refreshing
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis-controls">
          2.4.1.4. Dependency Analysis Controls
         </a>
        </li>
       </ul>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#analysis-view">
        2.4.2. Analysis View
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#guided-application-analysis">
          2.4.2.1. Guided Application Analysis
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#unguided-application-analysis">
          2.4.2.2. Unguided Application Analysis
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#pc-sampling-view">
          2.4.2.3. PC Sampling View
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#memory-statistics">
          2.4.2.4. Memory Statistics
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvlink-view">
          2.4.2.5. NVLink view
         </a>
        </li>
       </ul>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#source-disassembly-view">
        2.4.3. Source-Disassembly View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-details-view">
        2.4.4. GPU Details View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-details-view">
        2.4.5. CPU Details View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc-details-view">
        2.4.6. OpenACC Details View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp-details-view">
        2.4.7. OpenMP Details View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#properties-view">
        2.4.8. Properties View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#console-view">
        2.4.9. Console View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#settings-view">
        2.4.10. Settings View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-source-view">
        2.4.11. CPU Source View
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#customizing-the-profiler">
      2.5. Customizing the Profiler
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#resizing-a-view">
        2.5.1. Resizing a View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#reordering-a-view">
        2.5.2. Reordering a View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#moving-a-view">
        2.5.3. Moving a View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#undocking-a-view">
        2.5.4. Undocking a View
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#opening-and-closing-a-view">
        2.5.5. Opening and Closing a View
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#command-line-arguments">
      2.6. Command Line Arguments
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof">
    3. ânvprof
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#command-line-options">
      3.1. Command Line Options
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cuda-profiling-options">
        3.1.1. CUDA Profiling Options
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-profiling-options">
        3.1.2. CPU Profiling Options
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#print-options">
        3.1.3. Print Options
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#io-options">
        3.1.4. IO Options
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiling-modes">
      3.2. Profiling Modes
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#summary-mode">
        3.2.1. Summary Mode
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-trace-and-api-trace-modes">
        3.2.2. GPU-Trace and API-Trace Modes
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#event-metric-summary-mode">
        3.2.3. Event/metric Summary Mode
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#event-metric-trace-mode">
        3.2.4. Event/metric Trace Mode
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiling-controls">
      3.3. Profiling Controls
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeout">
        3.3.1. Timeout
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#concurrent-kernels">
        3.3.2. Concurrent Kernels
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiling-scope">
        3.3.3. Profiling Scope
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#multiprocess-profiling">
        3.3.4. Multiprocess Profiling
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#system-profiling">
        3.3.5. System Profiling
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#unified-memory-profiling">
        3.3.6. Unified Memory Profiling
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-thread-tracing">
        3.3.7. CPU Thread Tracing
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#output">
      3.4. Output
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#adjust-units">
        3.4.1. Adjust Units
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#csv">
        3.4.2. CSV
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#export-import">
        3.4.3. Export/Import
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#demangling">
        3.4.4. Demangling
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#redirecting-output">
        3.4.5. Redirecting Output
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis">
        3.4.6. Dependency Analysis
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-sampling">
      3.5. CPU Sampling
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-sampling-limitations">
        3.5.1. CPU Sampling Limitations
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc">
      3.6. OpenACC
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc-options">
        3.6.1. OpenACC Options
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc-summary-modes">
        3.6.2. OpenACC Summary Modes
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp">
      3.7. OpenMP
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp-options">
        3.7.1. OpenMP Options
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#remote-profiling">
    4. Remote Profiling
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#remote-profiling-with-visual-profiler">
      4.1. Remote Profiling With Visual Profiler
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#one-hop-remote-profiling">
        4.1.1. One-hop remote profiling
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#remote-profiling-with-nvprof">
      4.2. Remote Profiling With
      <span class="pre">
       nvprof
      </span>
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#collect-data-on-remote-system">
        4.2.1. Collect Data On Remote System
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#view-and-analyze-data">
        4.2.2. View And Analyze Data
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvidia-tools-extension">
    5. NVIDIA Tools Extension
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-api-overview">
      5.1. NVTX API Overview
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-api-events">
      5.2. NVTX API Events
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-markers">
        5.2.1. NVTX Markers
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-range-start-stop">
        5.2.2. NVTX Range Start/Stop
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-range-push-pop">
        5.2.3. NVTX Range Push/Pop
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#event-attributes-structure">
        5.2.4. Event Attributes Structure
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-synchronization-markers">
        5.2.5. NVTX Synchronization Markers
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-domains">
      5.3. NVTX Domains
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-resource-naming">
      5.4. NVTX Resource Naming
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-string-registration">
      5.5. NVTX String Registration
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#mpi-profiling">
    6. MPI Profiling
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#automatic-mpi-annotation-with-nvtx">
      6.1. Automatic MPI Annotation with NVTX
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#manual-mpi-profiling">
      6.2. Manual MPI Profiling
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#further-reading">
      6.3. Further Reading
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#mps-profiling">
    7. MPS Profiling
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#mps-profiling-with-visual-profiler">
      7.1. MPS profiling with Visual Profiler
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#mps-profiling-with-nvprof">
      7.2. MPS profiling with nvprof
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#viewing-nvprof-mps-timeline-in-visual-profiler">
      7.3. Viewing nvprof MPS timeline in Visual Profiler
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#id1">
    8. Dependency Analysis
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#background">
      8.1. Background
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics">
      8.2. Metrics
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#support">
      8.3. Support
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#limitations">
      8.4. Limitations
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-reference">
    9. Metrics Reference
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-for-capability-5-x">
      9.1. Metrics for Capability 5.x
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-for-capability-6-x">
      9.2. Metrics for Capability 6.x
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-for-capability-7-x">
      9.3. Metrics for Capability 7.x
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#warp-state">
    10. Warp State
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#migrating-to-nsight-tools-from-visual-profiler-and-nvprof">
    11. Migrating to Nsight Tools from Visual Profiler and nvprof
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiler-known-issues">
    12. Profiler Known Issues
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#changelog">
    13. Changelog
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#notices">
    14. Notices
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#notice">
      14.1. Notice
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#opencl">
      14.2. OpenCL
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#trademarks">
      14.3. Trademarks
     </a>
    </li>
   </ul>
  </li>
 </ul>
 <a href="https://docs.nvidia.com/cuda/profiler-users-guide/contents.html">
  Profiler
 </a>
 <ul class="wy-breadcrumbs">
  <li>
   <a class="icon icon-home" href="https://docs.nvidia.com/cuda/index.html">
   </a>
   Â»
  </li>
  <li>
   <span class="section-number">
    1.
   </span>
   Preparing An Application For Profiling
  </li>
  <li class="wy-breadcrumbs-aside">
   <span>
    v12.5 |
   </span>
   <a class="reference external" href="https://docs.nvidia.com/cuda/pdf/CUDA_Profiler_Users_Guide.pdf">
    PDF
   </a>
   <span>
    |
   </span>
   <a class="reference external" href="https://developer.nvidia.com/cuda-toolkit-archive">
    Archive
   </a>
   <span>
    Â
   </span>
  </li>
 </ul>
 <p class="rubric-h1 rubric">
  Profiler Userâs Guide
 </p>
 <p>
  The user manual for NVIDIA profiling tools for optimizing performance of CUDA applications.
 </p>
 <p class="rubric-h1 rubric">
  Profiling Overview
 </p>
 <p>
  This document describes NVIDIA profiling tools that enable you to understand and optimize the performance of your CUDA, OpenACC or OpenMP applications. The
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual-profiler">
   Visual Profiler
  </a>
  is a graphical profiling tool that displays a timeline of your applicationâs CPU and GPU activity, and that includes an automated analysis engine to identify optimization opportunities. The
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof">
   nvprof
  </a>
  profiling tool enables you to collect and view profiling data from the command-line.
 </p>
 <p>
  Note that Visual Profiler and nvprof will be deprecated in a future CUDA release.
  The NVIDIA Volta platform is the last architecture on which these tools are fully supported. It is recommended to use next-generation tools
  <a class="reference external" href="https://developer.nvidia.com/nsight-systems">
   NVIDIA Nsight Systems
  </a>
  for GPU and CPU sampling and tracing and
  <a class="reference external" href="https://developer.nvidia.com/nsight-compute">
   NVIDIA Nsight Compute
  </a>
  for GPU kernel profiling.
 </p>
 <p>
  Refer the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#migrating-to-nsight-tools-from-visual-profiler-and-nvprof">
   Migrating to Nsight Tools from Visual Profiler and nvprof
  </a>
  section for more details.
 </p>
 <p>
  Terminology
 </p>
 <p>
  An
  event
  is a countable activity, action, or occurrence on a device. It corresponds to a single hardware counter value which is collected during kernel execution. To see a list of all available events on a particular NVIDIA GPU, type
  <span class="pre">
   nvprof
  </span>
  <span class="pre">
   --query-events
  </span>
  .
 </p>
 <p>
  A
  metric
  is a characteristic of an application that is calculated from one or more event values. To see a list of all available metrics on a particular NVIDIA GPU, type
  <span class="pre">
   nvprof
  </span>
  <span class="pre">
   --query-metrics
  </span>
  . You can also refer to the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-reference">
   metrics reference
  </a>
  .
 </p>
 <h1>
  <span class="section-number">
   1.
  </span>
  Preparing An Application For Profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#preparing-an-application-for-profiling" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  The CUDA profiling tools do not require any application changes to enable profiling; however, by making some simple modifications and additions, you can greatly increase the usability and effectiveness profiling. This section describes these modifications and how they can improve your profiling results.
 </p>
 <h2>
  <span class="section-number">
   1.1.
  </span>
  Focused Profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#focused-profiling" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  By default, the profiling tools collect profile data over the entire run of your application. But, as explained below, you typically only want to profile the region(s) of your application containing some or all of the performance-critical code. Limiting profiling to performance-critical regions reduces the amount of profile data that both you and the tools must process, and focuses attention on the code where optimization will result in the greatest performance gains.
 </p>
 <p>
  There are several common situations where profiling a region of the application is helpful.
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    The application is a test harness that contains a CUDA implementation of all or part of your algorithm. The test harness initializes the data, invokes the CUDA functions to perform the algorithm, and then checks the results for correctness. Using a test harness is a common and productive way to quickly iterate and test algorithm changes. When profiling, you want to collect profile data for the CUDA functions implementing the algorithm, but not for the test harness code that initializes the data or checks the results.
   </p>
  </li>
  <li>
   <p>
    The application operates in phases, where a different set of algorithms is active in each phase. When the performance of each phase of the application can be optimized independently of the others, you want to profile each phase separately to focus your optimization efforts.
   </p>
  </li>
  <li>
   <p>
    The application contains algorithms that operate over a large number of iterations, but the performance of the algorithm does not vary significantly across those iterations. In this case you can collect profile data from a subset of the iterations.
   </p>
  </li>
 </ol>
 <p>
  To limit profiling to a region of your application, CUDA provides functions to start and stop profile data collection.
  <span class="pre">
   cudaProfilerStart()
  </span>
  is used to start profiling and
  <span class="pre">
   cudaProfilerStop()
  </span>
  is used to stop profiling (using the CUDA driver API, you get the same functionality with
  <span class="pre">
   cuProfilerStart()
  </span>
  and
  <span class="pre">
   cuProfilerStop()
  </span>
  ). To use these functions you must include
  <span class="pre">
   cuda_profiler_api.h
  </span>
  (or
  <span class="pre">
   cudaProfiler.h
  </span>
  for the driver API).
 </p>
 <p>
  When using the start and stop functions, you also need to instruct the profiling tool to disable profiling at the start of the application. For
  <span class="pre">
   nvprof
  </span>
  you do this with the
  <span class="pre">
   --profile-from-start
  </span>
  <span class="pre">
   off
  </span>
  flag. For the Visual Profiler you use the Start execution with profiling enabled checkbox in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#settings-view">
   Settings View
  </a>
  .
 </p>
 <h2>
  <span class="section-number">
   1.2.
  </span>
  Marking Regions of CPU Activity
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#marking-regions-of-cpu-activity" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The Visual Profiler can collect a trace of the CUDA function calls made by your application. The Visual Profiler shows these calls in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
   Timeline View
  </a>
  , allowing you to see where each CPU thread in the application is invoking CUDA functions. To understand what the applicationâs CPU threads are doing outside of CUDA function calls, you can use the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvidia-tools-extension">
   NVIDIA Tools Extension API
  </a>
  (NVTX). When you add NVTX markers and ranges to your application, the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
   Timeline View
  </a>
  shows when your CPU threads are executing within those regions.
 </p>
 <p>
  <span class="pre">
   nvprof
  </span>
  also supports NVTX markers and ranges. Markers and ranges are shown in the API trace output in the timeline. In summary mode, each range is shown with CUDA activities associated with that range.
 </p>
 <h2>
  <span class="section-number">
   1.3.
  </span>
  Naming CPU and CUDA Resources
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#naming-cpu-and-cuda-resources" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The Visual Profiler
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
   Timeline View
  </a>
  shows default naming for CPU thread and GPU devices, context and streams. Using custom names for these resources can improve understanding of the application behavior, especially for CUDA applications that have many host threads, devices, contexts, or streams. You can use the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvidia-tools-extension">
   NVIDIA Tools Extension API
  </a>
  to assign custom names for your CPU and GPU resources. Your custom names will then be displayed in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
   Timeline View
  </a>
  .
 </p>
 <p>
  <span class="pre">
   nvprof
  </span>
  also supports NVTX naming. Names of CUDA devices, contexts and streams are displayed in summary and trace mode. Thread names are displayed in summary mode.
 </p>
 <h2>
  <span class="section-number">
   1.4.
  </span>
  Flush Profile Data
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#flush-profile-data" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To reduce profiling overhead, the profiling tools collect and record profile information into internal buffers. These buffers are then flushed asynchronously to disk with low priority to avoid perturbing application behavior. To avoid losing profile information that has not yet been flushed, the application being profiled should make sure, before exiting, that all GPU work is done (using CUDA synchronization calls), and then call
  <span class="pre">
   cudaProfilerStop()
  </span>
  or
  <span class="pre">
   cuProfilerStop()
  </span>
  . Doing so forces buffered profile information on corresponding context(s) to be flushed.
 </p>
 <p>
  If your CUDA application includes graphics that operate using a display or main loop, care must be taken to call
  <span class="pre">
   cudaProfilerStop()
  </span>
  or
  <span class="pre">
   cuProfilerStop()
  </span>
  before the thread executing that loop calls
  <span class="pre">
   exit()
  </span>
  . Failure to call one of these APIs may result in the loss of some or all of the collected profile data.
 </p>
 <p>
  For some graphics applications like the ones use OpenGL, the application exits when the escape key is pressed. In those cases where calling the above functions before exit is not feasible, use
  <span class="pre">
   nvprof
  </span>
  option
  <span class="pre">
   --timeout
  </span>
  or set the âExecution timeoutâ in the Visual Profiler. The profiler will force a data flush just before the timeout.
 </p>
 <h2>
  <span class="section-number">
   1.5.
  </span>
  Profiling CUDA Fortran Applications
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiling-cuda-fortran-applications" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA Fortran applications compiled with the PGI CUDA Fortran compiler can be profiled by nvprof and the Visual Profiler. In cases where the profiler needs source file and line information (kernel profile analysis, global memory access pattern analysis, divergent execution analysis, etc.), use the â-Mcuda=lineinfoâ option when compiling. This option is supported on Linux 64-bit targets in PGI 2019 version 19.1 or later.
 </p>
 <h1>
  <span class="section-number">
   2.
  </span>
  âVisual Profiler
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual-profiler" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  The NVIDIA Visual Profiler allows you to visualize and optimize the performance of your application. The Visual Profiler displays a timeline of your applicationâs activity on both the CPU and GPU so that you can identify opportunities for performance improvement. In addition, the Visual Profiler will analyze your application to detect potential performance bottlenecks and direct you on how to take action to eliminate or reduce those bottlenecks.
 </p>
 <p>
  The Visual Profiler is available as both a standalone application and as part of Nsight Eclipse Edition. The standalone version of the Visual Profiler,
  <span class="pre">
   nvvp
  </span>
  , is included in the CUDA Toolkit for all supported OSes except for macOS. Starting with the CUDA 11.0, Visual Profiler and nvprof donât support macOS as the target platform. However Visual Profiler supports remote profiling from the macOS host. This support is deprecated in the CUDA 12.5 release. Visual Profiler is provided in a separate installer package to maintain the remote profiling workflow for CUDA developers on macOS. See
  <a class="reference external" href="https://developer.nvidia.com/nvidia-cuda-toolkit-developer-tools-mac-hosts">
   Developer Tools for macOS
  </a>
  for download instructions. Within Nsight Eclipse Edition, the Visual Profiler is located in the Profile Perspective and is activated when an application is run in profile mode.
 </p>
 <h2>
  <span class="section-number">
   2.1.
  </span>
  Getting Started
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#getting-started" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This section describes steps you might take as you begin profiling.
 </p>
 <h3>
  <span class="section-number">
   2.1.1.
  </span>
  Setting up Java Runtime Environment
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#setting-up-java-runtime-environment" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Visual Profiler requires Java Runtime Environment (JRE) 1.8 to be available on the local system. However, starting with CUDA Toolkit version 10.1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes. The user must install the required version of JRE 1.8 in order to use Visual Profiler. See
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#installing-jre">
   Installing JRE
  </a>
  .
 </p>
 <ul>
  <li>
   <p>
    To run Visual Profiler on OpenSUSE15 or SLES15:
   </p>
   <ul>
    <li>
     <p>
      Make sure that you invoke Visual Profiler with the command-line option included as shown below:
     </p>
     <pre>nvvp -vm /usr/lib64/jvm/jre-1.8.0/bin/java
</pre>
     <p class="admonition-title">
      Note
     </p>
     <p>
      The
      <span class="pre">
       -vm
      </span>
      option is only required when JRE 1.8 is not in the default path.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    To run Visual Profiler on Ubuntu 18.04 or Ubuntu 18.10:
   </p>
   <ul>
    <li>
     <p>
      Make sure that you invoke Visual Profiler with the command-line option included as shown below:
     </p>
     <pre>nvvp -vm /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
</pre>
     <p class="admonition-title">
      Note
     </p>
     <p>
      The
      <span class="pre">
       -vm
      </span>
      option is only required when JRE 1.8 is not in the default path.
     </p>
    </li>
    <li>
     <p>
      On Ubuntu 18.10, if you get error â
      <span class="pre">
       no
      </span>
      <span class="pre">
       swt-pi-gtk
      </span>
      <span class="pre">
       in
      </span>
      <span class="pre">
       java.library.path
      </span>
      â when running Visual Profiler, then you need to install GTK2. Type the below command to install the required GTK2.
     </p>
     <pre>apt-get install libgtk2.0-0
</pre>
    </li>
   </ul>
  </li>
  <li>
   <p>
    To run Visual Profiler on Fedora 29:
   </p>
   <ul>
    <li>
     <p>
      Make sure that you invoke Visual Profiler with the command-line option included as shown below:
     </p>
     <pre>nvvp -vm /usr/bin/java
</pre>
     <p class="admonition-title">
      Note
     </p>
     <p>
      The
      <span class="pre">
       -vm
      </span>
      option is only required when JRE 1.8 is not in the default path.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    To run Visual Profiler on macOS:
   </p>
   <ul>
    <li>
     <p>
      Visual Profiler requires Java Runtime Environment (JRE) 1.8 update 151. Visual Profiler does not work with newer versions i.e. JRE 1.8 update 152 or later. Make sure that you invoke Visual Profiler with the command-line option included as shown below:
     </p>
     <pre>nvvp -vm /usr/bin/java
</pre>
     <p class="admonition-title">
      Note
     </p>
     <p>
      The
      <span class="pre">
       -vm
      </span>
      option is only required when JRE 1.8 update 151 is not in the default path.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    To run Visual Profiler on Windows:
   </p>
   <ul>
    <li>
     <p>
      Make sure that you invoke Visual Profiler with the command-line option included as shown below:
     </p>
     <pre>nvvp -vm "C:\Program Files\Java\jdk1.8.0_77\jre\bin\java"
</pre>
     <p class="admonition-title">
      Note
     </p>
     <p>
      The
      <span class="pre">
       -vm
      </span>
      option is only required when JRE 1.8 is not in the default path.
     </p>
    </li>
   </ul>
  </li>
 </ul>
 <h3>
  <span class="section-number">
   2.1.2.
  </span>
  Installing JRE
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#installing-jre" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Visual Profiler require Java Runtime Environment (JRE) 1.8 to be available on the local system. However, as of CUDA Toolkit version 10.1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes. The user must install JRE 1.8 in order to use Visual Profiler. See below for available options. Also see
  <a class="reference external" href="https://www.oracle.com/technetwork/java/javase/jdk8-naming-2157130.html">
   Java Platform, Standard Edition 8 Names and Versions.
  </a>
 </p>
 <p class="title sectiontitle rubric" id="windows">
  Windows
 </p>
 <ul class="simple">
  <li>
   <p>
    Oracle JRE 1.8 (may require paid updates)
   </p>
  </li>
  <li>
   <p>
    OpenJDK JRE 1.8
   </p>
  </li>
 </ul>
 <p class="title sectiontitle rubric" id="linux">
  Linux
 </p>
 <ul class="simple">
  <li>
   <p>
    Oracle JRE 1.8 (may require paid updates)
   </p>
  </li>
  <li>
   <p>
    OpenJDK JRE 1.8
   </p>
  </li>
 </ul>
 <p class="title sectiontitle rubric" id="mac">
  Mac
 </p>
 <ul>
  <li>
   <p>
    Oracle JRE 1.8 (may require paid updates)
   </p>
   <p class="admonition-title">
    Note
   </p>
   <p>
    JRE 1.8u152 or later is not supported for Visual Profiler. You can find the JRE update 151 on the Oracle Download Archive site here:
    <a class="reference external" href="https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html?printOnly=1">
     https://www.oracle.com/technetwork/java/javase/downloads/java-archive-javase8-2177648.html?printOnly=1
    </a>
    .
   </p>
  </li>
 </ul>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/profiler-users-guide/_images/jre8u151downloadpage.png">
 </a>
 <h3>
  <span class="section-number">
   2.1.3.
  </span>
  Modify Your Application For Profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#modify-your-application-for-profiling" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The Visual Profiler does not require any application changes; however, by making some simple modifications and additions, you can greatly increase its usability and effectiveness. Section
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#preparing-an-application-for-profiling">
   Preparing An Application For Profiling
  </a>
  describes how you can focus your profiling efforts and add extra annotations to your application that will greatly improve your profiling experience.
 </p>
 <h3>
  <span class="section-number">
   2.1.4.
  </span>
  Creating a Session
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#creating-a-session" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The first step in using the Visual Profiler to profile your application is to create a new profiling session. A session contains the settings, data, and results associated with your application. The
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#sessions">
   Sessions
  </a>
  section gives more information on working with sessions.
 </p>
 <p>
  You can create a new session by selecting the Profile An Application link on the Welcome page, or by selecting New Session from the File menu. In the Create New Session dialog enter the executable for your application. Optionally, you can also specify the working directory, arguments, multi-process profiling option and environment.
 </p>
 <p>
  The muti-process profiling options are:
 </p>
 <ul class="simple">
  <li>
   <p>
    Profile child processes - If selected, profile all processes launched by the specified application.
   </p>
  </li>
  <li>
   <p>
    Profile all processes - If selected, profile every CUDA process launched on the same system by the same user who launched nvprof. In this mode the Visual Profiler will launch nvprof and user needs to run his application in another terminal outside the Visual Profiler. User can exit this mode by pressing âCancelâ button on progress dialog in Visual Profiler to load the profile data
   </p>
  </li>
  <li>
   <p>
    Profile current process only - If selected, only profile specified application.
   </p>
  </li>
 </ul>
 <p>
  Press Next to choose some additional profiling options.
 </p>
 <p>
  CUDA options:
 </p>
 <ul class="simple">
  <li>
   <p>
    Start execution with profiling enabled - If selected profile data is collected from the start of application execution. If not selected profile data is not collected until
    <span class="pre">
     cudaProfilerStart()
    </span>
    is called in the application. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#focused-profiling">
     Focused Profiling
    </a>
    for more information about
    <span class="pre">
     cudaProfilerStart()
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    Enable concurrent kernel profiling - This option should be selected for an application that uses CUDA streams to launch kernels that can execute concurrently. If the application uses only a single stream (and therefore cannot have concurrent kernel execution), deselecting this option may decrease profiling overhead.
   </p>
  </li>
  <li>
   <p>
    Enable CUDA API tracing in the timeline - If selected, the CUDA driver and runtime API call trace is collected and displayed on timeline.
   </p>
  </li>
  <li>
   <p>
    Enable power, clock, and thermal profiling - If selected, power, clock, and thermal conditions on the GPUs will be sampled and displayed on the timeline. Collection of this data is not supported on all GPUs. See the description of the Device timeline in
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
     Timeline View
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    Enable unified memory profiling - If selected for the GPU that supports Unified Memory, the Unified Memory related memory traffic to and from each GPU is collected on your system and displayed on timeline.
   </p>
  </li>
  <li>
   <p>
    Replay application to collect events and metrics - If selected, the whole application is re-run instead of replaying each kernel, in order to collect all events/metrics.
   </p>
  </li>
  <li>
   <p>
    Run guided analysis - If selected, the guided analysis is run immediately after the creation of a new session. Uncheck this option to disable this behavior.
   </p>
  </li>
 </ul>
 <p>
  CPU (host) options:
 </p>
 <ul class="simple">
  <li>
   <p>
    Profile execution on the CPU - If selected the CPU threads are sampled and data collected about the CPU performance is shown in the
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-details-view">
     CPU Details View
    </a>
    .
   </p>
  </li>
  <li>
   <p>
    Enable OpenACC profiling - If selected and an OpenACC application is profiled, OpenACC activities will be recorded and displayed on a new OpenACC timeline. Collection of this data is only supported on Linux and PGI 19.1 or later. See the description of the OpenACC timeline in
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
     Timeline View
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    Enable CPU thread tracing - If enabled, selected CPU thread API calls will be recorded and displayed on a new thread API timeline. This currently includes the Pthread API, mutexes and condition variables. For performance reasons, only those API calls that influence concurrent execution are recorded and collection of this data is not supported on Windows. See the description of the thread timeline in
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
     Timeline View
    </a>
    for more information. This option should be selected for dependency analysis of applications with multiple CPU threads using CUDA.
   </p>
  </li>
 </ul>
 <p>
  Timeline Options:
 </p>
 <ul class="simple">
  <li>
   <p>
    Load data for time range - If selected the start and end time stamps for the range of data to be loaded can be specified. This option is useful to select a subset of a large data.
   </p>
  </li>
  <li>
   <p>
    Enable timelines in the session - By default all timelines are enabled. If a timeline is un-checked, the data associated with that timeline will not be loaded and it will not be displayed.
   </p>
  </li>
 </ul>
 <p class="admonition-title">
  Note
 </p>
 <p>
  If some timelines are disabled by un-checking the option the analyses results which use this timeline data will be incorrect.
 </p>
 <p>
  Press Finish.
 </p>
 <h3>
  <span class="section-number">
   2.1.5.
  </span>
  Analyzing Your Application
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#analyzing-your-application" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  If the Donât run guided analysis option was not selected when you created your session, the Visual Profiler will immediately run your application to collect the data needed for the first stage of guided analysis. As described in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#analysis-view">
   Analysis View
  </a>
  section, you can use the guided analysis system to get recommendations on performance limiting behavior in your application.
 </p>
 <h3>
  <span class="section-number">
   2.1.6.
  </span>
  Exploring the Timeline
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#exploring-the-timeline" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  In addition to the guided analysis results, you will see a timeline for your application showing the CPU and GPU activity that occurred as your application executed. Read
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
   Timeline View
  </a>
  and
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#properties-view">
   Properties View
  </a>
  to learn how to explore the profiling information that is available in the timeline.
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#navigating-the-timeline">
   Navigating the Timeline
  </a>
  describes how you can zoom and scroll the timeline to focus on specific areas of your application.
 </p>
 <h3>
  <span class="section-number">
   2.1.7.
  </span>
  Looking at the Details
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#looking-at-the-details" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  In addition to the results provided in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#analysis-view">
   Analysis View
  </a>
  , you can also look at the specific metric and event values collected as part of the analysis. Metric and event values are displayed in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-details-view">
   GPU Details View
  </a>
  . You can collect specific metric and event values that reveal how the kernels in your application are behaving. You collect metrics and events as described in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-details-view">
   GPU Details View
  </a>
  section.
 </p>
 <h3>
  <span class="section-number">
   2.1.8.
  </span>
  Improve Loading of Large Profiles
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#improve-loading-of-large-profiles" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Some applications launch many tiny kernels, making them prone to very large (100s of megabytes or larger) output, even for application runs of only a few seconds. The Visual Profiler needs roughly the same amount of memory as the size of the profile it is opening/importing. The Java virtual machine may use a fraction of the main memory if no âmax heap sizeâ setting is specified. So depending on the size of main memory, the Visual Profiler may fail to load some large files.
 </p>
 <p>
  If the Visual Profiler fails to load a large profile, try setting the max heap size that JVM is allowed to use according to main memory size. You can modify the config file
  <span class="pre">
   libnvvp/nvvp.ini
  </span>
  in the toolkit installation directory. On macOS the
  <span class="pre">
   nvvp.ini
  </span>
  file is present in folder
  <span class="pre">
   /Developer/{cuda_install_dir}/libnvvp/nvvp.app/Contents/MacOS/
  </span>
  . The
  <span class="pre">
   nvvp.ini
  </span>
  configuration file looks like this:
 </p>
 <pre>-startup
plugins/org.eclipse.equinox.launcher_1.3.0.v20140415-2008.jar
--launcher.library
plugins/org.eclipse.equinox.launcher.gtk.linux.x86_64_1.1.200.v20140603-1326
-data
@user.home/nvvp_workspace
-vm
../jre/bin/java
-vmargs
-Dorg.eclipse.swt.browser.DefaultType=mozilla
</pre>
 <p>
  To force the JVM to use 3 gigabytes of memory, for example, add a new line with
  <span class="pre">
   âXmx3G
  </span>
  after
  <span class="pre">
   âvmargs
  </span>
  . The -Xmx setting should be tailored to the available system memory and input size. For example, if your system has 24GB of system memory, and you happen to know that you wonât need to run any other memory-intensive applications at the same time as the Visual Profiler, so itâs okay for the profiler to take up the vast majority of that space. So you might pick, say, 22GB as the maximum heap size, leaving a few gigabytes for the OS, GUI, and any other programs that might be running.
 </p>
 <p>
  Some other
  <span class="pre">
   nvvp.ini
  </span>
  configuration settings can also be modified:
 </p>
 <ul class="simple">
  <li>
   <p>
    Increase the default heap size (the one Java automatically starts up with) to, say, 2GB. (
    <span class="pre">
     -Xms
    </span>
    )
   </p>
  </li>
  <li>
   <p>
    Tell Java to run in 64-bit mode instead of the default 32-bit mode (only works on 64-bit systems); this is required if you want heap sizes &gt;4GB. (
    <span class="pre">
     -d64
    </span>
    )
   </p>
  </li>
  <li>
   <p>
    Enable Javas parallel garbage collection system, which helps both to decrease the required memory space for a given input size as well as to catch out of memory errors more gracefully. (
    <span class="pre">
     -XX:+UseConcMarkSweepGC
    </span>
    <span class="pre">
     -XX:+CMSIncrementalMode
    </span>
    )
   </p>
  </li>
 </ul>
 <p>
  Note: most installations require administrator/root-level access to modify this file.
 </p>
 <p>
  The modified
  <span class="pre">
   nvvp.ini
  </span>
  file as per examples given above is as follows:
 </p>
 <pre>-data
@user.home/nvvp_workspace
-vm
../jre/bin/java
-d64
-vmargs
-Xms2g
-Xmx22g
-XX:+UseConcMarkSweepGC
-XX:+CMSIncrementalMode
-Dorg.eclipse.swt.browser.DefaultType=Mozilla
</pre>
 <p>
  For more details on JVM settings, consult the Java virtual machine manual.
 </p>
 <p>
  In addition to this you can use timeline options Load data for time range and Enable timelines in the session mentioned in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#creating-a-session">
   Creating a Session
  </a>
  section to limit the data which is loaded and displayed.
 </p>
 <h2>
  <span class="section-number">
   2.2.
  </span>
  Sessions
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#sessions" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  A session contains the settings, data, and profiling results associated with your application. Each session is saved in a separate file; so you can delete, move, copy, or share a session by simply deleting, moving, copying, or sharing the session file. By convention, the file extension
  <span class="pre">
   .nvvp
  </span>
  is used for Visual Profiler session files.
 </p>
 <p>
  There are two types of sessions: an executable session that is associated with an application that is executed and profiled from within the Visual Profiler, and an import session that is created by importing data generated by
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof">
   nvprof
  </a>
  .
 </p>
 <h3>
  <span class="section-number">
   2.2.1.
  </span>
  Executable Session
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#executable-session" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  You can create a new executable session for your application by selecting the Profile An Application link on the Welcome page, or by selecting New Session from the File menu. Once a session is created, you can edit the sessionâs settings as described in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#settings-view">
   Settings View
  </a>
  .
 </p>
 <p>
  You can open and save existing sessions using the open and save options in the File menu.
 </p>
 <p>
  To analyze your application and to collect metric and event values, the Visual Profiler will execute your application multiple times. To get accurate profiling results, it is important that your application conform to the requirements detailed in
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#application-requirements">
   Application Requirements
  </a>
  .
 </p>
 <h3>
  <span class="section-number">
   2.2.2.
  </span>
  Import Session
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-session" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  You create an import session from the output of nvprof by using the Importâ¦ option in the File menu. Selecting this option opens the import dialog which guides you through the import process.
 </p>
 <p>
  Because an executable application is not associated with an import session, the Visual Profiler cannot execute the application to collect additional profile data. As a result, analysis can only be performed with the data that is imported. Also, the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-details-view">
   GPU Details View
  </a>
  will show any imported event and metrics values but new metrics and events cannot be selected and collected for the import session.
 </p>
 <h4>
  <span class="section-number">
   2.2.2.1.
  </span>
  Import Single-Process
  <span class="pre">
   nvprof
  </span>
  Session
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-single-process-nvprof-session" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Using the import dialog you can select one or more
  <span class="pre">
   nvprof
  </span>
  data files for import into the new session.
 </p>
 <p>
  You must have one
  <span class="pre">
   nvprof
  </span>
  data file that contains the timeline information for the session. This data file should be collected by running nvprof with the
  <span class="pre">
   --export-profile
  </span>
  option. You can optionally enable other options such as
  <span class="pre">
   --system-profiling
  </span>
  <span class="pre">
   on
  </span>
  , but you should not collect any events or metrics as that will distort the timeline so that it is not representative of the applications true behavior.
 </p>
 <p>
  You may optionally specify one or more event/metric data files that contain event and metric values for the application. These data files should be collected by running nvprof with one or both of the
  <span class="pre">
   --events
  </span>
  and
  <span class="pre">
   --metrics
  </span>
  options. To collect all the events and metrics that are needed for the analysis system, you can simply use the
  <span class="pre">
   --analysis-metrics
  </span>
  option along with the
  <span class="pre">
   --kernels
  </span>
  option to select the kernel(s) to collect events and metrics for. See
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#remote-profiling">
   Remote Profiling
  </a>
  for more information.
 </p>
 <p>
  If you are importing multiple
  <span class="pre">
   nvprof
  </span>
  output files into the session, it is important that your application conform to the requirements detailed in
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#application-requirements">
   Application Requirements
  </a>
  .
 </p>
 <h4>
  <span class="section-number">
   2.2.2.2.
  </span>
  Import Multi-Process
  <span class="pre">
   nvprof
  </span>
  Session
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-multi-process-nvprof-session" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Using the import wizard you can select multiple
  <span class="pre">
   nvprof
  </span>
  data files for import into the new multi-process session.
 </p>
 <p>
  Each
  <span class="pre">
   nvprof
  </span>
  data file must contain the timeline information for one of the processes. This data file should be collected by running nvprof with the
  <span class="pre">
   --export-profile
  </span>
  option. You can optionally enable other options such as
  <span class="pre">
   --system-profiling
  </span>
  <span class="pre">
   on
  </span>
  , but you should not collect any events or metrics as that will distort the timeline so that it is not representative of the applications true behavior.
 </p>
 <p>
  Select the Multiple Processes option in the Import nvprof Data dialog as shown in the figure below.
 </p>
 <p>
  When importing timeline data from multiple processes you may not specify any event/metric data files for those processes. Multi-processes profiling is only supported for timeline data.
 </p>
 <h4>
  <span class="section-number">
   2.2.2.3.
  </span>
  Import Command-Line Profiler Session
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-command-line-profiler-session" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Support for command-line profiler (using the environment variable COMPUTE_PROFILE) has been dropped, but CSV files generated using earlier versions can still be imported.
 </p>
 <p>
  Using the import wizard you can select one or more command-line profiler generated CSV files for import into the new session. When you import multiple CSV files, their contents are combined and displayed in a single timeline.
 </p>
 <p>
  The command-line profiler CSV file must be generated with the gpustarttimestamp and streamid configuration parameters. It is fine to include other configuration parameters, including events.
 </p>
 <h2>
  <span class="section-number">
   2.3.
  </span>
  Application Requirements
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#application-requirements" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To collect performance data about your application, the Visual Profiler must be able to execute your application repeatedly in a deterministic manner. Due to software and hardware limitations, it is not possible to collect all the necessary profile data in a single execution of your application. Each time your application is run, it must operate on the same data and perform the same kernel and memory copy invocations in the same order. Specifically,
 </p>
 <ul class="simple">
  <li>
   <p>
    For a device, the order of context creation must be the same each time the application executes. For a multi-threaded application where each thread creates its own context(s), care must be taken to ensure that the order of those context creations is consistent across multiple runs. For example, it may be necessary to create the contexts on a single thread and then pass the contexts to the other threads. Alternatively, the
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvidia-tools-extension">
     NVIDIA Tools Extension API
    </a>
    can be used to provide a custom name for each context. As long as the same custom name is applied to the same context on each execution of the application, the Visual Profiler will be able to correctly associate those contexts across multiple runs.
   </p>
  </li>
  <li>
   <p>
    For a context, the order of stream creation must be the same each time the application executes. Alternatively, the
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvidia-tools-extension">
     NVIDIA Tools Extension API
    </a>
    can be used to provide a custom name for each stream. As long as the same custom name is applied to the same stream on each execution of the application, the Visual Profiler will be able to correctly associate those streams across multiple runs.
   </p>
  </li>
  <li>
   <p>
    Within a stream, the order of kernel and memcpy invocations must be the same each time the application executes.
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   2.4.
  </span>
  Visual Profiler Views
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual-profiler-views" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The Visual Profiler is organized into views. Together, the views allow you to analyze and visualize the performance of your application. This section describes each view and how you use it while profiling your application.
 </p>
 <h3>
  <span class="section-number">
   2.4.1.
  </span>
  Timeline View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The Timeline View shows CPU and GPU activity that occurred while your application was being profiled. Multiple timelines can be opened in the Visual Profiler at the same time in different tabs. The following figure shows a Timeline View for a CUDA application.
 </p>
 <p>
  Along the top of the view is a horizontal ruler that shows elapsed time from the start of application profiling. Along the left of the view is a vertical ruler that describes what is being shown for each horizontal row of the timeline, and that contains various controls for the timeline. These controls are described in
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-controls">
   Timeline Controls
  </a>
 </p>
 <p>
  The timeline view is composed of timeline rows. Each row shows intervals that represent the start and end times of the activities that correspond to the type of the row. For example, timeline rows representing kernels have intervals representing the start and end times of executions of that kernel. In some cases (as noted below) a timeline row can display multiple sub-rows of activity. Sub-rows are used when there is overlapping activity. These sub-rows are created dynamically as necessary depending on how much activity overlap there is. The placement of intervals within certain sub-rows does not convey any particular meaning. Intervals are just packed into sub-rows using a heuristic that attempts to minimize the number of needed sub-rows. The height of the sub-rows is scaled to keep vertical space reasonable.
 </p>
 <p>
  The types of timeline rows that are displayed in the Timeline View are:
 </p>
 Process
 <p>
  A timeline will contain a Process row for each application profiled. The process identifier represents the pid of the process. The timeline row for a process does not contain any intervals of activity. Threads within the process are shown as children of the process.
 </p>
 Thread
 <p>
  A timeline will contain a Thread row for each CPU thread in the profiled application that performed either a CUDA driver or CUDA runtime API call. The thread identifier is a unique id for that CPU thread. The timeline row for a thread is does not contain any intervals of activity.
 </p>
 Runtime API
 <p>
  A timeline will contain a Runtime API row for each CPU thread that performs a CUDA Runtime API call. Each interval in the row represents the duration of the call on the corresponding thread.
 </p>
 Driver API
 <p>
  A timeline will contain a Driver API row for each CPU thread that performs a CUDA Driver API call. Each interval in the row represents the duration of the call on the corresponding thread.
 </p>
 OpenACC
 <p>
  A timeline will contain one or multiple OpenACC rows for each CPU thread that calls OpenACC directives. Each interval in the row represents the duration of the call on the corresponding thread. Each OpenACC timeline may consist of multiple rows. Within one timeline, OpenACC activities on rows further down are called from within activities on the rows above.
 </p>
 OpenMP
 <p>
  A timeline will contain one OpenMP row for each CPU thread that calls OpenMP. Each interval in the row represents how long the application spends in a given OpenMP region or state. The application may be in multiple states at the same time, this is shown by drawing multiple rows where some intervals overlap.
 </p>
 Pthread
 <p>
  A timeline will contain one Pthread row for each CPU thread that performs Pthread API calls, given that host thread API calls have been recorded during measurement. Each interval in the row represents the duration of the call. Note that for performance reasons, only selected Pthread API calls may have been recorded.
 </p>
 Markers and Ranges
 <p>
  A timeline will contain a single Markers and Ranges row for each CPU thread that uses the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvidia-tools-extension">
   NVIDIA Tools Extension API
  </a>
  to annotate a time range or marker. Each interval in the row represents the duration of a time range, or the instantaneous point of a marker. This row will have sub-rows if there are overlapping ranges.
 </p>
 Profiling Overhead
 <p>
  A timeline will contain a single Profiling Overhead row for each process. Each interval in the row represents the duration of execution of some activity required for profiling. These intervals represent activity that does not occur when the application is not being profiled.
 </p>
 Device
 <p>
  A timeline will contain a Device row for each GPU device utilized by the application being profiled. The name of the timeline row indicates the device ID in square brackets followed by the name of the device. After running the Compute Utilization analysis, the row will contain an estimate of the compute utilization of the device over time. If power, clock, and thermal profiling are enabled, the row will also contain points representing those readings.
 </p>
 Unified Memory
 <p>
  A timeline will contain a Unified Memory row for each CPU thread and device that uses unified memory. The Unified memory may contain CPU Page Faults, GPU Page Faults, Data Migration (DtoH) and Data Migration (HtoD) rows. When creating a session user can select segment mode or non-segment mode for Unified Memory timelines. In the segment mode the timeline is split into equal width segments and only aggregated data values for each time segment are shown. The number of segments can be changed. In non-segment mode each interval on the timeline will represent the actual data collected and the properties for each interval can be viewed. The segments are colored using a heat-map color scheme. Under properties for the timeline the property which is used for selecting the color is given and also a legend displays the mapping of colors to different range of property values.
 </p>
 CPU Page Faults
 <p>
  This will contain a CPU Page Faults row for each CPU thread. In the non-segment mode each interval on the timeline corresponds to one CPU page fault.
 </p>
 Data Migration (DtoH)
 <p>
  A timeline will contain Data Migration (DtoH) row for each device. In the non-segment mode each interval on the timeline corresponds to one data migration from device to host.
 </p>
 GPU Page Faults
 <p>
  A timeline will contain GPU Page Faults. row for each CPU thread. In the non-segment mode each interval on the timeline corresponds to one GPU page fault group.
 </p>
 Data Migration (DtoH)
 <p>
  A timeline will contain Data Migration (HtoD) row for each device. In the non-segment mode each interval on the timeline corresponds to one data migration from host to device.
 </p>
 Context
 <p>
  A timeline will contains a Context row for each CUDA context on a GPU device. The name of the timeline row indicates the context ID or the custom context name if the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvidia-tools-extension">
   NVIDIA Tools Extension API
  </a>
  was used to name the context. The row for a context does not contain any intervals of activity.
 </p>
 Memcpy
 <p>
  A timeline will contain memory copy row(s) for each context that performs memcpys. A context may contain up to four memcpy rows for device-to-host, host-to-device, device-to-device, and peer-to-peer memory copies. Each interval in a row represents the duration of a memcpy executing on the GPU.
 </p>
 Compute
 <p>
  A timeline will contain a Compute row for each context that performs computation on the GPU. Each interval in a row represents the duration of a kernel on the GPU device. The Compute row indicates all the compute activity for the context. Sub-rows are used when concurrent kernels are executed on the context. All kernel activity, including kernels launched using CUDA Dynamic Parallelism, is shown on the Compute row. The Kernel rows following the Compute row show activity of each individual application kernel.
 </p>
 Kernel
 <p>
  A timeline will contain a Kernel row for each kernel executed by the application. Each interval in a row represents the duration of execution of an instance of that kernel in the containing context. Each row is labeled with a percentage that indicates the total execution time of all instances of that kernel compared to the total execution time of all kernels. For each context, the kernels are ordered top to bottom by this execution time percentage. Sub-rows are used to show concurrent kernel execution. For CUDA Dynamic Parallelism applications, the kernels are organized in a hierarchy that represents the parent/child relationship between the kernels. Host-launched kernels are shown as direct children of the Context row. Kernels that use CUDA Dynamic Parallelism to launch other kernels can be expanded using the â+â icon to show the kernel rows representing those child kernels. For kernels that donât launch child kernels, the kernel execution is represented by a solid interval, showing the time that that instance of the kernel was executing on the GPU. For kernels that launch child kernels, the interval can also include a hollow part at the end. The hollow part represents the time after the kernel has finished executing where it is waiting for child kernels to finish executing. The CUDA Dynamic Parallelism execution model requires that a parent kernel not complete until all child kernels complete and this is what the hollow part is showing. The Focus control described in
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-controls">
   Timeline Controls
  </a>
  can be used to control display of the parent/child timelines.
 </p>
 Stream
 <p>
  A timeline will contain a Stream row for each stream used by the application (including both the default stream and any application created streams). Each interval in a Stream row represents the duration of a memcpy or kernel execution performed on that stream.
 </p>
 <h4>
  <span class="section-number">
   2.4.1.1.
  </span>
  Timeline Controls
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-controls" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  The
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
   Timeline View
  </a>
  has several controls that you use to control how the timeline is displayed. Some of these controls also influence the presentation of data in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-details-view">
   GPU Details View
  </a>
  and the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#analysis-view">
   Analysis View
  </a>
  .
 </p>
 <p>
  Resizing the Vertical Timeline Ruler
 </p>
 <p>
  The width of the vertical ruler can be adjusted by placing the mouse pointer over the right edge of the ruler. When the double arrow pointer appears, click and hold the left mouse button while dragging. The vertical ruler width is saved with your session.
 </p>
 <p>
  Reordering Timelines
 </p>
 <p>
  The Kernel and Stream timeline rows can be reordered. You may want to reorder these rows to aid in visualizing related kernels and streams, or to move unimportant kernels and streams to the bottom of the timeline. To reorder a row, left-click and hold onto the row label. When the double arrow pointer appears, drag up or down to position the row. The timeline ordering is saved with your session.
 </p>
 <p>
  Filtering Timelines
 </p>
 <p>
  Memcpy and Kernel rows can be filtered to exclude their activities from presentation in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-details-view">
   GPU Details View
  </a>
  and the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#analysis-view">
   Analysis View
  </a>
  . To filter out a row, left-click on the filter icon just to the left of the row label. To filter all Kernel or Memcpy rows, Shift-left-click one of the rows. When a row is filtered, any intervals on that row are dimmed to indicate their filtered status.
 </p>
 <p>
  Expanding and Collapsing Timelines
 </p>
 <p>
  Groups of timeline rows can be expanded and collapsed using the [+] and [-] controls just to the left of the row labels. There are three expand/collapse states:
 </p>
 Collapsed
 <p>
  No timeline rows contained in the collapsed row are shown.
 </p>
 Expanded
 <p>
  All non-filtered timeline rows are shown.
 </p>
 All-Expanded
 <p>
  All timeline rows, filtered and non-filtered, are shown.
 </p>
 <p>
  Intervals associated with collapsed rows may not be shown in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-details-view">
   GPU Details View
  </a>
  and the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#analysis-view">
   Analysis View
  </a>
  , depending on the filtering mode set for those views (see view documentation for more information). For example, if you collapse a device row, then all memcpys, memsets, and kernels associated with that device are excluded from the results shown in those views.
 </p>
 <p>
  Coloring Timelines
 </p>
 <p>
  There are three modes for timeline coloring. The coloring mode can be selected in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar. In kernel coloring mode, each type of kernel is assigned a unique color (that is, all activity intervals in a kernel row have the same color). In stream coloring mode, each stream is assigned a unique color (that is, all memcpy and kernel activity occurring on a stream are assigned the same color). In process coloring mode, each process is assigned a unique color (that is, all memcpy and kernel activity occurring in a process are assigned the same color).
 </p>
 <p>
  Focusing Kernel Timelines
 </p>
 <p>
  For applications using CUDA Dynamic Parallelism, the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
   Timeline View
  </a>
  displays a hierarchy of kernel activity that shows the parent/child relationship between kernels. By default all parent/child relationships are shown simultaneously. The focus timeline control can be used to focus the displayed parent/child relationships to a specific, limited set of âfamily treesâ. The focus timeline mode can be selected and deselected in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar.
 </p>
 <p>
  To see the âfamily treeâ of a particular kernel, select a kernel and then enable Focus mode. All kernels except those that are ancestors or descendants of the selected kernel will be hidden. Ctrl-select can be used to select multiple kernels before enabling Focus mode. Use the âDonât Focusâ option to disable focus mode and restore all kernels to the Timeline view.
 </p>
 <p>
  Dependency Analysis Controls
 </p>
 <p>
  There are two modes for visualizing dependency analysis results in the timeline: Focus Critical Path and Highlight Execution Dependencies. These modes can be selected in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the Visual Profiler toolbar.
 </p>
 <p>
  These options become available after the Dependency Analysis application analysis stage has been run (see
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#unguided-application-analysis">
   Unguided Application Analysis
  </a>
  ). A detailed explanation of these modes is given in
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis-controls">
   Dependency Analysis Controls
  </a>
 </p>
 <h4>
  <span class="section-number">
   2.4.1.2.
  </span>
  Navigating the Timeline
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#navigating-the-timeline" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  The timeline can be scrolled, zoomed, and focused in several ways to help you better understand and visualize your applicationâs performance.
 </p>
 <p>
  Zooming
 </p>
 <p>
  The zoom controls are available in the View menu, in the timeline context menu (accessed by right-clicking in the timeline view), and on the profiler toolbar. Zoom-in reduces the timespan displayed in the view, zoom-out increases the timespan displayed in the view, and zoom-to-fit scales the view so that the entire timeline is visible.
 </p>
 <p>
  You can also zoom-in and zoom-out with the mouse wheel while holding the Ctrl key (for macOS use the Command key).
 </p>
 <p>
  Another useful zoom mode is zoom-to-region. Select a region of the timeline by holding Ctrl (for macOS use the Command key) while left-clicking and dragging the mouse. The highlighted region will be expanded to occupy the entire view when the mouse button is released.
 </p>
 <p>
  Scrolling
 </p>
 <p>
  The timeline can be scrolled vertically with the scrollbar of the mouse wheel. The timeline can be scrolled horizontally with the scrollbar or by using the mouse wheel while holding the Shift key.
 </p>
 <p>
  Highlighting/Correlation
 </p>
 <p>
  When you move the mouse pointer over an activity interval on the timeline, that interval is highlighted in all places where the corresponding activity is shown. For example, if you move the mouse pointer over an interval representing a kernel execution, that kernel execution is also highlighted in the Stream and in the Compute timeline row. When a kernel or memcpy interval is highlighted, the corresponding driver or runtime API interval will also highlight. This allows you to see the correlation between the invocation of a driver or runtime API or OpenACC directive on the CPU and the corresponding activity on the GPU. Information about the highlighted interval is shown in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#properties-view">
   Properties View
  </a>
  .
 </p>
 <p>
  Selecting
 </p>
 <p>
  You can left-click on a timeline interval or row to select it. Multi-select is done using Ctrl-left-click. To unselect an interval or row simply Ctrl-left-click on it again. When a single interval or row is selected, the information about that interval or row is pinned in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#properties-view">
   Properties View
  </a>
  . In the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-details-view">
   GPU Details View
  </a>
  , the detailed information for the selected interval is shown in the table.
 </p>
 <p>
  Measuring Time Deltas
 </p>
 <p>
  Measurement rulers can be created by left-click dragging in the horizontal ruler at the top of the timeline. Once a ruler is created it can be activated and deactivated by left-clicking. Multiple rulers can be activated by Ctrl-left-click. Any number of rulers can be created. Active rulers are deleted with the Delete or Backspace keys. After a ruler is created, it can be resized by dragging the vertical guide lines that appear over the timeline. If the mouse is dragged over a timeline interval, the guideline will snap to the nearest edge of that interval.
 </p>
 <h4>
  <span class="section-number">
   2.4.1.3.
  </span>
  Timeline Refreshing
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-refreshing" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  The profiler loads the timeline gradually as it reads the data. This is more apparent if the data file being loaded is big, or the application has generated a lot of data. In such cases, the timeline may be partially rendered. At the same time, a spinning circle replaces the icon of the current session tab, indicating the timeline is not fully loaded. Loading is finished when the icon changes back.
 </p>
 <p>
  To reduce its memory footprint, the profiler may skip loading some timeline contents if they are not visible at the current zoom level. These contents will be automatically loaded when they become visible on a new zoom level.
 </p>
 <h4>
  <span class="section-number">
   2.4.1.4.
  </span>
  Dependency Analysis Controls
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis-controls" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  The profiler allows the visualization of dependency analysis results in the timeline once the respective analysis stage has been run. For a detailed description on how dependency analysis works, see
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis">
   Dependency Analysis
  </a>
  .
 </p>
 <p>
  Focus Critical Path visualizes the critical path through the application by focusing on all intervals on the critical path and fading others. When the mode is enabled and any timeline interval is selected (by left-clicking it), the selected interval will have focus. However, the critical path will still be visible as hollow intervals. This allows you to âfollowâ the critical path through the execution and to inspect individual intervals.
 </p>
 <p>
  Highlight Execution Dependencies allows you to analyze the execution dependencies for each interval (Note that for certain intervals, no dependency information is collected). When this mode is enabled, the highlighting color changes from yellow (representing correlated intervals) to red (representing dependencies). Both the selected interval as well as all incoming and outgoing dependencies are highlighted.
 </p>
 <h3>
  <span class="section-number">
   2.4.2.
  </span>
  Analysis View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#analysis-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The Analysis View is used to control application analysis and to display the analysis results. There are two analysis modes: guided and unguided. In guided mode the analysis system will guide you through multiple analysis stages to help you understand the likely performance limiters and optimization opportunities in your application. In unguided mode you can manually explore all the analysis results collected for your application. The following figure shows the analysis view in guided analysis mode. The left part of the view provides step-by-step directions to help you analyze and optimize your application. The right part of the view shows detailed analysis results appropriate for each part of the analysis.
 </p>
 <h4>
  <span class="section-number">
   2.4.2.1.
  </span>
  Guided Application Analysis
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#guided-application-analysis" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  In guided mode, the analysis view will guide you step-by-step through analysis of your entire application with specific analysis guidance provided for each kernel within your application. Guided analysis starts with CUDA Application Analysis and from there will guide you to optimization opportunities within your application.
 </p>
 <h4>
  <span class="section-number">
   2.4.2.2.
  </span>
  Unguided Application Analysis
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#unguided-application-analysis" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  In unguided analysis mode each application analysis stage has a Run analysis button that can be used to generate the analysis results for that stage. When the Run analysis button is selected, the profiler will execute the application to collect the profiling data needed to perform the analysis. The green check-mark next to an analysis stage indicates that the analysis results for that stage are available. Each analysis result contains a brief description of the analysis and a Moreâ¦ link to detailed documentation on the analysis. When you select an analysis result, the timeline rows or intervals associated with that result are highlighted in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
   Timeline View
  </a>
  .
 </p>
 <p>
  When a single kernel instance is selected in the timeline, additional kernel-specific analysis stages are available. Each kernel-specific analysis stage has a Run analysis button that operates in the same manner as for the application analysis stages. The following figure shows the analysis results for the Divergent Execution analysis stage. Some kernel instance analysis results, like Divergent Execution, are associated with specific source-lines within the kernel. To see the source associated with each result, select an entry from the table. The source file associated with that entry will open.
 </p>
 <h4>
  <span class="section-number">
   2.4.2.3.
  </span>
  PC Sampling View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#pc-sampling-view" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Devices with compute capability 5.2 and higher, excluding mobile devices, have a feature for PC sampling. In this feature PC and state of warp are sampled at regular interval for one of the active warps per SM. The warp state indicates if that warp issued an instruction in a cycle or why it was stalled and could not issue an instruction. When a warp that is sampled is stalled, there is a possibility that in the same cycle some other warp is issuing an instruction. Hence the stall for the sampled warp need not necessarily indicate that there is a hole in the instruction issue pipeline. Refer to the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#warp-state">
   Warp State
  </a>
  section for a description of different states.
 </p>
 <p>
  Devices with compute capability 6.0 and higher have a new feature that gives latency reasons. The latency samples indicate the reasons for holes in the issue pipeline. While collecting these samples, there is no instruction issued in the respective warp scheduler and hence these give the latency reasons. The latency reasons will be one of the stall reasons in
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#warp-state">
   Warp State
  </a>
  section except ânot selectedâ stall reason.
 </p>
 <p>
  The profiler collects this information and presents it in the Kernel Profile - PC Sampling view. In this view, the sample distribution for all functions and kernels is given in a table. A pie chart shows the distribution of stall reasons collected for each kernel. After clicking on the source file or device function the Kernel Profile - PC Sampling view is opened. The hotspots shown next to the vertical scroll bar are determined by the number of samples collected for each source and assembly line. The distribution of the stall reasons is shown as a stacked bar for each source and assembly line. This helps in pinpointing the latency reasons at the source code level.
 </p>
 <p>
  For devices with compute capability 6.0 and higher, Visual Profiler show two views: âKernel Profile - PC Samplingâ which gives the warp state view and âKernel Profile - PC Sampling - Latencyâ which gives the latency reasons. Hotspots can be seleted to point to hotspot of âWarp Stateâ or âLatency Reasonsâ. The tables in result section give percentage distribution for total latency samples, issue pipeline busy samples and instruction issued samples.
 </p>
 <p>
  The blog post
  <a class="reference external" href="https://devblogs.nvidia.com/parallelforall/cuda-7-5-pinpoint-performance-problems-instruction-level-profiling">
   Pinpoint Performance Problems with Instruction-Level Profiling
  </a>
  shows how PC Sampling can be used to optimize a CUDA kernel.
 </p>
 <h4>
  <span class="section-number">
   2.4.2.4.
  </span>
  Memory Statistics
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#memory-statistics" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Devices with compute capability 5.0 and higher have a feature to show usage of the memory sub-system during kernel execution. The chart shows a summary view of the memory hierarchy of the CUDA programming model. The green nodes in the diagram depict logical memory space whereas blue nodes depicts actual hardware unit on the chip. For the various caches the reported percentage number states the cache hit rate; that is the ratio of requests that could be served with data locally available to the cache over all requests made.
 </p>
 <p>
  The links between the nodes in the diagram depict the data paths between the SMs to the memory spaces into the memory system. Different metrics are shown per data path. The data paths from the SMs to the memory spaces (Global, Local, Texture, Surface and Shared) report the total number of memory instructions executed, it includes both read and write operations. The data path between memory spaces and âUnified Cacheâ or âShared Memoryâ reports the total amount of memory requests made. All other data paths report the total amount of transferred memory in bytes. The arrow pointing to right direction indicates WRITE operation whereas the arrow pointing to left direction indicates the READ operations.
 </p>
 <h4>
  <span class="section-number">
   2.4.2.5.
  </span>
  NVLink view
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvlink-view" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  NVIDIA NVLink is a high-bandwidth, energy-efficient interconnect that enables fast communication between the CPU and GPU, and between GPUs.
 </p>
 <p>
  Visual Profiler collects NVLink topology and NVLink transmit/receive throughput metrics and maps the metrics on to the topology. The topology is collected by default along with the timeline. Throughput/ utilization metrics are generated only when NVLink option is chosen.
 </p>
 <p>
  NVLink information is presented in the Results section of Examine GPU Usage in CUDA Application Analysis in Guided Analysis. NVLink Analysis shows topology that shows the logical NVLink connections between different devices. A logical link comprises of 1 to 4 physical NVLinks of same properties connected between two devices. Visual profiler lists the properties and achieved utilization for logical NVLinks in âLogical NVLink Propertiesâ table. It also lists the transmit and receive throughputs for logical NVLink in âLogical NVLink Throughputâ table.
 </p>
 <h3>
  <span class="section-number">
   2.4.3.
  </span>
  Source-Disassembly View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#source-disassembly-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The Source-Disassembly View is used to display the analysis results for a kernel at the source and assembly instruction level. To be able to view the kernel source you need to compile the code using the
  <span class="pre">
   -lineinfo
  </span>
  option. If this compiler option is not used, only the disassembly view will be shown.
 </p>
 <p>
  This view is displayed for the following types of analysis:
 </p>
 <ul class="simple">
  <li>
   <p>
    Global Memory Access Pattern Analysis
   </p>
  </li>
  <li>
   <p>
    Shared Memory Access Pattern Analysis
   </p>
  </li>
  <li>
   <p>
    Divergent Execution Analysis
   </p>
  </li>
  <li>
   <p>
    Kernel Profile - Instruction Execution Analysis
   </p>
  </li>
  <li>
   <p>
    Kernel Profile - PC Sampling Analysis
   </p>
  </li>
 </ul>
 <p>
  As part of the Guided Analysis or Unguided Analysis for a kernel the analysis results are displayed under the Analysis view. After clicking on the source file or device function the Source-Disassembly view is opened. If the source file is not found a dialog is opened to select and point to the new location of the source file. This can happen for example when the profiling is done on a different system.
 </p>
 <p>
  The Source-Disassembly view contains:
 </p>
 <ul class="simple">
  <li>
   <p>
    High level source
   </p>
  </li>
  <li>
   <p>
    Assembly instructions
   </p>
  </li>
  <li>
   <p>
    Hotspots at the source level
   </p>
  </li>
  <li>
   <p>
    Hotspots at the assembly instruction level
   </p>
  </li>
  <li>
   <p>
    Columns for profiling data aggregated to the source level
   </p>
  </li>
  <li>
   <p>
    Columns for profiling data collected at the assembly instruction level
   </p>
  </li>
 </ul>
 <p>
  The information shown in the Source-Disassembly view can be customized by the following toolbar options:
 </p>
 <ul class="simple">
  <li>
   <p>
    View menu - Select one or more out of the available profiler data columns to display. This is chosen by default based on the analysis type.
   </p>
  </li>
  <li>
   <p>
    Hot Spot menu - Select which profiler data to use for hot spots. This is chosen by default based on the analysis type.
   </p>
  </li>
  <li>
   <p>
    Show the source and disassembly views side by side.
   </p>
  </li>
  <li>
   <p>
    Show the source and disassembly views top to bottom.
   </p>
  </li>
  <li>
   <p>
    Maximize the source view
   </p>
  </li>
  <li>
   <p>
    Maximize the disassembly view
   </p>
  </li>
 </ul>
 <p>
  Hotspots are colored based on level of importance - low, medium or high. Hovering the mouse over the hotspot displays the value of the profiler data, the level of importance and the source or disassembly line. You can click on a hotspot at the source level or assembly instruction level to view the source or disassembly line corresponding to the hotspot.
 </p>
 <p>
  In the disassembly view the assembly instructions corresponding to the selected source line are highlighted. You can click on the up and down arrow buttons displayed at the right of the disassembly column header to navigate to the next or previous instruction block.
 </p>
 <h3>
  <span class="section-number">
   2.4.4.
  </span>
  GPU Details View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-details-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The GPU Details View displays a table of information for each memory copy and kernel execution in the profiled application. The following figure shows the table containing several memcpy and kernel executions. Each row of the table contains general information for a kernel execution or memory copy. For kernels, the table will also contain a column for each metric or event value collected for that kernel. In the figure, the Achieved Occupancy column shows the value of that metric for each of the kernel executions.
 </p>
 <p>
  You can sort the data by column by left clicking on the column header, and you can rearrange the columns by left clicking on a column header and dragging it to its new location. If you select a row in the table, the corresponding interval will be selected in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
   Timeline View
  </a>
  . Similarly, if you select a kernel or memcpy interval in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
   Timeline View
  </a>
  the table will be scrolled to show the corresponding data.
 </p>
 <p>
  If you hover the mouse over a column header, a tooltip will display the data shown in that column. For a column containing event or metric data, the tooltip will describe the corresponding event or metric. The
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-reference">
   Metrics Reference
  </a>
  section contains more detailed information about each metric.
 </p>
 <p>
  The information shown in the GPU Details View can be filtered in various ways using the menu accessible from the Details View toolbar. The following modes are available:
 </p>
 <ul class="simple">
  <li>
   <p>
    Filter By Selection - If selected, the GPU Details View shows data only for the selected kernel and memcpy intervals.
   </p>
  </li>
  <li>
   <p>
    Show Hidden Timeline Data - If not selected, data is shown only for kernels and memcpys that are visible in the timeline. Kernels and memcpys that are not visible because they are inside collapsed parts of the timeline are not shown.
   </p>
  </li>
  <li>
   <p>
    Show Filtered Timeline Data - If not selected, data is shown only for kernels and memcpys that are in timeline rows that are not filtered.
   </p>
  </li>
 </ul>
 <p>
  Collecting Events and Metrics
 </p>
 <p>
  Specific event and metric values can be collected for each kernel and displayed in the details table. Use the toolbar icon in the upper right corner of the view to configure the events and metrics to collect for each device, and to run the application to collect those events and metrics.
 </p>
 <p>
  Show Summary Data
 </p>
 <p>
  By default the table shows one row for each memcpy and kernel invocation. Alternatively, the table can show summary results for each kernel function. Use the toolbar icon in the upper right corner of the view to select or deselect summary format.
 </p>
 <p>
  Formatting Table Contents
 </p>
 <p>
  The numbers in the table can be displayed either with or without grouping separators. Use the toolbar icon in the upper right corner of the view to select or deselect grouping separators.
 </p>
 <p>
  Exporting Details
 </p>
 <p>
  The contents of the table can be exported in CSV format using the toolbar icon in the upper right corner of the view.
 </p>
 <h3>
  <span class="section-number">
   2.4.5.
  </span>
  CPU Details View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-details-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  CPU Details view
 </p>
 <p>
  This view details the amount of time your application spends executing functions on the CPU. Each thread is sampled periodically to capture its callstack and the summary of these measurements are displayed in this view. You can manipulate the view by selecting different orientations for organizing the callstack: Top-down, Bottom-up, Code Structure (3), choosing which thread to view (1), and by sorting or highlighting a specific thread (7, 8).
 </p>
 <ol class="arabic">
  <li>
   <p>
    All the threads profiled are shown in one view when the âall threadsâ option is selected (default). You can use this drop-down menu to instead select an individual thread.
   </p>
  </li>
  <li>
   <p>
    This column displays a tree of events representing the structure of the applicationâs execution on the CPU. Each of the remaining columns show the measurements collected for this event. The events shown here are determined by which tree orientation mode is selected (3).
   </p>
  </li>
  <li>
   <p>
    The tree is organized to show the calling hierarchy among functions. The following modes are available:
   </p>
   <ul class="simple">
    <li>
     <p>
      Top-down (callers first) call tree view - The CPU details tree is organized as a call tree with each function shown as a child of its caller. In this mode you can see the callstack starting at the âmainâ function.
     </p>
    </li>
    <li>
     <p>
      Bottom-up (callees first) call tree view - The CPU details tree is organized in such a way that each function is shown as a child of any functions it calls. In this mode you can quickly identify the call path that contributes the most time to the applicationâs execution.
     </p>
    </li>
    <li>
     <p>
      Code structure (file and line) tree view - The CPU details tree shows which functions belong to each source file and library as well as how much of the applicationâs execution is attributed to a given line of source code.
     </p>
    </li>
   </ul>
   <p>
    In every mode the time listed for each function is âinclusiveâ and includes time spent both in this function and any functions that it calls. For the code structure view the region of code is inclusive (i.e. the file entry lists the time spent in every function contained within a file).
   </p>
  </li>
  <li>
   <p>
    This column displays the total amount of time spent by all threads in this event as a percentage of the total amount of time spent in all events.
   </p>
  </li>
  <li>
   <p>
    This column displays a bar denoting a range where the amount of time spent in an event by any thread is always within this this range. On the left the minimum value is written, and on the right the maximum value is written. Also, if there is space, a small âdiamondâ is drawn in the middle of the bar where the mean time is spent in this event across all threads.
   </p>
  </li>
  <li>
   <p>
    These columns display a distinct chart for each event. On the left is a vertical scale showing the same minimum and maximum values as shown on the range chart. The following columns each show the amount of time spent in this event by thread. If the cell for the given event / thread combination is greyed out then no time was spent by this thread in this event (for this example both threads 1 and 2 spent no time in the event âx_solveâ). Furthermore, the thread(s) with the minimum or maximum amount of time spent in the event across all threads are annotated with the âtriangle / lineâ. In this example thread 3 spent the most and thread 6 the least amount of time in the event âx_solveâ.
   </p>
  </li>
  <li>
   <p>
    To reorder the rows by the time spent on a given thread click on the thread column header.
   </p>
  </li>
  <li>
   <p>
    To highlight a given thread click on one of its bars in this chart.
   </p>
  </li>
 </ol>
 <p>
  This change to the view is the result of sorting by thread 3 (7) and highlighting it (8).
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    Having highlighted thread 3 we now see a vertical line on the range chart showing the amount of time this thread spent in this event compared to the range across all thread.
   </p>
  </li>
  <li>
   <p>
    This thread is also highlighted on each row.
   </p>
  </li>
 </ol>
 <p>
  CPU Threads
 </p>
 <p>
  CPU Source Code
 </p>
 <p>
  You can open the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-source-view">
   CPU Source View
  </a>
  for any function by double-clicking on it in the tree. To be displayed the source files must be on the local file system. By default the directory containing the executable or profile file is searched. If the source file cannot be found a prompt will appear asking for its location. Sometimes a file within a specific directory is being sought, in this case you should give the path to where this directory resides.
 </p>
 <p class="admonition-title">
  Tip
 </p>
 <p>
  The CPU profile is gathered by periodically sampling the state of the running application. For this reason a function will only appear in this view if it was sampled during execution. Short-running or very infrequently called functions are less likely to be sampled. If a function was not sampled the time it was running is accounted to the function that called it. In order to gather a CPU profile that is representative of the applicationâs performance the code of interest must execute for enough to gather enough samples. Usually a minute of runtime is sufficient.
 </p>
 <p class="admonition-title">
  Tip
 </p>
 <p>
  The file and line information is gathered from the applicationâs debug information obtained by the compiler. To ensure that this information is available it is recommended that you compile with â-gâ or a similar option.
 </p>
 <h3>
  <span class="section-number">
   2.4.6.
  </span>
  OpenACC Details View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc-details-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  OpenACC table view
 </p>
 <p>
  The OpenACC Details View displays each OpenACC runtime activity executed by the profiled application. Each activity is grouped by source location: each activity which occurs at the same file and line number in the applicationâs source code is placed under a node labeled with the source location. Each activity shows the amount of time spent by the profiled application as both a unit of time and as a percentage of the total time this application was executing any OpenACC activity. Also the number of times this activity was called is shown. There are two ways to count how much time is spent in a particular OpenACC activity:
 </p>
 <ul class="simple">
  <li>
   <p>
    Show the Inclusive durations (counting any other OpenACC activities running at the same time) in the OpenACC details view - The OpenACC details view shows the total time spent in each activity including any activities that were executed as the result of this activity. In this case the amount of time spent in each activity occurring at a given application source location is totaled and displayed on the row displaying the source location.
   </p>
  </li>
  <li>
   <p>
    Show the Exclusive durations (excluding any other OpenACC activities running at the same time) in the OpenACC details view - The OpenACC details view shows the time spent only in a given activity. In this case the amount of time spent at a given source location is always zeroâtime is attributed solely to each activity occurring at this source location.
   </p>
  </li>
 </ul>
 <h3>
  <span class="section-number">
   2.4.7.
  </span>
  OpenMP Details View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp-details-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  OpenMP table view
 </p>
 <p>
  The OpenMP Details view displays the activity of the OpenMP runtime on the CPU. The time your application spends in a parallel region or idling is shown both on the timeline and is summarized in this view. The reference for the percentage of time spent in each type of activity is the time from the start of the first parallel region to the end of the last parallel region. The sum of the percentages of each activity type often exceeds 100% because the OpenMP runtime can be in multiple states at the same time.
 </p>
 <h3>
  <span class="section-number">
   2.4.8.
  </span>
  Properties View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#properties-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The Properties View shows information about the row or interval highlighted or selected in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeline-view">
   Timeline View
  </a>
  . If a row or interval is not selected, the displayed information tracks the motion of the mouse pointer. If a row or interval is selected, the displayed information is pinned to that row or interval.
 </p>
 <p>
  When an OpenACC interval with an associated source file is selected, this filename is shown in the Source File table entry. Double-clicking on the filename opens the respective source file if it is available on the file-system.
 </p>
 <h3>
  <span class="section-number">
   2.4.9.
  </span>
  Console View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#console-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The Console View shows stdout and stderr output of the application each time it executes. If you need to provide stdin input to your application, do so by typing into the console view.
 </p>
 <h3>
  <span class="section-number">
   2.4.10.
  </span>
  Settings View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#settings-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The Settings View allows you to specify execution settings for the application being profiled. As shown in the following figure, the Executable settings tab allows you to specify the executable file, the working directory, the command-line arguments, and the environment for the application. Only the executable file is required, all other fields are optional.
 </p>
 <p>
  Exection Timeout
 </p>
 <p>
  The Executable settings tab also allows you to specify an optional execution timeout. If the execution timeout is specified, the application execution will be terminated after that number of seconds. If the execution timeout is not specified, the application will be allowed to continue execution until it terminates normally.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The timer starts counting from the moment the CUDA driver is initialized. If the application doesnât call any CUDA APIs, a timeout wonât be triggered.
 </p>
 <p>
  Start execution with profiling enabled
 </p>
 <p>
  The Start execution with profiling enabled checkbox is set by default to indicate that application profiling begins at the start of application execution. If you are using
  <span class="pre">
   cudaProfilerStart()
  </span>
  and
  <span class="pre">
   cudaProfilerStop()
  </span>
  to control profiling within your application as described in
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#focused-profiling">
   Focused Profiling
  </a>
  , then you should uncheck this box.
 </p>
 <p>
  Enable concurrent kernel profiling
 </p>
 <p>
  The Enable concurrent kernel profiling checkbox is set by default to enable profiling of applications that exploit concurrent kernel execution. If this checkbox is unset, the profiler will disable concurrent kernel execution. Disabling concurrent kernel execution can reduce profiling overhead in some cases and so may be appropriate for applications that do not exploit concurrent kernels.
 </p>
 <p>
  Enable power, clock, and thermal profiling
 </p>
 <p>
  The Enable power, clock, and thermal profiling checkbox can be set to enable low frequency sampling of the power, clock, and thermal behavior of each GPU used by the application.
 </p>
 <h3>
  <span class="section-number">
   2.4.11.
  </span>
  CPU Source View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-source-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The CPU source code view allows you to inspect the files that comprise the profiled applicationâs CPU source. This view can be opened in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-details-view">
   CPU Details View
  </a>
  by double-clicking on a function in the treeâthe source file that corresponds to this function is then opened. Line numbers can be enabled by right-clicking left side ruler.
 </p>
 <p>
  When compiling using the PGIÂ® compilers annotations can be added to this view (see
  <a class="reference external" href="http://www.pgroup.com/resources/ccff.htm">
   Common Compiler Feedback Format
  </a>
  for more information). These annotation are notes about how a given line of code is compiled. PGI compilers save information about how your program was optimized, or why a particular optimization was not made. This can be combined with the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-details-view">
   CPU Details View
  </a>
  to help identify why certain lines of code performed the way they did. For example, the message may tell you about the following:
 </p>
 <ul class="simple">
  <li>
   <p>
    vector instructions generated by the compiler.
   </p>
  </li>
  <li>
   <p>
    compute-intensity of a loop, a ratio computation to memory operationsâhigher numbers mean that there is more computation than memory loads and stores.
   </p>
  </li>
  <li>
   <p>
    information about parallelization, with a hint for how it might be possible to make the loop run in parallel if the compiler could not auto-parallelize it.
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   2.5.
  </span>
  Customizing the Profiler
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#customizing-the-profiler" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  When you first start the Visual Profiler
  <span class="pre">
   ,
  </span>
  <span class="pre">
   and
  </span>
  <span class="pre">
   after
  </span>
  <span class="pre">
   closing
  </span>
  <span class="pre">
   the
  </span>
  <span class="pre">
   Welcome
  </span>
  <span class="pre">
   page,
  </span>
  you will be presented with a default placement of the views. By moving and resizing the views, you can customize the profiler to meet your development needs. Any changes you make are restored the next time you start the profiler.
 </p>
 <h3>
  <span class="section-number">
   2.5.1.
  </span>
  Resizing a View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#resizing-a-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  To resize a view, simply left click and drag on the dividing area between the views. All views stacked together in one area are resized at the same time.
 </p>
 <h3>
  <span class="section-number">
   2.5.2.
  </span>
  Reordering a View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#reordering-a-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  To reorder a view in a stacked set of views, left click and drag the view tab to the new location within the view stack.
 </p>
 <h3>
  <span class="section-number">
   2.5.3.
  </span>
  Moving a View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#moving-a-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  to move a view, left click the view tab and drag it to its new location. As you drag the view, an outline will show the target location for the view. You can place the view in a new location, or stack it in the same location as other views.
 </p>
 <h3>
  <span class="section-number">
   2.5.4.
  </span>
  Undocking a View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#undocking-a-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  You can undock a view from the profiler window so that the view occupies its own stand-alone window. You may want to do this to take advantage of multiple monitors or to maximum the size of an individual view. To undock a view, left click the view tab and drag it outside of the profiler window. To dock a view, left click the view tab (not the window decoration) and drag it into the profiler window.
 </p>
 <h3>
  <span class="section-number">
   2.5.5.
  </span>
  Opening and Closing a View
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#opening-and-closing-a-view" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Use the X icon on a view tab to close a view. To open a view, use the View menu.
 </p>
 <h2>
  <span class="section-number">
   2.6.
  </span>
  Command Line Arguments
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#command-line-arguments" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  When the Visual Profiler is started from the command line, it is possible, using command line arguments, to specify executable to start new session with or import profile files exported from nvprof using one of the following patterns:
 </p>
 <ul>
  <li>
   <p>
    Start new executable session by launching nvvp with name of executable followed, optionally, by its arguments:
   </p>
   <pre><span class="n">nvvp</span><span class="n">executableName</span>
<span class="p">[[</span><span class="n">executableArguments</span><span class="p">]...]</span>
</pre>
  </li>
  <li>
   <p>
    Import single-process nvprof session by launching nvvp with single .nvprof file as argument(see
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#export-import">
     nvprofâs export/import options
    </a>
    section for more details):
   </p>
   <pre><span class="n">nvvp</span>
<span class="n">data</span><span class="p">.</span><span class="n">nvprof</span>
</pre>
  </li>
  <li>
   <p>
    Import multi-process nvprof session, by launching nvvp with multiple .nvprof files as arguments:
   </p>
   <pre><span class="n">nvvp</span>
<span class="n">data1</span><span class="p">.</span><span class="n">nvprof</span><span class="n">data2</span><span class="p">.</span><span class="n">nvprof</span><span class="p">...</span>
</pre>
  </li>
 </ul>
 <h1>
  <span class="section-number">
   3.
  </span>
  ânvprof
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  The
  <span class="pre">
   nvprof
  </span>
  profiling tool enables you to collect and view profiling data from the command-line.
  <span class="pre">
   nvprof
  </span>
  enables the collection of a timeline of CUDA-related activities on both CPU and GPU, including kernel execution, memory transfers, memory set and CUDA API calls and events or metrics for CUDA kernels. Profiling options are provided to
  <span class="pre">
   nvprof
  </span>
  through command-line options. Profiling results are displayed in the console after the profiling data is collected, and may also be saved for laterviewing by either
  <span class="pre">
   nvprof
  </span>
  or the Visual Profiler.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The textual output of the profiler is redirected to
  <span class="pre">
   stderr
  </span>
  by default. Use
  <span class="pre">
   --log-file
  </span>
  to redirect the output to another file. See
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#redirecting-output">
   Redirecting Output
  </a>
  .
 </p>
 <p>
  To profile an application from the command-line:
 </p>
 <pre><span class="n">nvprof</span><span class="p">[</span><span class="n">options</span><span class="p">]</span><span class="p">[</span><span class="n">application</span><span class="p">]</span>
<span class="p">[</span><span class="n">application</span><span class="o">-</span><span class="n">arguments</span><span class="p">]</span>
</pre>
 <p>
  To view the full help page, type
  <span class="pre">
   nvprof
  </span>
  <span class="pre">
   --help
  </span>
  .
 </p>
 <h2>
  <span class="section-number">
   3.1.
  </span>
  Command Line Options
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#command-line-options" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <h3>
  <span class="section-number">
   3.1.1.
  </span>
  CUDA Profiling Options
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cuda-profiling-options" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <table class="table-no-stripes docutils align-default">
  <tr class="row-odd">
   <th class="head">
    <p>
     Option
    </p>
   </th>
   <th class="head">
    <p>
     Values
    </p>
   </th>
   <th class="head">
    <p>
     Default
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     aggregate-mode
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     on
    </p>
   </td>
   <td>
    <p>
     Turn on/off aggregate mode for events and metrics specified by subsequent
     <span class="pre">
      --events
     </span>
     and
     <span class="pre">
      --metrics
     </span>
     options. Those event/metric values will be collected for each domain instance, instead of the whole device.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#event-metric-trace-mode">
      Event/metric Trace Mode
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     analysis-metrics
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Collect profiling data that can be imported to Visual Profilerâs âanalysisâ mode. Note: Use
     <span class="pre">
      --export-profile
     </span>
     to specify an export file.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     annotate-mpi
    </p>
   </td>
   <td>
    <p>
     off, openmpi, mpich
    </p>
   </td>
   <td>
    <p>
     off
    </p>
   </td>
   <td>
    <p>
     Automatically annotate MPI calls with NVTX markers. Specify the MPI implementation installed on your machine. Currently, Open MPI and MPICH implementations are supported.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#automatic-mpi-annotation-with-nvtx">
      Automatic MPI Annotation with NVTX
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     concurrent-kernels
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     on
    </p>
   </td>
   <td>
    <p>
     Turn on/off concurrent kernel execution. If concurrent kernel execution is off, all kernels running on one device will be serialized.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     continuous-sampling-interval
    </p>
   </td>
   <td>
    <p>
     {interval in milliseconds}
    </p>
   </td>
   <td>
    <p>
     2 milliseconds
    </p>
   </td>
   <td>
    <p>
     Set the continuous mode sampling interval in milliseconds. Minimum is 1 ms.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     cpu-thread-tracing
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     off
    </p>
   </td>
   <td>
    <p>
     Collect information about CPU thread API activity.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-thread-tracing">
      CPU Thread Tracing
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dependency-analysis
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Generate event dependency graph for host and device activities and run dependency analysis.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis">
      Dependency Analysis
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     device-buffer-size
    </p>
   </td>
   <td>
    <p>
     {size in MBs}
    </p>
   </td>
   <td>
    <p>
     8 MB
    </p>
   </td>
   <td>
    <p>
     Set the device memory size (in MBs) reserved for storing profiling data for non-CDP operations, especially for concurrent kernel tracing, for each buffer on a context. The size should be a positive integer.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     device-cdp-buffer-size
    </p>
   </td>
   <td>
    <p>
     {size in MBs}
    </p>
   </td>
   <td>
    <p>
     8 MB
    </p>
   </td>
   <td>
    <p>
     Set the device memory size (in MBs) reserved for storing profiling data for CDP operations for each buffer on a context. The size should be a positive integer.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     devices
    </p>
   </td>
   <td>
    <p>
     {comma-separated device IDs}, all
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Change the scope of subsequent
     <span class="pre">
      --events
     </span>
     ,
     <span class="pre">
      --metrics
     </span>
     ,
     <span class="pre">
      --query-events
     </span>
     and
     <span class="pre">
      --query-metrics
     </span>
     options.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiling-scope">
      Profiling Scope
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     event-collection-mode
    </p>
   </td>
   <td>
    <p>
     kernel, continuous
    </p>
   </td>
   <td>
    <p>
     kernel
    </p>
   </td>
   <td>
    <p>
     Choose event collection mode for all events/metrics.
    </p>
    <ul class="simple">
     <li>
      <p>
       kernel: Events/metrics are collected only for durations of kernel executions
      </p>
     </li>
     <li>
      <p>
       continuous: Events/metrics are collected for duration of application. This is not applicable for non-Tesla devices. This mode is compatible only with NVLink events/metrics. This mode is incompatible with
       <span class="pre">
        --profile-all-processes
       </span>
       or
       <span class="pre">
        --profile-child-processes
       </span>
       or
       <span class="pre">
        --replay-mode
       </span>
       <span class="pre">
        kernel
       </span>
       or
       <span class="pre">
        --replay-mode
       </span>
       <span class="pre">
        application
       </span>
       .
      </p>
     </li>
    </ul>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     events (e)
    </p>
   </td>
   <td>
    <p>
     {comma-separated event names}, all
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Specify the events to be profiled on certain device(s). Multiple event names separated by comma can be specified. Which device(s) are profiled is controlled by the
     <span class="pre">
      --devices
     </span>
     option. Otherwise events will be collected on all devices. For a list of available events, use
     <span class="pre">
      --query-events
     </span>
     . Use
     <span class="pre">
      --events
     </span>
     <span class="pre">
      all
     </span>
     to profile all events available for each device. Use
     <span class="pre">
      --devices
     </span>
     and
     <span class="pre">
      --kernels
     </span>
     to select a specific kernel invocation.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     kernel-latency-timestamps
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     off
    </p>
   </td>
   <td>
    <p>
     Turn on/off collection of kernel latency timestamps, namely queued and submitted. The queued timestamp is captured when a kernel launch command was queued into the CPU command buffer. The submitted timestamp denotes when the CPU command buffer containing this kernel launch was submitted to the GPU. Turning this option on may incur an overhead during profiling.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     kernels
    </p>
   </td>
   <td>
    <p>
     {kernel name}, {[context id/name]:[stream id/name]:[kernel name]:[invocation]}
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Change the scope of subsequent
     <span class="pre">
      --events
     </span>
     ,
     <span class="pre">
      --metrics
     </span>
     options. The syntax is as follows:
    </p>
    <ul class="simple">
     <li>
      <p>
       {kernel name}: Limit scope to given kernel name.
      </p>
     </li>
     <li>
      <p>
       {[context id/name]:[stream id/name]:[kernel name]:[invocation]}: The context/stream IDs, names, kernel name and invocation can be regular expressions. Empty string matches any number or characters. If [context id/name] or [stream id/name] is a positive number, itâs strictly matched against the CUDA context/stream ID. Otherwise itâs treated as a regular expression and matched against the context/stream name specified by the NVTX library. If the invocation count is a positive number, itâs strictly matched against the invocation of the kernel. Otherwise itâs treated as a regular expression.
      </p>
     </li>
    </ul>
    <p>
     Example:
     <span class="pre">
      --kernels
     </span>
     <span class="pre">
      "1:foo:bar:2"
     </span>
     will profile any kernel whose name contains âbarâ and is the 2nd instance on context 1 and on stream named âfooâ.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiling-scope">
      Profiling Scope
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     metrics (m)
    </p>
   </td>
   <td>
    <p>
     {comma-separated metric names}, all
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Specify the metrics to be profiled on certain device(s). Multiple metric names separated by comma can be specified. Which device(s) are profiled is controlled by the
     <span class="pre">
      --devices
     </span>
     option. Otherwise metrics will be collected on all devices. For a list of available metrics, use
     <span class="pre">
      --query-metrics
     </span>
     . Use
     <span class="pre">
      --metrics
     </span>
     <span class="pre">
      all
     </span>
     to profile all metrics available for each device. Use
     <span class="pre">
      --devices
     </span>
     and
     <span class="pre">
      --kernels
     </span>
     to select a specific kernel invocation. Note:
     <span class="pre">
      --metrics
     </span>
     <span class="pre">
      all
     </span>
     does not include some metrics which are needed for Visual Profilerâs source level analysis. For that, use
     <span class="pre">
      --analysis-metrics
     </span>
     .
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     pc-sampling-period
    </p>
   </td>
   <td>
    <p>
     {period in cycles}
    </p>
   </td>
   <td>
    <p>
     Between 5 and 12 based on the setup
    </p>
   </td>
   <td>
    <p>
     Specify PC Sampling period in cycles, at which the sampling records will be dumped. Allowed values for the period are integers between 5 to 31 both inclusive. This will set the sampling period to (2^period) cycles Note: Only available for GM20X+.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     profile-all-processes
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Profile all processes launched by the same user who launched this nvprof instance. Note: Only one instance of nvprof can run with this option at the same time. Under this mode, thereâs no need to specify an application to run.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#multiprocess-profiling">
      Multiprocess Profiling
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     profile-api-trace
    </p>
   </td>
   <td>
    <p>
     none, runtime, driver, all
    </p>
   </td>
   <td>
    <p>
     all
    </p>
   </td>
   <td>
    <p>
     Turn on/off CUDA runtime/driver API tracing.
    </p>
    <ul class="simple">
     <li>
      <p>
       none: turn off API tracing
      </p>
     </li>
     <li>
      <p>
       runtime: only turn on CUDA runtime API tracing
      </p>
     </li>
     <li>
      <p>
       driver: only turn on CUDA driver API tracing
      </p>
     </li>
     <li>
      <p>
       all: turn on all API tracing
      </p>
     </li>
    </ul>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     profile-child-processes
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Profile the application and all child processes launched by it.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#multiprocess-profiling">
      Multiprocess Profiling
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     profile-from-start
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     on
    </p>
   </td>
   <td>
    <p>
     Enable/disable profiling from the start of the application. If itâs disabled, the application can use {cu,cuda}Profiler{Start,Stop} to turn on/off profiling.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#focused-profiling">
      Focused Profiling
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     profiling-semaphore-pool-size
    </p>
   </td>
   <td>
    <p>
     {count}
    </p>
   </td>
   <td>
    <p>
     65536
    </p>
   </td>
   <td>
    <p>
     Set the profiling semaphore pool size reserved for storing profiling data for serialized kernels and memory operations for each context. The size should be a positive integer.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     query-events
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     List all the events available on the device(s). Device(s) queried can be controlled by the
     <span class="pre">
      --devices
     </span>
     option.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     query-metrics
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     List all the metrics available on the device(s). Device(s) queried can be controlled by the
     <span class="pre">
      --devices
     </span>
     option.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     replay-mode
    </p>
   </td>
   <td>
    <p>
     disabled, kernel, application
    </p>
   </td>
   <td>
    <p>
     kernel
    </p>
   </td>
   <td>
    <p>
     Choose replay mode used when not all events/metrics can be collected in a single run.
    </p>
    <ul class="simple">
     <li>
      <p>
       disabled: replay is disabled, events/metrics couldnât be profiled will be dropped
      </p>
     </li>
     <li>
      <p>
       kernel: each kernel invocation is replayed
      </p>
     </li>
     <li>
      <p>
       application: the entire application is replayed. This mode is incompatible with
       <span class="pre">
        --profile-all-processes
       </span>
       or
       <span class="pre">
        profile-child-processes
       </span>
       .
      </p>
     </li>
    </ul>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     skip-kernel-replay-save-restore
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     off
    </p>
   </td>
   <td>
    <p>
     If enabled, this option can vastly improve kernel replay speed, as save and restore of the mutable state for each kernel pass will be skipped. Skipping of save/restore of input/output buffers allows you to specify that all profiled kernels on the context do not change the contents of their input buffers during execution, or call device malloc/free or new/delete, that leave the device heap in a different state. Specifically, a kernel can malloc and free a buffer in the same launch, but it cannot call an unmatched malloc or an unmatched free. Note: incorrectly using this mode while one of the kernels does modify the input buffer or uses unmatched malloc/free will result in undefined behavior, including kernel execution failure and/or corrupted device data.
    </p>
    <ul class="simple">
     <li>
      <p>
       on: skip save/restore of the input/output buffers
      </p>
     </li>
     <li>
      <p>
       off: save/restore input/output buffers for each kernel replay pass
      </p>
     </li>
    </ul>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     source-level-analysis (a)
    </p>
   </td>
   <td>
    <p>
     global_access, shared_access, branch, instruction_execution, pc_sampling
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Specify the source level metrics to be profiled on a certain kernel invocation. Use
     <span class="pre">
      --devices
     </span>
     and
     <span class="pre">
      --kernels
     </span>
     to select a specific kernel invocation. One or more of these may be specified, separated by commas
    </p>
    <ul class="simple">
     <li>
      <p>
       global_access: global access
      </p>
     </li>
     <li>
      <p>
       shared_access: shared access
      </p>
     </li>
     <li>
      <p>
       branch: divergent branch
      </p>
     </li>
     <li>
      <p>
       instruction_execution: instruction execution
      </p>
     </li>
     <li>
      <p>
       pc_sampling: pc sampling, available only for GM20X+
      </p>
     </li>
    </ul>
    <p>
     Note: Use
     <span class="pre">
      --export-profile
     </span>
     to specify an export file.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#source-disassembly-view">
      Source-Disassembly View
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     system-profiling
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     off
    </p>
   </td>
   <td>
    <p>
     Turn on/off power, clock, and thermal profiling.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#system-profiling">
      System Profiling
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     timeout (t)
    </p>
   </td>
   <td>
    <p>
     {seconds}
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Set an execution timeout (in seconds) for the CUDA application. Note: Timeout starts counting from the moment the CUDA driver is initialized. If the application doesnât call any CUDA APIs, timeout wonât be triggered.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeout">
      Timeout
     </a>
     and
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#flush-profile-data">
      Flush Profile Data
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     track-memory-allocations
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     off
    </p>
   </td>
   <td>
    <p>
     Turn on/off tracking of memory operations, which involves recording timestamps, memory size, memory type and program counters of the memory allocations and frees. Turning this option on may incur an overhead during profiling.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     unified-memory-profiling
    </p>
   </td>
   <td>
    <p>
     per-process-device, off
    </p>
   </td>
   <td>
    <p>
     per-process-device
    </p>
   </td>
   <td>
    <p>
     Configure unified memory profiling.
    </p>
    <ul class="simple">
     <li>
      <p>
       per-process-device: collect counts for each process and each device
      </p>
     </li>
     <li>
      <p>
       off: turn off unified memory profiling
      </p>
     </li>
    </ul>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#unified-memory-profiling">
      Unified Memory Profiling
     </a>
     for more information.
    </p>
   </td>
  </tr>
 </table>
 <h3>
  <span class="section-number">
   3.1.2.
  </span>
  CPU Profiling Options
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-profiling-options" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <table class="table-no-stripes docutils align-default">
  <tr class="row-odd">
   <th class="head">
    <p>
     Option
    </p>
   </th>
   <th class="head">
    <p>
     Values
    </p>
   </th>
   <th class="head">
    <p>
     Default
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cpu-profiling
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     off
    </p>
   </td>
   <td>
    <p>
     Turn on CPU profiling. Note: CPU profiling is not supported in multi-process mode.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     cpu-profiling-explain-ccff
    </p>
   </td>
   <td>
    <p>
     {filename}
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Set the path to a PGI pgexplain.xml file that should be used to interpret Common Compiler Feedback Format (CCFF) messages.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cpu-profiling-frequency
    </p>
   </td>
   <td>
    <p>
     {frequency}
    </p>
   </td>
   <td>
    <p>
     100Hz
    </p>
   </td>
   <td>
    <p>
     Set the CPU profiling frequency in samples per second. Maximum is 500Hz.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     cpu-profiling-max-depth
    </p>
   </td>
   <td>
    <p>
     {depth}
    </p>
   </td>
   <td>
    <p>
     0 (i.e. unlimited)
    </p>
   </td>
   <td>
    <p>
     Set the maximum depth of each call stack.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cpu-profiling-mode
    </p>
   </td>
   <td>
    <p>
     flat, top-down, bottom-up
    </p>
   </td>
   <td>
    <p>
     bottom-up
    </p>
   </td>
   <td>
    <p>
     Set the output mode of CPU profiling.
    </p>
    <ul class="simple">
     <li>
      <p>
       flat: Show flat profile
      </p>
     </li>
     <li>
      <p>
       top-down: Show parent functions at the top
      </p>
     </li>
     <li>
      <p>
       bottom-up: Show parent functions at the bottom
      </p>
     </li>
    </ul>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     cpu-profiling-percentage-threshold
    </p>
   </td>
   <td>
    <p>
     {threshold}
    </p>
   </td>
   <td>
    <p>
     0 (i.e. unlimited)
    </p>
   </td>
   <td>
    <p>
     Filter out the entries that are below the set percentage threshold. The limit should be an integer between 0 and 100, inclusive.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cpu-profiling-scope
    </p>
   </td>
   <td>
    <p>
     function, instruction
    </p>
   </td>
   <td>
    <p>
     function
    </p>
   </td>
   <td>
    <p>
     Choose the profiling scope.
    </p>
    <ul class="simple">
     <li>
      <p>
       function: Each level in the stack trace represents a distinct function
      </p>
     </li>
     <li>
      <p>
       instruction: Each level in the stack trace represents a distinct instruction address
      </p>
     </li>
    </ul>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     cpu-profiling-show-ccff
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     off
    </p>
   </td>
   <td>
    <p>
     Choose whether to print Common Compiler Feedback Format (CCFF) messages embedded in the binary. Note: this option implies
     <span class="pre">
      --cpu-profiling-scope
     </span>
     <span class="pre">
      instruction
     </span>
     .
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cpu-profiling-show-library
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     off
    </p>
   </td>
   <td>
    <p>
     Choose whether to print the library name for each sample.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     cpu-profiling-thread-mode
    </p>
   </td>
   <td>
    <p>
     separated, aggregated
    </p>
   </td>
   <td>
    <p>
     aggregated
    </p>
   </td>
   <td>
    <p>
     Set the thread mode of CPU profiling.
    </p>
    <ul class="simple">
     <li>
      <p>
       separated: Show separate profile for each thread
      </p>
     </li>
     <li>
      <p>
       aggregated: Aggregate data from all threads
      </p>
     </li>
    </ul>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cpu-profiling-unwind-stack
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     on
    </p>
   </td>
   <td>
    <p>
     Choose whether to unwind the CPU call-stack at each sample point.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     openacc-profiling
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     on
    </p>
   </td>
   <td>
    <p>
     Enable/disable recording information from the OpenACC profiling interface. Note: if the OpenACC profiling interface is available depends on the OpenACC runtime.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc">
      OpenACC
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     openmp-profiling
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     off
    </p>
   </td>
   <td>
    <p>
     Enable/disable recording information from the OpenMP profiling interface. Note: if the OpenMP profiling interface is available depends on the OpenMP runtime.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp">
      OpenMP
     </a>
     for more information.
    </p>
   </td>
  </tr>
 </table>
 <h3>
  <span class="section-number">
   3.1.3.
  </span>
  Print Options
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#print-options" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <table class="table-no-stripes docutils align-default">
  <tr class="row-odd">
   <th class="head">
    <p>
     Option
    </p>
   </th>
   <th class="head">
    <p>
     Values
    </p>
   </th>
   <th class="head">
    <p>
     Default
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     context-name
    </p>
   </td>
   <td>
    <p>
     {name}
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Name of the CUDA context.
    </p>
    <p>
     <span class="pre">
      %i
     </span>
     in the context name string is replaced with the ID of the context.
    </p>
    <p>
     <span class="pre">
      %p
     </span>
     in the context name string is replaced with the process ID of the application being profiled.
    </p>
    <p>
     <span class="pre">
      %q{&lt;ENV&gt;}
     </span>
     in the context name string is replaced with the value of the environment variable
     <span class="pre">
      &lt;ENV&gt;
     </span>
     . If the environment variable is not set itâs an error.
    </p>
    <p>
     <span class="pre">
      %h
     </span>
     in the context name string is replaced with the hostname of the system.
    </p>
    <p>
     <span class="pre">
      %%
     </span>
     in the context name string is replaced with
     <span class="pre">
      %
     </span>
     . Any other character following
     <span class="pre">
      %
     </span>
     is illegal.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     csv
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Use comma-separated values in the output.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#csv">
      CSV
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     demangling
    </p>
   </td>
   <td>
    <p>
     on, off
    </p>
   </td>
   <td>
    <p>
     on
    </p>
   </td>
   <td>
    <p>
     Turn on/off C++ name demangling of function names.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#demangling">
      Demangling
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     normalized-time-unit (u)
    </p>
   </td>
   <td>
    <p>
     s, ms, us, ns, col, auto
    </p>
   </td>
   <td>
    <p>
     auto
    </p>
   </td>
   <td>
    <p>
     Specify the unit of time that will be used in the output.
    </p>
    <ul class="simple">
     <li>
      <p>
       s: second
      </p>
     </li>
     <li>
      <p>
       ms: millisecond
      </p>
     </li>
     <li>
      <p>
       us: microsecond
      </p>
     </li>
     <li>
      <p>
       ns: nanosecond
      </p>
     </li>
     <li>
      <p>
       col: a fixed unit for each column
      </p>
     </li>
     <li>
      <p>
       auto: the scale is chosen for each value based on its length.
      </p>
     </li>
    </ul>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#adjust-units">
      Adjust Units
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     openacc-summary-mode
    </p>
   </td>
   <td>
    <p>
     exclusive, inclusive
    </p>
   </td>
   <td>
    <p>
     exclusive
    </p>
   </td>
   <td>
    <p>
     Set how durations are computed in the OpenACC summary.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc-summary-modes">
      OpenACC Summary Modes
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     trace
    </p>
   </td>
   <td>
    <p>
     api, gpu
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Specify the option (or options separated by commas) to be traced.
    </p>
    <ul class="simple">
     <li>
      <p>
       api - only turn on CUDA runtime and driver API tracing
      </p>
     </li>
     <li>
      <p>
       gpu - only turn on CUDA GPU tracing
      </p>
     </li>
    </ul>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     print-api-summary
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print a summary of CUDA runtime/driver API calls.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     print-api-trace
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print CUDA runtime/driver API trace.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-trace-and-api-trace-modes">
      GPU-Trace and API-Trace Modes
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     print-dependency-analysis-trace
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print dependency analysis trace.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis">
      Dependency Analysis
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     print-gpu-summary
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print a summary of the activities on the GPU (including CUDA kernels and memcpyâs/memsetâs).
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#summary-mode">
      Summary Mode
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     print-gpu-trace
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print individual kernel invocations (including CUDA memcpyâs/memsetâs) and sort them in chronological order. In event/metric profiling mode, show events/metrics for each kernel invocation.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-trace-and-api-trace-modes">
      GPU-Trace and API-Trace Modes
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     print-openacc-constructs
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Include parent construct names in OpenACC profile.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc-options">
      OpenACC Options
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     print-openacc-summary
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print a summary of the OpenACC profile.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     print-openacc-trace
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print a trace of the OpenACC profile.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     print-openmp-summary
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print a summary of the OpenMP profile.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     print-summary (s)
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print a summary of the profiling result on screen. Note: This is the default unless
     <span class="pre">
      --export-profile
     </span>
     or other print options are used.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     print-summary-per-gpu
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print a summary of the profiling result for each GPU.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     process-name
    </p>
   </td>
   <td>
    <p>
     {name}
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Name of the process.
    </p>
    <p>
     <span class="pre">
      %p
     </span>
     in the process name string is replaced with the process ID of the application being profiled.
    </p>
    <p>
     <span class="pre">
      %q{&lt;ENV&gt;}
     </span>
     in the process name string is replaced with the value of the environment variable
     <span class="pre">
      &lt;ENV&gt;
     </span>
     . If the environment variable is not set itâs an error.
    </p>
    <p>
     <span class="pre">
      %h
     </span>
     in the process name string is replaced with the hostname of the system.
    </p>
    <p>
     <span class="pre">
      %%
     </span>
     in the process name string is replaced with
     <span class="pre">
      %
     </span>
     . Any other character following
     <span class="pre">
      %
     </span>
     is illegal.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     quiet
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Suppress all nvprof output.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stream-name
    </p>
   </td>
   <td>
    <p>
     {name}
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Name of the CUDA stream.
    </p>
    <p>
     <span class="pre">
      %i
     </span>
     in the stream name string is replaced with the ID of the stream.
    </p>
    <p>
     <span class="pre">
      %p
     </span>
     in the stream name string is replaced with the process ID of the application being profiled.
    </p>
    <p>
     <span class="pre">
      %q{&lt;ENV&gt;}
     </span>
     in the stream name string is replaced with the value of the environment variable
     <span class="pre">
      &lt;ENV&gt;
     </span>
     . If the environment variable is not set itâs an error.
    </p>
    <p>
     <span class="pre">
      %h
     </span>
     in the stream name string is replaced with the hostname of the system.
    </p>
    <p>
     <span class="pre">
      %%
     </span>
     in the stream name string is replaced with
     <span class="pre">
      %
     </span>
     . Any other character following
     <span class="pre">
      %
     </span>
     is illegal.
    </p>
   </td>
  </tr>
 </table>
 <h3>
  <span class="section-number">
   3.1.4.
  </span>
  IO Options
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#io-options" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <table class="table-no-stripes docutils align-default">
  <tr class="row-odd">
   <th class="head">
    <p>
     Option
    </p>
   </th>
   <th class="head">
    <p>
     Values
    </p>
   </th>
   <th class="head">
    <p>
     Default
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     export-profile (o)
    </p>
   </td>
   <td>
    <p>
     {filename}
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Export the result file which can be imported later or opened by the NVIDIA Visual Profiler.
    </p>
    <p>
     <span class="pre">
      %p
     </span>
     in the file name string is replaced with the process ID of the application being profiled.
    </p>
    <p>
     <span class="pre">
      %q{&lt;ENV&gt;}
     </span>
     in the file name string is replaced with the value of the environment variable
     <span class="pre">
      &lt;ENV&gt;
     </span>
     . If the environment variable is not set itâs an error.
    </p>
    <p>
     <span class="pre">
      %h
     </span>
     in the file name string is replaced with the hostname of the system.
    </p>
    <p>
     <span class="pre">
      %%
     </span>
     in the file name string is replaced with
     <span class="pre">
      %
     </span>
     . Any other character following
     <span class="pre">
      %
     </span>
     is illegal.
    </p>
    <p>
     By default, this option disables the summary output. Note: If the application being profiled creates child processes, or if
     <span class="pre">
      --profile-all-processes
     </span>
     is used, the
     <span class="pre">
      %p
     </span>
     format is needed to get correct export files for each process.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#export-import">
      Export/Import
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     force-overwrite (f)
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Force overwriting all output files (any existing files will be overwritten).
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     import-profile (i)
    </p>
   </td>
   <td>
    <p>
     {filename}
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Import a result profile from a previous run.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#export-import">
      Export/Import
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     log-file
    </p>
   </td>
   <td>
    <p>
     {filename}
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Make nvprof send all its output to the specified file, or one of the standard channels. The file will be overwritten. If the file doesnât exist, a new one will be created.
    </p>
    <p>
     <span class="pre">
      %1
     </span>
     as the whole file name indicates standard output channel (stdout).
    </p>
    <p>
     <span class="pre">
      %2
     </span>
     as the whole file name indicates standard error channel (stderr). Note: This is the default.
    </p>
    <p>
     <span class="pre">
      %p
     </span>
     in the file name string is replaced with the process ID of the application being profiled.
    </p>
    <p>
     <span class="pre">
      %q{&lt;ENV&gt;}
     </span>
     in the file name string is replaced with the value of the environment variable
     <span class="pre">
      &lt;ENV&gt;
     </span>
     . If the environment variable is not set itâs an error.
    </p>
    <p>
     <span class="pre">
      %h
     </span>
     in the file name string is replaced with the hostname of the system.
    </p>
    <p>
     <span class="pre">
      %%
     </span>
     in the file name is replaced with
     <span class="pre">
      %
     </span>
     . Any other character following
     <span class="pre">
      %
     </span>
     is illegal.
    </p>
    <p>
     See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#redirecting-output">
      Redirecting Output
     </a>
     for more information.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     print-nvlink-topology
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print nvlink topology
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     print-pci-topology
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print PCI topology
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     help (h)
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print help information.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     version (V)
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     N/A
    </p>
   </td>
   <td>
    <p>
     Print version information of this tool.
    </p>
   </td>
  </tr>
 </table>
 <h2>
  <span class="section-number">
   3.2.
  </span>
  Profiling Modes
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiling-modes" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  <span class="pre">
   nvprof
  </span>
  operates in one of the modes listed below.
 </p>
 <h3>
  <span class="section-number">
   3.2.1.
  </span>
  Summary Mode
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#summary-mode" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Summary mode is the default operating mode for
  <span class="pre">
   nvprof
  </span>
  . In this mode,
  <span class="pre">
   nvprof
  </span>
  outputs a single result line for each kernel function and each type of CUDA memory copy/set performed by the application. For each kernel,
  <span class="pre">
   nvprof
  </span>
  outputs the total time of all instances of the kernel or type of memory copy as well as the average, minimum, and maximum time. The time for a kernel is the kernel execution time on the device. By default,
  <span class="pre">
   nvprof
  </span>
  also prints a summary of all the CUDA runtime/driver API calls. Output of
  <span class="pre">
   nvprof
  </span>
  (except for tables) are prefixed with
  <span class="pre">
   ==&lt;pid&gt;==
  </span>
  ,
  <span class="pre">
   &lt;pid&gt;
  </span>
  being the process ID of the application being profiled.
 </p>
 <p>
  Hereâs a simple example of running
  <span class="pre">
   nvprof
  </span>
  on the CUDA sample
  <span class="pre">
   matrixMul
  </span>
  :
 </p>
 <pre><span class="n">$</span><span class="n">nvprof</span><span class="n">matrixMul</span>
<span class="p">[</span><span class="n">Matrix</span><span class="n">Multiply</span><span class="n">Using</span><span class="n">CUDA</span><span class="p">]</span><span class="o">-</span><span class="n">Starting</span><span class="p">...</span>
<span class="o">==</span><span class="mi">27694</span><span class="o">==</span><span class="n">NVPROF</span><span class="n">is</span><span class="n">profiling</span><span class="n">process</span><span class="mi">27694</span><span class="p">,</span><span class="n">command</span><span class="o">:</span><span class="n">matrixMul</span>
<span class="n">GPU</span><span class="n">Device</span><span class="mi">0</span><span class="o">:</span><span class="s">"GeForce GT 640M LE"</span><span class="n">with</span><span class="n">compute</span><span class="n">capability</span><span class="mf">3.0</span>

<span class="n">MatrixA</span><span class="p">(</span><span class="mi">320</span><span class="p">,</span><span class="mi">320</span><span class="p">),</span><span class="n">MatrixB</span><span class="p">(</span><span class="mi">640</span><span class="p">,</span><span class="mi">320</span><span class="p">)</span>
<span class="n">Computing</span><span class="n">result</span><span class="k">using</span><span class="n">CUDA</span><span class="n">Kernel</span><span class="p">...</span>
<span class="n">done</span>
<span class="n">Performance</span><span class="o">=</span><span class="mf">35.35</span><span class="n">GFlop</span><span class="o">/</span><span class="n">s</span><span class="p">,</span><span class="n">Time</span><span class="o">=</span><span class="mf">3.708</span><span class="n">msec</span><span class="p">,</span><span class="n">Size</span><span class="o">=</span><span class="mi">131072000</span><span class="n">Ops</span><span class="p">,</span><span class="n">WorkgroupSize</span><span class="o">=</span><span class="mi">1024</span><span class="n">threads</span><span class="o">/</span><span class="n">block</span>
<span class="n">Checking</span><span class="n">computed</span><span class="n">result</span><span class="k">for</span><span class="n">correctness</span><span class="o">:</span><span class="n">OK</span>

<span class="nl">Note</span><span class="p">:</span><span class="n">For</span><span class="n">peak</span><span class="n">performance</span><span class="p">,</span><span class="n">please</span><span class="n">refer</span><span class="n">to</span><span class="n">the</span><span class="n">matrixMulCUBLAS</span><span class="n">example</span><span class="p">.</span>
<span class="o">==</span><span class="mi">27694</span><span class="o">==</span><span class="n">Profiling</span><span class="n">application</span><span class="o">:</span><span class="n">matrixMul</span>
<span class="o">==</span><span class="mi">27694</span><span class="o">==</span><span class="n">Profiling</span><span class="n">result</span><span class="o">:</span>
<span class="n">Time</span><span class="p">(</span><span class="o">%</span><span class="p">)</span><span class="n">Time</span><span class="n">Calls</span><span class="n">Avg</span><span class="n">Min</span><span class="n">Max</span><span class="n">Name</span>
<span class="mf">99.94</span><span class="o">%</span><span class="mf">1.11524</span><span class="n">s</span><span class="mi">301</span><span class="mf">3.7051</span><span class="n">ms</span><span class="mf">3.6928</span><span class="n">ms</span><span class="mf">3.7174</span><span class="n">ms</span><span class="kt">void</span><span class="n">matrixMulCUDA</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">=</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">0.04</span><span class="o">%</span><span class="mf">406.30</span><span class="n">us</span><span class="mi">2</span><span class="mf">203.15</span><span class="n">us</span><span class="mf">136.13</span><span class="n">us</span><span class="mf">270.18</span><span class="n">us</span><span class="p">[</span><span class="n">CUDA</span><span class="n">memcpy</span><span class="n">HtoD</span><span class="p">]</span>
<span class="mf">0.02</span><span class="o">%</span><span class="mf">248.29</span><span class="n">us</span><span class="mi">1</span><span class="mf">248.29</span><span class="n">us</span><span class="mf">248.29</span><span class="n">us</span><span class="mf">248.29</span><span class="n">us</span><span class="p">[</span><span class="n">CUDA</span><span class="n">memcpy</span><span class="n">DtoH</span><span class="p">]</span>

<span class="o">==</span><span class="mi">27964</span><span class="o">==</span><span class="n">API</span><span class="n">calls</span><span class="o">:</span>
<span class="n">Time</span><span class="p">(</span><span class="o">%</span><span class="p">)</span><span class="n">Time</span><span class="n">Calls</span><span class="n">Avg</span><span class="n">Min</span><span class="n">Max</span><span class="n">Name</span>
<span class="mf">49.81</span><span class="o">%</span><span class="mf">285.17</span><span class="n">ms</span><span class="mi">3</span><span class="mf">95.055</span><span class="n">ms</span><span class="mf">153.32</span><span class="n">us</span><span class="mf">284.86</span><span class="n">ms</span><span class="n">cudaMalloc</span>
<span class="mf">25.95</span><span class="o">%</span><span class="mf">148.57</span><span class="n">ms</span><span class="mi">1</span><span class="mf">148.57</span><span class="n">ms</span><span class="mf">148.57</span><span class="n">ms</span><span class="mf">148.57</span><span class="n">ms</span><span class="n">cudaEventSynchronize</span>
<span class="mf">22.23</span><span class="o">%</span><span class="mf">127.28</span><span class="n">ms</span><span class="mi">1</span><span class="mf">127.28</span><span class="n">ms</span><span class="mf">127.28</span><span class="n">ms</span><span class="mf">127.28</span><span class="n">ms</span><span class="n">cudaDeviceReset</span>
<span class="mf">1.33</span><span class="o">%</span><span class="mf">7.6314</span><span class="n">ms</span><span class="mi">301</span><span class="mf">25.353</span><span class="n">us</span><span class="mf">23.551</span><span class="n">us</span><span class="mf">143.98</span><span class="n">us</span><span class="n">cudaLaunch</span>
<span class="mf">0.25</span><span class="o">%</span><span class="mf">1.4343</span><span class="n">ms</span><span class="mi">3</span><span class="mf">478.09</span><span class="n">us</span><span class="mf">155.84</span><span class="n">us</span><span class="mf">984.38</span><span class="n">us</span><span class="n">cudaMemcpy</span>
<span class="mf">0.11</span><span class="o">%</span><span class="mf">601.45</span><span class="n">us</span><span class="mi">1</span><span class="mf">601.45</span><span class="n">us</span><span class="mf">601.45</span><span class="n">us</span><span class="mf">601.45</span><span class="n">us</span><span class="n">cudaDeviceSynchronize</span>
<span class="mf">0.10</span><span class="o">%</span><span class="mf">564.48</span><span class="n">us</span><span class="mi">1505</span><span class="mi">375</span><span class="n">ns</span><span class="mi">313</span><span class="n">ns</span><span class="mf">3.6790</span><span class="n">us</span><span class="n">cudaSetupArgument</span>
<span class="mf">0.09</span><span class="o">%</span><span class="mf">490.44</span><span class="n">us</span><span class="mi">76</span><span class="mf">6.4530</span><span class="n">us</span><span class="mi">307</span><span class="n">ns</span><span class="mf">221.93</span><span class="n">us</span><span class="n">cuDeviceGetAttribute</span>
<span class="mf">0.07</span><span class="o">%</span><span class="mf">406.61</span><span class="n">us</span><span class="mi">3</span><span class="mf">135.54</span><span class="n">us</span><span class="mf">115.07</span><span class="n">us</span><span class="mf">169.99</span><span class="n">us</span><span class="n">cudaFree</span>
<span class="mf">0.02</span><span class="o">%</span><span class="mf">143.00</span><span class="n">us</span><span class="mi">301</span><span class="mi">475</span><span class="n">ns</span><span class="mi">431</span><span class="n">ns</span><span class="mf">2.4370</span><span class="n">us</span><span class="n">cudaConfigureCall</span>
<span class="mf">0.01</span><span class="o">%</span><span class="mf">42.321</span><span class="n">us</span><span class="mi">1</span><span class="mf">42.321</span><span class="n">us</span><span class="mf">42.321</span><span class="n">us</span><span class="mf">42.321</span><span class="n">us</span><span class="n">cuDeviceTotalMem</span>
<span class="mf">0.01</span><span class="o">%</span><span class="mf">33.655</span><span class="n">us</span><span class="mi">1</span><span class="mf">33.655</span><span class="n">us</span><span class="mf">33.655</span><span class="n">us</span><span class="mf">33.655</span><span class="n">us</span><span class="n">cudaGetDeviceProperties</span>
<span class="mf">0.01</span><span class="o">%</span><span class="mf">31.900</span><span class="n">us</span><span class="mi">1</span><span class="mf">31.900</span><span class="n">us</span><span class="mf">31.900</span><span class="n">us</span><span class="mf">31.900</span><span class="n">us</span><span class="n">cuDeviceGetName</span>
<span class="mf">0.00</span><span class="o">%</span><span class="mf">21.874</span><span class="n">us</span><span class="mi">2</span><span class="mf">10.937</span><span class="n">us</span><span class="mf">8.5850</span><span class="n">us</span><span class="mf">13.289</span><span class="n">us</span><span class="n">cudaEventRecord</span>
<span class="mf">0.00</span><span class="o">%</span><span class="mf">16.513</span><span class="n">us</span><span class="mi">2</span><span class="mf">8.2560</span><span class="n">us</span><span class="mf">2.6240</span><span class="n">us</span><span class="mf">13.889</span><span class="n">us</span><span class="n">cudaEventCreate</span>
<span class="mf">0.00</span><span class="o">%</span><span class="mf">13.091</span><span class="n">us</span><span class="mi">1</span><span class="mf">13.091</span><span class="n">us</span><span class="mf">13.091</span><span class="n">us</span><span class="mf">13.091</span><span class="n">us</span><span class="n">cudaEventElapsedTime</span>
<span class="mf">0.00</span><span class="o">%</span><span class="mf">8.1410</span><span class="n">us</span><span class="mi">1</span><span class="mf">8.1410</span><span class="n">us</span><span class="mf">8.1410</span><span class="n">us</span><span class="mf">8.1410</span><span class="n">us</span><span class="n">cudaGetDevice</span>
<span class="mf">0.00</span><span class="o">%</span><span class="mf">2.6290</span><span class="n">us</span><span class="mi">2</span><span class="mf">1.3140</span><span class="n">us</span><span class="mi">509</span><span class="n">ns</span><span class="mf">2.1200</span><span class="n">us</span><span class="n">cuDeviceGetCount</span>
<span class="mf">0.00</span><span class="o">%</span><span class="mf">1.9970</span><span class="n">us</span><span class="mi">2</span><span class="mi">998</span><span class="n">ns</span><span class="mi">520</span><span class="n">ns</span><span class="mf">1.4770</span><span class="n">us</span><span class="n">cuDeviceGet</span>
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  API trace can be turned off, if not needed, by using
  <span class="pre">
   --profile-api-trace
  </span>
  <span class="pre">
   none
  </span>
  . This reduces some of the profiling overhead, especially when the kernels are short.
 </p>
 <p>
  If multiple CUDA capable devices are profiled,
  <span class="pre">
   nvprof
  </span>
  <span class="pre">
   --print-summary-per-gpu
  </span>
  can be used to print one summary per GPU.
 </p>
 <p>
  <span class="pre">
   nvprof
  </span>
  supports CUDA Dynamic Parallelism in summary mode. If your application uses Dynamic Parallelism, the output will contain one column for the number of host-launched kernels and one for the number of device-launched kernels. Hereâs an example of running
  <span class="pre">
   nvprof
  </span>
  on the CUDA Dynamic Parallelism sample
  <span class="pre">
   cdpSimpleQuicksort
  </span>
  :
 </p>
 <pre><span class="n">$</span><span class="n">nvprof</span><span class="n">cdpSimpleQuicksort</span>
<span class="o">==</span><span class="mi">27325</span><span class="o">==</span><span class="n">NVPROF</span><span class="n">is</span><span class="n">profiling</span><span class="n">process</span><span class="mi">27325</span><span class="p">,</span><span class="n">command</span><span class="o">:</span><span class="n">cdpSimpleQuicksort</span>
</pre>
 <pre><span class="n">Running</span><span class="n">on</span><span class="n">GPU</span><span class="mi">0</span><span class="p">(</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">)</span>
<span class="n">Initializing</span><span class="n">data</span><span class="o">:</span>
<span class="n">Running</span><span class="n">quicksort</span><span class="n">on</span><span class="mi">128</span><span class="n">elements</span>
<span class="n">Launching</span><span class="n">kernel</span><span class="n">on</span><span class="n">the</span><span class="n">GPU</span>
<span class="n">Validating</span><span class="n">results</span><span class="o">:</span><span class="n">OK</span>
<span class="o">==</span><span class="mi">27325</span><span class="o">==</span><span class="n">Profiling</span><span class="n">application</span><span class="o">:</span><span class="n">cdpSimpleQuicksort</span>
<span class="o">==</span><span class="mi">27325</span><span class="o">==</span><span class="n">Profiling</span><span class="n">result</span><span class="o">:</span>
<span class="n">Time</span><span class="p">(</span><span class="o">%</span><span class="p">)</span><span class="n">Time</span><span class="n">Calls</span><span class="p">(</span><span class="n">host</span><span class="p">)</span><span class="n">Calls</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="n">Avg</span><span class="n">Min</span><span class="n">Max</span><span class="n">Name</span>
<span class="mf">99.71</span><span class="o">%</span><span class="mf">1.2114</span><span class="n">ms</span><span class="mi">1</span><span class="mi">14</span><span class="mf">80.761</span><span class="n">us</span><span class="mf">5.1200</span><span class="n">us</span><span class="mf">145.66</span><span class="n">us</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">0.18</span><span class="o">%</span><span class="mf">2.2080</span><span class="n">us</span><span class="mi">1</span><span class="o">-</span><span class="mf">2.2080</span><span class="n">us</span><span class="mf">2.2080</span><span class="n">us</span><span class="mf">2.2080</span><span class="n">us</span><span class="p">[</span><span class="n">CUDA</span><span class="n">memcpy</span><span class="n">DtoH</span><span class="p">]</span>
<span class="mf">0.11</span><span class="o">%</span><span class="mf">1.2800</span><span class="n">us</span><span class="mi">1</span><span class="o">-</span><span class="mf">1.2800</span><span class="n">us</span><span class="mf">1.2800</span><span class="n">us</span><span class="mf">1.2800</span><span class="n">us</span><span class="p">[</span><span class="n">CUDA</span><span class="n">memcpy</span><span class="n">HtoD</span><span class="p">]</span>
</pre>
 <h3>
  <span class="section-number">
   3.2.2.
  </span>
  GPU-Trace and API-Trace Modes
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#gpu-trace-and-api-trace-modes" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  GPU-Trace and API-Trace modes can be enabled individually or together. GPU-Trace mode provides a timeline of all activities taking place on the GPU in chronological order. Each kernel execution and memory copy/set instance is shown in the output. For each kernel or memory copy, detailed information such as kernel parameters, shared memory usage and memory transfer throughput are shown. The number shown in the square brackets after the kernel name correlates to the CUDA API that launched that kernel.
 </p>
 <p>
  Hereâs an example:
 </p>
 <pre><span class="n">$</span><span class="n">nvprof</span><span class="o">--</span><span class="n">print</span><span class="o">-</span><span class="n">gpu</span><span class="o">-</span><span class="n">trace</span><span class="n">matrixMul</span>
<span class="o">==</span><span class="mi">27706</span><span class="o">==</span><span class="n">NVPROF</span><span class="n">is</span><span class="n">profiling</span><span class="n">process</span><span class="mi">27706</span><span class="p">,</span><span class="n">command</span><span class="o">:</span><span class="n">matrixMul</span>
</pre>
 <pre><span class="o">==</span><span class="mi">27706</span><span class="o">==</span><span class="n">Profiling</span><span class="n">application</span><span class="o">:</span><span class="n">matrixMul</span>
<span class="p">[</span><span class="n">Matrix</span><span class="n">Multiply</span><span class="n">Using</span><span class="n">CUDA</span><span class="p">]</span><span class="o">-</span><span class="n">Starting</span><span class="p">...</span>
<span class="n">GPU</span><span class="n">Device</span><span class="mi">0</span><span class="o">:</span><span class="s">"GeForce GT 640M LE"</span><span class="n">with</span><span class="n">compute</span><span class="n">capability</span><span class="mf">3.0</span>

<span class="n">MatrixA</span><span class="p">(</span><span class="mi">320</span><span class="p">,</span><span class="mi">320</span><span class="p">),</span><span class="n">MatrixB</span><span class="p">(</span><span class="mi">640</span><span class="p">,</span><span class="mi">320</span><span class="p">)</span>
<span class="n">Computing</span><span class="n">result</span><span class="k">using</span><span class="n">CUDA</span><span class="n">Kernel</span><span class="p">...</span>
<span class="n">done</span>
<span class="n">Performance</span><span class="o">=</span><span class="mf">35.36</span><span class="n">GFlop</span><span class="o">/</span><span class="n">s</span><span class="p">,</span><span class="n">Time</span><span class="o">=</span><span class="mf">3.707</span><span class="n">msec</span><span class="p">,</span><span class="n">Size</span><span class="o">=</span><span class="mi">131072000</span><span class="n">Ops</span><span class="p">,</span><span class="n">WorkgroupSize</span><span class="o">=</span><span class="mi">1024</span><span class="n">threads</span><span class="o">/</span><span class="n">block</span>
<span class="n">Checking</span><span class="n">computed</span><span class="n">result</span><span class="k">for</span><span class="n">correctness</span><span class="o">:</span><span class="n">OK</span>

<span class="nl">Note</span><span class="p">:</span><span class="n">For</span><span class="n">peak</span><span class="n">performance</span><span class="p">,</span><span class="n">please</span><span class="n">refer</span><span class="n">to</span><span class="n">the</span><span class="n">matrixMulCUBLAS</span><span class="n">example</span><span class="p">.</span>
<span class="o">==</span><span class="mi">27706</span><span class="o">==</span><span class="n">Profiling</span><span class="n">result</span><span class="o">:</span>
<span class="n">Start</span><span class="n">Duration</span><span class="n">Grid</span><span class="n">Size</span><span class="n">Block</span><span class="n">Size</span><span class="n">Regs</span><span class="o">*</span><span class="n">SSMem</span><span class="o">*</span><span class="n">DSMem</span><span class="o">*</span><span class="n">Size</span><span class="n">Throughput</span><span class="n">Device</span><span class="n">Context</span><span class="n">Stream</span><span class="n">Name</span>
<span class="mf">133.81</span><span class="n">ms</span><span class="mf">135.78</span><span class="n">us</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="mf">409.60</span><span class="n">KB</span><span class="mf">3.0167</span><span class="n">GB</span><span class="o">/</span><span class="n">s</span><span class="n">GeForce</span><span class="n">GT</span><span class="mi">640</span><span class="n">M</span><span class="mi">1</span><span class="mi">2</span><span class="p">[</span><span class="n">CUDA</span><span class="n">memcpy</span><span class="n">HtoD</span><span class="p">]</span>
<span class="mf">134.62</span><span class="n">ms</span><span class="mf">270.66</span><span class="n">us</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="mf">819.20</span><span class="n">KB</span><span class="mf">3.0267</span><span class="n">GB</span><span class="o">/</span><span class="n">s</span><span class="n">GeForce</span><span class="n">GT</span><span class="mi">640</span><span class="n">M</span><span class="mi">1</span><span class="mi">2</span><span class="p">[</span><span class="n">CUDA</span><span class="n">memcpy</span><span class="n">HtoD</span><span class="p">]</span>
<span class="mf">134.90</span><span class="n">ms</span><span class="mf">3.7037</span><span class="n">ms</span><span class="p">(</span><span class="mi">20</span><span class="mi">10</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">32</span><span class="mi">32</span><span class="mi">1</span><span class="p">)</span><span class="mi">29</span><span class="mf">8.1920</span><span class="n">KB</span><span class="mi">0</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">GeForce</span><span class="n">GT</span><span class="mi">640</span><span class="n">M</span><span class="mi">1</span><span class="mi">2</span><span class="kt">void</span><span class="n">matrixMulCUDA</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">=</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span><span class="p">[</span><span class="mi">94</span><span class="p">]</span>
<span class="mf">138.71</span><span class="n">ms</span><span class="mf">3.7011</span><span class="n">ms</span><span class="p">(</span><span class="mi">20</span><span class="mi">10</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">32</span><span class="mi">32</span><span class="mi">1</span><span class="p">)</span><span class="mi">29</span><span class="mf">8.1920</span><span class="n">KB</span><span class="mi">0</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">GeForce</span><span class="n">GT</span><span class="mi">640</span><span class="n">M</span><span class="mi">1</span><span class="mi">2</span><span class="kt">void</span><span class="n">matrixMulCUDA</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">=</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span><span class="p">[</span><span class="mi">105</span><span class="p">]</span>
<span class="o">&lt;</span><span class="p">...</span><span class="n">more</span><span class="n">output</span><span class="p">...</span><span class="o">&gt;</span>
<span class="mf">1.24341</span><span class="n">s</span><span class="mf">3.7011</span><span class="n">ms</span><span class="p">(</span><span class="mi">20</span><span class="mi">10</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">32</span><span class="mi">32</span><span class="mi">1</span><span class="p">)</span><span class="mi">29</span><span class="mf">8.1920</span><span class="n">KB</span><span class="mi">0</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">GeForce</span><span class="n">GT</span><span class="mi">640</span><span class="n">M</span><span class="mi">1</span><span class="mi">2</span><span class="kt">void</span><span class="n">matrixMulCUDA</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">=</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span><span class="p">[</span><span class="mi">2191</span><span class="p">]</span>
<span class="mf">1.24711</span><span class="n">s</span><span class="mf">3.7046</span><span class="n">ms</span><span class="p">(</span><span class="mi">20</span><span class="mi">10</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">32</span><span class="mi">32</span><span class="mi">1</span><span class="p">)</span><span class="mi">29</span><span class="mf">8.1920</span><span class="n">KB</span><span class="mi">0</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">GeForce</span><span class="n">GT</span><span class="mi">640</span><span class="n">M</span><span class="mi">1</span><span class="mi">2</span><span class="kt">void</span><span class="n">matrixMulCUDA</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">=</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span><span class="p">[</span><span class="mi">2198</span><span class="p">]</span>
<span class="mf">1.25089</span><span class="n">s</span><span class="mf">248.13</span><span class="n">us</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="mf">819.20</span><span class="n">KB</span><span class="mf">3.3015</span><span class="n">GB</span><span class="o">/</span><span class="n">s</span><span class="n">GeForce</span><span class="n">GT</span><span class="mi">640</span><span class="n">M</span><span class="mi">1</span><span class="mi">2</span><span class="p">[</span><span class="n">CUDA</span><span class="n">memcpy</span><span class="n">DtoH</span><span class="p">]</span>

<span class="nl">Regs</span><span class="p">:</span><span class="n">Number</span><span class="n">of</span><span class="n">registers</span><span class="n">used</span><span class="n">per</span><span class="n">CUDA</span><span class="kr">thread</span><span class="p">.</span><span class="n">This</span><span class="n">number</span><span class="n">includes</span><span class="n">registers</span><span class="n">used</span><span class="n">internally</span><span class="n">by</span><span class="n">the</span><span class="n">CUDA</span><span class="n">driver</span><span class="n">and</span><span class="o">/</span><span class="n">or</span><span class="n">tools</span><span class="n">and</span><span class="n">can</span><span class="n">be</span><span class="n">more</span><span class="n">than</span><span class="n">what</span><span class="n">the</span><span class="n">compiler</span><span class="n">shows</span><span class="p">.</span>
<span class="nl">SSMem</span><span class="p">:</span><span class="n">Static</span><span class="n">shared</span><span class="n">memory</span><span class="n">allocated</span><span class="n">per</span><span class="n">CUDA</span><span class="n">block</span><span class="p">.</span>
<span class="nl">DSMem</span><span class="p">:</span><span class="n">Dynamic</span><span class="n">shared</span><span class="n">memory</span><span class="n">allocated</span><span class="n">per</span><span class="n">CUDA</span><span class="n">block</span><span class="p">.</span>
</pre>
 <p>
  <span class="pre">
   nvprof
  </span>
  supports CUDA Dynamic Parallelism in GPU-Trace mode. For host kernel launch, the kernel ID will be shown. For device kernel launch, the kernel ID, parent kernel ID and parent block will be shown. Hereâs an example:
 </p>
 <pre><span class="n">$nvprof</span><span class="o">--</span><span class="n">print</span><span class="o">-</span><span class="n">gpu</span><span class="o">-</span><span class="n">trace</span><span class="n">cdpSimpleQuicksort</span>
<span class="o">==</span><span class="mi">28128</span><span class="o">==</span><span class="n">NVPROF</span><span class="n">is</span><span class="n">profiling</span><span class="n">process</span><span class="mi">28128</span><span class="p">,</span><span class="n">command</span><span class="o">:</span><span class="n">cdpSimpleQuicksort</span>
</pre>
 <pre><span class="n">Running</span><span class="n">on</span><span class="n">GPU</span><span class="mi">0</span><span class="p">(</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">)</span>
<span class="n">Initializing</span><span class="n">data</span><span class="o">:</span>
<span class="n">Running</span><span class="n">quicksort</span><span class="n">on</span><span class="mi">128</span><span class="n">elements</span>
<span class="n">Launching</span><span class="n">kernel</span><span class="n">on</span><span class="n">the</span><span class="n">GPU</span>
<span class="n">Validating</span><span class="n">results</span><span class="o">:</span><span class="n">OK</span>
<span class="o">==</span><span class="mi">28128</span><span class="o">==</span><span class="n">Profiling</span><span class="n">application</span><span class="o">:</span><span class="n">cdpSimpleQuicksort</span>
<span class="o">==</span><span class="mi">28128</span><span class="o">==</span><span class="n">Profiling</span><span class="n">result</span><span class="o">:</span>
<span class="n">Start</span><span class="n">Duration</span><span class="n">Grid</span><span class="n">Size</span><span class="n">Block</span><span class="n">Size</span><span class="n">Regs</span><span class="o">*</span><span class="n">SSMem</span><span class="o">*</span><span class="n">DSMem</span><span class="o">*</span><span class="n">Size</span><span class="n">Throughput</span><span class="n">Device</span><span class="n">Context</span><span class="n">Stream</span><span class="n">ID</span><span class="n">Parent</span><span class="n">ID</span><span class="n">Parent</span><span class="n">Block</span><span class="n">Name</span>
<span class="mf">192.76</span><span class="n">ms</span><span class="mf">1.2800</span><span class="n">us</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="mi">512</span><span class="n">B</span><span class="mf">400.00</span><span class="n">MB</span><span class="o">/</span><span class="n">s</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="p">[</span><span class="n">CUDA</span><span class="n">memcpy</span><span class="n">HtoD</span><span class="p">]</span>
<span class="mf">193.31</span><span class="n">ms</span><span class="mf">146.02</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">0</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">2</span><span class="o">-</span><span class="o">-</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span><span class="p">[</span><span class="mi">171</span><span class="p">]</span>
<span class="mf">193.41</span><span class="n">ms</span><span class="mf">110.53</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-5</span><span class="mi">2</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.45</span><span class="n">ms</span><span class="mf">125.57</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-6</span><span class="mi">2</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.48</span><span class="n">ms</span><span class="mf">9.2480</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-7</span><span class="mi">-5</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.52</span><span class="n">ms</span><span class="mf">107.23</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-8</span><span class="mi">-5</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.53</span><span class="n">ms</span><span class="mf">93.824</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-9</span><span class="mi">-6</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.57</span><span class="n">ms</span><span class="mf">117.47</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-10</span><span class="mi">-6</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.58</span><span class="n">ms</span><span class="mf">5.0560</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-11</span><span class="mi">-8</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.62</span><span class="n">ms</span><span class="mf">108.06</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-12</span><span class="mi">-8</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.65</span><span class="n">ms</span><span class="mf">113.34</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-13</span><span class="mi">-10</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.68</span><span class="n">ms</span><span class="mf">29.536</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-14</span><span class="mi">-12</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.69</span><span class="n">ms</span><span class="mf">22.848</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-15</span><span class="mi">-10</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.71</span><span class="n">ms</span><span class="mf">130.85</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-16</span><span class="mi">-13</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.73</span><span class="n">ms</span><span class="mf">62.432</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-17</span><span class="mi">-12</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.76</span><span class="n">ms</span><span class="mf">41.024</span><span class="n">us</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="p">(</span><span class="mi">1</span><span class="mi">1</span><span class="mi">1</span><span class="p">)</span><span class="mi">32</span><span class="mi">0</span><span class="n">B</span><span class="mi">256</span><span class="n">B</span><span class="o">-</span><span class="o">-</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="mi">-18</span><span class="mi">-13</span><span class="p">(</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span><span class="p">)</span><span class="n">cdp_simple_quicksort</span><span class="p">(</span><span class="kt">unsigned</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mf">193.92</span><span class="n">ms</span><span class="mf">2.1760</span><span class="n">us</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="mi">512</span><span class="n">B</span><span class="mf">235.29</span><span class="n">MB</span><span class="o">/</span><span class="n">s</span><span class="n">Tesla</span><span class="n">K20c</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="mi">1</span><span class="mi">2</span><span class="o">-</span><span class="o">-</span><span class="o">-</span><span class="p">[</span><span class="n">CUDA</span><span class="n">memcpy</span><span class="n">DtoH</span><span class="p">]</span>

<span class="nl">Regs</span><span class="p">:</span><span class="n">Number</span><span class="n">of</span><span class="n">registers</span><span class="n">used</span><span class="n">per</span><span class="n">CUDA</span><span class="kr">thread</span><span class="p">.</span><span class="n">This</span><span class="n">number</span><span class="n">includes</span><span class="n">registers</span><span class="n">used</span><span class="n">internally</span><span class="n">by</span><span class="n">the</span><span class="n">CUDA</span><span class="n">driver</span><span class="n">and</span><span class="o">/</span><span class="n">or</span><span class="n">tools</span><span class="n">and</span><span class="n">can</span><span class="n">be</span><span class="n">more</span><span class="n">than</span><span class="n">what</span><span class="n">the</span><span class="n">compiler</span><span class="n">shows</span><span class="p">.</span>
<span class="nl">SSMem</span><span class="p">:</span><span class="n">Static</span><span class="n">shared</span><span class="n">memory</span><span class="n">allocated</span><span class="n">per</span><span class="n">CUDA</span><span class="n">block</span><span class="p">.</span>
<span class="nl">DSMem</span><span class="p">:</span><span class="n">Dynamic</span><span class="n">shared</span><span class="n">memory</span><span class="n">allocated</span><span class="n">per</span><span class="n">CUDA</span><span class="n">block</span><span class="p">.</span>
</pre>
 <p>
  API-trace mode shows the timeline of all CUDA runtime and driver API calls invoked on the host in chronological order. Hereâs an example:
 </p>
 <pre><span class="n">$nvprof</span><span class="o">--</span><span class="n">print</span><span class="o">-</span><span class="n">api</span><span class="o">-</span><span class="n">trace</span><span class="n">matrixMul</span>
<span class="o">==</span><span class="mi">27722</span><span class="o">==</span><span class="n">NVPROF</span><span class="n">is</span><span class="n">profiling</span><span class="n">process</span><span class="mi">27722</span><span class="p">,</span><span class="n">command</span><span class="o">:</span><span class="n">matrixMul</span>
</pre>
 <pre><span class="o">==</span><span class="mi">27722</span><span class="o">==</span><span class="n">Profiling</span><span class="n">application</span><span class="o">:</span><span class="n">matrixMul</span>
<span class="p">[</span><span class="n">Matrix</span><span class="n">Multiply</span><span class="n">Using</span><span class="n">CUDA</span><span class="p">]</span><span class="o">-</span><span class="n">Starting</span><span class="p">...</span>
<span class="n">GPU</span><span class="n">Device</span><span class="mi">0</span><span class="o">:</span><span class="s">"GeForce GT 640M LE"</span><span class="n">with</span><span class="n">compute</span><span class="n">capability</span><span class="mf">3.0</span>

<span class="n">MatrixA</span><span class="p">(</span><span class="mi">320</span><span class="p">,</span><span class="mi">320</span><span class="p">),</span><span class="n">MatrixB</span><span class="p">(</span><span class="mi">640</span><span class="p">,</span><span class="mi">320</span><span class="p">)</span>
<span class="n">Computing</span><span class="n">result</span><span class="k">using</span><span class="n">CUDA</span><span class="n">Kernel</span><span class="p">...</span>
<span class="n">done</span>
<span class="n">Performance</span><span class="o">=</span><span class="mf">35.35</span><span class="n">GFlop</span><span class="o">/</span><span class="n">s</span><span class="p">,</span><span class="n">Time</span><span class="o">=</span><span class="mf">3.708</span><span class="n">msec</span><span class="p">,</span><span class="n">Size</span><span class="o">=</span><span class="mi">131072000</span><span class="n">Ops</span><span class="p">,</span><span class="n">WorkgroupSize</span><span class="o">=</span><span class="mi">1024</span><span class="n">threads</span><span class="o">/</span><span class="n">block</span>
<span class="n">Checking</span><span class="n">computed</span><span class="n">result</span><span class="k">for</span><span class="n">correctness</span><span class="o">:</span><span class="n">OK</span>

<span class="nl">Note</span><span class="p">:</span><span class="n">For</span><span class="n">peak</span><span class="n">performance</span><span class="p">,</span><span class="n">please</span><span class="n">refer</span><span class="n">to</span><span class="n">the</span><span class="n">matrixMulCUBLAS</span><span class="n">example</span><span class="p">.</span>
<span class="o">==</span><span class="mi">27722</span><span class="o">==</span><span class="n">Profiling</span><span class="n">result</span><span class="o">:</span>
<span class="n">Start</span><span class="n">Duration</span><span class="n">Name</span>
<span class="mf">108.38</span><span class="n">ms</span><span class="mf">6.2130</span><span class="n">us</span><span class="n">cuDeviceGetCount</span>
<span class="mf">108.42</span><span class="n">ms</span><span class="mi">840</span><span class="n">ns</span><span class="n">cuDeviceGet</span>
<span class="mf">108.42</span><span class="n">ms</span><span class="mf">22.459</span><span class="n">us</span><span class="n">cuDeviceGetName</span>
<span class="mf">108.45</span><span class="n">ms</span><span class="mf">11.782</span><span class="n">us</span><span class="n">cuDeviceTotalMem</span>
<span class="mf">108.46</span><span class="n">ms</span><span class="mi">945</span><span class="n">ns</span><span class="n">cuDeviceGetAttribute</span>
<span class="mf">149.37</span><span class="n">ms</span><span class="mf">23.737</span><span class="n">us</span><span class="n">cudaLaunch</span><span class="p">(</span><span class="kt">void</span><span class="n">matrixMulCUDA</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">=</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span><span class="p">[</span><span class="mi">2198</span><span class="p">])</span>
<span class="mf">149.39</span><span class="n">ms</span><span class="mf">6.6290</span><span class="n">us</span><span class="n">cudaEventRecord</span>
<span class="mf">149.40</span><span class="n">ms</span><span class="mf">1.10156</span><span class="n">s</span><span class="n">cudaEventSynchronize</span>
<span class="o">&lt;</span><span class="p">...</span><span class="n">more</span><span class="n">output</span><span class="p">...</span><span class="o">&gt;</span>
<span class="mf">1.25096</span><span class="n">s</span><span class="mf">21.543</span><span class="n">us</span><span class="n">cudaEventElapsedTime</span>
<span class="mf">1.25103</span><span class="n">s</span><span class="mf">1.5462</span><span class="n">ms</span><span class="n">cudaMemcpy</span>
<span class="mf">1.25467</span><span class="n">s</span><span class="mf">153.93</span><span class="n">us</span><span class="n">cudaFree</span>
<span class="mf">1.25483</span><span class="n">s</span><span class="mf">75.373</span><span class="n">us</span><span class="n">cudaFree</span>
<span class="mf">1.25491</span><span class="n">s</span><span class="mf">75.564</span><span class="n">us</span><span class="n">cudaFree</span>
<span class="mf">1.25693</span><span class="n">s</span><span class="mf">10.901</span><span class="n">ms</span><span class="n">cudaDeviceReset</span>
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Due to the way the profiler is setup, the first âcuInit()â driver API call is never traced.
 </p>
 <h3>
  <span class="section-number">
   3.2.3.
  </span>
  Event/metric Summary Mode
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#event-metric-summary-mode" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  To see a list of all available events on a particular NVIDIA GPU, use the
  <span class="pre">
   --query-events
  </span>
  option. To see a list of all available metrics on a particular NVIDIA GPU, use the
  <span class="pre">
   --query-metrics
  </span>
  option.
  <span class="pre">
   nvprof
  </span>
  is able to collect multiple events/metrics at the same time. Hereâs an example:
 </p>
 <pre><span class="n">$</span><span class="n">nvprof</span><span class="o">--</span><span class="n">events</span><span class="n">warps_launched</span><span class="p">,</span><span class="n">local_load</span><span class="o">--</span><span class="n">metrics</span><span class="n">ipc</span><span class="n">matrixMul</span>
<span class="p">[</span><span class="n">Matrix</span><span class="n">Multiply</span><span class="n">Using</span><span class="n">CUDA</span><span class="p">]</span><span class="o">-</span><span class="n">Starting</span><span class="p">...</span>
<span class="o">==</span><span class="mi">6461</span><span class="o">==</span><span class="n">NVPROF</span><span class="n">is</span><span class="n">profiling</span><span class="n">process</span><span class="mi">6461</span><span class="p">,</span><span class="n">command</span><span class="o">:</span><span class="n">matrixMul</span>
</pre>
 <pre><span class="n">GPU</span><span class="n">Device</span><span class="mi">0</span><span class="o">:</span><span class="s">"GeForce GTX TITAN"</span><span class="n">with</span><span class="n">compute</span><span class="n">capability</span><span class="mf">3.5</span>

<span class="n">MatrixA</span><span class="p">(</span><span class="mi">320</span><span class="p">,</span><span class="mi">320</span><span class="p">),</span><span class="n">MatrixB</span><span class="p">(</span><span class="mi">640</span><span class="p">,</span><span class="mi">320</span><span class="p">)</span>
<span class="n">Computing</span><span class="n">result</span><span class="k">using</span><span class="n">CUDA</span><span class="n">Kernel</span><span class="p">...</span>
<span class="o">==</span><span class="mi">6461</span><span class="o">==</span><span class="n">Warning</span><span class="o">:</span><span class="n">Some</span><span class="n">kernel</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="n">will</span><span class="n">be</span><span class="n">replayed</span><span class="n">on</span><span class="n">device</span><span class="mi">0</span><span class="n">in</span><span class="n">order</span><span class="n">to</span><span class="n">collect</span><span class="n">all</span><span class="n">events</span><span class="o">/</span><span class="n">metrics</span><span class="p">.</span>
<span class="n">done</span>
<span class="n">Performance</span><span class="o">=</span><span class="mf">6.39</span><span class="n">GFlop</span><span class="o">/</span><span class="n">s</span><span class="p">,</span><span class="n">Time</span><span class="o">=</span><span class="mf">20.511</span><span class="n">msec</span><span class="p">,</span><span class="n">Size</span><span class="o">=</span><span class="mi">131072000</span><span class="n">Ops</span><span class="p">,</span><span class="n">WorkgroupSize</span><span class="o">=</span><span class="mi">1024</span><span class="n">threads</span><span class="o">/</span><span class="n">block</span>
<span class="n">Checking</span><span class="n">computed</span><span class="n">result</span><span class="k">for</span><span class="n">correctness</span><span class="o">:</span><span class="n">Result</span><span class="o">=</span><span class="n">PASS</span>

<span class="nl">NOTE</span><span class="p">:</span><span class="n">The</span><span class="n">CUDA</span><span class="n">Samples</span><span class="n">are</span><span class="n">not</span><span class="n">meant</span><span class="k">for</span><span class="n">performance</span><span class="n">measurements</span><span class="p">.</span><span class="n">Results</span><span class="n">may</span><span class="n">vary</span><span class="n">when</span><span class="n">GPU</span><span class="n">Boost</span><span class="n">is</span><span class="n">enabled</span><span class="p">.</span>
<span class="o">==</span><span class="mi">6461</span><span class="o">==</span><span class="n">Profiling</span><span class="n">application</span><span class="o">:</span><span class="n">matrixMul</span>
<span class="o">==</span><span class="mi">6461</span><span class="o">==</span><span class="n">Profiling</span><span class="n">result</span><span class="o">:</span>
<span class="o">==</span><span class="mi">6461</span><span class="o">==</span><span class="n">Event</span><span class="n">result</span><span class="o">:</span>
<span class="n">Invocations</span><span class="n">Event</span><span class="n">Name</span><span class="n">Min</span><span class="n">Max</span><span class="n">Avg</span>
<span class="n">Device</span><span class="s">"GeForce GTX TITAN (0)"</span>
<span class="nl">Kernel</span><span class="p">:</span><span class="kt">void</span><span class="n">matrixMulCUDA</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">=</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mi">301</span><span class="n">warps_launched</span><span class="mi">6400</span><span class="mi">6400</span><span class="mi">6400</span>
<span class="mi">301</span><span class="n">local_load</span><span class="mi">0</span><span class="mi">0</span><span class="mi">0</span>

<span class="o">==</span><span class="mi">6461</span><span class="o">==</span><span class="n">Metric</span><span class="n">result</span><span class="o">:</span>
<span class="n">Invocations</span><span class="n">Metric</span><span class="n">Name</span><span class="n">Metric</span><span class="n">Description</span><span class="n">Min</span><span class="n">Max</span><span class="n">Avg</span>
<span class="n">Device</span><span class="s">"GeForce GTX TITAN (0)"</span>
<span class="nl">Kernel</span><span class="p">:</span><span class="kt">void</span><span class="n">matrixMulCUDA</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">=</span><span class="mi">32</span><span class="o">&gt;</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="kt">int</span><span class="p">,</span><span class="kt">int</span><span class="p">)</span>
<span class="mi">301</span><span class="n">ipc</span><span class="n">Executed</span><span class="n">IPC</span><span class="mf">1.282576</span><span class="mf">1.299736</span><span class="mf">1.291500</span>
</pre>
 <p>
  If the specified events/metrics canât be profiled in a single run of the application,
  <span class="pre">
   nvprof
  </span>
  by default replays each kernel multiple times until all the events/metrics are collected.
 </p>
 <p>
  The
  <span class="pre">
   --replay-mode
  </span>
  <span class="pre">
   &lt;mode&gt;
  </span>
  option can be used to change the replay mode. In âapplication replayâ mode,
  <span class="pre">
   nvprof
  </span>
  re-runs the whole application instead of replaying each kernel, in order to collect all events/metrics. In some cases this mode can be faster than kernel replay mode if the application allocates large amount of device memory. Replay can also be turned off entirely, in which case the profiler will not collect some events/metrics.
 </p>
 <p>
  To collect all events available on each device, use the option
  <span class="pre">
   --events
  </span>
  <span class="pre">
   all
  </span>
  .
 </p>
 <p>
  To collect all metrics available on each device, use the option
  <span class="pre">
   --metrics
  </span>
  <span class="pre">
   all
  </span>
  .
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Events or metrics collection may significantly change the overall performance characteristics of the application because all kernel executions are serialized on the GPU.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  If a large number of events or metrics are requested, no matter which replay mode is chosen, the overall application execution time may increase significantly.
 </p>
 <h3>
  <span class="section-number">
   3.2.4.
  </span>
  Event/metric Trace Mode
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#event-metric-trace-mode" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  In event/metric trace mode, event and metric values are shown for each kernel execution. By default, event and metric values are aggregated across all units in the GPU. For example, multiprocessor specific events are aggregated across all multiprocessors on the GPU. If
  <span class="pre">
   --aggregate-mode
  </span>
  <span class="pre">
   off
  </span>
  is specified, values of each unit are shown. For example, in the following example, the âbranchâ event value is shown for each multiprocessor on the GPU:
 </p>
 <pre><span class="n">$</span><span class="n">nvprof</span><span class="o">--</span><span class="n">aggregate</span><span class="o">-</span><span class="n">mode</span><span class="n">off</span><span class="o">--</span><span class="n">events</span><span class="n">local_load</span><span class="o">--</span><span class="n">print</span><span class="o">-</span><span class="n">gpu</span><span class="o">-</span><span class="n">trace</span><span class="n">matrixMul</span>
<span class="p">[</span><span class="n">Matrix</span><span class="n">Multiply</span><span class="n">Using</span><span class="n">CUDA</span><span class="p">]</span><span class="o">-</span><span class="n">Starting</span><span class="p">...</span>
<span class="o">==</span><span class="mi">6740</span><span class="o">==</span><span class="n">NVPROF</span><span class="n">is</span><span class="n">profiling</span><span class="n">process</span><span class="mi">6740</span><span class="p">,</span><span class="n">command</span><span class="o">:</span><span class="n">matrixMul</span>
</pre>
 <pre><span class="n">GPU</span><span class="n">Device</span><span class="mi">0</span><span class="o">:</span><span class="s">"GeForce GTX TITAN"</span><span class="n">with</span><span class="n">compute</span><span class="n">capability</span><span class="mf">3.5</span>

<span class="n">MatrixA</span><span class="p">(</span><span class="mi">320</span><span class="p">,</span><span class="mi">320</span><span class="p">),</span><span class="n">MatrixB</span><span class="p">(</span><span class="mi">640</span><span class="p">,</span><span class="mi">320</span><span class="p">)</span>
<span class="n">Computing</span><span class="n">result</span><span class="k">using</span><span class="n">CUDA</span><span class="n">Kernel</span><span class="p">...</span>
<span class="n">done</span>
<span class="n">Performance</span><span class="o">=</span><span class="mf">16.76</span><span class="n">GFlop</span><span class="o">/</span><span class="n">s</span><span class="p">,</span><span class="n">Time</span><span class="o">=</span><span class="mf">7.822</span><span class="n">msec</span><span class="p">,</span><span class="n">Size</span><span class="o">=</span><span class="mi">131072000</span><span class="n">Ops</span><span class="p">,</span><span class="n">WorkgroupSize</span><span class="o">=</span><span class="mi">1024</span><span class="n">threads</span><span class="o">/</span><span class="n">block</span>
<span class="n">Checking</span><span class="n">computed</span><span class="n">result</span><span class="k">for</span><span class="n">correctness</span><span class="o">:</span><span class="n">Result</span><span class="o">=</span><span class="n">PASS</span>

<span class="nl">NOTE</span><span class="p">:</span><span class="n">The</span><span class="n">CUDA</span><span class="n">Samples</span><span class="n">are</span><span class="n">not</span><span class="n">meant</span><span class="k">for</span><span class="n">performance</span><span class="n">measurements</span><span class="p">.</span><span class="n">Results</span><span class="n">may</span><span class="n">vary</span><span class="n">when</span><span class="n">GPU</span><span class="n">Boost</span><span class="n">is</span><span class="n">enabled</span><span class="p">.</span>
<span class="o">==</span><span class="mi">6740</span><span class="o">==</span><span class="n">Profiling</span><span class="n">application</span><span class="o">:</span><span class="n">matrixMul</span>
<span class="o">==</span><span class="mi">6740</span><span class="o">==</span><span class="n">Profiling</span><span class="n">result</span><span class="o">:</span>
<span class="n">Device</span><span class="n">Context</span><span class="n">Stream</span><span class="n">Kernel</span><span class="n">local_load</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="n">local_load</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="p">...</span>
<span class="n">GeForce</span><span class="n">GTX</span><span class="n">TIT</span><span class="mi">1</span><span class="mi">7</span><span class="kt">void</span><span class="n">matrixMulCUDA</span><span class="o">&lt;</span><span class="n">i</span><span class="mi">0</span><span class="mi">0</span><span class="p">...</span>
<span class="n">GeForce</span><span class="n">GTX</span><span class="n">TIT</span><span class="mi">1</span><span class="mi">7</span><span class="kt">void</span><span class="n">matrixMulCUDA</span><span class="o">&lt;</span><span class="n">i</span><span class="mi">0</span><span class="mi">0</span><span class="p">...</span>
<span class="o">&lt;</span><span class="p">...</span><span class="n">more</span><span class="n">output</span><span class="p">...</span><span class="o">&gt;</span>
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Although
  <span class="pre">
   --aggregate-mode
  </span>
  applies to metrics, some metrics are only available in aggregate mode and some are only available in non-aggregate mode.
 </p>
 <h2>
  <span class="section-number">
   3.3.
  </span>
  Profiling Controls
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiling-controls" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <h3>
  <span class="section-number">
   3.3.1.
  </span>
  Timeout
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#timeout" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  A timeout (in seconds) can be provided to
  <span class="pre">
   nvprof
  </span>
  . The CUDA application being profiled will be killed by
  <span class="pre">
   nvprof
  </span>
  after the timeout. Profiling result collected before the timeout will be shown.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Timeout starts counting from the moment the CUDA driver is initialized. If the application doesnât call any CUDA APIs, timeout wonât be triggered.
 </p>
 <h3>
  <span class="section-number">
   3.3.2.
  </span>
  Concurrent Kernels
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#concurrent-kernels" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Concurrent-kernel profiling is supported, and is turned on by default. To turn the feature off, use the option
  <span class="pre">
   --concurrent-kernels
  </span>
  <span class="pre">
   off
  </span>
  . This forces concurrent kernel executions to be serialized when a CUDA application is run with
  <span class="pre">
   nvprof
  </span>
  .
 </p>
 <h3>
  <span class="section-number">
   3.3.3.
  </span>
  Profiling Scope
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiling-scope" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  When collecting events/metrics,
  <span class="pre">
   nvprof
  </span>
  profiles all kernels launched on all visible CUDA devices by default. This profiling scope can be limited by the following options.
 </p>
 <p>
  <span class="pre">
   --devices
  </span>
  <span class="pre">
   &lt;device
  </span>
  <span class="pre">
   IDs&gt;
  </span>
  applies to
  <span class="pre">
   --events
  </span>
  ,
  <span class="pre">
   --metrics
  </span>
  ,
  <span class="pre">
   --query-events
  </span>
  and
  <span class="pre">
   --query-metrics
  </span>
  options
  that follows it
  . It limits these options to collect events/metrics only on the devices specified by
  <span class="pre">
   &lt;device
  </span>
  Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â
  <span class="pre">
   IDs&gt;
  </span>
  , which can be a list of device ID numbers separated by comma.
 </p>
 <p>
  <span class="pre">
   --kernels
  </span>
  <span class="pre">
   &lt;kernel
  </span>
  <span class="pre">
   filter&gt;
  </span>
  applies to
  <span class="pre">
   --events
  </span>
  and
  <span class="pre">
   --metrics
  </span>
  options
  that follows it
  . It limits these options to collect events/metrics only on the kernels specified by
  <span class="pre">
   &lt;kernel
  </span>
  <span class="pre">
   filter&gt;
  </span>
  , which has the following syntax:
 </p>
 <pre>&lt;kernel name&gt;
</pre>
 <p>
  or
 </p>
 <pre>&lt;context id/name&gt;:&lt;stream id/name&gt;:&lt;kernel
        name&gt;:&lt;invocation&gt;
</pre>
 <p>
  Each string in the angle brackets can be a standard Perl regular expression. Empty string matches any number or character combination.
 </p>
 <p>
  Invocation number
  n
  indicates the
  n
  th invocation of the kernel. If invocation is a positive number, itâs strictly matched against the invocation of the kernel. Otherwise itâs treated as a regular expression. Invocation number is counted separately for each kernel. So for instance
  <span class="pre">
   :::3
  </span>
  will match the 3rd invocation of every kernel.
 </p>
 <p>
  If the context/stream string is a positive number, itâs strictly matched against the cuda context/stream ID. Otherwise itâs treated as a regular expression and matched against the context/stream name provided by the NVIDIA Tools Extension.
 </p>
 <p>
  Both
  <span class="pre">
   --devices
  </span>
  and
  <span class="pre">
   --kernels
  </span>
  can be specified multiple times, with distinct events/metrics associated.
 </p>
 <p>
  <span class="pre">
   --events
  </span>
  ,
  <span class="pre">
   --metrics
  </span>
  ,
  <span class="pre">
   --query-events
  </span>
  and
  <span class="pre">
   --query-metrics
  </span>
  are controlled by the nearest scope options before them.
 </p>
 <p>
  As an example, the following command,
 </p>
 <pre>nvprof --devices 0 --metrics ipc
        --kernels "1:foo:bar:2" --events local_load a.out
</pre>
 <p>
  collects metric
  <span class="pre">
   ipc
  </span>
  on all kernels launched on device 0. It also collects event
  <span class="pre">
   local_load
  </span>
  for any kernel whose name contains
  <span class="pre">
   bar
  </span>
  and is the 2nd instance launched on context 1 and on stream named
  <span class="pre">
   foo
  </span>
  on device 0.
 </p>
 <h3>
  <span class="section-number">
   3.3.4.
  </span>
  Multiprocess Profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#multiprocess-profiling" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  By default,
  <span class="pre">
   nvprof
  </span>
  only profiles the application specified by the command-line argument. It doesnât trace child processes launched by that process. To profile all processes launched by an application, use the
  <span class="pre">
   --profile-child-processes
  </span>
  option.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  <span class="pre">
   nvprof
  </span>
  cannot profile processes that
  <span class="pre">
   fork()
  </span>
  but do not then
  <span class="pre">
   exec()
  </span>
  .
 </p>
 <p>
  <span class="pre">
   nvprof
  </span>
  also has a âprofile all processesâ mode, in which it profiles every CUDA process launched on the same system by the same user who launched
  <span class="pre">
   nvprof
  </span>
  . Exit this mode by typing âCtrl-câ.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  CPU profiling is not supported in multi-process mode.
 </p>
 <h3>
  <span class="section-number">
   3.3.5.
  </span>
  System Profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#system-profiling" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  For devices that support system profiling, nvprof can enable low frequency sampling of the power, clock, and thermal behavior of each GPU used by the application. This feature is turned off by default. To turn on this feature, use
  <span class="pre">
   --system-profiling
  </span>
  <span class="pre">
   on
  </span>
  . To see the detail of each sample point, combine the above option with
  <span class="pre">
   --print-gpu-trace
  </span>
  .
 </p>
 <h3>
  <span class="section-number">
   3.3.6.
  </span>
  Unified Memory Profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#unified-memory-profiling" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  For GPUs that support Unified Memory,
  <span class="pre">
   nvprof
  </span>
  collects the Unified Memory related memory traffic to and from each GPU on your system. This feature is enabled by default. This feature can be disabled with
  <span class="pre">
   --unified-memory-profiling
  </span>
  <span class="pre">
   off
  </span>
  . To see the detail of each memory transfer while this feature is enabled, use
  <span class="pre">
   --print-gpu-trace
  </span>
  .
 </p>
 <p>
  On multi-GPU configurations without P2P support between any pair of devices that support Unified Memory, managed memory allocations are placed in zero-copy memory. In this case Unified Memory profiling is not supported. In certain cases, the environment variable
  <span class="pre">
   CUDA_MANAGED_FORCE_DEVICE_ALLOC
  </span>
  can be set to force managed allocations to be in device memory and to enable migration on these hardware configurations. In this case Unified Memory profiling is supported. Normally, using the environment variable
  <span class="pre">
   CUDA_VISIBLE_DEVICES
  </span>
  is recommended to restrict CUDA to only use those GPUs that have P2P support. Please refer to the environment variables section in the
  CUDA C++ Programming Guide
  for further details.
 </p>
 <h3>
  <span class="section-number">
   3.3.7.
  </span>
  CPU Thread Tracing
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-thread-tracing" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  In order to allow a correct
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis">
   Dependency Analysis
  </a>
  ,
  <span class="pre">
   nvprof
  </span>
  can collect information about CPU-side threading APIs. This can be enabled by specifying
  <span class="pre">
   --cpu-thread-tracing
  </span>
  <span class="pre">
   on
  </span>
  during measurement. Recording this information is necessary if
 </p>
 <ul class="simple">
  <li>
   <p>
    the application uses multiple CPU threads and
   </p>
  </li>
  <li>
   <p>
    at least two of these threads call the CUDA API.
   </p>
  </li>
 </ul>
 <p>
  Currently, only POSIX threads (Pthreads) are supported. For performance reasons, only selected Pthread API calls may be recorded.
  <span class="pre">
   nvprof
  </span>
  tries to detect which calls are necessary to model the execution behavior and filters others. Filtered calls include
  <span class="pre">
   pthread_mutex_lock
  </span>
  and
  <span class="pre">
   pthread_mutex_unlock
  </span>
  when those do not cause any concurrent thread to block.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  CPU thread tracing is not available on Windows.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  CPU thread tracing starts after the first CUDA API call, from the thread issuing this call. Therefore, the application must call e.g.
  <span class="pre">
   cuInit
  </span>
  from its main thread before spawning any other user threads that call the CUDA API.
 </p>
 <h2>
  <span class="section-number">
   3.4.
  </span>
  Output
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#output" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <h3>
  <span class="section-number">
   3.4.1.
  </span>
  Adjust Units
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#adjust-units" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  By default,
  <span class="pre">
   nvprof
  </span>
  adjusts the time units automatically to get the most precise time values. The
  <span class="pre">
   --normalized-time-unit
  </span>
  options can be used to get fixed time units throughout the results.
 </p>
 <h3>
  <span class="section-number">
   3.4.2.
  </span>
  CSV
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#csv" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  For each profiling mode, option
  <span class="pre">
   --csv
  </span>
  can be used to generate output in comma-separated values (CSV) format. The result can be directly imported to spreadsheet software such as Excel.
 </p>
 <h3>
  <span class="section-number">
   3.4.3.
  </span>
  Export/Import
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#export-import" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  For each profiling mode, option
  <span class="pre">
   --export-profile
  </span>
  can be used to generate a result file. This file is not human-readable, but can be imported back to
  <span class="pre">
   nvprof
  </span>
  using the option
  <span class="pre">
   --import-profile
  </span>
  , or into the Visual Profiler.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The profilers use SQLite as the format of the export profiles. Writing files in such format may require more disk operations than writing a plain file. Thus, exporting profiles to slower devices such as a network drive may slow down the execution of the application.
 </p>
 <h3>
  <span class="section-number">
   3.4.4.
  </span>
  Demangling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#demangling" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  By default,
  <span class="pre">
   nvprof
  </span>
  demangles C++ function names. Use option
  <span class="pre">
   --demangling
  </span>
  <span class="pre">
   off
  </span>
  to turn this feature off.
 </p>
 <h3>
  <span class="section-number">
   3.4.5.
  </span>
  Redirecting Output
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#redirecting-output" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  By default,
  <span class="pre">
   nvprof
  </span>
  sends most of its output to
  <span class="pre">
   stderr
  </span>
  . To redirect the output, use
  <span class="pre">
   --log-file
  </span>
  .
  <span class="pre">
   --log-file
  </span>
  <span class="pre">
   %1
  </span>
  tells
  <span class="pre">
   nvprof
  </span>
  to redirect all output to
  <span class="pre">
   stdout
  </span>
  .
  <span class="pre">
   --log-file
  </span>
  <span class="pre">
   &lt;filename&gt;
  </span>
  redirects output to a file. Use
  <span class="pre">
   %p
  </span>
  in the filename to be replaced by the process ID of
  <span class="pre">
   nvprof
  </span>
  ,
  <span class="pre">
   %h
  </span>
  by the hostname ,
  <span class="pre">
   %q{ENV}
  </span>
  by the value of environment variable
  <span class="pre">
   ENV
  </span>
  , and
  <span class="pre">
   %%
  </span>
  by
  <span class="pre">
   %
  </span>
  .
 </p>
 <h3>
  <span class="section-number">
   3.4.6.
  </span>
  Dependency Analysis
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  <span class="pre">
   nvprof
  </span>
  can run a
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis">
   Dependency Analysis
  </a>
  after the application has been profiled, using the
  <span class="pre">
   --dependency-analysis
  </span>
  option. This analysis can also be applied to imported profiles. It requires to collect the full CUDA API and GPU activity trace during measurement. This is the default for
  <span class="pre">
   nvprof
  </span>
  if not disabled using
  <span class="pre">
   --profile-api-trace
  </span>
  <span class="pre">
   none
  </span>
  .
 </p>
 <p>
  For applications using CUDA from multiple CPU threads,
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-thread-tracing">
   CPU Thread Tracing
  </a>
  should be enabled, too. The option
  <span class="pre">
   --print-dependency-analysis-trace
  </span>
  can be specified to change from a summary output to a trace output, showing computed metrics such as time on the critical path per function instance rather than per function type.
 </p>
 <p>
  An example for dependency analysis summary output with all computed metrics aggregated per function type is shown below. The table is sorted first by time on the critical path and second by waiting time. The summary contains an entry named
  <span class="pre">
   Other
  </span>
  , referring to all CPU activity that is not tracked by
  <span class="pre">
   nvprof
  </span>
  (e.g. the applicationâs
  <span class="pre">
   main
  </span>
  function).
 </p>
 <pre><span class="o">==</span><span class="mi">20704</span><span class="o">==</span><span class="n">Dependency</span><span class="n">Analysis</span><span class="o">:</span>
<span class="o">==</span><span class="mi">20704</span><span class="o">==</span><span class="n">Analysis</span><span class="n">progress</span><span class="o">:</span><span class="mi">100</span><span class="o">%</span>
<span class="n">Critical</span><span class="n">path</span><span class="p">(</span><span class="o">%</span><span class="p">)</span><span class="n">Critical</span><span class="n">path</span><span class="n">Waiting</span><span class="n">time</span><span class="n">Name</span>
<span class="o">%</span><span class="n">s</span><span class="n">s</span>
<span class="mf">92.06</span><span class="mf">4.061817</span><span class="mf">0.000000</span><span class="n">clock_block</span><span class="p">(</span><span class="kt">long</span><span class="o">*</span><span class="p">,</span><span class="kt">long</span><span class="p">)</span>
<span class="mf">4.54</span><span class="mf">0.200511</span><span class="mf">0.000000</span><span class="n">cudaMalloc</span>
<span class="mf">3.25</span><span class="mf">0.143326</span><span class="mf">0.000000</span><span class="n">cudaDeviceReset</span>
<span class="mf">0.13</span><span class="mf">5.7273280e-03</span><span class="mf">0.000000</span><span class="o">&lt;</span><span class="n">Other</span><span class="o">&gt;</span>
<span class="mf">0.01</span><span class="mf">2.7200900e-04</span><span class="mf">0.000000</span><span class="n">cudaFree</span>
<span class="mf">0.00</span><span class="mf">0.000000</span><span class="mf">4.062506</span><span class="n">pthread_join</span>
<span class="mf">0.00</span><span class="mf">0.000000</span><span class="mf">4.061790</span><span class="n">cudaStreamSynchronize</span>
<span class="mf">0.00</span><span class="mf">0.000000</span><span class="mf">1.015485</span><span class="n">pthread_mutex_lock</span>
<span class="mf">0.00</span><span class="mf">0.000000</span><span class="mf">1.013711</span><span class="n">pthread_cond_wait</span>
<span class="mf">0.00</span><span class="mf">0.000000</span><span class="mf">0.000000</span><span class="n">pthread_mutex_unlock</span>
<span class="mf">0.00</span><span class="mf">0.000000</span><span class="mf">0.000000</span><span class="n">pthread_exit</span>
<span class="mf">0.00</span><span class="mf">0.000000</span><span class="mf">0.000000</span><span class="n">pthread_enter</span>
<span class="mf">0.00</span><span class="mf">0.000000</span><span class="mf">0.000000</span><span class="n">pthread_create</span>
<span class="mf">0.00</span><span class="mf">0.000000</span><span class="mf">0.000000</span><span class="n">pthread_cond_signal</span>
<span class="mf">0.00</span><span class="mf">0.000000</span><span class="mf">0.000000</span><span class="n">cudaLaunch</span>
</pre>
 <h2>
  <span class="section-number">
   3.5.
  </span>
  CPU Sampling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-sampling" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Sometimes itâs useful to profile the CPU portion of your application, in order to better understand the bottlenecks and identify potential hotspots for the entire CUDA application. For the CPU portion of the application,
  <span class="pre">
   nvprof
  </span>
  is able to sample the program counter and call stacks at a certain frequency. The data is then used to construct a graph, with nodes being frames in each call stack. Function and library symbols are also extracted if available. A sample graph is shown below:
 </p>
 <pre><span class="o">========</span><span class="n">CPU</span><span class="n">profiling</span><span class="n">result</span><span class="p">(</span><span class="n">bottom</span><span class="n">up</span><span class="p">)</span><span class="o">:</span>
<span class="mf">45.45</span><span class="o">%</span><span class="n">cuInit</span>
<span class="o">|</span><span class="mf">45.45</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">globalState</span><span class="o">::</span><span class="n">loadDriverInternal</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="o">|</span><span class="mf">45.45</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">__loadDriverInternalUtil</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="o">|</span><span class="mf">45.45</span><span class="o">%</span><span class="n">pthread_once</span>
<span class="o">|</span><span class="mf">45.45</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">cuosOnce</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="p">,</span><span class="kt">void</span><span class="p">(</span><span class="o">*</span><span class="p">)</span><span class="p">(</span><span class="kt">void</span><span class="p">))</span>
<span class="o">|</span><span class="mf">45.45</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">globalState</span><span class="o">::</span><span class="n">loadDriver</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="o">|</span><span class="mf">45.45</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">globalState</span><span class="o">::</span><span class="n">initializeDriver</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="o">|</span><span class="mf">45.45</span><span class="o">%</span><span class="n">cudaMalloc</span>
<span class="o">|</span><span class="mf">45.45</span><span class="o">%</span><span class="n">main</span>
<span class="mf">33.33</span><span class="o">%</span><span class="n">cuDevicePrimaryCtxRetain</span>
<span class="o">|</span><span class="mf">33.33</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">contextStateManager</span><span class="o">::</span><span class="n">initPrimaryContext</span><span class="p">(</span><span class="n">cudart</span><span class="o">::</span><span class="n">device</span><span class="o">*</span><span class="p">)</span>
<span class="o">|</span><span class="mf">33.33</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">contextStateManager</span><span class="o">::</span><span class="n">tryInitPrimaryContext</span><span class="p">(</span><span class="n">cudart</span><span class="o">::</span><span class="n">device</span><span class="o">*</span><span class="p">)</span>
<span class="o">|</span><span class="mf">33.33</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">contextStateManager</span><span class="o">::</span><span class="n">initDriverContext</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="o">|</span><span class="mf">33.33</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">contextStateManager</span><span class="o">::</span><span class="n">getRuntimeContextState</span><span class="p">(</span><span class="n">cudart</span><span class="o">::</span><span class="n">contextState</span><span class="o">**</span><span class="p">,</span><span class="kt">bool</span><span class="p">)</span>
<span class="o">|</span><span class="mf">33.33</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">getLazyInitContextState</span><span class="p">(</span><span class="n">cudart</span><span class="o">::</span><span class="n">contextState</span><span class="o">**</span><span class="p">)</span>
<span class="o">|</span><span class="mf">33.33</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">doLazyInitContextState</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="o">|</span><span class="mf">33.33</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">cudaApiMalloc</span><span class="p">(</span><span class="kt">void</span><span class="o">**</span><span class="p">,</span><span class="kt">unsigned</span><span class="kt">long</span><span class="p">)</span>
<span class="o">|</span><span class="mf">33.33</span><span class="o">%</span><span class="n">cudaMalloc</span>
<span class="o">|</span><span class="mf">33.33</span><span class="o">%</span><span class="n">main</span>
<span class="mf">18.18</span><span class="o">%</span><span class="n">cuDevicePrimaryCtxReset</span>
<span class="o">|</span><span class="mf">18.18</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">device</span><span class="o">::</span><span class="n">resetPrimaryContext</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="o">|</span><span class="mf">18.18</span><span class="o">%</span><span class="n">cudart</span><span class="o">::</span><span class="n">cudaApiThreadExit</span><span class="p">(</span><span class="kt">void</span><span class="p">)</span>
<span class="o">|</span><span class="mf">18.18</span><span class="o">%</span><span class="n">cudaThreadExit</span>
<span class="o">|</span><span class="mf">18.18</span><span class="o">%</span><span class="n">main</span>
<span class="mf">3.03</span><span class="o">%</span><span class="n">cudbgGetAPIVersion</span>
<span class="mf">3.03</span><span class="o">%</span><span class="n">start_thread</span>
<span class="mf">3.03</span><span class="o">%</span><span class="n">clone</span>
</pre>
 <p>
  The graph can be presented in different âviewsâ (
  <span class="pre">
   top-down
  </span>
  ,
  <span class="pre">
   bottom-up
  </span>
  or
  <span class="pre">
   flat
  </span>
  ), allowing the user to analyze the sampling data from different perspectives. For instance, the
  <span class="pre">
   bottom-up
  </span>
  view (shown above) can be useful in identifying the âhotâ functions in which the application is spending most of its time. The
  <span class="pre">
   top-down
  </span>
  view gives a break-down of the application execution time, starting from the
  <span class="pre">
   main
  </span>
  function, allowing you to find âcall pathsâ which are executed frequently.
 </p>
 <p>
  By default the CPU sampling feature is disabled. To enable it, use the option
  <span class="pre">
   --cpu-profiling
  </span>
  <span class="pre">
   on
  </span>
  . The next section describes all the options controlling the CPU sampling behavior.
 </p>
 <p>
  CPU sampling is supported on Linux and Windows for Intel x86/x86_64 architecture.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  When using the CPU profiling feature on POSIX systems, the profiler samples the application by sending periodic signals. Applications should therefore ensure that system calls are handled appropriately when interrupted.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  On Windows, nvprof requires Visual Studio installation (2010 or later) and compiler-generated .PDB (program database) files to resolve symbol information. When building your application, ensure that .PDB files are created and placed next to the profiled executable and libraries.
 </p>
 <h3>
  <span class="section-number">
   3.5.1.
  </span>
  CPU Sampling Limitations
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-sampling-limitations" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The following are known issues with the current release.
 </p>
 <ul class="simple">
  <li>
   <p>
    CPU sampling is not supported on the mobile devices.
   </p>
  </li>
  <li>
   <p>
    CPU sampling is currently not supported in multi-process profiling mode.
   </p>
  </li>
  <li>
   <p>
    The result stack traces might not be complete under some compiler optimizations, notably frame pointer omission and function inlining.
   </p>
  </li>
  <li>
   <p>
    The CPU sampling result does not support CSV mode.
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   3.6.
  </span>
  OpenACC
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  On 64bit Linux platforms,
  <span class="pre">
   nvprof
  </span>
  supports recording OpenACC activities using the CUPTI Activity API. This allows to investigate the performance on the level of OpenACC constructs in addition to the underlying, compiler-generated CUDA API calls.
 </p>
 <p>
  OpenACC profiling in
  <span class="pre">
   nvprof
  </span>
  requires the targeted application to use PGI OpenACC runtime 19.1 or later.
 </p>
 <p>
  Even though recording OpenACC activities is only supported on x86_64 Linux systems, importing and viewing previously generated profile data is available on all platforms supported by
  <span class="pre">
   nvprof
  </span>
  .
 </p>
 <p>
  An example for OpenACC summary output is shown below. The CUPTI OpenACC activities are mapped to the original OpenACC constructs using their source file and line information. For
  <span class="pre">
   acc_enqueue_launch
  </span>
  activities, it will furthermore show the launched CUDA kernel name which is generated by the OpenACC compiler. By default, nvprof will demangle kernel names generated by the OpenACC compiler. You can pass
  <span class="pre">
   --demangling
  </span>
  <span class="pre">
   off
  </span>
  to disable this behavior.
 </p>
 <pre>==20854== NVPROF is profiling process 20854, command: ./acc_saxpy
</pre>
 <pre>==20854== Profiling application: ./acc_saxpy
==20854== Profiling result:
==20854== OpenACC (excl):
Time(%)      Time     Calls       Avg       Min       Max  Name
 33.16%  1.27944s       200  6.3972ms  24.946us  12.770ms  acc_implicit_wait@acc_saxpy.cpp:42
 33.12%  1.27825s       100  12.783ms  12.693ms  12.787ms  acc_wait@acc_saxpy.cpp:54
 33.12%  1.27816s       100  12.782ms  12.720ms  12.786ms  acc_wait@acc_saxpy.cpp:61
  0.14%  5.4550ms       100  54.549us  51.858us  71.461us  acc_enqueue_download@acc_saxpy.cpp:43
  0.07%  2.5190ms       100  25.189us  23.877us  60.269us  acc_enqueue_launch@acc_saxpy.cpp:50 (kernel2(int, float, float*, float*)_50_gpu)
  0.06%  2.4988ms       100  24.987us  24.161us  29.453us  acc_enqueue_launch@acc_saxpy.cpp:60 (kernel3(int, float, float*, float*)_60_gpu)
  0.06%  2.2799ms       100  22.798us  21.654us  56.674us  acc_enqueue_launch@acc_saxpy.cpp:42 (kernel1(int, float, float*, float*)_42_gpu)
  0.05%  2.1068ms       100  21.068us  20.444us  33.159us  acc_enqueue_download@acc_saxpy.cpp:51
  0.05%  2.0854ms       100  20.853us  19.453us  23.697us  acc_enqueue_download@acc_saxpy.cpp:61
  0.04%  1.6265ms       100  16.265us  15.284us  49.632us  acc_enqueue_upload@acc_saxpy.cpp:50
  0.04%  1.5963ms       100  15.962us  15.052us  19.749us  acc_enqueue_upload@acc_saxpy.cpp:60
  0.04%  1.5393ms       100  15.393us  14.592us  56.414us  acc_enqueue_upload@acc_saxpy.cpp:42
  0.01%  558.54us       100  5.5850us  5.3700us  6.2090us  acc_implicit_wait@acc_saxpy.cpp:43
  0.01%  266.13us       100  2.6610us  2.4630us  4.7590us  acc_compute_construct@acc_saxpy.cpp:42
  0.01%  211.77us       100  2.1170us  1.9980us  4.1770us  acc_compute_construct@acc_saxpy.cpp:50
  0.01%  209.14us       100  2.0910us  1.9880us  2.2500us  acc_compute_construct@acc_saxpy.cpp:60
  0.00%  55.066us         1  55.066us  55.066us  55.066us  acc_enqueue_launch@acc_saxpy.cpp:70 (initVec(int, float, float*)_70_gpu)
  0.00%  13.209us         1  13.209us  13.209us  13.209us  acc_compute_construct@acc_saxpy.cpp:70
  0.00%  10.901us         1  10.901us  10.901us  10.901us  acc_implicit_wait@acc_saxpy.cpp:70
  0.00%       0ns       200       0ns       0ns       0ns  acc_delete@acc_saxpy.cpp:61
  0.00%       0ns       200       0ns       0ns       0ns  acc_delete@acc_saxpy.cpp:43
  0.00%       0ns       200       0ns       0ns       0ns  acc_create@acc_saxpy.cpp:60
  0.00%       0ns       200       0ns       0ns       0ns  acc_create@acc_saxpy.cpp:42
  0.00%       0ns       200       0ns       0ns       0ns  acc_delete@acc_saxpy.cpp:51
  0.00%       0ns       200       0ns       0ns       0ns  acc_create@acc_saxpy.cpp:50
  0.00%       0ns         2       0ns       0ns       0ns  acc_alloc@acc_saxpy.cpp:42
</pre>
 <h3>
  <span class="section-number">
   3.6.1.
  </span>
  OpenACC Options
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc-options" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Table 1 contains OpenACC profiling related command-line options of
  <span class="pre">
   nvprof
  </span>
  .
 </p>
 <table class="table-no-stripes docutils align-default" id="openacc-options-options-table">
  <span class="caption-text">
   Table 1. OpenACC Options
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc-options-options-table" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Option
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      --openacc-profiling
     </span>
     <span class="pre">
      &lt;on|off&gt;
     </span>
    </p>
   </td>
   <td>
    <p>
     Turn on/off OpenACC profiling. Note: OpenACC profiling is only supported on x86_64 Linux. Default is on.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      --print-openacc-summary
     </span>
    </p>
   </td>
   <td>
    <p>
     Print a summary of all recorded OpenACC activities.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      --print-openacc-trace
     </span>
    </p>
   </td>
   <td>
    <p>
     Print a detailed trace of all recorded OpenACC activities, including each activityâs timestamp and duration.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      --print-openacc-constructs
     </span>
    </p>
   </td>
   <td>
    <p>
     Include the name of the OpenACC parent construct that caused an OpenACC activity to be emitted. Note that for applications using PGI OpenACC runtime before 19.1, this value will always be
     <span class="pre">
      unknown
     </span>
     .
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      --openacc-summary-mode
     </span>
     <span class="pre">
      &lt;exclusive|inclusive&gt;
     </span>
    </p>
   </td>
   <td>
    <p>
     Specify how activity durations are presented in the OpenACC summary. Allowed values: âexclusiveâ - exclusive durations (default). âinclusiveâ - inclusive durations. See
     <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc-summary-modes">
      OpenACC Summary Modes
     </a>
     for more information.
    </p>
   </td>
  </tr>
 </table>
 <h3>
  <span class="section-number">
   3.6.2.
  </span>
  OpenACC Summary Modes
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc-summary-modes" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  <span class="pre">
   nvprof
  </span>
  supports two modes for presenting OpenACC activity durations in the OpenACC summary mode (enabled with
  <span class="pre">
   --print-openacc-summary
  </span>
  ): âexclusiveâ and âinclusiveâ.
 </p>
 <ul class="simple">
  <li>
   <p>
    Inclusive: In this mode, all durations represent the actual runtime of an activity. This includes the time spent in this activity as well as in all its children (callees).
   </p>
  </li>
  <li>
   <p>
    Exclusive: In this mode, all durations represent the time spent solely in this activity. This includes the time spent in this activity but excludes the runtime of all of its children (callees).
   </p>
  </li>
 </ul>
 <p>
  As an example, consider the OpenACC
  <span class="pre">
   acc_compute_construct
  </span>
  which itself calls
  <span class="pre">
   acc_enqueue_launch
  </span>
  to launch a kernel to the device and
  <span class="pre">
   acc_implicit_wait
  </span>
  , which waits on the completion of this kernel. In âinclusiveâ mode, the duration for
  <span class="pre">
   acc_compute_construct
  </span>
  will include the time spent in
  <span class="pre">
   acc_enqueue_launch
  </span>
  and
  <span class="pre">
   acc_implicit_wait
  </span>
  . In âexclusiveâ mode, those two durations are subtracted. In the summary profile, this is helpful to identify if a long
  <span class="pre">
   acc_compute_construct
  </span>
  represents a high launch overhead or rather a long wait (synchronization) time.
 </p>
 <h2>
  <span class="section-number">
   3.7.
  </span>
  OpenMP
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  On 64bit Linux platforms,
  <span class="pre">
   nvprof
  </span>
  supports recording OpenMP activities
 </p>
 <p>
  OpenMP profiling in
  <span class="pre">
   nvprof
  </span>
  requires the targeted application to use a runtime supporting the OpenMP Tools interface (OMPT). (PGI version 19.1 or greater using the LLVM code generator supports OMPT).
 </p>
 <p>
  Even though recording OpenMP activities is only supported on x86_64 Linux systems, importing and viewing previously generated profile data is available on all platforms supported by
  <span class="pre">
   nvprof
  </span>
  .
 </p>
 <p>
  An example for the OpenMP summary output is shown below:
 </p>
 <pre><span class="o">==</span><span class="mi">20854</span><span class="o">==</span><span class="n">NVPROF</span><span class="n">is</span><span class="n">profiling</span><span class="n">process</span><span class="mi">20854</span><span class="p">,</span><span class="n">command</span><span class="o">:</span><span class="p">.</span><span class="o">/</span><span class="n">openmp</span>
</pre>
 <pre><span class="o">==</span><span class="mi">20854</span><span class="o">==</span><span class="n">Profiling</span><span class="n">application</span><span class="o">:</span><span class="p">.</span><span class="o">/</span><span class="n">openmp</span>
<span class="o">==</span><span class="mi">20854</span><span class="o">==</span><span class="n">Profiling</span><span class="n">result</span><span class="o">:</span>
<span class="n">No</span><span class="n">kernels</span><span class="n">were</span><span class="n">profiled</span><span class="p">.</span>
<span class="n">No</span><span class="n">API</span><span class="n">activities</span><span class="n">were</span><span class="n">profiled</span><span class="p">.</span>
<span class="n">Type</span><span class="n">Time</span><span class="p">(</span><span class="o">%</span><span class="p">)</span><span class="n">Time</span><span class="n">Calls</span><span class="n">Avg</span><span class="n">Min</span><span class="n">Max</span><span class="n">Name</span>
<span class="n">OpenMP</span><span class="p">(</span><span class="n">incl</span><span class="p">)</span><span class="o">:</span><span class="mf">99.97</span><span class="o">%</span><span class="mf">277.10</span><span class="n">ms</span><span class="mi">20</span><span class="mf">13.855</span><span class="n">ms</span><span class="mf">13.131</span><span class="n">ms</span><span class="mf">18.151</span><span class="n">ms</span><span class="n">omp_parallel</span>
<span class="mf">0.03</span><span class="o">%</span><span class="mf">72.728</span><span class="n">us</span><span class="mi">19</span><span class="mf">3.8270</span><span class="n">us</span><span class="mf">2.9840</span><span class="n">us</span><span class="mf">9.5610</span><span class="n">us</span><span class="n">omp_idle</span>
<span class="mf">0.00</span><span class="o">%</span><span class="mf">7.9170</span><span class="n">us</span><span class="mi">7</span><span class="mf">1.1310</span><span class="n">us</span><span class="mf">1.0360</span><span class="n">us</span><span class="mf">1.5330</span><span class="n">us</span><span class="n">omp_wait_barrier</span>
</pre>
 <h3>
  <span class="section-number">
   3.7.1.
  </span>
  OpenMP Options
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp-options" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Table 2 contains OpenMP profiling related command-line options of
  <span class="pre">
   nvprof
  </span>
  .
 </p>
 <table class="table-no-stripes docutils align-default" id="openmp-options-options-table">
  <span class="caption-text">
   Table 2. OpenMP Options
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp-options-options-table" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Option
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      --print-openmp-summary
     </span>
    </p>
   </td>
   <td>
    <p>
     Print a summary of all recorded OpenMP activities.
    </p>
   </td>
  </tr>
 </table>
 <h1>
  <span class="section-number">
   4.
  </span>
  Remote Profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#remote-profiling" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  Remote profiling is the process of collecting profile data from a remote system that is different than the host system at which that profile data will be viewed and analyzed. There are two ways to perform remote profiling. You can profile your remote application directly from
  <span class="pre">
   nsight
  </span>
  orthe Visual Profiler. Or you can use
  <span class="pre">
   nvprof
  </span>
  to collect the profile data on the remote system and then use
  <span class="pre">
   nvvp
  </span>
  on the host system to view and analyze the data.
 </p>
 <h2>
  <span class="section-number">
   4.1.
  </span>
  Remote Profiling With Visual Profiler
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#remote-profiling-with-visual-profiler" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This section describes how to perform remote profiling by using the remote capabilities of nsight and the Visual Profiler.
 </p>
 <p>
  Nsight Eclipse Edition supports full remote development including remote building, debugging, and profiling. Using these capabilities you can create a project and launch configuration that allows you to remotely profile your application. See the Nsight Eclipse Edition documentation for more information.
 </p>
 <p>
  The
  <span class="pre">
   Visual
  </span>
  <span class="pre">
   Profiler
  </span>
  also enables remote profiling. As shown in the following figure, when creating a new session or editing an existing session you can specify that the application being profiled resides on a remote system. Once you have configured your session to use a remote application, you can perform all profiler functions in the same way as you would with a local application, including timeline generation, guided analysis, and event and metric collection.
 </p>
 <p>
  To use the Visual Profiler remote profiling you must install the same version of the CUDA Toolkit on both the host and remote systems. It is not necessary for the host system to have an NVIDIA GPU, but ensure that the CUDA Toolkit installed on the host system supports the target device. The host and remote systems may run different operating systems or have different CPU architectures. Only a remote system running Linux is supported. The remote system must be accessible via SSH.
 </p>
 <h3>
  <span class="section-number">
   4.1.1.
  </span>
  One-hop remote profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#one-hop-remote-profiling" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  In certain remote profiling setups, the machine running the actual CUDA program is not accessible from the machine running the Visual Profiler. These two machines are connected via an intermediate machine, which we refer to as the login node.
 </p>
 <p>
  The host machine is the one which is running the Visual Profiler.
 </p>
 <p>
  The login node is where the one-hop profiling script will run. We only need ssh, scp and perl on this machine.
 </p>
 <p>
  The compute node is where the actual CUDA application will run and profiled. The profiling data generated will be copied over to the login node, so that it can be used by the Visual Profiler on the host.
 </p>
 <p>
  To configure one-hop profiling, you need to do the following one-time setup:
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    Copy the
    <a class="reference external" href="https://github.com/NVIDIA/cuda-profiler/tree/master/one_hop_profiling">
     one-hop profiling Perl script
    </a>
    onto the login node.
   </p>
  </li>
  <li>
   <p>
    In Visual Profiler, add the login node as a new remote connection.
   </p>
  </li>
  <li>
   <p>
    In Visual Profilerâs New Session wizard, use the Configure button to open the toolkit configuration window. Here, use the radio button to select the custom script option, and browse to point to the Perl script on the login node.
   </p>
  </li>
 </ol>
 <p>
  Once this setup is complete, you can profile the application as you would on any remote machine. Copying all data to and from the login and compute nodes happens transparently and automatically.
 </p>
 <h2>
  <span class="section-number">
   4.2.
  </span>
  Remote Profiling With
  <span class="pre">
   nvprof
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#remote-profiling-with-nvprof" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This section describes how to perform remote profiling by running
  <span class="pre">
   nvprof
  </span>
  manually on the remote system and then importing the collected profile data into the
  <span class="pre">
   Visual
  </span>
  <span class="pre">
   Profiler
  </span>
  .
 </p>
 <h3>
  <span class="section-number">
   4.2.1.
  </span>
  Collect Data On Remote System
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#collect-data-on-remote-system" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  There are three common remote profiling use cases that can be addressed by using
  <span class="pre">
   nvprof
  </span>
  and the Visual Profiler.
 </p>
 <p>
  Timeline
 </p>
 <p>
  The first use case is to collect a timeline of the application executing on the remote system. The timeline should be collected in a way that most accurately reflects the behavior of the application. To collect the timeline execute the following on the remote system. See
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof">
   ânvprof
  </a>
  for more information on
  <span class="pre">
   nvprof
  </span>
  options.
 </p>
 <pre><span class="n">$</span><span class="n">nvprof</span><span class="o">--</span><span class="k">export</span><span class="o">-</span><span class="n">profile</span><span class="n">timeline</span><span class="p">.</span><span class="n">prof</span><span class="o">&lt;</span><span class="n">app</span><span class="o">&gt;</span><span class="o">&lt;</span><span class="n">app</span><span class="n">args</span><span class="o">&gt;</span>
</pre>
 <p>
  The profile data will be collected in timeline.prof. You should copy this file back to the host system and then import it into the Visual Profiler as described in the next section.
 </p>
 <p>
  Metrics And Events
 </p>
 <p>
  The second use case is to collect events or metrics for all kernels in an application for which you have already collected a timeline. Collecting events or metrics for all kernels will significantly change the overall performance characteristics of the application because all kernel executions will be serialized on the GPU. Even though overall application performance is changed, the event or metric values for individual kernels will be correct and so you can merge the collected event and metric values onto a previously collected timeline to get an accurate picture of the applications behavior. To collect events or metrics you use the
  <span class="pre">
   --events
  </span>
  or
  <span class="pre">
   --metrics
  </span>
  flag. The following shows an example using just the
  <span class="pre">
   --metrics
  </span>
  flag to collect two metrics.
 </p>
 <pre><span class="n">$</span><span class="n">nvprof</span><span class="o">--</span><span class="n">metrics</span><span class="n">achieved_occupancy</span><span class="p">,</span><span class="n">ipc</span><span class="o">-</span><span class="n">o</span><span class="n">metrics</span><span class="p">.</span><span class="n">prof</span><span class="o">&lt;</span><span class="n">app</span><span class="o">&gt;</span><span class="o">&lt;</span><span class="n">app</span><span class="n">args</span><span class="o">&gt;</span>
</pre>
 <p>
  You can collect any number of events and metrics for each
  <span class="pre">
   nvprof
  </span>
  invocation, and you can invoke
  <span class="pre">
   nvprof
  </span>
  multiple times to collect multiple metrics.prof files. To get accurate profiling results, it is important that your application conform to the requirements detailed in
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#application-requirements">
   Application Requirements
  </a>
  .
 </p>
 <p>
  The profile data will be collected in the metrics.prof file(s). You should copy these files back to the host system and then import it into the Visual Profiler as described in the next section.
 </p>
 <p>
  Analysis For Individual Kernel
 </p>
 <p>
  The third common remote profiling use case is to collect the metrics needed by the analysis system for an individual kernel. When imported into the Visual Profiler this data will enable the analysis system to analyze the kernel and report optimization opportunities for that kernel. To collect the analysis data execute the following on the remote system. It is important that the
  <span class="pre">
   --kernels
  </span>
  option appear before the
  <span class="pre">
   --analysis-metrics
  </span>
  option so that metrics are collected only for the kernel(s) specified by
  <span class="pre">
   kernel
  </span>
  <span class="pre">
   specifier
  </span>
  . See
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiling-scope">
   Profiling Scope
  </a>
  for more information on the
  <span class="pre">
   --kernels
  </span>
  option.
 </p>
 <pre><span class="n">$</span><span class="n">nvprof</span><span class="o">--</span><span class="n">kernels</span><span class="o">&lt;</span><span class="n">kernel</span><span class="n">specifier</span><span class="o">&gt;</span><span class="o">--</span><span class="n">analysis</span><span class="o">-</span><span class="n">metrics</span><span class="o">-</span><span class="n">o</span><span class="n">analysis</span><span class="p">.</span><span class="n">prof</span><span class="o">&lt;</span><span class="n">app</span><span class="o">&gt;</span><span class="o">&lt;</span><span class="n">app</span><span class="n">args</span><span class="o">&gt;</span>
</pre>
 <p>
  The profile data will be collected in analysis.prof. You should copy this file back to the host system and then import it into the Visual Profiler as described in the next section.
 </p>
 <h3>
  <span class="section-number">
   4.2.2.
  </span>
  View And Analyze Data
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#view-and-analyze-data" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The collected profile data is viewed and analyzed by importing it into the Visual Profiler on the host system. See
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-session">
   Import Session
  </a>
  for more information about importing.
 </p>
 <p>
  Timeline, Metrics And Events
 </p>
 <p>
  To view collected timeline data, the timeline.prof file can be imported into the Visual Profiler as described in
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-single-process-nvprof-session">
   Import Single-Process nvprof Session
  </a>
  . If metric or event data was also collected for the application, the corresponding metrics.prof file(s) can be imported into the Visual Profiler along with the timeline so that the events and metrics collected for each kernel are associated with the corresponding kernel in the timeline.
 </p>
 <p>
  Guided Analysis For Individual Kernel
 </p>
 <p>
  To view collected analysis data for an individual kernel, the analysis.prof file can be imported into the
  <span class="pre">
   Visual
  </span>
  <span class="pre">
   Profiler
  </span>
  as described in
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-single-process-nvprof-session">
   Import Single-Process nvprof Session
  </a>
  . The analysis.prof must be imported by itself. The timeline will show just the individual kernel that we specified during data collection. After importing, the guided analysis system can be used to explore the optimization opportunities for the kernel.
 </p>
 <h1>
  <span class="section-number">
   5.
  </span>
  NVIDIA Tools Extension
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvidia-tools-extension" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  NVIDIA Tools Extension (NVTX) is a C-based Application Programming Interface (API) for annotating events, code ranges, and resources in your applications. Applications which integrate NVTX can use the Visual Profiler to capture and visualize these events and ranges. The NVTX API provides two core services:
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    Tracing of CPU events and time ranges.
   </p>
  </li>
  <li>
   <p>
    Naming of OS and CUDA resources.
   </p>
  </li>
 </ol>
 <p>
  NVTX can be quickly integrated into an application. The sample program below shows the use of marker events, range events, and resource naming.
 </p>
 <pre>void Wait(int waitMilliseconds) {
  nvtxNameOsThread(âMAINâ);
  nvtxRangePush(__FUNCTION__);
  nvtxMark("Waiting...");
  Sleep(waitMilliseconds);
  nvtxRangePop();
}

int main(void) {
  nvtxNameOsThread("MAIN");
  nvtxRangePush(__FUNCTION__);
  Wait();
  nvtxRangePop();
}
</pre>
 <h2>
  <span class="section-number">
   5.1.
  </span>
  NVTX API Overview
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-api-overview" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Files
 </p>
 <p>
  The core NVTX API is defined in file nvToolsExt.h, whereas CUDA-specific extensions to the NVTX interface are defined in nvToolsExtCuda.h and nvToolsExtCudaRt.h. On Linux the NVTX shared library is called
  <span class="pre">
   libnvToolsExt.so
  </span>
  and on macOS the shared library is called
  <span class="pre">
   libnvToolsExt.dylib
  </span>
  . On Windows the library (.lib) and runtime components (.dll) are named
  <span class="pre">
   nvToolsExt[bitness=32|64]_[version].{dll|lib}
  </span>
  .
 </p>
 <p>
  Function Calls
 </p>
 <p>
  All NVTX API functions start with an nvtx name prefix and may end with one of the three suffixes: A, W, or Ex. NVTX functions with these suffixes exist in multiple variants, performing the same core functionality with different parameter encodings. Depending on the version of the NVTX library, available encodings may include ASCII (A), Unicode (W), or event structure (Ex).
 </p>
 <p>
  The CUDA implementation of NVTX only implements the ASCII (A) and event structure (Ex) variants of the API, the Unicode (W) versions are not supported and have no effect when called.
 </p>
 <p>
  Return Values
 </p>
 <p>
  Some of the NVTX functions are defined to have return values. For example, the
  <span class="pre">
   nvtxRangeStart()
  </span>
  function returns a unique range identifier and
  <span class="pre">
   nvtxRangePush()
  </span>
  function outputs the current stack level. It is recommended not to use the returned values as part of conditional code in the instrumented application. The returned values can differ between various implementations of the NVTX library and, consequently, having added dependencies on the return values might work with one tool, but may fail with another.
 </p>
 <h2>
  <span class="section-number">
   5.2.
  </span>
  NVTX API Events
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-api-events" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Markers are used to describe events that occur at a specific time during the execution of an application, while ranges detail the time span in which they occur. This information is presented alongside all of the other captured data, which makes it easier to understand the collected information. All markers and ranges are identified by a message string. The Ex version of the marker and range APIs also allows category, color, and payload attributes to be associated with the event using the event attributes structure.
 </p>
 <h3>
  <span class="section-number">
   5.2.1.
  </span>
  NVTX Markers
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-markers" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  A marker is used to describe an instantaneous event. A marker can contain a text message or specify additional information using the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#event-attributes-structure">
   event attributes structure
  </a>
  . Use
  <span class="pre">
   nvtxMarkA
  </span>
  to create a marker containing an ASCII message. Use
  <span class="pre">
   nvtxMarkEx()
  </span>
  to create a marker containing additional attributes specified by the event attribute structure. The
  <span class="pre">
   nvtxMarkW()
  </span>
  function is not supported in the CUDA implementation of NVTX and has no effect if called.
 </p>
 <p>
  Code Example
 </p>
 <pre><span class="n">nvtxMarkA</span><span class="p">(</span><span class="s">"My mark"</span><span class="p">);</span>

<span class="n">nvtxEventAttributes_t</span><span class="n">eventAttrib</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">version</span><span class="o">=</span><span class="n">NVTX_VERSION</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">size</span><span class="o">=</span><span class="n">NVTX_EVENT_ATTRIB_STRUCT_SIZE</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">colorType</span><span class="o">=</span><span class="n">NVTX_COLOR_ARGB</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">color</span><span class="o">=</span><span class="n">COLOR_RED</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">messageType</span><span class="o">=</span><span class="n">NVTX_MESSAGE_TYPE_ASCII</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">message</span><span class="p">.</span><span class="n">ascii</span><span class="o">=</span><span class="s">"my mark with attributes"</span><span class="p">;</span>
<span class="n">nvtxMarkEx</span><span class="p">(</span><span class="o">&amp;</span><span class="n">eventAttrib</span><span class="p">);</span>
</pre>
 <h3>
  <span class="section-number">
   5.2.2.
  </span>
  NVTX Range Start/Stop
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-range-start-stop" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  A start/end range is used to denote an arbitrary, potentially non-nested, time span. The start of a range can occur on a different thread than the end of the range. A range can contain a text message or specify additional information using the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#event-attributes-structure">
   event attributes structure
  </a>
  . Use
  <span class="pre">
   nvtxRangeStartA()
  </span>
  to create a marker containing an ASCII message. Use
  <span class="pre">
   nvtxRangeStartEx()
  </span>
  to create a range containing additional attributes specified by the event attribute structure. The
  <span class="pre">
   nvtxRangeStartW()
  </span>
  function is not supported in the CUDA implementation of NVTX and has no effect if called. For the correlation of a start/end pair, a unique correlation ID is created that is returned from
  <span class="pre">
   nvtxRangeStartA()
  </span>
  or
  <span class="pre">
   nvtxRangeStartEx()
  </span>
  , and is then passed into
  <span class="pre">
   nvtxRangeEnd()
  </span>
  .
 </p>
 <p>
  Code Example
 </p>
 <pre><span class="c1">// non-overlapping range</span>
<span class="n">nvtxRangeId_t</span><span class="n">id1</span><span class="o">=</span><span class="n">nvtxRangeStartA</span><span class="p">(</span><span class="s">"My range"</span><span class="p">);</span>
<span class="n">nvtxRangeEnd</span><span class="p">(</span><span class="n">id1</span><span class="p">);</span>

<span class="n">nvtxEventAttributes_t</span><span class="n">eventAttrib</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">version</span><span class="o">=</span><span class="n">NVTX_VERSION</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">size</span><span class="o">=</span><span class="n">NVTX_EVENT_ATTRIB_STRUCT_SIZE</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">colorType</span><span class="o">=</span><span class="n">NVTX_COLOR_ARGB</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">color</span><span class="o">=</span><span class="n">COLOR_BLUE</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">messageType</span><span class="o">=</span><span class="n">NVTX_MESSAGE_TYPE_ASCII</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">message</span><span class="p">.</span><span class="n">ascii</span><span class="o">=</span><span class="s">"my start/stop range"</span><span class="p">;</span>
<span class="n">nvtxRangeId_t</span><span class="n">id2</span><span class="o">=</span><span class="n">nvtxRangeStartEx</span><span class="p">(</span><span class="o">&amp;</span><span class="n">eventAttrib</span><span class="p">);</span>
<span class="n">nvtxRangeEnd</span><span class="p">(</span><span class="n">id2</span><span class="p">);</span>

<span class="c1">// overlapping ranges</span>
<span class="n">nvtxRangeId_t</span><span class="n">r1</span><span class="o">=</span><span class="n">nvtxRangeStartA</span><span class="p">(</span><span class="s">"My range 0"</span><span class="p">);</span>
<span class="n">nvtxRangeId_t</span><span class="n">r2</span><span class="o">=</span><span class="n">nvtxRangeStartA</span><span class="p">(</span><span class="s">"My range 1"</span><span class="p">);</span>
<span class="n">nvtxRangeEnd</span><span class="p">(</span><span class="n">r1</span><span class="p">);</span>
<span class="n">nvtxRangeEnd</span><span class="p">(</span><span class="n">r2</span><span class="p">);</span>
</pre>
 <h3>
  <span class="section-number">
   5.2.3.
  </span>
  NVTX Range Push/Pop
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-range-push-pop" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  A push/pop range is used to denote nested time span. The start of a range must occur on the same thread as the end of the range. A range can contain a text message or specify additional information using the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#event-attributes-structure">
   event attributes structure
  </a>
  . Use
  <span class="pre">
   nvtxRangePushA()
  </span>
  to create a marker containing an ASCII message. Use
  <span class="pre">
   nvtxRangePushEx()
  </span>
  to create a range containing additional attributes specified by the event attribute structure. The
  <span class="pre">
   nvtxRangePushW()
  </span>
  function is not supported in the CUDA implementation of NVTX and has no effect if called. Each push function returns the zero-based depth of the range being started. The
  <span class="pre">
   nvtxRangePop()
  </span>
  function is used to end the most recently pushed range for the thread.
  <span class="pre">
   nvtxRangePop()
  </span>
  returns the zero-based depth of the range being ended. If the pop does not have a matching push, a negative value is returned to indicate an error.
 </p>
 <p>
  Code Example
 </p>
 <pre><span class="n">nvtxRangePushA</span><span class="p">(</span><span class="s">"outer"</span><span class="p">);</span>
<span class="n">nvtxRangePushA</span><span class="p">(</span><span class="s">"inner"</span><span class="p">);</span>
<span class="n">nvtxRangePop</span><span class="p">();</span><span class="c1">// end "inner" range</span>
<span class="n">nvtxRangePop</span><span class="p">();</span><span class="c1">// end "outer" range</span>

<span class="n">nvtxEventAttributes_t</span><span class="n">eventAttrib</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">version</span><span class="o">=</span><span class="n">NVTX_VERSION</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">size</span><span class="o">=</span><span class="n">NVTX_EVENT_ATTRIB_STRUCT_SIZE</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">colorType</span><span class="o">=</span><span class="n">NVTX_COLOR_ARGB</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">color</span><span class="o">=</span><span class="n">COLOR_GREEN</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">messageType</span><span class="o">=</span><span class="n">NVTX_MESSAGE_TYPE_ASCII</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">message</span><span class="p">.</span><span class="n">ascii</span><span class="o">=</span><span class="s">"my push/pop range"</span><span class="p">;</span>
<span class="n">nvtxRangePushEx</span><span class="p">(</span><span class="o">&amp;</span><span class="n">eventAttrib</span><span class="p">);</span>
<span class="n">nvtxRangePop</span><span class="p">();</span>
</pre>
 <h3>
  <span class="section-number">
   5.2.4.
  </span>
  Event Attributes Structure
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#event-attributes-structure" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The events attributes structure,
  <span class="pre">
   nvtxEventAttributes_t
  </span>
  , is used to describe the attributes of an event. The layout of the structure is defined by a specific version of NVTX and can change between different versions of the Tools Extension library.
 </p>
 <p>
  Attributes
 </p>
 <p>
  Markers and ranges can use attributes to provide additional information for an event or to guide the toolâs visualization of the data. Each of the attributes is optional and if left unspecified, the attributes fall back to a default value.
 </p>
 Message
 <p>
  The message field can be used to specify an optional string. The caller must set both the
  <span class="pre">
   messageType
  </span>
  and
  <span class="pre">
   message
  </span>
  fields. The default value is
  <span class="pre">
   NVTX_MESSAGE_UNKNOWN
  </span>
  . The CUDA implementation of NVTX only supports ASCII type messages.
 </p>
 Category
 <p>
  The category attribute is a user-controlled ID that can be used to group events. The tool may use category IDs to improve filtering, or for grouping events. The default value is 0.
 </p>
 Color
 <p>
  The color attribute is used to help visually identify events in the tool. The caller must set both the
  <span class="pre">
   colorType
  </span>
  and
  <span class="pre">
   color
  </span>
  fields.
 </p>
 Payload
 <p>
  The payload attribute can be used to provide additional data for markers and ranges. Range events can only specify values at the beginning of a range. The caller must specify valid values for both the
  <span class="pre">
   payloadType
  </span>
  and
  <span class="pre">
   payload
  </span>
  fields.
 </p>
 <p>
  Initialization
 </p>
 <p>
  The caller should always perform the following three tasks when using attributes:
 </p>
 <ul class="simple">
  <li>
   <p>
    Zero the structure
   </p>
  </li>
  <li>
   <p>
    Set the version field
   </p>
  </li>
  <li>
   <p>
    Set the size field
   </p>
  </li>
 </ul>
 <p>
  Zeroing the structure sets all the event attributes types and values to the default value. The version and size field are used by NVTX to handle multiple versions of the attributes structure.
 </p>
 <p>
  It is recommended that the caller use the following method to initialize the event attributes structure.
 </p>
 <pre><span class="n">nvtxEventAttributes_t</span><span class="n">eventAttrib</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">version</span><span class="o">=</span><span class="n">NVTX_VERSION</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">size</span><span class="o">=</span><span class="n">NVTX_EVENT_ATTRIB_STRUCT_SIZE</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">colorType</span><span class="o">=</span><span class="n">NVTX_COLOR_ARGB</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">color</span><span class="o">=</span><span class="o">::</span><span class="n">COLOR_YELLOW</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">messageType</span><span class="o">=</span><span class="n">NVTX_MESSAGE_TYPE_ASCII</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">message</span><span class="p">.</span><span class="n">ascii</span><span class="o">=</span><span class="s">"My event"</span><span class="p">;</span>
<span class="n">nvtxMarkEx</span><span class="p">(</span><span class="o">&amp;</span><span class="n">eventAttrib</span><span class="p">);</span>
</pre>
 <h3>
  <span class="section-number">
   5.2.5.
  </span>
  NVTX Synchronization Markers
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-synchronization-markers" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The NVTX synchronization module provides functions to support tracking additional synchronization details of the target application. Naming OS synchronization primitives may allow users to better understand the data collected by traced synchronization APIs. Additionally, annotating a user-defined synchronization object can allow the user to tell the tools when the user is building their own synchronization system that does not rely on the OS to provide behaviors, and instead uses techniques like atomic operations and spinlocks.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Synchronization marker support is not available on Windows.
 </p>
 <p>
  Code Example
 </p>
 <pre><span class="k">class</span><span class="nc">MyMutex</span>
<span class="p">{</span>
<span class="k">volatile</span><span class="kt">long</span><span class="n">bLocked</span><span class="p">;</span>
<span class="n">nvtxSyncUser_t</span><span class="n">hSync</span><span class="p">;</span>

<span class="k">public</span><span class="o">:</span>
<span class="n">MyMutex</span><span class="p">(</span><span class="k">const</span><span class="kt">char</span><span class="o">*</span><span class="n">name</span><span class="p">,</span><span class="n">nvtxDomainHandle_t</span><span class="n">d</span><span class="p">)</span><span class="p">{</span>
<span class="n">bLocked</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>
<span class="n">nvtxSyncUserAttributes_t</span><span class="n">attribs</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="n">attribs</span><span class="p">.</span><span class="n">version</span><span class="o">=</span><span class="n">NVTX_VERSION</span><span class="p">;</span>
<span class="n">attribs</span><span class="p">.</span><span class="n">size</span><span class="o">=</span><span class="n">NVTX_SYNCUSER_ATTRIB_STRUCT_SIZE</span><span class="p">;</span>
<span class="n">attribs</span><span class="p">.</span><span class="n">messageType</span><span class="o">=</span><span class="n">NVTX_MESSAGE_TYPE_ASCII</span><span class="p">;</span>
<span class="n">attribs</span><span class="p">.</span><span class="n">message</span><span class="p">.</span><span class="n">ascii</span><span class="o">=</span><span class="n">name</span><span class="p">;</span>
<span class="n">hSync</span><span class="o">=</span><span class="n">nvtxDomainSyncUserCreate</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="o">&amp;</span><span class="n">attribs</span><span class="p">);</span>
<span class="p">}</span>

<span class="o">~</span><span class="n">MyMutex</span><span class="p">()</span><span class="p">{</span>
<span class="n">nvtxDomainSyncUserDestroy</span><span class="p">(</span><span class="n">hSync</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">bool</span><span class="n">Lock</span><span class="p">()</span><span class="p">{</span>
<span class="n">nvtxDomainSyncUserAcquireStart</span><span class="p">(</span><span class="n">hSync</span><span class="p">);</span>

<span class="c1">//atomic compiler intrinsic</span>
<span class="kt">bool</span><span class="n">acquired</span><span class="o">=</span><span class="n">__sync_bool_compare_and_swap</span><span class="p">(</span><span class="o">&amp;</span><span class="n">bLocked</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>

<span class="k">if</span><span class="p">(</span><span class="n">acquired</span><span class="p">)</span><span class="p">{</span>
<span class="n">nvtxDomainSyncUserAcquireSuccess</span><span class="p">(</span><span class="n">hSync</span><span class="p">);</span>
<span class="p">}</span>
<span class="k">else</span><span class="p">{</span>
<span class="n">nvtxDomainSyncUserAcquireFailed</span><span class="p">(</span><span class="n">hSync</span><span class="p">);</span>
<span class="p">}</span>
<span class="k">return</span><span class="n">acquired</span><span class="p">;</span>
<span class="p">}</span>

<span class="kt">void</span><span class="n">Unlock</span><span class="p">()</span><span class="p">{</span>
<span class="n">nvtxDomainSyncUserReleasing</span><span class="p">(</span><span class="n">hSync</span><span class="p">);</span>
<span class="n">bLocked</span><span class="o">=</span><span class="nb">false</span><span class="p">;</span>
<span class="p">}</span>
<span class="p">};</span>
</pre>
 <h2>
  <span class="section-number">
   5.3.
  </span>
  NVTX Domains
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-domains" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Domains enable developers to scope annotations. By default all events and annotations are in the default domain. Additional domains can be registered. This allows developers to scope markers and ranges to avoid conflicts.
 </p>
 <p>
  The function
  <span class="pre">
   nvtxDomainCreateA()
  </span>
  or
  <span class="pre">
   nvtxDomainCreateW()
  </span>
  is used to create a named domain.
 </p>
 <p>
  Each domain maintains its own
 </p>
 <ul class="simple">
  <li>
   <p>
    categories
   </p>
  </li>
  <li>
   <p>
    thread range stacks
   </p>
  </li>
  <li>
   <p>
    registered strings
   </p>
  </li>
 </ul>
 <p>
  The function
  <span class="pre">
   nvtxDomainDestroy()
  </span>
  marks the end of the domain. Destroying a domain unregisters and destroys all objects associated with it such as registered strings, resource objects, named categories, and started ranges.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Domain support is not available on Windows.
 </p>
 <p>
  Code Example
 </p>
 <pre><span class="n">nvtxDomainHandle_t</span><span class="n">domain</span><span class="o">=</span><span class="n">nvtxDomainCreateA</span><span class="p">(</span><span class="s">"Domain_A"</span><span class="p">);</span>

<span class="n">nvtxMarkA</span><span class="p">(</span><span class="s">"Mark_A"</span><span class="p">);</span>
<span class="n">nvtxEventAttributes_t</span><span class="n">attrib</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="n">attrib</span><span class="p">.</span><span class="n">version</span><span class="o">=</span><span class="n">NVTX_VERSION</span><span class="p">;</span>
<span class="n">attrib</span><span class="p">.</span><span class="n">size</span><span class="o">=</span><span class="n">NVTX_EVENT_ATTRIB_STRUCT_SIZE</span><span class="p">;</span>
<span class="n">attrib</span><span class="p">.</span><span class="n">message</span><span class="p">.</span><span class="n">ascii</span><span class="o">=</span><span class="s">"Mark A Message"</span><span class="p">;</span>
<span class="n">nvtxDomainMarkEx</span><span class="p">(</span><span class="nb">NULL</span><span class="p">,</span><span class="o">&amp;</span><span class="n">attrib</span><span class="p">);</span>

<span class="n">nvtxDomainDestroy</span><span class="p">(</span><span class="n">domain</span><span class="p">);</span>
</pre>
 <h2>
  <span class="section-number">
   5.4.
  </span>
  NVTX Resource Naming
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-resource-naming" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  NVTX resource naming allows custom names to be associated with host OS threads and CUDA resources such as devices, contexts, and streams. The names assigned using NVTX are displayed by the Visual Profiler.
 </p>
 <p>
  OS Thread
 </p>
 <p>
  The
  <span class="pre">
   nvtxNameOsThreadA()
  </span>
  function is used to name a host OS thread. The
  <span class="pre">
   nvtxNameOsThreadW()
  </span>
  function is not supported in the CUDA implementation of NVTX and has no effect if called. The following example shows how the current host OS thread can be named.
 </p>
 <pre><span class="c1">// Windows</span>
<span class="n">nvtxNameOsThread</span><span class="p">(</span><span class="n">GetCurrentThreadId</span><span class="p">(),</span><span class="s">"MAIN_THREAD"</span><span class="p">);</span>

<span class="c1">// Linux/Mac</span>
<span class="n">nvtxNameOsThread</span><span class="p">(</span><span class="n">pthread_self</span><span class="p">(),</span><span class="s">"MAIN_THREAD"</span><span class="p">);</span>
</pre>
 <p>
  CUDA Runtime Resources
 </p>
 <p>
  The
  <span class="pre">
   nvtxNameCudaDeviceA()
  </span>
  and
  <span class="pre">
   nvtxNameCudaStreamA()
  </span>
  functions are used to name CUDA device and stream objects, respectively. The
  <span class="pre">
   nvtxNameCudaDeviceW()
  </span>
  and
  <span class="pre">
   nvtxNameCudaStreamW()
  </span>
  functions are not supported in the CUDA implementation of NVTX and have no effect if called. The
  <span class="pre">
   nvtxNameCudaEventA()
  </span>
  and
  <span class="pre">
   nvtxNameCudaEventW()
  </span>
  functions are also not supported. The following example shows how a CUDA device and stream can be named.
 </p>
 <pre><span class="n">nvtxNameCudaDeviceA</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="s">"my cuda device 0"</span><span class="p">);</span>

<span class="n">cudaStream_t</span><span class="n">cudastream</span><span class="p">;</span>
<span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">cudastream</span><span class="p">);</span>
<span class="n">nvtxNameCudaStreamA</span><span class="p">(</span><span class="n">cudastream</span><span class="p">,</span><span class="s">"my cuda stream"</span><span class="p">);</span>
</pre>
 <p>
  CUDA Driver Resources
 </p>
 <p>
  The
  <span class="pre">
   nvtxNameCuDeviceA()
  </span>
  ,
  <span class="pre">
   nvtxNameCuContextA()
  </span>
  and
  <span class="pre">
   nvtxNameCuStreamA()
  </span>
  functions are used to name CUDA driver device, context and stream objects, respectively. The
  <span class="pre">
   nvtxNameCuDeviceW()
  </span>
  ,
  <span class="pre">
   nvtxNameCuContextW()
  </span>
  and
  <span class="pre">
   nvtxNameCuStreamW()
  </span>
  functions are not supported in the CUDA implementation of NVTX and have no effect if called. The
  <span class="pre">
   nvtxNameCuEventA()
  </span>
  and
  <span class="pre">
   nvtxNameCuEventW()
  </span>
  functions are also not supported. The following example shows how a CUDA device, context and stream can be named.
 </p>
 <pre><span class="n">CUdevice</span><span class="n">device</span><span class="p">;</span>
<span class="n">cuDeviceGet</span><span class="p">(</span><span class="o">&amp;</span><span class="n">device</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="n">nvtxNameCuDeviceA</span><span class="p">(</span><span class="n">device</span><span class="p">,</span><span class="s">"my device 0"</span><span class="p">);</span>

<span class="n">CUcontext</span><span class="n">context</span><span class="p">;</span>
<span class="n">cuCtxCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">context</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">device</span><span class="p">);</span>
<span class="n">nvtxNameCuContextA</span><span class="p">(</span><span class="n">context</span><span class="p">,</span><span class="s">"my context"</span><span class="p">);</span>

<span class="n">cuStream</span><span class="n">stream</span><span class="p">;</span>
<span class="n">cuStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="n">nvtxNameCuStreamA</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span><span class="s">"my stream"</span><span class="p">);</span>
</pre>
 <h2>
  <span class="section-number">
   5.5.
  </span>
  NVTX String Registration
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvtx-string-registration" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Registered strings are intended to increase performance by lowering instrumentation overhead. String may be registered once and the handle may be passed in place of a string where an the APIs may allow.
 </p>
 <p>
  The
  <span class="pre">
   nvtxDomainRegisterStringA()
  </span>
  function is used to register a string. The
  <span class="pre">
   nvtxDomainRegisterStringW()
  </span>
  function is not supported in the CUDA implementation of NVTX and has no effect if called.
 </p>
 <pre><span class="n">nvtxDomainHandle_t</span><span class="n">domain</span><span class="o">=</span><span class="n">nvtxDomainCreateA</span><span class="p">(</span><span class="s">"Domain_A"</span><span class="p">);</span>
<span class="n">nvtxStringHandle_t</span><span class="n">message</span><span class="o">=</span><span class="n">nvtxDomainRegisterStringA</span><span class="p">(</span><span class="n">domain</span><span class="p">,</span><span class="s">"registered string"</span><span class="p">);</span>
<span class="n">nvtxEventAttributes_t</span><span class="n">eventAttrib</span><span class="o">=</span><span class="p">{</span><span class="mi">0</span><span class="p">};</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">version</span><span class="o">=</span><span class="n">NVTX_VERSION</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">size</span><span class="o">=</span><span class="n">NVTX_EVENT_ATTRIB_STRUCT_SIZE</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">messageType</span><span class="o">=</span><span class="n">NVTX_MESSAGE_TYPE_REGISTERED</span><span class="p">;</span>
<span class="n">eventAttrib</span><span class="p">.</span><span class="n">message</span><span class="p">.</span><span class="n">registered</span><span class="o">=</span><span class="n">message</span><span class="p">;</span>
</pre>
 <h1>
  <span class="section-number">
   6.
  </span>
  MPI Profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#mpi-profiling" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   6.1.
  </span>
  Automatic MPI Annotation with NVTX
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#automatic-mpi-annotation-with-nvtx" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  You can annotate MPI calls with
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvidia-tools-extension">
   NVTX
  </a>
  markers to profile, trace and visualize them. It can get tedious to wrap every MPI call with NVTX markers, but there are two ways to do this automatically:
 </p>
 <p>
  Built-in annotation
 </p>
 <p>
  nvprof has a built-in option that supports two MPI implementations - OpenMPI and MPICH. If you have either of these installed on your system, you can use the
  <span class="pre">
   --annotate-mpi
  </span>
  option and specify your installed MPI implementation.
 </p>
 <p>
  If you use this option, nvprof will generate NVTX markers every time your application makes MPI calls. Only synchronous MPI calls are annotated using this built-in option. Additionally, we use NVTX to rename the current thread and current device object to indicate the MPI rank.
 </p>
 <p>
  For example if you have OpenMPI installed, you can annotate your application using the command:
 </p>
 <pre><span class="n">$</span><span class="n">mpirun</span><span class="o">-</span><span class="n">np</span><span class="mi">2</span><span class="n">nvprof</span><span class="o">--</span><span class="n">annotate</span><span class="o">-</span><span class="n">mpi</span><span class="n">openmpi</span><span class="p">.</span><span class="o">/</span><span class="n">my_mpi_app</span>
</pre>
 <p>
  This will give you output that looks something like this:
 </p>
 <pre><span class="n">NVTX</span><span class="n">result</span><span class="o">:</span>
<span class="n">Thread</span><span class="s">"MPI Rank 0"</span><span class="p">(</span><span class="n">id</span><span class="o">=</span><span class="mi">583411584</span><span class="p">)</span>
<span class="n">Domain</span><span class="s">"&lt;unnamed&gt;"</span>
<span class="n">Range</span><span class="s">"MPI_Reduce"</span>
<span class="n">Type</span><span class="n">Time</span><span class="p">(</span><span class="o">%</span><span class="p">)</span><span class="n">Time</span><span class="n">Calls</span><span class="n">Avg</span><span class="n">Min</span><span class="n">Max</span><span class="n">Name</span>
<span class="nl">Range</span><span class="p">:</span><span class="mf">100.00</span><span class="o">%</span><span class="mf">16.652</span><span class="n">us</span><span class="mi">1</span><span class="mf">16.652</span><span class="n">us</span><span class="mf">16.652</span><span class="n">us</span><span class="mf">16.652</span><span class="n">us</span><span class="n">MPI_Reduce</span>
<span class="p">...</span>

<span class="n">Range</span><span class="s">"MPI_Scatter"</span>
<span class="n">Type</span><span class="n">Time</span><span class="p">(</span><span class="o">%</span><span class="p">)</span><span class="n">Time</span><span class="n">Calls</span><span class="n">Avg</span><span class="n">Min</span><span class="n">Max</span><span class="n">Name</span>
<span class="nl">Range</span><span class="p">:</span><span class="mf">100.00</span><span class="o">%</span><span class="mf">3.0320</span><span class="n">ms</span><span class="mi">1</span><span class="mf">3.0320</span><span class="n">ms</span><span class="mf">3.0320</span><span class="n">ms</span><span class="mf">3.0320</span><span class="n">ms</span><span class="n">MPI_Scatter</span>
<span class="p">...</span>

<span class="n">NVTX</span><span class="n">result</span><span class="o">:</span>
<span class="n">Thread</span><span class="s">"MPI Rank 1"</span><span class="p">(</span><span class="n">id</span><span class="o">=</span><span class="mi">199923584</span><span class="p">)</span>
<span class="n">Domain</span><span class="s">"&lt;unnamed&gt;"</span>
<span class="n">Range</span><span class="s">"MPI_Reduce"</span>
<span class="n">Type</span><span class="n">Time</span><span class="p">(</span><span class="o">%</span><span class="p">)</span><span class="n">Time</span><span class="n">Calls</span><span class="n">Avg</span><span class="n">Min</span><span class="n">Max</span><span class="n">Name</span>
<span class="nl">Range</span><span class="p">:</span><span class="mf">100.00</span><span class="o">%</span><span class="mf">21.062</span><span class="n">us</span><span class="mi">1</span><span class="mf">21.062</span><span class="n">us</span><span class="mf">21.062</span><span class="n">us</span><span class="mf">21.062</span><span class="n">us</span><span class="n">MPI_Reduce</span>
<span class="p">...</span>

<span class="n">Range</span><span class="s">"MPI_Scatter"</span>
<span class="n">Type</span><span class="n">Time</span><span class="p">(</span><span class="o">%</span><span class="p">)</span><span class="n">Time</span><span class="n">Calls</span><span class="n">Avg</span><span class="n">Min</span><span class="n">Max</span><span class="n">Name</span>
<span class="nl">Range</span><span class="p">:</span><span class="mf">100.00</span><span class="o">%</span><span class="mf">85.296</span><span class="n">ms</span><span class="mi">1</span><span class="mf">85.296</span><span class="n">ms</span><span class="mf">85.296</span><span class="n">ms</span><span class="mf">85.296</span><span class="n">ms</span><span class="n">MPI_Scatter</span>
<span class="p">...</span>
</pre>
 <p>
  Custom annotation
 </p>
 <p>
  If your system has a version of MPI that is not supported by nvprof, or if you want more control over which MPI functions are annotated and how the NVTX markers are generated, you can create your own annotation library, and use the environment variable
  <span class="pre">
   LD_PRELOAD
  </span>
  to intercept MPI calls and wrap them with NVTX markers.
 </p>
 <p>
  You can create this annotation library conveniently using the documentation and open-source scripts located
  <a class="reference external" href="https://github.com/NVIDIA/cuda-profiler/tree/master/nvtx_pmpi_wrappers">
   here
  </a>
  .
 </p>
 <h2>
  <span class="section-number">
   6.2.
  </span>
  Manual MPI Profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#manual-mpi-profiling" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To use
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof">
   nvprof
  </a>
  to collect the profiles of the individual MPI processes, you must tell
  <span class="pre">
   nvprof
  </span>
  to send its output to unique files. In CUDA 5.0 and earlier versions, it was recommended to use a script for this. However, you can now easily do it utilizing the
  <span class="pre">
   %h
  </span>
  ,
  <span class="pre">
   %p
  </span>
  and
  <span class="pre">
   %q{ENV}
  </span>
  features of the
  <span class="pre">
   --export-profile
  </span>
  argument to the
  <span class="pre">
   nvprof
  </span>
  command. Below is example run using Open MPI.
 </p>
 <pre><span class="n">$</span><span class="n">mpirun</span><span class="o">-</span><span class="n">np</span><span class="mi">2</span><span class="o">-</span><span class="n">host</span><span class="n">c0</span><span class="mi">-0</span><span class="p">,</span><span class="n">c0</span><span class="mi">-1</span><span class="n">nvprof</span><span class="o">-</span><span class="n">o</span><span class="n">output</span><span class="p">.</span><span class="o">%</span><span class="n">h</span><span class="p">.</span><span class="o">%</span><span class="n">p</span><span class="p">.</span><span class="o">%</span><span class="n">q</span><span class="p">{</span><span class="n">OMPI_COMM_WORLD_RANK</span><span class="p">}</span><span class="p">.</span><span class="o">/</span><span class="n">my_mpi_app</span>
</pre>
 <p>
  Alternatively, one can make use of the new feature to turn on profiling on the nodes of interest using the
  <span class="pre">
   --profile-all-processes
  </span>
  argument to
  <span class="pre">
   nvprof
  </span>
  . To do this, you first log into the node you want to profile and start up
  <span class="pre">
   nvprof
  </span>
  there.
 </p>
 <pre><span class="n">$</span><span class="n">nvprof</span><span class="o">--</span><span class="n">profile</span><span class="o">-</span><span class="n">all</span><span class="o">-</span><span class="n">processes</span><span class="o">-</span><span class="n">o</span><span class="n">output</span><span class="p">.</span><span class="o">%</span><span class="n">h</span><span class="p">.</span><span class="o">%</span><span class="n">p</span>
</pre>
 <p>
  Then you can just run the MPI job as your normally would.
 </p>
 <pre><span class="n">$</span><span class="n">mpirun</span><span class="o">-</span><span class="n">np</span><span class="mi">2</span><span class="o">-</span><span class="n">host</span><span class="n">c0</span><span class="mi">-0</span><span class="p">,</span><span class="n">c0</span><span class="mi">-1</span><span class="p">.</span><span class="o">/</span><span class="n">my_mpi_app</span>
</pre>
 <p>
  Any processes that run on the node where the
  <span class="pre">
   --profile-all-processes
  </span>
  is running will automatically get profiled. The profiling data will be written to the output files. Note that the
  <span class="pre">
   %q{OMPI_COMM_WORLD_RANK}
  </span>
  option will not work here, because this environment variable will not be available in the shell where nvprof is running.
 </p>
 <p>
  Starting CUDA 7.5, you can name threads and CUDA contexts just as you name output files with the options âprocess-name and âcontext-name, by passing a string like
  <span class="pre">
   "MPI
  </span>
  <span class="pre">
   Rank
  </span>
  <span class="pre">
   %q{OMPI_COMM_WORLD_RANK}"
  </span>
  as a parameter. This feature is useful to spot resources associated with a specific rank when user imports multiple files into the same time-line in the Visual Profiler.
 </p>
 <pre><span class="n">$</span><span class="n">mpirun</span><span class="o">-</span><span class="n">np</span><span class="mi">2</span><span class="o">-</span><span class="n">host</span><span class="n">c0</span><span class="mi">-0</span><span class="p">,</span><span class="n">c0</span><span class="mi">-1</span><span class="n">nvprof</span><span class="o">--</span><span class="n">process</span><span class="o">-</span><span class="n">name</span><span class="s">"MPI Rank %q{OMPI_COMM_WORLD_RANK}"</span><span class="o">--</span><span class="n">context</span><span class="o">-</span><span class="n">name</span><span class="s">"MPI Rank %q{OMPI_COMM_WORLD_RANK}"</span><span class="o">-</span><span class="n">o</span><span class="n">output</span><span class="p">.</span><span class="o">%</span><span class="n">h</span><span class="p">.</span><span class="o">%</span><span class="n">p</span><span class="p">.</span><span class="o">%</span><span class="n">q</span><span class="p">{</span><span class="n">OMPI_COMM_WORLD_RANK</span><span class="p">}</span><span class="p">.</span><span class="o">/</span><span class="n">my_mpi_app</span>
</pre>
 <h2>
  <span class="section-number">
   6.3.
  </span>
  Further Reading
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#further-reading" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Details about what types of additional arguments to use with
  <span class="pre">
   nvprof
  </span>
  can be found in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#multiprocess-profiling">
   Multiprocess Profiling
  </a>
  and
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#redirecting-output">
   Redirecting Output
  </a>
  section. Additional information about how to view the data with the Visual Profiler can be found in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-single-process-nvprof-session">
   Import Single-Process nvprof Session
  </a>
  and
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-multi-process-nvprof-session">
   Import Multi-Process nvprof Session
  </a>
  sections.
 </p>
 <p>
  The blog post
  <a class="reference external" href="http://devblogs.nvidia.com/parallelforall/cuda-pro-tip-profiling-mpi-applications">
   Profiling MPI Applications
  </a>
  shows how to use new output file naming of nvprof introduced in CUDA 6.5 and NVTX library to name various resources to analyze the performance of a MPI application.
 </p>
 <p>
  The blog post
  <a class="reference external" href="http://devblogs.nvidia.com/parallelforall/gpu-pro-tip-track-mpi-calls-nvidia-visual-profiler">
   Track MPI Calls in the Visual Profiler
  </a>
  shows how Visual Profiler, combined with PMPI and NVTX can give interesting insights into how the MPI calls in your application interact with the GPU.
 </p>
 <h1>
  <span class="section-number">
   7.
  </span>
  MPS Profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#mps-profiling" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  You can collect profiling data for a CUDA application using Multi-Process Service(MPS) with
  <span class="pre">
   nvprof
  </span>
  and then view the timeline by importing the data in the Visual Profiler.
 </p>
 <h2>
  <span class="section-number">
   7.1.
  </span>
  MPS profiling with Visual Profiler
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#mps-profiling-with-visual-profiler" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Visual Profiler can be run on a particular MPS client or for all MPS clients. Timeline profiling can be done for all MPS clients on the same server. Event or metric profiling results in serialization - only one MPS client will execute at a time.
 </p>
 <p>
  To profile a CUDA application using MPS:
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    Launch the MPS daemon. Refer the MPS document for details.
   </p>
  </li>
 </ol>
 <pre>nvidia-cuda-mps-control -d
</pre>
 <ol class="arabic simple" start="2">
  <li>
   <p>
    In Visual Profiler open âNew Sessionâ wizard using main menu âFile-&gt;New Sessionâ. Select âProfile all processesâ option from drop down, press âNextâ and then âFinishâ.
   </p>
  </li>
  <li>
   <p>
    Run the application in a separate terminal
   </p>
  </li>
  <li>
   <p>
    To end profiling press the âCancelâ button on progress dialog in Visual Profiler.
   </p>
  </li>
 </ol>
 <p>
  Note that the profiling output also includes data for the CUDA MPS server processes which have process name
  <span class="pre">
   nvidia-cuda-mps-server
  </span>
  .
 </p>
 <h2>
  <span class="section-number">
   7.2.
  </span>
  MPS profiling with nvprof
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#mps-profiling-with-nvprof" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  <span class="pre">
   nvprof
  </span>
  can be run on a particular MPS client or for all MPS clients. Timeline profiling can be done for all MPS clients on the same server. Event or metric profiling results in serialization - only one MPS client will execute at a time.
 </p>
 <p>
  To profile a CUDA application using MPS:
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    Launch the MPS daemon. Refer to the MPS document for details.
   </p>
  </li>
 </ol>
 <pre>nvidia-cuda-mps-control -d
</pre>
 <ol class="arabic simple" start="2">
  <li>
   <p>
    Run
    <span class="pre">
     nvprof
    </span>
    with
    <span class="pre">
     --profile-all-processes
    </span>
    argument and to generate separate output files for each process use the
    <span class="pre">
     %p
    </span>
    feature of the
    <span class="pre">
     --export-profile
    </span>
    argument. Note that
    <span class="pre">
     %p
    </span>
    will be replaced by the process id.
   </p>
  </li>
 </ol>
 <pre>nvprof --profile-all-processes -o output_%p
</pre>
 <ol class="arabic simple" start="3">
  <li>
   <p>
    Run the application in a separate terminal
   </p>
  </li>
  <li>
   <p>
    Exit
    <span class="pre">
     nvprof
    </span>
    by typing âCtrl-câ.
   </p>
  </li>
 </ol>
 <p>
  Note that the profiling output also includes data for the CUDA MPS server processes which have process name
  <span class="pre">
   nvidia-cuda-mps-server
  </span>
  .
 </p>
 <h2>
  <span class="section-number">
   7.3.
  </span>
  Viewing nvprof MPS timeline in Visual Profiler
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#viewing-nvprof-mps-timeline-in-visual-profiler" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Import the nvprof generated data files for each process using the multi-process import option. Refer the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#import-multi-process-nvprof-session">
   Import Multi-Process Session
  </a>
  section.
 </p>
 <p>
  The figure below shows the MPS timeline view for three processes. The MPS context is identified in the timeline row label as Context MPS. Note that the Compute and kernel timeline row shows three kernels overlapping.
 </p>
 <h1>
  <span class="section-number">
   8.
  </span>
  Dependency Analysis
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#id1" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  The dependency analysis feature enables optimization of the program runtime and concurrency of applications utilizing multiple CPU threads and CUDA streams. It allows to compute the critical path of a specific execution, detect waiting time and inspect dependencies between functions executing in different threads or streams.
 </p>
 <h2>
  <span class="section-number">
   8.1.
  </span>
  Background
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#background" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The dependency analysis in
  <span class="pre">
   nvprof
  </span>
  and the
  <span class="pre">
   Visual
  </span>
  <span class="pre">
   Profiler
  </span>
  is based on execution traces of applications. A trace captures all relevant activities such as API function calls or CUDA kernels along with their timestamps and durations. Given this execution trace and a model of the dependencies between those activities on different threads/streams, a dependency graph can be constructed. Typical dependencies modelled in this graph would be that a CUDA kernel can not start before its respective launch API call or that a blocking CUDA stream synchronization call can not return before all previously enqueued work in this stream has been completed. These dependencies are defined by the CUDA API contract.
 </p>
 <p>
  From this dependency graph and the API model(s), wait states can be computed. A wait state is the duration for which an activity such as an API function call is blocked waiting on an event in another thread or stream. Given the previous stream synchronization example, the synchronizing API call is blocked for the time it has to wait on any GPU activity in the respective CUDA stream. Knowledge about where wait states occur and how long functions are blocked is helpful to identify optimization opportunities for more high-level concurrency in the application.
 </p>
 <p>
  In addition to individual wait states, the critical path through the captured event graph enables to pinpoint those function calls, kernel and memory copies that are responsible for the total application runtime. The critical path is the longest path through an event graph that does not contain wait states, i.e. optimizing activities on this path can directly improve the execution time.
 </p>
 <h2>
  <span class="section-number">
   8.2.
  </span>
  Metrics
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Waiting Time
 </p>
 <p>
  A wait state is the duration for which an activity such as an API function call is blocked waiting on an event in another thread or stream. Waiting time is an inidicator for load-imbalances between execution streams. In the example below, the blocking CUDA synchronization API calls are waiting on their respective kernels to finish executing on the GPU. Instead of waiting immediately, one should attempt to overlap the kernel executions with concurrent CPU work with a similar runtime, thereby reducing the time that any computing device (CPU or GPU) is blocked.
 </p>
 <p>
  Time on Critical Path
 </p>
 <p>
  The critical path is the longest path through an event graph that does not contain wait states, i.e. optimizing activities on this path can directly improve the execution time. Activities with a high time on the critical path have a high direct impact on the application runtime. In the example pictured below,
  <span class="pre">
   copy_kernel
  </span>
  is on the critical path since the CPU is blocked waiting for it to finish in
  <span class="pre">
   cudeDeviceSynchronize
  </span>
  . Reducing the kernel runtime allows the CPU to return earlier from the API call and continue program execution. On the other hand,
  <span class="pre">
   jacobi_kernel
  </span>
  is fully overlapped with CPU work, i.e. the synchronizing API call is triggered after the kernel is already finished. Since no execution stream is waiting on this kernel to finish, reducing its duration will likely not improve the overall application runtime.
 </p>
 <h2>
  <span class="section-number">
   8.3.
  </span>
  Support
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#support" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The following programming APIs are currently supported for dependency analysis
 </p>
 <ul class="simple">
  <li>
   <p>
    CUDA runtime and driver API
   </p>
  </li>
  <li>
   <p>
    POSIX threads (Pthreads), POSIX mutexes and condition variables
   </p>
  </li>
 </ul>
 <p>
  Dependency analysis is available in Visual Profiler and
  <span class="pre">
   nvprof
  </span>
  . A Dependency Analysis stage can be selected in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#unguided-application-analysis">
   Unguided Application Analysis
  </a>
  and new
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis-controls">
   Dependency Analysis Controls
  </a>
  are available for the timeline. See section
  <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis-controls">
   Dependency Analysis
  </a>
  on how to use this feature in
  <span class="pre">
   nvprof
  </span>
  .
 </p>
 <h2>
  <span class="section-number">
   8.4.
  </span>
  Limitations
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#limitations" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The dependency and wait time analysis between different threads and CUDA streams only takes into account execution dependencies stated in the respective supported API contracts. This especially does not include synchronization as a result of resource contention. For example, asynchronous memory copies enqueued into independent CUDA streams will not be marked dependent even if the concrete GPU has only a single copy engine. Furthermore, the analysis does not account for synchronization using a not-supported API. For example, a CPU thread actively polling for a value at some memory location (busy-waiting) will not be considered blocked on another concurrent activity.
 </p>
 <p>
  The dependency analysis has only limited support for applications using CUDA Dynamic Parallelism (CDP). CDP kernels can use CUDA API calls from the GPU which are not tracked via the CUPTI Activity API. Therefore, the analysis cannot determine the full dependencies and waiting time for CDP kernels. However, it utilizes the parent-child launch dependencies between CDP kernels. As a result the critical path will always include the last CDP kernel of each host-launched kernel.
 </p>
 <p>
  The POSIX semaphores API is currently not supported.
 </p>
 <p>
  The dependency analysis does not support API functions
  <span class="pre">
   cudaLaunchCooperativeKernelMultiDevice
  </span>
  or
  <span class="pre">
   cuLaunchCooperativeKernelMultiDevice
  </span>
  . Kernel launched by either of these API functions might not be tracked correctly.
 </p>
 <h1>
  <span class="section-number">
   9.
  </span>
  Metrics Reference
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-reference" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  This section contains detailed descriptions of the metrics that can be collected by
  <span class="pre">
   nvprof
  </span>
  and the Visual Profiler. A scope value of âSingle-contextâ indicates that the metric can only be accurately collected when a single context (CUDA or graphic) is executing on the GPU. A scope value of âMulti-contextâ indicates that the metric can be accurately collected when multiple contexts are executing on the GPU. A scope value of âDeviceâ indicates that the metric will be collected at device level, that is it will include values for all the contexts executing on the GPU.
  Note that, NVLink metrics collected for kernel mode exhibit the behavior of âSingle-contextâ.
 </p>
 <h2>
  <span class="section-number">
   9.1.
  </span>
  Metrics for Capability 5.x
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-for-capability-5-x" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Devices with compute capability 5.x implement the metrics shown in the following table. Note that for some metrics the âMulti-contextâ scope is supported only for specific devices. Such metrics are marked with âMulti-context
  *
  â under the âScopeâ column. Refer to the note at the bottom of the table.
 </p>
 <table class="table-no-stripes docutils align-default" id="id2">
  <span class="caption-text">
   Table 4. Capability 5.x Metrics
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#id2" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Metric Name
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
   <th class="head">
    <p>
     Scope
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     achieved_occupancy
    </p>
   </td>
   <td>
    <p>
     Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     atomic_transactions
    </p>
   </td>
   <td>
    <p>
     Global memory atomic and reduction transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     atomic_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of global memory atomic and reduction transactions performed for each atomic and reduction instruction
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     branch_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of non-divergent branches to total branches expressed as percentage
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cf_executed
    </p>
   </td>
   <td>
    <p>
     Number of executed control-flow instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     cf_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute control-flow instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cf_issued
    </p>
   </td>
   <td>
    <p>
     Number of issued control-flow instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     double_precision_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute double-precision floating-point instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_read_bytes
    </p>
   </td>
   <td>
    <p>
     Total bytes read from DRAM to L2 cache. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     dram_read_throughput
    </p>
   </td>
   <td>
    <p>
     Device memory read throughput. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_read_transactions
    </p>
   </td>
   <td>
    <p>
     Device memory read transactions. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     dram_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the device memory relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_write_bytes
    </p>
   </td>
   <td>
    <p>
     Total bytes written from L2 cache to DRAM. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     dram_write_throughput
    </p>
   </td>
   <td>
    <p>
     Device memory write throughput. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_write_transactions
    </p>
   </td>
   <td>
    <p>
     Device memory write transactions. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     ecc_throughput
    </p>
   </td>
   <td>
    <p>
     ECC throughput from L2 to DRAM. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     ecc_transactions
    </p>
   </td>
   <td>
    <p>
     Number of ECC transactions between L2 and DRAM. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     eligible_warps_per_cycle
    </p>
   </td>
   <td>
    <p>
     Average number of warps that are eligible to issue per active cycle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_dp
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_dp_add
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point add operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_dp_fma
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_dp_mul
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point multiply operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_hp
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point operations executed by non-predicated threads (add, multiply and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. This is available for compute capability 5.3.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_hp_add
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point add operations executed by non-predicated threads. This is available for compute capability 5.3.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_hp_fma
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count. This is available for compute capability 5.3.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_hp_mul
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point multiply operations executed by non-predicated threads. This is available for compute capability 5.3.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_sp
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. The count does not include special operations.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_sp_add
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point add operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_sp_fma
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_sp_mul
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point multiply operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_sp_special
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point special operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_dp_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of achieved to peak double-precision floating-point operations
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_hp_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of achieved to peak half-precision floating-point operations. This is available for compute capability 5.3.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_sp_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of achieved to peak single-precision floating-point operations
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gld_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of requested global memory load throughput to required global memory load throughput expressed as percentage.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gld_requested_throughput
    </p>
   </td>
   <td>
    <p>
     Requested global memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gld_throughput
    </p>
   </td>
   <td>
    <p>
     Global memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gld_transactions
    </p>
   </td>
   <td>
    <p>
     Number of global memory load transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gld_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of global memory load transactions performed for each global memory load.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     global_atomic_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global atomic(Atom and Atom CAS) requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     global_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate for global loads in unified l1/tex cache. Metric value maybe wrong if malloc is used in kernel.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     global_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     global_reduction_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global reduction requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     global_store_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global store requests from Multiprocessor. This does not include atomic requests.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gst_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of requested global memory store throughput to required global memory store throughput expressed as percentage.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gst_requested_throughput
    </p>
   </td>
   <td>
    <p>
     Requested global memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gst_throughput
    </p>
   </td>
   <td>
    <p>
     Global memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gst_transactions
    </p>
   </td>
   <td>
    <p>
     Number of global memory store transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gst_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of global memory store transactions performed for each global memory store
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     half_precision_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute 16 bit floating-point instructions and integer instructions on a scale of 0 to 10. This is available for compute capability 5.3.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_bit_convert
    </p>
   </td>
   <td>
    <p>
     Number of bit-conversion instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_compute_ld_st
    </p>
   </td>
   <td>
    <p>
     Number of compute load/store instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_control
    </p>
   </td>
   <td>
    <p>
     Number of control-flow instructions executed by non-predicated threads (jump, branch, etc.)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed
    </p>
   </td>
   <td>
    <p>
     The number of instructions executed
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_global_atomics
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global atom and atom cas
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_global_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_global_reductions
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global reductions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_global_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_local_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for local loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_local_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for local stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_shared_atomics
    </p>
   </td>
   <td>
    <p>
     Warp level shared instructions for atom and atom CAS
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_shared_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for shared loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_shared_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for shared stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_surface_atomics
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface atom and atom cas
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_surface_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_surface_reductions
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface reductions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_surface_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_tex_ops
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for texture
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_fp_16
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.) This is available for compute capability 5.3.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_fp_32
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_fp_64
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_integer
    </p>
   </td>
   <td>
    <p>
     Number of integer instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_inter_thread_communication
    </p>
   </td>
   <td>
    <p>
     Number of inter-thread communication instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_issued
    </p>
   </td>
   <td>
    <p>
     The number of instructions issued
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_misc
    </p>
   </td>
   <td>
    <p>
     Number of miscellaneous instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_per_warp
    </p>
   </td>
   <td>
    <p>
     Average number of instructions executed by each warp
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_replay_overhead
    </p>
   </td>
   <td>
    <p>
     Average number of replays for each instruction executed
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     ipc
    </p>
   </td>
   <td>
    <p>
     Instructions executed per cycle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     issue_slot_utilization
    </p>
   </td>
   <td>
    <p>
     Percentage of issue slots that issued at least one instruction, averaged across all cycles
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     issue_slots
    </p>
   </td>
   <td>
    <p>
     The number of issue slots used
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     issued_ipc
    </p>
   </td>
   <td>
    <p>
     Instructions issued per cycle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_atomic_throughput
    </p>
   </td>
   <td>
    <p>
     Memory read throughput seen at L2 cache for atomic and reduction requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_atomic_transactions
    </p>
   </td>
   <td>
    <p>
     Memory read transactions seen at L2 cache for atomic and reduction requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_global_atomic_store_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from Unified cache for global atomics (ATOM and ATOM CAS)
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_global_load_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes read from L2 for misses in Unified Cache for global loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_global_reduction_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from Unified cache for global reductions
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_local_global_store_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from Unified Cache for local and global stores. This does not include global atomics.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_local_load_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes read from L2 for misses in Unified Cache for local loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_read_throughput
    </p>
   </td>
   <td>
    <p>
     Memory read throughput seen at L2 cache for all read requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_read_transactions
    </p>
   </td>
   <td>
    <p>
     Memory read transactions seen at L2 cache for all read requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_surface_atomic_store_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes transferred between Unified Cache and L2 for surface atomics (ATOM and ATOM CAS)
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_surface_load_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes read from L2 for misses in Unified Cache for surface loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_surface_reduction_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from Unified Cache for surface reductions
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_surface_store_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from Unified Cache for surface stores. This does not include surface atomics.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_tex_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate at L2 cache for all requests from texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_tex_read_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate at L2 cache for all read requests from texture cache. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_tex_read_throughput
    </p>
   </td>
   <td>
    <p>
     Memory read throughput seen at L2 cache for read requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_tex_read_transactions
    </p>
   </td>
   <td>
    <p>
     Memory read transactions seen at L2 cache for read requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_tex_write_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit Rate at L2 cache for all write requests from texture cache. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_tex_write_throughput
    </p>
   </td>
   <td>
    <p>
     Memory write throughput seen at L2 cache for write requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_tex_write_transactions
    </p>
   </td>
   <td>
    <p>
     Memory write transactions seen at L2 cache for write requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the L2 cache relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_write_throughput
    </p>
   </td>
   <td>
    <p>
     Memory write throughput seen at L2 cache for all write requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_write_transactions
    </p>
   </td>
   <td>
    <p>
     Memory write transactions seen at L2 cache for all write requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     ldst_executed
    </p>
   </td>
   <td>
    <p>
     Number of executed local, global, shared and texture memory load and store instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     ldst_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute shared load, shared store and constant load instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     ldst_issued
    </p>
   </td>
   <td>
    <p>
     Number of issued local, global, shared and texture memory load and store instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate for local loads and stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of local load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_load_throughput
    </p>
   </td>
   <td>
    <p>
     Local memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_load_transactions
    </p>
   </td>
   <td>
    <p>
     Number of local memory load transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_load_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of local memory load transactions performed for each local memory load
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_memory_overhead
    </p>
   </td>
   <td>
    <p>
     Ratio of local memory traffic to total memory traffic between the L1 and L2 caches expressed as percentage
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_store_requests
    </p>
   </td>
   <td>
    <p>
     Total number of local store requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_store_throughput
    </p>
   </td>
   <td>
    <p>
     Local memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_store_transactions
    </p>
   </td>
   <td>
    <p>
     Number of local memory store transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_store_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of local memory store transactions performed for each local memory store
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     pcie_total_data_received
    </p>
   </td>
   <td>
    <p>
     Total data bytes received through PCIe
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     pcie_total_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total data bytes transmitted through PCIe
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of requested shared memory throughput to required shared memory throughput expressed as percentage
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_load_throughput
    </p>
   </td>
   <td>
    <p>
     Shared memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_load_transactions
    </p>
   </td>
   <td>
    <p>
     Number of shared memory load transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_load_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of shared memory load transactions performed for each shared memory load
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_store_throughput
    </p>
   </td>
   <td>
    <p>
     Shared memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_store_transactions
    </p>
   </td>
   <td>
    <p>
     Number of shared memory store transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_store_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of shared memory store transactions performed for each shared memory store
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the shared memory relative to peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     single_precision_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute single-precision floating-point instructions and integer instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sm_efficiency
    </p>
   </td>
   <td>
    <p>
     The percentage of time at least one warp is active on a specific multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     special_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute sin, cos, ex2, popc, flo, and similar instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_constant_memory_dependency
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because of immediate constant cache miss
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_exec_dependency
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because an input required by the instruction is not yet available
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_inst_fetch
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because the next assembly instruction has not yet been fetched
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_memory_dependency
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because a memory operation cannot be performed due to the required resources not being available or fully utilized, or because too many requests of a given type are outstanding
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_memory_throttle
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because of memory throttle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_not_selected
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because warp was not selected
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_other
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring due to miscellaneous reasons
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_pipe_busy
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because a compute operation cannot be performed because the compute pipeline is busy
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_sync
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because the warp is blocked at a __syncthreads() call
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_texture
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because the texture sub-system is fully utilized or has too many outstanding requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     surface_atomic_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface atomic(Atom and Atom CAS) requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     surface_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     surface_reduction_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface reduction requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     surface_store_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface store requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_read_bytes
    </p>
   </td>
   <td>
    <p>
     Number of bytes read from system memory
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_read_throughput
    </p>
   </td>
   <td>
    <p>
     System memory read throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_read_transactions
    </p>
   </td>
   <td>
    <p>
     Number of system memory read transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_read_utilization
    </p>
   </td>
   <td>
    <p>
     The read utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_write_bytes
    </p>
   </td>
   <td>
    <p>
     Number of bytes written to system memory
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_write_throughput
    </p>
   </td>
   <td>
    <p>
     System memory write throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_write_transactions
    </p>
   </td>
   <td>
    <p>
     Number of system memory write transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_write_utilization
    </p>
   </td>
   <td>
    <p>
     The write utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 5.0 and 5.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     tex_cache_hit_rate
    </p>
   </td>
   <td>
    <p>
     Unified cache hit rate
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     tex_cache_throughput
    </p>
   </td>
   <td>
    <p>
     Unified cache throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     tex_cache_transactions
    </p>
   </td>
   <td>
    <p>
     Unified cache read transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     tex_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute global, local and texture memory instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     tex_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the unified cache relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
     *
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     texture_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of texture Load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     warp_execution_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of the average active threads per warp to the maximum number of threads per warp supported on a multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     warp_nonpred_execution_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of the average active threads per warp executing non-predicated instructions to the maximum number of threads per warp supported on a multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
 </table>
 <p>
  * The âMulti-contextâ scope for this metric is supported only for devices with compute capability 5.0 and 5.2.
 </p>
 <h2>
  <span class="section-number">
   9.2.
  </span>
  Metrics for Capability 6.x
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-for-capability-6-x" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Devices with compute capability 6.x implement the metrics shown in the following table.
 </p>
 <table class="table-no-stripes docutils align-default" id="id3">
  <span class="caption-text">
   Table 5. Capability 6.x Metrics
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#id3" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Metric Name
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
   <th class="head">
    <p>
     Scope
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     achieved_occupancy
    </p>
   </td>
   <td>
    <p>
     Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     atomic_transactions
    </p>
   </td>
   <td>
    <p>
     Global memory atomic and reduction transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     atomic_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of global memory atomic and reduction transactions performed for each atomic and reduction instruction
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     branch_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of non-divergent branches to total branches expressed as percentage
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cf_executed
    </p>
   </td>
   <td>
    <p>
     Number of executed control-flow instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     cf_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute control-flow instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cf_issued
    </p>
   </td>
   <td>
    <p>
     Number of issued control-flow instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     double_precision_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute double-precision floating-point instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_read_bytes
    </p>
   </td>
   <td>
    <p>
     Total bytes read from DRAM to L2 cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     dram_read_throughput
    </p>
   </td>
   <td>
    <p>
     Device memory read throughput. This is available for compute capability 6.0 and 6.1.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_read_transactions
    </p>
   </td>
   <td>
    <p>
     Device memory read transactions. This is available for compute capability 6.0 and 6.1.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     dram_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the device memory relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_write_bytes
    </p>
   </td>
   <td>
    <p>
     Total bytes written from L2 cache to DRAM
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     dram_write_throughput
    </p>
   </td>
   <td>
    <p>
     Device memory write throughput. This is available for compute capability 6.0 and 6.1.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_write_transactions
    </p>
   </td>
   <td>
    <p>
     Device memory write transactions. This is available for compute capability 6.0 and 6.1.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     ecc_throughput
    </p>
   </td>
   <td>
    <p>
     ECC throughput from L2 to DRAM. This is available for compute capability 6.1.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     ecc_transactions
    </p>
   </td>
   <td>
    <p>
     Number of ECC transactions between L2 and DRAM. This is available for compute capability 6.1.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     eligible_warps_per_cycle
    </p>
   </td>
   <td>
    <p>
     Average number of warps that are eligible to issue per active cycle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_dp
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_dp_add
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point add operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_dp_fma
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_dp_mul
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point multiply operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_hp
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_hp_add
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point add operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_hp_fma
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_hp_mul
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point multiply operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_sp
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. The count does not include special operations.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_sp_add
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point add operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_sp_fma
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_sp_mul
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point multiply operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_sp_special
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point special operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_dp_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of achieved to peak double-precision floating-point operations
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_hp_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of achieved to peak half-precision floating-point operations
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_sp_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of achieved to peak single-precision floating-point operations
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gld_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of requested global memory load throughput to required global memory load throughput expressed as percentage.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gld_requested_throughput
    </p>
   </td>
   <td>
    <p>
     Requested global memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gld_throughput
    </p>
   </td>
   <td>
    <p>
     Global memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gld_transactions
    </p>
   </td>
   <td>
    <p>
     Number of global memory load transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gld_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of global memory load transactions performed for each global memory load.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     global_atomic_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global atomic(Atom and Atom CAS) requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     global_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate for global loads in unified l1/tex cache. Metric value maybe wrong if malloc is used in kernel.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     global_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     global_reduction_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global reduction requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     global_store_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global store requests from Multiprocessor. This does not include atomic requests.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gst_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of requested global memory store throughput to required global memory store throughput expressed as percentage.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gst_requested_throughput
    </p>
   </td>
   <td>
    <p>
     Requested global memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gst_throughput
    </p>
   </td>
   <td>
    <p>
     Global memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gst_transactions
    </p>
   </td>
   <td>
    <p>
     Number of global memory store transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gst_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of global memory store transactions performed for each global memory store
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     half_precision_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute 16 bit floating-point instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_bit_convert
    </p>
   </td>
   <td>
    <p>
     Number of bit-conversion instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_compute_ld_st
    </p>
   </td>
   <td>
    <p>
     Number of compute load/store instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_control
    </p>
   </td>
   <td>
    <p>
     Number of control-flow instructions executed by non-predicated threads (jump, branch, etc.)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed
    </p>
   </td>
   <td>
    <p>
     The number of instructions executed
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_global_atomics
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global atom and atom cas
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_global_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_global_reductions
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global reductions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_global_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_local_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for local loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_local_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for local stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_shared_atomics
    </p>
   </td>
   <td>
    <p>
     Warp level shared instructions for atom and atom CAS
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_shared_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for shared loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_shared_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for shared stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_surface_atomics
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface atom and atom cas
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_surface_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_surface_reductions
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface reductions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_surface_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_tex_ops
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for texture
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_fp_16
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_fp_32
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_fp_64
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_integer
    </p>
   </td>
   <td>
    <p>
     Number of integer instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_inter_thread_communication
    </p>
   </td>
   <td>
    <p>
     Number of inter-thread communication instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_issued
    </p>
   </td>
   <td>
    <p>
     The number of instructions issued
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_misc
    </p>
   </td>
   <td>
    <p>
     Number of miscellaneous instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_per_warp
    </p>
   </td>
   <td>
    <p>
     Average number of instructions executed by each warp
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_replay_overhead
    </p>
   </td>
   <td>
    <p>
     Average number of replays for each instruction executed
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     ipc
    </p>
   </td>
   <td>
    <p>
     Instructions executed per cycle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     issue_slot_utilization
    </p>
   </td>
   <td>
    <p>
     Percentage of issue slots that issued at least one instruction, averaged across all cycles
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     issue_slots
    </p>
   </td>
   <td>
    <p>
     The number of issue slots used
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     issued_ipc
    </p>
   </td>
   <td>
    <p>
     Instructions issued per cycle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_atomic_throughput
    </p>
   </td>
   <td>
    <p>
     Memory read throughput seen at L2 cache for atomic and reduction requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_atomic_transactions
    </p>
   </td>
   <td>
    <p>
     Memory read transactions seen at L2 cache for atomic and reduction requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_global_atomic_store_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from Unified cache for global atomics (ATOM and ATOM CAS)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_global_load_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes read from L2 for misses in Unified Cache for global loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_global_reduction_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from Unified cache for global reductions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_local_global_store_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from Unified Cache for local and global stores. This does not include global atomics.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_local_load_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes read from L2 for misses in Unified Cache for local loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_read_throughput
    </p>
   </td>
   <td>
    <p>
     Memory read throughput seen at L2 cache for all read requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_read_transactions
    </p>
   </td>
   <td>
    <p>
     Memory read transactions seen at L2 cache for all read requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_surface_atomic_store_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes transferred between Unified Cache and L2 for surface atomics (ATOM and ATOM CAS)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_surface_load_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes read from L2 for misses in Unified Cache for surface loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_surface_reduction_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from Unified Cache for surface reductions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_surface_store_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from Unified Cache for surface stores. This does not include surface atomics.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_tex_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate at L2 cache for all requests from texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_tex_read_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate at L2 cache for all read requests from texture cache. This is available for compute capability 6.0 and 6.1.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_tex_read_throughput
    </p>
   </td>
   <td>
    <p>
     Memory read throughput seen at L2 cache for read requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_tex_read_transactions
    </p>
   </td>
   <td>
    <p>
     Memory read transactions seen at L2 cache for read requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_tex_write_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit Rate at L2 cache for all write requests from texture cache. This is available for compute capability 6.0 and 6.1.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_tex_write_throughput
    </p>
   </td>
   <td>
    <p>
     Memory write throughput seen at L2 cache for write requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_tex_write_transactions
    </p>
   </td>
   <td>
    <p>
     Memory write transactions seen at L2 cache for write requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the L2 cache relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_write_throughput
    </p>
   </td>
   <td>
    <p>
     Memory write throughput seen at L2 cache for all write requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_write_transactions
    </p>
   </td>
   <td>
    <p>
     Memory write transactions seen at L2 cache for all write requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     ldst_executed
    </p>
   </td>
   <td>
    <p>
     Number of executed local, global, shared and texture memory load and store instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     ldst_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute shared load, shared store and constant load instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     ldst_issued
    </p>
   </td>
   <td>
    <p>
     Number of issued local, global, shared and texture memory load and store instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate for local loads and stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of local load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_load_throughput
    </p>
   </td>
   <td>
    <p>
     Local memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_load_transactions
    </p>
   </td>
   <td>
    <p>
     Number of local memory load transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_load_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of local memory load transactions performed for each local memory load
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_memory_overhead
    </p>
   </td>
   <td>
    <p>
     Ratio of local memory traffic to total memory traffic between the L1 and L2 caches expressed as percentage
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_store_requests
    </p>
   </td>
   <td>
    <p>
     Total number of local store requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_store_throughput
    </p>
   </td>
   <td>
    <p>
     Local memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_store_transactions
    </p>
   </td>
   <td>
    <p>
     Number of local memory store transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_store_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of local memory store transactions performed for each local memory store
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_overhead_data_received
    </p>
   </td>
   <td>
    <p>
     Ratio of overhead data to the total data, received through NVLink. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_overhead_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Ratio of overhead data to the total data, transmitted through NVLink. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_receive_throughput
    </p>
   </td>
   <td>
    <p>
     Number of bytes received per second through NVLinks. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_total_data_received
    </p>
   </td>
   <td>
    <p>
     Total data bytes received through NVLinks including headers. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_total_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total data bytes transmitted through NVLinks including headers. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_total_nratom_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total non-reduction atomic data bytes transmitted through NVLinks. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_total_ratom_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total reduction atomic data bytes transmitted through NVLinks This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_total_response_data_received
    </p>
   </td>
   <td>
    <p>
     Total response data bytes received through NVLink, response data includes data for read requests and result of non-reduction atomic requests. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_total_write_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total write data bytes transmitted through NVLinks. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_transmit_throughput
    </p>
   </td>
   <td>
    <p>
     Number of Bytes Transmitted per second through NVLinks. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_user_data_received
    </p>
   </td>
   <td>
    <p>
     User data bytes received through NVLinks, doesnât include headers. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_user_data_transmitted
    </p>
   </td>
   <td>
    <p>
     User data bytes transmitted through NVLinks, doesnât include headers. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_user_nratom_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total non-reduction atomic user data bytes transmitted through NVLinks. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_user_ratom_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total reduction atomic user data bytes transmitted through NVLinks. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_user_response_data_received
    </p>
   </td>
   <td>
    <p>
     Total user response data bytes received through NVLink, response data includes data for read requests and result of non-reduction atomic requests. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_user_write_data_transmitted
    </p>
   </td>
   <td>
    <p>
     User write data bytes transmitted through NVLinks. This is available for compute capability 6.0.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     pcie_total_data_received
    </p>
   </td>
   <td>
    <p>
     Total data bytes received through PCIe
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     pcie_total_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total data bytes transmitted through PCIe
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of requested shared memory throughput to required shared memory throughput expressed as percentage
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_load_throughput
    </p>
   </td>
   <td>
    <p>
     Shared memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_load_transactions
    </p>
   </td>
   <td>
    <p>
     Number of shared memory load transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_load_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of shared memory load transactions performed for each shared memory load
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_store_throughput
    </p>
   </td>
   <td>
    <p>
     Shared memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_store_transactions
    </p>
   </td>
   <td>
    <p>
     Number of shared memory store transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_store_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of shared memory store transactions performed for each shared memory store
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the shared memory relative to peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     single_precision_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute single-precision floating-point instructions and integer instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sm_efficiency
    </p>
   </td>
   <td>
    <p>
     The percentage of time at least one warp is active on a specific multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     special_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute sin, cos, ex2, popc, flo, and similar instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_constant_memory_dependency
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because of immediate constant cache miss
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_exec_dependency
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because an input required by the instruction is not yet available
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_inst_fetch
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because the next assembly instruction has not yet been fetched
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_memory_dependency
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because a memory operation cannot be performed due to the required resources not being available or fully utilized, or because too many requests of a given type are outstanding
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_memory_throttle
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because of memory throttle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_not_selected
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because warp was not selected
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_other
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring due to miscellaneous reasons
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_pipe_busy
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because a compute operation cannot be performed because the compute pipeline is busy
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_sync
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because the warp is blocked at a __syncthreads() call
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_texture
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because the texture sub-system is fully utilized or has too many outstanding requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     surface_atomic_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface atomic(Atom and Atom CAS) requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     surface_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     surface_reduction_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface reduction requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     surface_store_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface store requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_read_bytes
    </p>
   </td>
   <td>
    <p>
     Number of bytes read from system memory
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_read_throughput
    </p>
   </td>
   <td>
    <p>
     System memory read throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_read_transactions
    </p>
   </td>
   <td>
    <p>
     Number of system memory read transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_read_utilization
    </p>
   </td>
   <td>
    <p>
     The read utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 6.0 and 6.1.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 6.0 and 6.1.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_write_bytes
    </p>
   </td>
   <td>
    <p>
     Number of bytes written to system memory
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_write_throughput
    </p>
   </td>
   <td>
    <p>
     System memory write throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_write_transactions
    </p>
   </td>
   <td>
    <p>
     Number of system memory write transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_write_utilization
    </p>
   </td>
   <td>
    <p>
     The write utilization level of the system memory relative to the peak utilization on a scale of 0 to 10. This is available for compute capability 6.0 and 6.1.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     tex_cache_hit_rate
    </p>
   </td>
   <td>
    <p>
     Unified cache hit rate
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     tex_cache_throughput
    </p>
   </td>
   <td>
    <p>
     Unified cache throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     tex_cache_transactions
    </p>
   </td>
   <td>
    <p>
     Unified cache read transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     tex_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute global, local and texture memory instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     tex_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the unified cache relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     texture_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of texture Load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     unique_warps_launched
    </p>
   </td>
   <td>
    <p>
     Number of warps launched. Value is unaffected by compute preemption.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     warp_execution_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of the average active threads per warp to the maximum number of threads per warp supported on a multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     warp_nonpred_execution_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of the average active threads per warp executing non-predicated instructions to the maximum number of threads per warp supported on a multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
 </table>
 <h2>
  <span class="section-number">
   9.3.
  </span>
  Metrics for Capability 7.x
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-for-capability-7-x" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Devices with compute capability 7.x implement the metrics shown in the following table. (7.x refers to 7.0 and 7.2 here.)
 </p>
 <table class="table-no-stripes docutils align-default" id="id4">
  <span class="caption-text">
   Table 6. Capability 7.x (7.0 and 7.2) Metrics
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#id4" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Metric Name
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
   <th class="head">
    <p>
     Scope
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     achieved_occupancy
    </p>
   </td>
   <td>
    <p>
     Ratio of the average active warps per active cycle to the maximum number of warps supported on a multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     atomic_transactions
    </p>
   </td>
   <td>
    <p>
     Global memory atomic and reduction transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     atomic_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of global memory atomic and reduction transactions performed for each atomic and reduction instruction
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     branch_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of branch instruction to sum of branch and divergent branch instruction
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cf_executed
    </p>
   </td>
   <td>
    <p>
     Number of executed control-flow instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     cf_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute control-flow instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     cf_issued
    </p>
   </td>
   <td>
    <p>
     Number of issued control-flow instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     double_precision_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute double-precision floating-point instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_read_bytes
    </p>
   </td>
   <td>
    <p>
     Total bytes read from DRAM to L2 cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     dram_read_throughput
    </p>
   </td>
   <td>
    <p>
     Device memory read throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_read_transactions
    </p>
   </td>
   <td>
    <p>
     Device memory read transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     dram_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the device memory relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_write_bytes
    </p>
   </td>
   <td>
    <p>
     Total bytes written from L2 cache to DRAM
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     dram_write_throughput
    </p>
   </td>
   <td>
    <p>
     Device memory write throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     dram_write_transactions
    </p>
   </td>
   <td>
    <p>
     Device memory write transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     eligible_warps_per_cycle
    </p>
   </td>
   <td>
    <p>
     Average number of warps that are eligible to issue per active cycle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_dp
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_dp_add
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point add operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_dp_fma
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_dp_mul
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point multiply operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_hp
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate contributes 2 or 4 to the count based on the number of inputs.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_hp_add
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point add operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_hp_fma
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate contributes 2 or 4 to the count based on the number of inputs.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_hp_mul
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point multiply operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_sp
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point operations executed by non-predicated threads (add, multiply, and multiply-accumulate). Each multiply-accumulate operation contributes 2 to the count. The count does not include special operations.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_sp_add
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point add operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_sp_fma
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point multiply-accumulate operations executed by non-predicated threads. Each multiply-accumulate operation contributes 1 to the count.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_count_sp_mul
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point multiply operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_count_sp_special
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point special operations executed by non-predicated threads.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_dp_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of achieved to peak double-precision floating-point operations
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     flop_hp_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of achieved to peak half-precision floating-point operations
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     flop_sp_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of achieved to peak single-precision floating-point operations
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gld_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of requested global memory load throughput to required global memory load throughput expressed as percentage.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gld_requested_throughput
    </p>
   </td>
   <td>
    <p>
     Requested global memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gld_throughput
    </p>
   </td>
   <td>
    <p>
     Global memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gld_transactions
    </p>
   </td>
   <td>
    <p>
     Number of global memory load transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gld_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of global memory load transactions performed for each global memory load.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     global_atomic_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global atomic(Atom and Atom CAS) requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     global_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate for global load and store in unified l1/tex cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     global_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     global_reduction_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global reduction requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     global_store_requests
    </p>
   </td>
   <td>
    <p>
     Total number of global store requests from Multiprocessor. This does not include atomic requests.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gst_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of requested global memory store throughput to required global memory store throughput expressed as percentage.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gst_requested_throughput
    </p>
   </td>
   <td>
    <p>
     Requested global memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gst_throughput
    </p>
   </td>
   <td>
    <p>
     Global memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     gst_transactions
    </p>
   </td>
   <td>
    <p>
     Number of global memory store transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     gst_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of global memory store transactions performed for each global memory store
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     half_precision_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute 16 bit floating-point instructions on a scale of 0 to 10. Note that this doesnât specify the utilization level of tensor core unit
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_bit_convert
    </p>
   </td>
   <td>
    <p>
     Number of bit-conversion instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_compute_ld_st
    </p>
   </td>
   <td>
    <p>
     Number of compute load/store instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_control
    </p>
   </td>
   <td>
    <p>
     Number of control-flow instructions executed by non-predicated threads (jump, branch, etc.)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed
    </p>
   </td>
   <td>
    <p>
     The number of instructions executed
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_global_atomics
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global atom and atom cas
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_global_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_global_reductions
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global reductions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_global_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for global stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_local_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for local loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_local_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for local stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_shared_atomics
    </p>
   </td>
   <td>
    <p>
     Warp level shared instructions for atom and atom CAS
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_shared_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for shared loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_shared_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for shared stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_surface_atomics
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface atom and atom cas
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_surface_loads
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_surface_reductions
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface reductions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_executed_surface_stores
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for surface stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_executed_tex_ops
    </p>
   </td>
   <td>
    <p>
     Warp level instructions for texture
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_fp_16
    </p>
   </td>
   <td>
    <p>
     Number of half-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_fp_32
    </p>
   </td>
   <td>
    <p>
     Number of single-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_fp_64
    </p>
   </td>
   <td>
    <p>
     Number of double-precision floating-point instructions executed by non-predicated threads (arithmetic, compare, etc.)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_integer
    </p>
   </td>
   <td>
    <p>
     Number of integer instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_inter_thread_communication
    </p>
   </td>
   <td>
    <p>
     Number of inter-thread communication instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_issued
    </p>
   </td>
   <td>
    <p>
     The number of instructions issued
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_misc
    </p>
   </td>
   <td>
    <p>
     Number of miscellaneous instructions executed by non-predicated threads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     inst_per_warp
    </p>
   </td>
   <td>
    <p>
     Average number of instructions executed by each warp
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     inst_replay_overhead
    </p>
   </td>
   <td>
    <p>
     Average number of replays for each instruction executed
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     ipc
    </p>
   </td>
   <td>
    <p>
     Instructions executed per cycle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     issue_slot_utilization
    </p>
   </td>
   <td>
    <p>
     Percentage of issue slots that issued at least one instruction, averaged across all cycles
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     issue_slots
    </p>
   </td>
   <td>
    <p>
     The number of issue slots used
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     issued_ipc
    </p>
   </td>
   <td>
    <p>
     Instructions issued per cycle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_atomic_throughput
    </p>
   </td>
   <td>
    <p>
     Memory read throughput seen at L2 cache for atomic and reduction requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_atomic_transactions
    </p>
   </td>
   <td>
    <p>
     Memory read transactions seen at L2 cache for atomic and reduction requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_global_atomic_store_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from L1 for global atomics (ATOM and ATOM CAS)
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_global_load_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes read from L2 for misses in L1 for global loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_local_global_store_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes written to L2 from L1 for local and global stores. This does not include global atomics.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_local_load_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes read from L2 for misses in L1 for local loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_read_throughput
    </p>
   </td>
   <td>
    <p>
     Memory read throughput seen at L2 cache for all read requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_read_transactions
    </p>
   </td>
   <td>
    <p>
     Memory read transactions seen at L2 cache for all read requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_surface_load_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes read from L2 for misses in L1 for surface loads
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_surface_store_bytes
    </p>
   </td>
   <td>
    <p>
     Bytes read from L2 for misses in L1 for surface stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_tex_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate at L2 cache for all requests from texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_tex_read_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate at L2 cache for all read requests from texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_tex_read_throughput
    </p>
   </td>
   <td>
    <p>
     Memory read throughput seen at L2 cache for read requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_tex_read_transactions
    </p>
   </td>
   <td>
    <p>
     Memory read transactions seen at L2 cache for read requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_tex_write_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit Rate at L2 cache for all write requests from texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_tex_write_throughput
    </p>
   </td>
   <td>
    <p>
     Memory write throughput seen at L2 cache for write requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_tex_write_transactions
    </p>
   </td>
   <td>
    <p>
     Memory write transactions seen at L2 cache for write requests from the texture cache
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the L2 cache relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     l2_write_throughput
    </p>
   </td>
   <td>
    <p>
     Memory write throughput seen at L2 cache for all write requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     l2_write_transactions
    </p>
   </td>
   <td>
    <p>
     Memory write transactions seen at L2 cache for all write requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     ldst_executed
    </p>
   </td>
   <td>
    <p>
     Number of executed local, global, shared and texture memory load and store instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     ldst_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute shared load, shared store and constant load instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     ldst_issued
    </p>
   </td>
   <td>
    <p>
     Number of issued local, global, shared and texture memory load and store instructions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_hit_rate
    </p>
   </td>
   <td>
    <p>
     Hit rate for local loads and stores
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of local load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_load_throughput
    </p>
   </td>
   <td>
    <p>
     Local memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_load_transactions
    </p>
   </td>
   <td>
    <p>
     Number of local memory load transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_load_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of local memory load transactions performed for each local memory load
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_memory_overhead
    </p>
   </td>
   <td>
    <p>
     Ratio of local memory traffic to total memory traffic between the L1 and L2 caches expressed as percentage
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_store_requests
    </p>
   </td>
   <td>
    <p>
     Total number of local store requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_store_throughput
    </p>
   </td>
   <td>
    <p>
     Local memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     local_store_transactions
    </p>
   </td>
   <td>
    <p>
     Number of local memory store transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     local_store_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of local memory store transactions performed for each local memory store
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_overhead_data_received
    </p>
   </td>
   <td>
    <p>
     Ratio of overhead data to the total data, received through NVLink.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_overhead_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Ratio of overhead data to the total data, transmitted through NVLink.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_receive_throughput
    </p>
   </td>
   <td>
    <p>
     Number of bytes received per second through NVLinks.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_total_data_received
    </p>
   </td>
   <td>
    <p>
     Total data bytes received through NVLinks including headers.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_total_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total data bytes transmitted through NVLinks including headers.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_total_nratom_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total non-reduction atomic data bytes transmitted through NVLinks.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_total_ratom_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total reduction atomic data bytes transmitted through NVLinks.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_total_response_data_received
    </p>
   </td>
   <td>
    <p>
     Total response data bytes received through NVLink, response data includes data for read requests and result of non-reduction atomic requests.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_total_write_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total write data bytes transmitted through NVLinks.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_transmit_throughput
    </p>
   </td>
   <td>
    <p>
     Number of Bytes Transmitted per second through NVLinks.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_user_data_received
    </p>
   </td>
   <td>
    <p>
     User data bytes received through NVLinks, doesnât include headers.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_user_data_transmitted
    </p>
   </td>
   <td>
    <p>
     User data bytes transmitted through NVLinks, doesnât include headers.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_user_nratom_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total non-reduction atomic user data bytes transmitted through NVLinks.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_user_ratom_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total reduction atomic user data bytes transmitted through NVLinks.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     nvlink_user_response_data_received
    </p>
   </td>
   <td>
    <p>
     Total user response data bytes received through NVLink, response data includes data for read requests and result of non-reduction atomic requests.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     nvlink_user_write_data_transmitted
    </p>
   </td>
   <td>
    <p>
     User write data bytes transmitted through NVLinks.
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     pcie_total_data_received
    </p>
   </td>
   <td>
    <p>
     Total data bytes received through PCIe
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     pcie_total_data_transmitted
    </p>
   </td>
   <td>
    <p>
     Total data bytes transmitted through PCIe
    </p>
   </td>
   <td>
    <p>
     Device
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of requested shared memory throughput to required shared memory throughput expressed as percentage
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_load_throughput
    </p>
   </td>
   <td>
    <p>
     Shared memory load throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_load_transactions
    </p>
   </td>
   <td>
    <p>
     Number of shared memory load transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_load_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of shared memory load transactions performed for each shared memory load
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_store_throughput
    </p>
   </td>
   <td>
    <p>
     Shared memory store throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_store_transactions
    </p>
   </td>
   <td>
    <p>
     Number of shared memory store transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     shared_store_transactions_per_request
    </p>
   </td>
   <td>
    <p>
     Average number of shared memory store transactions performed for each shared memory store
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     shared_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the shared memory relative to peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     single_precision_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute single-precision floating-point instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sm_efficiency
    </p>
   </td>
   <td>
    <p>
     The percentage of time at least one warp is active on a specific multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     special_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute sin, cos, ex2, popc, flo, and similar instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_constant_memory_dependency
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because of immediate constant cache miss
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_exec_dependency
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because an input required by the instruction is not yet available
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_inst_fetch
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because the next assembly instruction has not yet been fetched
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_memory_dependency
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because a memory operation cannot be performed due to the required resources not being available or fully utilized, or because too many requests of a given type are outstanding
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_memory_throttle
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because of memory throttle
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_not_selected
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because warp was not selected
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_other
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring due to miscellaneous reasons
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_pipe_busy
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because a compute operation cannot be performed because the compute pipeline is busy
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_sleeping
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because warp was sleeping
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     stall_sync
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because the warp is blocked at a __syncthreads() call
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     stall_texture
    </p>
   </td>
   <td>
    <p>
     Percentage of stalls occurring because the texture sub-system is fully utilized or has too many outstanding requests
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     surface_atomic_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface atomic(Atom and Atom CAS) requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     surface_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     surface_reduction_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface reduction requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     surface_store_requests
    </p>
   </td>
   <td>
    <p>
     Total number of surface store requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_read_bytes
    </p>
   </td>
   <td>
    <p>
     Number of bytes read from system memory
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_read_throughput
    </p>
   </td>
   <td>
    <p>
     System memory read throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_read_transactions
    </p>
   </td>
   <td>
    <p>
     Number of system memory read transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_read_utilization
    </p>
   </td>
   <td>
    <p>
     The read utilization level of the system memory relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the system memory relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_write_bytes
    </p>
   </td>
   <td>
    <p>
     Number of bytes written to system memory
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_write_throughput
    </p>
   </td>
   <td>
    <p>
     System memory write throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     sysmem_write_transactions
    </p>
   </td>
   <td>
    <p>
     Number of system memory write transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     sysmem_write_utilization
    </p>
   </td>
   <td>
    <p>
     The write utilization level of the system memory relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     tensor_precision_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute tensor core instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     tensor_int_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute tensor core int8 instructions on a scale of 0 to 10. This metric is only available for device with compute capability 7.2.
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     tex_cache_hit_rate
    </p>
   </td>
   <td>
    <p>
     Unified cache hit rate
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     tex_cache_throughput
    </p>
   </td>
   <td>
    <p>
     Unified cache to Multiprocessor read throughput
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     tex_cache_transactions
    </p>
   </td>
   <td>
    <p>
     Unified cache to Multiprocessor read transactions
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     tex_fu_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the multiprocessor function units that execute global, local and texture memory instructions on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     tex_utilization
    </p>
   </td>
   <td>
    <p>
     The utilization level of the unified cache relative to the peak utilization on a scale of 0 to 10
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     texture_load_requests
    </p>
   </td>
   <td>
    <p>
     Total number of texture Load requests from Multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     warp_execution_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of the average active threads per warp to the maximum number of threads per warp supported on a multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     warp_nonpred_execution_efficiency
    </p>
   </td>
   <td>
    <p>
     Ratio of the average active threads per warp executing non-predicated instructions to the maximum number of threads per warp supported on a multiprocessor
    </p>
   </td>
   <td>
    <p>
     Multi-context
    </p>
   </td>
  </tr>
 </table>
 <h1>
  <span class="section-number">
   10.
  </span>
  Warp State
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#warp-state" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  This section contains a description of each warp state. The warp can have following states:
 </p>
 <ul>
  <li>
   <p>
    Instruction issued
    - An instruction or a pair of independent instructions was issued from a warp.
   </p>
  </li>
  <li>
   <p>
    Stalled
    - Warp can be stalled for one of the following reasons. The stall reason distribution can be seen at source level in
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#pc-sampling-view">
     PC Sampling View
    </a>
    or at kernel level in Latency analysis using âExamine Stall Reasonsâ
   </p>
   <ul>
    <li>
     <p>
      Stalled for instruction fetch
      - The next instruction was not yet available.
     </p>
     <p>
      To reduce instruction fetch stalls:
     </p>
     <ul class="simple">
      <li>
       <p>
        If large loop have been unrolled in kernel, try reducing them.
       </p>
      </li>
      <li>
       <p>
        If the kernel contains many calls to small function, try inlining more of them with the __inline__ or __forceinline__ qualifiers. Conversely, if inlining many functions or large functions, try __noinline__ to disable inlining of those functions.
       </p>
      </li>
      <li>
       <p>
        For very short kernels, consider fusing into a single kernels.
       </p>
      </li>
      <li>
       <p>
        If blocks with fewer threads are used, consider using fewer blocks of more threads. Occasional calls to __syncthreads() will then keep the warps in sync which may improve instruction cache hit rate.
       </p>
      </li>
     </ul>
    </li>
   </ul>
   <ul>
    <li>
     <p>
      Stalled for execution dependency
      - The next instruction is waiting for one or more of its inputs to be computed by earlier instruction(s).
     </p>
     <p>
      To reduce execution dependency stalls, try to increase instruction-level parallelism (ILP). This can be done by, for example, increasing loop unrolling or processing several elements per thread. This prevents the thread from idling through the full latency of each instruction.
     </p>
    </li>
    <li>
     <p>
      Stalled for memory dependency
      - The next instruction is waiting for a previous memory accesses to complete.
     </p>
     <p>
      To reduce the memory dependency stalls
     </p>
     <ul class="simple">
      <li>
       <p>
        Try to improve memory coalescing and/or efficiency of bytes fetched (alignment, etc.). Look at the source level analysis âGlobal Memory Access Patternâ and/or the metrics gld_efficiency and gst_efficiency.
       </p>
      </li>
      <li>
       <p>
        Try to increase memory-level parallelism (MLP): the number of independent memory operations in flight per thread. Loop unrolling, loading vector types such as float4, and processing multiple elements per thread are all ways to increase memory-level parallelism.
       </p>
      </li>
      <li>
       <p>
        Consider moving frequently-accessed data closer to SM, such as by use of shared memory or read-only data cache.
       </p>
      </li>
      <li>
       <p>
        Consider re-computing data where possible instead of loading it from device memory.
       </p>
      </li>
      <li>
       <p>
        If local memory accesses are high, consider increasing register count per thread to reduce spilling, even at the expense of occupancy since local memory accesses are cached only in L2 for GPUs with compute capability major = 5.
       </p>
      </li>
     </ul>
    </li>
    <li>
     <p>
      Stalled for memory throttle
      - A large number of outstanding memory requests prevents forward progress. On GPUs with compute capability major = 3, memory throttle indicates high number of memory replays.
     </p>
     <p>
      To reduce memory throttle stalls:
     </p>
     <ul class="simple">
      <li>
       <p>
        Try to find ways to combine several memory transactions into one (e.g., use 64-bit memory requests instead of two 32-bit requests).
       </p>
      </li>
      <li>
       <p>
        Check for un-coalesced memory accesses using the source level analysis âGlobal Memory Access Patternâ and/or the profiler metrics gld_efficiency and gst_efficiency; minimize them wherever possible.
       </p>
      </li>
      <li>
       <p>
        On GPUs with compute capability major &gt;= 3, consider using read-only data cache using LDG for un-coalesced global reads
       </p>
      </li>
     </ul>
    </li>
    <li>
     <p>
      Stalled for texture
      - The texture sub-system is fully utilized or has too many outstanding requests.
     </p>
     <p>
      To reduce texture stalls:
     </p>
     <ul class="simple">
      <li>
       <p>
        Consider combining several texture fetch operations into one (e.g., packing data in texture and unpacking in SM or using vector loads).
       </p>
      </li>
      <li>
       <p>
        Consider moving frequently-accessed data closer to SM by use of shared memory.
       </p>
      </li>
      <li>
       <p>
        Consider re-computing data where possible instead of fetching it from memory.
       </p>
      </li>
      <li>
       <p>
        On GPUs with compute capability major &lt; 5: Consider changing some texture accesses into regular global loads to reduce pressure on the texture unit, especially if you do not use texture-specific features such as interpolation.
       </p>
      </li>
      <li>
       <p>
        On GPUs with compute capability major = 3: If global loads through the read-only data cache (LDG) are the source of texture accesses for this kernel, consider changing some of them back to regular global loads. Note that if LDG is being generated due to use of the __ldg() intrinsic, this simply means changing back to a normal pointer dereference, but if LDG is being generated automatically by the compiler due to the use of the const and __restrict__ qualifiers, this may be more difficult.
       </p>
      </li>
     </ul>
    </li>
    <li>
     <p>
      Stalled for sync
      - The warp is waiting for all threads to synchronize after a barrier instruction.
     </p>
     <p>
      To reduce sync stalls:
     </p>
     <ul class="simple">
      <li>
       <p>
        Try to improve load balancing i.e. try to increase work done between synchronization points; consider reducing thread block size.
       </p>
      </li>
      <li>
       <p>
        Minimize use of threadfence_*().
       </p>
      </li>
      <li>
       <p>
        On GPUs with compute capability major &gt;= 3: If __syncthreads() is being used because of data exchange through shared memory within a threadblock, consider whether warp shuffle operations can be used in place of some of these exchange/synchronize sequences.
       </p>
      </li>
     </ul>
    </li>
    <li>
     <p>
      Stalled for constant memory dependency
      - The warp is stalled on a miss in the cache for __constant__ memory and immediate.
     </p>
     <p>
      This may be high the first time each constant is accessed (e.g., at the beginning of a kernel). To reduce these stalls,
     </p>
     <ul class="simple">
      <li>
       <p>
        Consider reducing use of __constant__ or increase kernel runtime by increasing block count
       </p>
      </li>
      <li>
       <p>
        Consider increasing number of items processed per thread
       </p>
      </li>
      <li>
       <p>
        Consider merging several kernels that use the same __constant__ data to amortize the cost of misses in the constant cache.
       </p>
      </li>
      <li>
       <p>
        Try using regular global memory accesses instead of constant memory accesses.
       </p>
      </li>
     </ul>
    </li>
    <li>
     <p>
      Stalled for pipe busy - The warp is stalled because the functional unit required to execute the next instruction is busy.
     </p>
     <p>
      To reduce stalls due to pipe busy:
     </p>
     <ul class="simple">
      <li>
       <p>
        Prefer high-throughput operations over low-throughput operations. If precision doesnât matter, use float instead of double precision arithmetic.
       </p>
      </li>
      <li>
       <p>
        Look for arithmetic improvements (e.g., order-of-operations changes) that may be mathematically valid but unsafe for the compiler to do automatically. Due to e.g. floating-point non-associativity.
       </p>
      </li>
     </ul>
    </li>
    <li>
     <p>
      Stalled for not selected
      - Warp was ready but did not get a chance to issue as some other warp was selected for issue. This reason generally indicates that kernel is possibly optimized well but in some cases, you may be able to decrease occupancy without impacting latency hiding, and doing so may help improve cache hit rates.
     </p>
    </li>
    <li>
     <p>
      Stalled for other
      - Warp is blocked for an uncommon reason like compiler or hardware reasons. Developers do not have control over these stalls.
     </p>
    </li>
   </ul>
  </li>
 </ul>
 <h1>
  <span class="section-number">
   11.
  </span>
  Migrating to Nsight Tools from Visual Profiler and nvprof
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#migrating-to-nsight-tools-from-visual-profiler-and-nvprof" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  Visual Profiler and nvprof will be deprecated in a future CUDA release. It is recommended to use next-generation tools
  <a class="reference external" href="https://developer.nvidia.com/nsight-systems">
   NVIDIA Nsight Systems
  </a>
  for GPU and CPU sampling and tracing and
  <a class="reference external" href="https://developer.nvidia.com/nsight-compute">
   NVIDIA Nsight Compute
  </a>
  for GPU kernel profiling. The new tools still offer the same profiling / optimization / deployment workflow. The type of data you need to look at is the same. The commands have changed and the output looks a little different. The new tools are powerful, fast, and feature rich, allowing you to find solutions even more quickly.
 </p>
 <p>
  <a class="reference external" href="https://developer.nvidia.com/nsight-systems">
   NVIDIA Nsight Systems
  </a>
  is a system-wide performance analysis tool designed to visualize an applicationâs algorithms, help you identify the largest opportunities to optimize, and tune to scale efficiently across any quantity or size of CPUs and GPUs; from large servers to our smallest SoC. Refer to the
  <a class="reference external" href="https://docs.nvidia.com/nsight-systems/UserGuide/index.html#migrate-nvprof">
   Migrating from NVIDIA nvprof section
  </a>
  section in the NVIDIA Nsight Systems User Guide
 </p>
 <p>
  <a class="reference external" href="https://developer.nvidia.com/nsight-compute">
   NVIDIA Nsight Compute
  </a>
  is an interactive kernel profiler for CUDA applications. It provides detailed performance metrics and API debugging via a user interface and command line tool. In addition, its baseline feature allows users to compare results within the tool. Nsight Compute provides a customizable and data-driven user interface and metric collection and can be extended with analysis scripts for post-processing results. Refer to the
  <a class="reference external" href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#nvprof-guide">
   nvprof Transition Guide
  </a>
  section in the Nsight Compute CLI document. Refer to the
  <a class="reference external" href="https://docs.nvidia.com/nsight-compute/NsightCompute/index.html#nvvp-guide">
   Visual Profiler Transition Guide
  </a>
  section in the Nsight Compute document.
 </p>
 <p>
  Also refer to the blog posts on how to move your development to the next-generation tools:
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    <a class="reference external" href="https://devblogs.nvidia.com/migrating-nvidia-nsight-tools-nvvp-nvprof">
     Migrating to Nsight Tools from Visual Profiler and nvprof
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="reference external" href="https://devblogs.nvidia.com/transitioning-nsight-systems-nvidia-visual-profiler-nvprof">
     Transitioning to Nsight Systems from Visual Profiler and nvprof
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="reference external" href="https://devblogs.nvidia.com/using-nsight-compute-to-inspect-your-kernels">
     Using Nsight Compute to Inspect your Kernels
    </a>
   </p>
  </li>
 </ol>
 <table class="table-no-stripes docutils align-default" id="id5">
  <span class="caption-text">
   Table 7. Which tools are available on which GPU architectures
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#id5" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     GPU architecture
    </p>
   </th>
   <th class="head">
    <p>
     Visual Profiler and nvprof
    </p>
   </th>
   <th class="head">
    <p>
     Nsight Systems
    </p>
   </th>
   <th class="head">
    <p>
     Nsight Compute
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Maxwell
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
   <td>
    <p>
     No
    </p>
   </td>
   <td>
    <p>
     No
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Pascal
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
   <td>
    <p>
     No
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Volta
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Turing
    </p>
   </td>
   <td>
    <p>
     Yes*
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Ampere and later GPU architectures
    </p>
   </td>
   <td>
    <p>
     No
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
 </table>
 <p>
  * Only Tracing functionality is supported - Timeline, Activity, API. CUDA kernel profiling functionality i.e. collecting GPU performance metrics is not supported.
 </p>
 The following table maps the key features of Visual Profiler and nvprof to the NVIDIA Nsight tools
 <table class="table-no-stripes docutils align-default" id="id6">
  <span class="caption-text">
   Table 8. Mapping of key Visual Profiler and nvprof features
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#id6" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Visual Profiler/nvprof feature categories
    </p>
   </th>
   <th class="head">
    <p>
     Nsight Systems
    </p>
   </th>
   <th class="head">
    <p>
     Nsight Compute
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Timeline/Activity/API Tracing
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     CPU Sampling
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     OpenACC
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     OpenMP
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     MPI
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     MPS
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Application Dependency Analysis
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Unified Memory Transfers
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Unified Memory Page Faults
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Application Unified Memory Analysis
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Application NVLink Analysis
    </p>
   </td>
   <td>
    <p>
     Yes (per kernel)
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Events and Metrics (per kernel)
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Guided and Unguided Kernel Analysis
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Kernel Source-Disassembly View
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Kernel PC Sampling
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     NVTX
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Remote Profiling
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
  </tr>
 </table>
 <h1>
  <span class="section-number">
   12.
  </span>
  Profiler Known Issues
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#profiler-known-issues" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  The following are known issues with the current release.
 </p>
 <ul>
  <li>
   <p>
    Visual Profiler and nvprof donât support devices with compute capability 8.0 and higher. Next-gen tools NVIDIA Nsight Compute and NVIDIA Nsight Systems should be used instead.
   </p>
  </li>
  <li>
   <p>
    Starting with the CUDA 11.0, Visual Profiler and nvprof wonât support macOS as the target platform. However Visual Profiler supports remote profiling from the macOS host. This support is deprecated in the CUDA 12.5 release. Visual Profiler is provided in a separate installer package to maintain the remote profiling workflow for CUDA developers on macOS. See
    <a class="reference external" href="https://developer.nvidia.com/nvidia-cuda-toolkit-developer-tools-mac-hosts">
     Developer Tools for macOS
    </a>
    for download instructions.
   </p>
  </li>
  <li>
   <p>
    Starting with CUDA 10.2, Visual Profiler and nvprof use dynamic/shared CUPTI library. Thus itâs required to set the path to the CUPTI library before launching Visual Profiler and nvprof on Windows. CUPTI library can be found at
    <span class="pre">
     "C:\Program
    </span>
    <span class="pre">
     Files\NVIDIA
    </span>
    <span class="pre">
     GPU
    </span>
    <span class="pre">
     Computing
    </span>
    <span class="pre">
     Toolkit\CUDA\&lt;cuda-toolkit&gt;\extras\CUPTI\lib64"
    </span>
    for Windows.
   </p>
  </li>
  <li>
   <p>
    A security vulnerability issue required profiling tools to disable features using GPU performance counters for non-root or non-admin users when using a Windows 419.17 or Linux 418.43 or later driver. By default, NVIDIA drivers require elevated permissions to access GPU performance counters. On Tegra platforms, profile as root or using sudo. On other platforms, you can either start profiling as root or using sudo, or by enabling non-admin profiling. More details about the issue and the solutions can be found on the ERR_NVGPUCTRPERM
    <a class="reference external" href="https://developer.nvidia.com/ERR_NVGPUCTRPERM">
     web page
    </a>
    .
   </p>
   <p class="admonition-title">
    Note
   </p>
   <p>
    Visual Profiler and nvprof allow tracing features for non-root and non-admin users on desktop platforms only, Tegra platforms require root or sudo access.
   </p>
  </li>
  <li>
   <p>
    Use of the environment variable LD_PRELOAD to load some versions of MPI libraries may result in a crash on Linux platforms. The workaround is to start the profiling session as a root user. For the normal user, the SUID permission for
    <span class="pre">
     nvprof
    </span>
    must be set.
   </p>
  </li>
  <li>
   <p>
    To ensure that all profile data is collected and flushed to a file, cudaDeviceSynchronize() followed by either cudaProfilerStop() or cuProfilerStop() should be called before the application exits. Refer the section
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#flush-profile-data">
     Flush Profile Data
    </a>
    .
   </p>
  </li>
  <li>
   <p>
    Concurrent kernel mode can add significant overhead if used on kernels that execute a large number of blocks and that have short execution durations.
   </p>
  </li>
  <li>
   <p>
    If the kernel launch rate is very high, the device memory used to collect profiling data can run out. In such a case some profiling data might be dropped. This will be indicated by a warning.
   </p>
  </li>
  <li>
   <p>
    When profiling an application that uses CUDA Dynamic Parallelism (CDP) there are several limitations to the profiling tools.
   </p>
   <ul class="simple">
    <li>
     <p>
      CDP kernel launch tracing has a limitation for devices with compute capability 7.0 and higher. Profiler traces all the host launched kernels until it encounters a host launched kernel which launches child kernels. Subsequent kernels are not traced.
     </p>
    </li>
    <li>
     <p>
      Source level analysis is not supported on devices with compute capability 7.0 and higher.
     </p>
    </li>
    <li>
     <p>
      The Visual Profiler timeline does not display CUDA API calls invoked from within device-launched kernels.
     </p>
    </li>
    <li>
     <p>
      The Visual Profiler does not display detailed event, metric, and source-level results for device-launched kernels. Event, metric, and source-level results collected for CPU-launched kernels will include event, metric, and source-level results for the entire call-tree of kernels launched from within that kernel.
     </p>
    </li>
    <li>
     <p>
      The
      <span class="pre">
       nvprof
      </span>
      event/metric output does not include results for device-launched kernels. Events/metrics collected for CPU-launched kernels will include events/metrics for the entire call-tree of kernels launched from within that kernel.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    Profiling APK binaries is not supported.
   </p>
  </li>
  <li>
   <p>
    Unified memory profiling is not supported on the ARM architecture (aarch64).
   </p>
  </li>
  <li>
   <p>
    When profiling an application in which a device kernel was stopped due to an assertion the profiling data will be incomplete and a warning or error message is displayed. But the message is not precise as the exact cause of the failure is not detected.
   </p>
  </li>
  <li>
   <p>
    For dependency analysis, in cases where activity timestamps in the trace are slightly distorted such that they violate the programming model constraints, no dependencies or waiting times can be analyzed.
   </p>
  </li>
  <li>
   <p>
    Devices with compute capability 6.0 and higher introduce a new feature, compute preemption, to give fair chance for all compute contexts while running long tasks. With compute preemption feature-
   </p>
   <ul class="simple">
    <li>
     <p>
      If multiple contexts are running in parallel it is possible that long kernels will get preempted.
     </p>
    </li>
    <li>
     <p>
      Some kernels may get preempted occasionally due to timeslice expiry for the context.
     </p>
    </li>
   </ul>
   <p>
    If kernel has been preempted, the time the kernel spends preempted is still counted towards kernel duration. This can affect the kernel optimization priorities given by Visual Profiler as there is randomness introduced due to preemption.
   </p>
  </li>
 </ul>
 <p>
  Compute preemption can affect events and metrics collection. The following are known issues with the current release:
 </p>
 <ul class="simple">
  <li>
   <p>
    Events and metrics collection for a MPS client can result in higher counts than expected on devices with compute capability 7.0 and higher, since MPS client may get preempted due to termination of another MPS client.
   </p>
  </li>
  <li>
   <p>
    Events warps_launched and sm_cta_launched and metric inst_per_warp might provide higher counts than expected on devices with compute capability 6.0 and 6.1. Metric unique_warps_launched can be used in place of warps_launched to get correct count of actual warps launched as it is not affected by compute preemption.
   </p>
  </li>
  <li>
   <p>
    To avoid compute preemption affecting profiler results try to isolate the context being profiled:
   </p>
   <ul>
    <li>
     <p>
      Run the application on secondary GPU where display is not connected.
     </p>
    </li>
    <li>
     <p>
      On Linux if the application is running on the primary GPU where the display driver is connected then unload the display driver.
     </p>
    </li>
    <li>
     <p>
      Run only one process that uses GPU at one time.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    Devices with compute capability 6.0 and higher support demand paging. When the kernel is scheduled for the first time, all the pages allocated using cudaMallocManaged and that are required for execution of the kernel are fetched in the global memory when GPU faults are generated. Profiler requires multiple passes to collect all the metrics required for kernel analysis. The kernel state needs to be saved and restored for each kernel replay pass. For devices with compute capability 6.0 and higher and platforms supporting Unified memory, in the first kernel iteration the GPU faults will be generated and all pages will be fetched in the global memory. Second iteration onwards GPU page faults will not occur. This will significantly affect the memory related events and timing. The time taken from trace will include the time required to fetch the pages but most of the metrics profiled in multiple iterations will not include time/cycles required to fetch the pages. This causes inconsistency in the profiler results.
   </p>
  </li>
  <li>
   <p>
    CUDA device enumeration and order, typically controlled through environment variables
    <span class="pre">
     CUDA_VISIBLE_DEVICES
    </span>
    and
    <span class="pre">
     CUDA_DEVICE_ORDER
    </span>
    , should remain the same for the profiler and the application.
   </p>
  </li>
  <li>
   <p>
    CUDA profiling might not work on systems that contain a mixture of supported and unsupported GPUs. On such systems, either set option
    <span class="pre">
     --devices
    </span>
    to supported devices in
    <span class="pre">
     nvprof
    </span>
    , or set environment variable
    <span class="pre">
     CUDA_VISIBLE_DEVICES
    </span>
    before launching
    <span class="pre">
     nvprof
    </span>
    or the Visual Profiler.
   </p>
  </li>
  <li>
   <p>
    Because of the low resolution of the timer on Windows, the start and end timestamps can be same for activities having short execution duration on Windows. As a result, the
    <span class="pre">
     nvprof
    </span>
    and Visual Profiler report the following warning: âFound N invalid records in the result.â
   </p>
  </li>
  <li>
   <p>
    Profiler cannot interoperate with other Nvidia tools such as cuda-gdb, cuda-memcheck, Nsight Systems and Nsight Compute.
   </p>
  </li>
  <li>
   <p>
    OpenACC profiling might fail when OpenACC library is linked statically in the user application. This happens due to the missing definition of the OpenACC API routines needed for the OpenACC profiling, as compiler might ignore definitions for the functions not used in the application. This issue can be mitigated by linking the OpenACC library dynamically.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler and nvprof versions shipped in the CUDA Toolkit 11.7 and CUDA Toolkit 11.8 donât support Kepler (sm_35 and sm_37) devices. This issue can be resolved by upgrading the CUPTI library. Refer to the webpages
    <a class="reference external" href="https://developer.nvidia.com/cupti-ctk11_7">
     CUPTI 11.7
    </a>
    and
    <a class="reference external" href="https://developer.nvidia.com/cupti-ctk11_8">
     CUPTI 11.8
    </a>
    for location of the CUPTI packages having the support for these Kepler devices.
   </p>
  </li>
  <li>
   <p>
    Profiler is not supported on below system configurations:
   </p>
   <ul>
    <li>
     <p>
      64-bit ARM Server CPU architecture (arm64 SBSA).
     </p>
    </li>
    <li>
     <p>
      Virtual GPUs (vGPU).
     </p>
    </li>
    <li>
     <p>
      Windows Subsystem for Linux (WSL).
     </p>
    </li>
    <li>
     <p>
      NVIDIA Crypto Mining Processors (CMP). For more information, please visit the
      <a class="reference external" href="https://developer.nvidia.com/ERR_NVCMPGPU">
       web page
      </a>
      .
     </p>
    </li>
   </ul>
  </li>
 </ul>
 <p>
  Visual Profiler
 </p>
 <p>
  The following are known issues related to Visual Profiler:
 </p>
 <ul>
  <li>
   <p>
    Visual Profiler requires Java Runtime Environment (JRE) 1.8 to be available on the local system. However, starting with CUDA Toolkit version 10.1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes. The user must install the required version of JRE 1.8 in order to use Visual Profiler. Refer to the section
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#setting-up-java-runtime-environment">
     Setting up Java Runtime Environment
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    Some analysis results require metrics that are not available on all devices. When these analyses are attempted on a device where the metric is not available the analysis results will show that the required data is ânot availableâ.
   </p>
  </li>
  <li>
   <p>
    Using the mouse wheel button to scroll does not work within the Visual Profiler on Windows.
   </p>
  </li>
  <li>
   <p>
    Since Visual Profiler uses
    <span class="pre">
     nvprof
    </span>
    for collecting profiling data,
    <span class="pre">
     nvprof
    </span>
    limitations also apply to Visual Profiler.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler cannot load profiler data larger than the memory size limited by JVM or available memory on the system. Refer
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#improve-loading-of-large-profiles">
     Improve Loading of Large Profiles
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler global menus do not show properly or are empty on some versions of Ubuntu. One workaround is to set environment variable âUBUNTU_MENUPROXY=0â before running Visual Profiler
   </p>
  </li>
  <li>
   <p>
    In the Visual Profiler the NVLink Analysis diagram can be incorrect after scrolling the diagram. This can be corrected by horizontally resizing the diagram panel.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler might not be able to show NVLink events on the timeline when large number of samples are collected. To work around this issue, refresh the timeline by doing zoom-in or zoom-out. Alternate solution is to save and open the session.
   </p>
  </li>
  <li>
   <p>
    For unified memory profiling on a remote setup having different version of GCC than host machine, Visual Profiler might not be able to show the source code location for CPU page fault events.
   </p>
  </li>
  <li>
   <p>
    For unified memory profiling on a remote setup having different architecture than the host machine (x86 versus POWER), Visual Profiler might not be able to show the source code location for CPU page fault and allocation tracking events.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler is not supported on the ARM architecture (aarch64). You can use Remote Profiling. Refer the
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#remote-profiling">
     Remote Profiling
    </a>
    section for more information.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler doesnât support remote profiling for the Android target. The workaround is to run nvprof on the target and load the nvprof output in the Visual Profiler.
   </p>
  </li>
  <li>
   <p>
    For remote profiling, the CUDA Toolkit installed on the host system must support the target device on the remote system.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler might show strange symbol fonts on platforms which donât have required fonts installed.
   </p>
  </li>
  <li>
   <p>
    The splash screen that appears on Visual Profiler start-up is disabled on macOS.
   </p>
  </li>
  <li>
   <p>
    When using remote profiling if there is a connection failure due to key exchange failure, then you will get an error message âUnable to establish shell connection to
    <a class="reference external" href="mailto:'user%40xxx">
     âuser
     <span>
      @
     </span>
     xxx
    </a>
    ââ. You can follow these steps to mitigate the issue.
   </p>
   <ol class="arabic">
    <li>
     <p>
      Check the SSH daemon config file (default path is /etc/ssh/sshd_config) on the target
     </p>
    </li>
    <li>
     <p>
      Comment out lines starting with:
     </p>
     <pre>KexAlgorithms
</pre>
     <pre>HostbasedAcceptedKeyTypes
</pre>
     <pre>Ciphers
</pre>
     <pre>HostKey
</pre>
     <pre>AuthorizedKeysFile
</pre>
    </li>
    <li>
     <p>
      Re-generate keys
     </p>
     <pre>sudo ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key
</pre>
    </li>
    <li>
     <p>
      Restart sshd service
     </p>
     <pre>sudo services sshd restart
</pre>
    </li>
   </ol>
  </li>
  <li>
   <p>
    Accessing the local help document from Visual Profiler leads to HTTP Error 500. The workaround is to refer to this document (online document or pdf).
   </p>
  </li>
  <li>
   <p>
    Visual Profiler canât remote into a target machine running Ubuntu 20.04 and later.
   </p>
  </li>
 </ul>
 <p>
  nvprof
 </p>
 <p>
  The following are known issues related to
  <span class="pre">
   nvprof
  </span>
  :
 </p>
 <ul class="simple">
  <li>
   <p>
    <span class="pre">
     nvprof
    </span>
    cannot profile processes that
    <span class="pre">
     fork()
    </span>
    but do not then
    <span class="pre">
     exec()
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     nvprof
    </span>
    assumes it has access to the temporary directory on the system, which it uses to store temporary profiling data. On Linux the default is
    <span class="pre">
     /tmp
    </span>
    . On Windows itâs specified by the system environment variables. To specify a custom location, change
    <span class="pre">
     $TMPDIR
    </span>
    on Linux or
    <span class="pre">
     %TMP%
    </span>
    on Windows.
   </p>
  </li>
  <li>
   <p>
    When multiple
    <span class="pre">
     nvprof
    </span>
    processes are run simultaneously on the same node, there is an issue of contention for files under the temporary directory. One workaround is to set a different temporary directory for each process.
   </p>
  </li>
  <li>
   <p>
    Multiple
    <span class="pre">
     nvprof
    </span>
    processes running concurrently using application replay may generate incorrect results or no results at all. To work around this issue you need to set a unique temporary directory per process. Set NVPROF_TMPDIR before launching
    <span class="pre">
     nvprof
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    To profile application on Android
    <span class="pre">
     $TMPDIR
    </span>
    environment variable has to be defined and point to a user-writable folder.
   </p>
  </li>
  <li>
   <p>
    Profiling results might be inconsistent when auto boost is enabled.
    <span class="pre">
     nvprof
    </span>
    tries to disable auto boost by default, it might fail to do so in some conditions, but profiling will continue.
    <span class="pre">
     nvprof
    </span>
    will report a warning when auto boost cannot be disabled. Note that auto boost is supported only on certain Tesla devices from the Kepler+ family.
   </p>
  </li>
  <li>
   <p>
    Profiling a C++ application which overloads the new operator at the global scope and uses any CUDA APIs like
    <span class="pre">
     cudaMalloc()
    </span>
    or
    <span class="pre">
     cudaMallocManaged()
    </span>
    inside the overloaded new operator will result in a hang.
   </p>
  </li>
  <li>
   <p>
    NVTX annotations will not work when profiling all processes using the
    <span class="pre">
     nvprof
    </span>
    option
    <span class="pre">
     --profile-all-processes
    </span>
    . It is advised to set the environment variable NVTX_INJECTION64_PATH to point to the profiler injection library, libcuinj64.so on Linux and cuinj64_*.dll on Windows, before launching the application.
   </p>
  </li>
 </ul>
 <p>
  Events and Metrics
 </p>
 <p>
  The following are known issues related to Events and Metrics profiling:
 </p>
 <ul class="simple">
  <li>
   <p>
    Profiling features for devices with compute capability 7.5 and higher are supported in the
    <a class="reference external" href="https://developer.nvidia.com/nsight-compute">
     NVIDIA Nsight Compute
    </a>
    . Visual Profiler does not support Guided Analysis, some stages under Unguided Analysis and events and metrics collection for devices with compute capability 7.5 and higher. One can launch the NVIDIA Nsight Compute UI for devices with compute capability 7.5 and higher from Visual Profiler. Also nvprof does not support query and collection of events and metrics, source level analysis and other options used for profiling on devices with compute capability 7.5 and higher. The NVIDIA Nsight Compute command line interface can be used for these features.
   </p>
  </li>
  <li>
   <p>
    Events or metrics collection may significantly change the overall performance characteristics of the application because all kernel executions are serialized on the GPU.
   </p>
  </li>
  <li>
   <p>
    In event or metric profiling, kernel launches are blocking. Thus kernels waiting on updates from host or another kernel may hang. This includes synchronization between the host and the device build upon value-based CUDA stream synchronization APIs such as
    <span class="pre">
     cuStreamWaitValue32()
    </span>
    and
    <span class="pre">
     cuStreamWriteValue32()
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    Event and metric collection requiring multiple passes will not work with the
    <span class="pre">
     nvprof
    </span>
    kernel replay option for any kernel performing IPC or data communication between the kernel and CPU, kernel and regular CPU allocated memory, kernel and Peer GPU, or kernel and other Peer devices (e.g. GPU direct).
   </p>
  </li>
  <li>
   <p>
    For some metrics, the required events can only be collected for a single CUDA context. For an application that uses multiple CUDA contexts, these metrics will only be collected for one of the contexts. The metrics that can be collected only for a single CUDA context are indicated in the
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-reference">
     metric reference tables
    </a>
    .
   </p>
  </li>
  <li>
   <p>
    Some metric values are calculated assuming a kernel is large enough to occupy all device multiprocessors with approximately the same amount of work. If a kernel launch does not have this characteristic, then those metric values may not be accurate.
   </p>
  </li>
  <li>
   <p>
    Some metrics are not available on all devices. To see a list of all available metrics on a particular NVIDIA GPU, type
    <span class="pre">
     nvprof
    </span>
    <span class="pre">
     --query-metrics
    </span>
    . You can also refer to the
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#metrics-reference">
     metric reference tables
    </a>
    .
   </p>
  </li>
  <li>
   <p>
    The profilers may fail to collect events or metrics when âapplication replayâ mode is turned on. This is most likely to happen if the application is multi-threaded and non-deterministic. Instead use âkernel replayâ mode in this case.
   </p>
  </li>
  <li>
   <p>
    For applications that allocate large amount of device memory, the profiler may take significant time to collect all events or metrics when âkernel replayâ mode is used. Instead use âapplication replayâ mode in this case.
   </p>
  </li>
  <li>
   <p>
    Here are a couple of reasons why Visual Profiler may fail to gather metric or event information.
   </p>
   <ul>
    <li>
     <p>
      More than one tool is trying to access the GPU. To fix this issue please make sure only one tool is using the GPU at any given point. Tools include Nsight Compute, Nsight Systems, Nsight Graphics, and applications that use either CUPTI or PerfKit API (NVPM) to read event values.
     </p>
    </li>
    <li>
     <p>
      More than one application is using the GPU at the same time Visual Profiler is profiling a CUDA application. To fix this issue please close all applications and just run the one with Visual Profiler. Interacting with the active desktop should be avoided while the application is generating event information. Please note that for some types of event Visual Profiler gathers events for only one context if the application is using multiple contexts within the same application.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    When collecting events or metrics with the
    <span class="pre">
     --events
    </span>
    ,
    <span class="pre">
     --metrics
    </span>
    , or
    <span class="pre">
     --analysis-metrics
    </span>
    options,
    <span class="pre">
     nvprof
    </span>
    will use kernel replay to execute each kernel multiple times as needed to collect all the requested data. If a large number of events or metrics are requested then a large number of replays may be required, resulting in a significant increase in application execution time.
   </p>
  </li>
  <li>
   <p>
    Some events are not available on all devices. To see a list of all available events on a particular device, type
    <span class="pre">
     nvprof
    </span>
    <span class="pre">
     --query-events
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    Enabling certain events can cause GPU kernels to run longer than the driverâs watchdog time-out limit. In these cases the driver will terminate the GPU kernel resulting in an application error and profiling data will not be available. Please disable the driver watchdog time out before profiling such long running CUDA kernels
   </p>
   <ul>
    <li>
     <p>
      On Linux, setting the X Config option Interactive to false is recommended.
     </p>
    </li>
    <li>
     <p>
      For Windows, detailed information about TDR (Timeout Detection and Recovery) and how to disable it is available at
      <a class="reference external" href="https://docs.microsoft.com/en-us/windows-hardware/drivers/display/timeout-detection-and-recovery">
       https://docs.microsoft.com/en-us/windows-hardware/drivers/display/timeout-detection-and-recovery
      </a>
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    <span class="pre">
     nvprof
    </span>
    can give out of memory error for event and metrics profiling, it could be due to large number of instructions in the kernel.
   </p>
  </li>
  <li>
   <p>
    Profiling results might be incorrect for CUDA applications compiled with nvcc version older than 9.0 for devices with compute capability 6.0 and 6.1. It is advised to recompile the application with nvcc version 9.0 or later. Ignore this warning if code is already compiled with the recommended nvcc version.
   </p>
  </li>
  <li>
   <p>
    PC Sampling is not supported on Tegra platforms.
   </p>
  </li>
  <li>
   <p>
    Profiling is not supported for multidevice cooperative kernels, that is, kernels launched by using the API functions cudaLaunchCooperativeKernelMultiDevice or cuLaunchCooperativeKernelMultiDevice.
   </p>
  </li>
  <li>
   <p>
    Profiling is not supported for CUDA kernel nodes launched by a CUDA Graph.
   </p>
  </li>
 </ul>
 <h1>
  <span class="section-number">
   13.
  </span>
  Changelog
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#changelog" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  Profiler changes in CUDA 12.5
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 12.5 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    Visual Profilerâs remote profiling support from the macOS host is deprecated. It will be dropped in an upcoming release.
   </p>
  </li>
  <li>
   <p>
    Support for IBM Power architecture is dropped.
   </p>
  </li>
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 12.4
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 12.4 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 12.3
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 12.3 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 12.2
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 12.2 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 12.1
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 12.1 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 12.0
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 12.0 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 11.8
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 11.8 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 11.7
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 11.7 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 11.6
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 11.6 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 11.5
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 11.5 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 11.4
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 11.4 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 11.3
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 11.3 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    Visual Profiler extends remote profiling support to macOS host running version 11 (Big Sur) on Intel x86_64 architecture.
   </p>
  </li>
  <li>
   <p>
    General bug fixes.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 11.2
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 11.2 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 11.1
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 11.1 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    General bug fixes. No new feature is added in this release.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 11.0
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 11.0 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    Visual Profiler and nvprof donât support devices with compute capability 8.0 and higher. Next-gen tools NVIDIA Nsight Compute and NVIDIA Nsight Systems should be used instead.
   </p>
  </li>
  <li>
   <p>
    Starting with the CUDA 11.0, Visual Profiler and nvprof wonât support Mac as the target platform. However Visual Profiler will continue to support remote profiling from the Mac host. Visual Profiler will be provided in a separate installer package to maintain the remote profiling workflow for CUDA developers on Mac.
   </p>
  </li>
  <li>
   <p>
    Added support to trace Optix applications.
   </p>
  </li>
  <li>
   <p>
    Fixed the nvprof option âannotate-mpi which was broken since CUDA 10.0.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 10.2
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 10.2 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    Visual Profiler and nvprof allow tracing features for non-root and non-admin users on desktop platforms. Note that events and metrics profiling is still restricted for non-root and non-admin users. More details about the issue and the solutions can be found on this
    <a class="reference external" href="https://developer.nvidia.com/nvidia-development-tools-solutions-ERR_NVGPUCTRPERM-permission-issue-performance-counters">
     web page
    </a>
    .
   </p>
  </li>
  <li>
   <p>
    Starting with CUDA 10.2, Visual Profiler and nvprof use dynamic/shared CUPTI library. Thus itâs required to set the path to the CUPTI library before launching Visual Profiler and nvprof. CUPTI library can be found at
    <span class="pre">
     /usr/local/&lt;cuda-toolkit&gt;/extras/CUPTI/lib64
    </span>
    or
    <span class="pre">
     /usr/local/&lt;cuda-toolkit&gt;/targets/&lt;arch&gt;/lib
    </span>
    for POSIX platforms and
    <span class="pre">
     "C:\Program
    </span>
    <span class="pre">
     Files\NVIDIA
    </span>
    <span class="pre">
     GPU
    </span>
    <span class="pre">
     Computing
    </span>
    <span class="pre">
     Toolkit\CUDA\&lt;cuda-toolkit&gt;\extras\CUPTI\lib64"
    </span>
    for Windows.
   </p>
  </li>
  <li>
   <p>
    Profilers no longer turn off the performance characteristics of CUDA Graph when tracing the application.
   </p>
  </li>
  <li>
   <p>
    Added an option to enable/disable the OpenMP profiling in Visual Profiler.
   </p>
  </li>
  <li>
   <p>
    Fixed the incorrect timing issue for the asynchronous cuMemset/cudaMemset activity.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 10.1 Update 2
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 10.1 Update 2 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    This release is focused on bug fixes and stability of the profiling tools.
   </p>
  </li>
  <li>
   <p>
    A security vulnerability issue required profiling tools to disable all the features for non-root or non-admin users. As a result, Visual Profiler and nvprof cannot profile the application when using a Windows 419.17 or Linux 418.43 or later driver. More details about the issue and the solutions can be found on this
    <a class="reference external" href="https://developer.nvidia.com/nvidia-development-tools-solutions-ERR_NVGPUCTRPERM-permission-issue-performance-counters">
     web page
    </a>
    .
   </p>
  </li>
  <li>
   <p>
    Visual Profiler requires Java Runtime Environment (JRE) 1.8 to be available on the local system. However, starting with CUDA Toolkit version 10.1 Update 2, the JRE is no longer included in the CUDA Toolkit due to Oracle upgrade licensing changes. The user must install the required version of JRE 1.8 in order to use Visual Profiler. Refer to the section
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#setting-up-java-runtime-environment">
     Setting up Java Runtime Environment
    </a>
    for more information.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 10.1
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 10.1 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    This release is focused on bug fixes and stability of the profiling tools.
   </p>
  </li>
  <li>
   <p>
    Support for NVTX string registration API nvtxDomainRegisterStringA().
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 10.0
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 10.0 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    Added tracing support for devices with compute capability 7.5.
   </p>
  </li>
  <li>
   <p>
    Profiling features for devices with compute capability 7.5 and higher are supported in the
    <a class="reference external" href="https://developer.nvidia.com/nsight-compute">
     NVIDIA Nsight Compute
    </a>
    . Visual Profiler does not support Guided Analysis, some stages under Unguided Analysis and events and metrics collection for devices with compute capability 7.5 and higher. One can launch the NVIDIA Nsight Compute UI for devices with compute capability 7.5 and higher from Visual Profiler. Also nvprof does not support query and collection of events and metrics, source level analysis and other options used for profiling on devices with compute capability 7.5 and higher. The NVIDIA Nsight Compute command line interface can be used for these features.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler and nvprof now support OpenMP profiling where available. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openmp">
     OpenMP
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    Tracing support for CUDA kernels, memcpy and memset nodes launched by a CUDA Graph.
   </p>
  </li>
  <li>
   <p>
    Profiler supports version 3 of NVIDIA Tools Extension API (NVTX). This is a header-only implementation of NVTX version 2.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 9.2
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 9.2 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    The Visual Profiler allows to switch multiple segments to non-segment mode for Unified Memory profiling on the timeline. Earlier it was restircted to single segment only.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler shows a summary view of the memory hierarchy of the CUDA programming model. This is available for devices with compute capability 5.0 and higher. Refer
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#memory-statistics">
     Memory Statistics
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler can correctly import profiler data generated by nvprof when the option
    <span class="pre">
     --kernels
    </span>
    <span class="pre">
     kernel-filter
    </span>
    is used.
   </p>
  </li>
  <li>
   <p>
    nvprof supports display of basic PCIe topolgy including PCI bridges between NVIDIA GPUs and Host Bridge.
   </p>
  </li>
  <li>
   <p>
    To view and analyze bandwidth of memory transfers over PCIe topologies, new set of metrics to collect total data bytes transmitted and recieved through PCIe are added. Those give accumulated count for all devices in the system. These metrics are collected at the device level for the entire application. And those are made available for devices with compute capability 5.2 and higher.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler and nvprof added support for new metrics:
   </p>
   <ul>
    <li>
     <p>
      Instruction executed for different types of load and store
     </p>
    </li>
    <li>
     <p>
      Total number of cached global/local load requests from SM to texture cache
     </p>
    </li>
    <li>
     <p>
      Global atomic/non-atomic/reduction bytes written to L2 cache from texture cache
     </p>
    </li>
    <li>
     <p>
      Surface atomic/non-atomic/reduction bytes written to L2 cache from texture cache
     </p>
    </li>
    <li>
     <p>
      Hit rate at L2 cache for all requests from texture cache
     </p>
    </li>
    <li>
     <p>
      Device memory (DRAM) read and write bytes
     </p>
    </li>
    <li>
     <p>
      The utilization level of the multiprocessor function units that execute tensor core instructions for devices with compute capability 7.0
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    nvprof allows to collect tracing infromation along with the profiling information in the same pass. Use new option
    <span class="pre">
     --trace
    </span>
    <span class="pre">
     &lt;api|gpu&gt;
    </span>
    to enable trace along with collection of events/metrics.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 9.1
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 9.1 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    The Visual Profiler shows the breakdown of the time spent on the CPU for each thread in the
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-details-view">
     CPU Details View
    </a>
    .
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler supports a new option to select the PC sampling frequency.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler shows NVLink version in the NVLink topology.
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     nvprof
    </span>
    provides the correlation ID when profiling data is generated in CSV format.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 9.0
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 9.0 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    Visual Profiler and
    <span class="pre">
     nvprof
    </span>
    now support profiling on devices with compute capability 7.0.
   </p>
  </li>
  <li>
   <p>
    Tools and extensions for profiling are hosted on Github at
    <a class="reference external" href="https://github.com/NVIDIA/cuda-profiler">
     https://github.com/NVIDIA/cuda-profiler
    </a>
   </p>
  </li>
  <li>
   <p>
    There are several enhancements to Unified Memory profiling:
   </p>
   <ul>
    <li>
     <p>
      The Visual Profiler now associates unified memory events with the source code at which the memory is allocated.
     </p>
    </li>
    <li>
     <p>
      The Visual Profiler now correlates a CPU page fault to the source code resulting in the page fault.
     </p>
    </li>
    <li>
     <p>
      New Unified Memory profiling events for page thrashing, throttling and remote map are added.
     </p>
    </li>
    <li>
     <p>
      The Visual Profiler provides an option to switch between segment and non-segment mode on the timeline.
     </p>
    </li>
    <li>
     <p>
      The Visual Profiler supports filtering of Unified Memory profiling events based on the virtual address, migration reason or the page fault access type.
     </p>
    </li>
    <li>
     <p>
      CPU page fault support is extended to Mac platforms.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    Tracing and profiling of cooperative kernel launches is supported.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler shows NVLink events on the timeline.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler color codes links in the NVLink topology diagram based on throughput.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler supports new options to make it easier to do multi-hop remote profiling.
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     nvprof
    </span>
    supports a new option to select the PC sampling frequency.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler supports remote profiling to systems supporting
    <span class="pre">
     ssh
    </span>
    key exchange algorithms with a key length of 2048 bits.
   </p>
  </li>
  <li>
   <p>
    OpenACC profiling is now also supported on non-NVIDIA systems.
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     nvprof
    </span>
    flushes all profiling data when a
    <span class="pre">
     SIGINT
    </span>
    or
    <span class="pre">
     SIGKILL
    </span>
    signal is encountered.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 8.0
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 8.0 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    Visual Profiler and nvprof now support NVLink analysis for devices with compute capability 6.0. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvlink-view">
     NVLink view
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler and nvprof now support dependency analysis which enables optimization of the program runtime and concurrency of applications utilizing multiple CPU threads and CUDA streams. It allows computing the critical path of a specific execution, detect waiting time and inspect dependencies between functions executing in different threads or streams. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#dependency-analysis">
     Dependency Analysis
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler and nvprof now support OpenACC profiling. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#openacc">
     OpenACC
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler now supports CPU profiling. Refer
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-details-view">
     CPU Details View
    </a>
    and
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-source-view">
     CPU Source View
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    Unified Memory profiling now provides GPU page fault information on devices with compute capability 6.0 and 64 bit Linux platforms.
   </p>
  </li>
  <li>
   <p>
    Unified Memory profiling now provides CPU page fault information on 64 bit Linux platforms.
   </p>
  </li>
  <li>
   <p>
    Unified Memory profiling support is extended to the Mac platform.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler source-disassembly view has several enhancements. There is now a single integrated view for the different source level analysis results collected for a kernel instance. Results of different analysis steps can be viewed together. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#source-disassembly-view">
     Source-Disassembly View
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    The PC sampling feature is enhanced to point out the true latency issues for devices with compute capability 6.0 and higher.
   </p>
  </li>
  <li>
   <p>
    Support for 16-bit floating point (FP16) data format profiling.
   </p>
  </li>
  <li>
   <p>
    If the new NVIDIA Tools Extension API(NVTX) feature of domains is used then Visual Profiler and nvprof will show the NVTX markers and ranges grouped by domain.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler now adds a default file extension
    <span class="pre">
     .nvvp
    </span>
    if an extension is not specified when saving or opening a session file.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler now supports timeline filtering options in create new session and import dialogs. Refer âTimeline Optionsâ section under
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#creating-a-session">
     Creating a Session
    </a>
    for more details.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 7.5
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 7.5 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    <span class="pre">
     Visual
    </span>
    <span class="pre">
     Profiler
    </span>
    now supports PC sampling for devices with compute capability 5.2. Warp state including stall reasons are shown at source level for kernel latency analysis. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#pc-sampling-view">
     PC Sampling View
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     Visual
    </span>
    <span class="pre">
     Profiler
    </span>
    now supports profiling child processes and profiling all processes launched on the same system. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#creating-a-session">
     Creating a Session
    </a>
    for more information on the new multi-process profiling options. For profiling CUDA applications using Multi-Process Service(MPS) see
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#mps-profiling-with-visual-profiler">
     MPS profiling with Visual Profiler
    </a>
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     Visual
    </span>
    <span class="pre">
     Profiler
    </span>
    import now supports browsing and selecting files on a remote system.
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     nvprof
    </span>
    now supports CPU profiling. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#cpu-sampling">
     CPU Sampling
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    All events and metrics for devices with compute capability 5.2 can now be collected accurately in presence of multiple contexts on the GPU.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 7.0
 </p>
 <p>
  The profiling tools contain a number of changes and new features as part of the CUDA Toolkit 7.0 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    The Visual Profiler has been updated with several enhancements:
   </p>
   <ul>
    <li>
     <p>
      Performance is improved when loading large data file. Memory usage is also reduced.
     </p>
    </li>
    <li>
     <p>
      Visual Profiler timeline is improved to view multi-gpu MPS profile data.
     </p>
    </li>
    <li>
     <p>
      Unified memory profiling is enhanced by providing fine grain data transfers to and from the GPU, coupled with more accurate timestamps with each transfer.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    <span class="pre">
     nvprof
    </span>
    has been updated with several enhancements:
   </p>
   <ul>
    <li>
     <p>
      All events and metrics for devices with compute capability 3.x and 5.0 can now be collected accurately in presence of multiple contexts on the GPU.
     </p>
    </li>
   </ul>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 6.5
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 6.5 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    The Visual Profiler kernel memory analysis has been updated with several enhancements:
   </p>
   <ul>
    <li>
     <p>
      ECC overhead is added which provides a count of memory transactions required for ECC
     </p>
    </li>
    <li>
     <p>
      Under L2 cache a split up of transactions for L1 Reads, L1 Writes, Texture Reads, Atomic and Noncoherent reads is shown
     </p>
    </li>
    <li>
     <p>
      Under L1 cache a count of Atomic transactions is shown
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    The Visual Profiler kernel profile analysis view has been updated with several enhancements:
   </p>
   <ul>
    <li>
     <p>
      Initially the instruction with maximum execution count is highlighted
     </p>
    </li>
    <li>
     <p>
      A bar is shown in the background of the counter value for the âExec Countâ column to make it easier to identify instruction with high execution counts
     </p>
    </li>
    <li>
     <p>
      The current assembly instruction block is highlighted using two horizontal lines around the block. Also ânextâ and âpreviousâ buttons are added to move to the next or previous block of assembly instructions.
     </p>
    </li>
    <li>
     <p>
      Syntax highlighting is added for the CUDA C source.
     </p>
    </li>
    <li>
     <p>
      Support is added for showing or hiding columns.
     </p>
    </li>
    <li>
     <p>
      A tooltip describing each column is added.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    <span class="pre">
     nvprof
    </span>
    now supports a new application replay mode for collecting multiple events and metrics. In this mode the application is run multiple times instead of using kernel replay. This is useful for cases when the kernel uses a large amount of device memory and use of kernel replay can be slow due to a high overhead of saving and restoring device memory for each kernel replay run. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#event-metric-summary-mode">
     Event/metric Summary Mode
    </a>
    for more information. Visual Profiler also supports this new application replay mode and it can enabled in the Visual Profiler âNew Sessionâ dialog.
   </p>
  </li>
  <li>
   <p>
    Visual Profiler now displays peak single precision flops and double precision flops for a GPU under device properties.
   </p>
  </li>
  <li>
   <p>
    Improved source-to-assembly code correlation for CUDA Fortran applications compiled by the PGI CUDA Fortran compiler.
   </p>
  </li>
 </ul>
 <p>
  Profiler changes in CUDA 6.0
 </p>
 <p>
  List of changes done as part of the CUDA Toolkit 6.0 release.
 </p>
 <ul class="simple">
  <li>
   <p>
    Unified Memory is fully supported by both the Visual Profiler and
    <span class="pre">
     nvprof
    </span>
    . Both profilers allow you to see the Unified Memory related memory traffic to and from each GPU on your system.
   </p>
  </li>
  <li>
   <p>
    The standalone Visual Profiler,
    <span class="pre">
     nvvp
    </span>
    , now provides a multi-process timeline view. You can import multiple timeline data sets collected with
    <span class="pre">
     nvprof
    </span>
    into
    <span class="pre">
     nvvp
    </span>
    and view them on the same timeline to see how they are sharing the GPU(s). This multi-process import capability also includes support for CUDA applications using MPS. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#mps-profiling">
     MPS Profiling
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler now supports a remote profiling mode that allows you to collect a profile on a remote Linux system and view the timeline, analysis results, and detailed results on your local Linux, Mac, or Windows system. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#remote-profiling">
     Remote Profiling
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler analysis system now includes a side-by-side source and disassembly view annotated with instruction execution counts, inactive thread counts, and predicated instruction counts. This new view enables you to find hotspots and inefficient code sequences within your kernels.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler analysis system has been updated with several new analysis passes: 1) kernel instructions are categorized into classes so that you can see if instruction mix matches your expectations, 2) inefficient shared memory access patterns are detected and reported, and 3) per-SM activity level is presented to help you detect detect load-balancing issues across the blocks of your kernel.
   </p>
  </li>
  <li>
   <p>
    The Visual Profiler guided analysis system can now generate a kernel analysis report. The report is a PDF version of the per-kernel information presented by the guided analysis system.
   </p>
  </li>
  <li>
   <p>
    Both
    <span class="pre">
     nvvp
    </span>
    and
    <span class="pre">
     nvprof
    </span>
    can now operate on a system that does not have an NVIDIA GPU. You can import profile data collected from another system and view and analyze it on your GPU-less system.
   </p>
  </li>
  <li>
   <p>
    Profiling overheads for both
    <span class="pre">
     nvvp
    </span>
    and
    <span class="pre">
     nvprof
    </span>
    have been significantly reduced.
   </p>
  </li>
 </ul>
 <h1>
  <span class="section-number">
   14.
  </span>
  Notices
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#notices" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   14.1.
  </span>
  Notice
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#notice" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
 </p>
 <p>
  NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
 </p>
 <p>
  Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
 </p>
 <p>
  NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
 </p>
 <p>
  NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
 </p>
 <p>
  NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
 </p>
 <p>
  No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
 </p>
 <p>
  Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
 </p>
 <p>
  THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.
 </p>
 <h2>
  <span class="section-number">
   14.2.
  </span>
  OpenCL
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#opencl" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.
 </p>
 <h2>
  <span class="section-number">
   14.3.
  </span>
  Trademarks
  <a class="headerlink" href="https://docs.nvidia.com/cuda/profiler-users-guide/index.html#trademarks" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.
 </p>
 <p class="notices">
  <a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">
   Privacy Policy
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">
   Manage My Privacy
  </a>
  |
  <a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">
   Do Not Sell or Share My Data
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">
   Terms of Service
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">
   Accessibility
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">
   Corporate Policies
  </a>
  |
  <a href="https://www.nvidia.com/en-us/product-security/" target="_blank">
   Product Security
  </a>
  |
  <a href="https://www.nvidia.com/en-us/contact/" target="_blank">
   Contact
  </a>
 </p>
 <p>
  Copyright Â© 2007-2024, NVIDIA Corporation &amp; affiliates. All rights reserved.
 </p>
 <p>
  <span class="lastupdated">
   Last updated on Jul 1, 2024.
  </span>
 </p>
</body>
</body></html>