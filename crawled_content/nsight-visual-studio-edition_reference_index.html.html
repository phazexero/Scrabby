<html><head><title>Reference — nsight-visual-studio-edition 12.5 documentation</title></head><body><body class="wy-body-for-nav">
 <a href="https://docs.nvidia.com/nsight-visual-studio-edition/index.html">
 </a>
 <ul>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/introduction/index.html">
    Introduction
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/release-notes/index.html">
    Release Notes
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/install-setup/index.html">
    Installation and Setup
   </a>
  </li>
 </ul>
 <p class="caption" role="heading">
  <span class="caption-text">
   CUDA Debugger
  </span>
 </p>
 <ul>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/cuda-debugger/index.html">
    Getting Started with the CUDA Debugger
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/cuda-build-run/index.html">
    Build and Run
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/cuda-control-gpu-execution/index.html">
    Control GPU Execution
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/cuda-inspect-state/index.html">
    Inspect State
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/cuda-advanced-topics/index.html">
    Advanced Topics
   </a>
  </li>
 </ul>
 <p class="caption" role="heading">
  <span class="caption-text">
   Reference
  </span>
 </p>
 <ul class="current">
  <li class="toctree-l1 current">
   <a class="current reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html">
    Reference
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#reference-topics">
      Reference Topics
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#debugging-external-applications">
      Debugging External Applications
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#timeout-detection-recovery-tdr">
      Timeout Detection &amp; Recovery (TDR)
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#local-debugging-with-a-single-gpu">
        Local Debugging with a Single GPU
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#local-debugging-with-multiple-gpus-or-remote-debugging">
        Local Debugging with Multiple GPUs or Remote Debugging
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#tesla-compute-cluster">
      Tesla Compute Cluster (TCC)
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#limitations">
        Limitations
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#setting-tcc-mode-for-tesla-products">
        Setting TCC Mode for Tesla Products
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#about-tcc">
        About TCC
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#environment-variables">
      Environment Variables
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#directcompute">
      DirectCompute
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#troubleshooting-nvidia-nsight-visual-studio-edition">
      Troubleshooting NVIDIA Nsight Visual Studio Edition
     </a>
    </li>
   </ul>
  </li>
 </ul>
 <p class="caption" role="heading">
  <span class="caption-text">
   Release Information
  </span>
 </p>
 <ul>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/archives/index.html">
    Archives
   </a>
  </li>
 </ul>
 <p class="caption" role="heading">
  <span class="caption-text">
   Copyright and License Notices
  </span>
 </p>
 <ul>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/nsight-visual-studio-edition/eula/index.html">
    EULA
   </a>
  </li>
 </ul>
 <a href="https://docs.nvidia.com/nsight-visual-studio-edition/index.html">
  nsight-visual-studio-edition
 </a>
 <ul class="wy-breadcrumbs">
  <li>
   <a class="icon icon-home" href="https://docs.nvidia.com/index.html">
   </a>
   »
  </li>
  <li>
   Reference
  </li>
  <li class="wy-breadcrumbs-aside">
   <a class="reference external" href="https://developer.nvidia.com/nsight-visual-studio-edition-2024_2-new-features">
    v2024.2.1 |
   </a>
   <a class="reference external" href="https://developer.nvidia.com/nsight-visual-studio-edition-archive">
    Archive
   </a>
  </li>
 </ul>
 <h1>
  Reference
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#reference" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  Additional resources for NVIDIA Nsight VSE.
 </p>
 <h2>
  Reference Topics
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#reference-topics" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This section provides additional information and resources for the NVIDIA Nsight Visual Studio Edition User Guide.
 </p>
 <ul class="simple">
  <li>
   <p>
    <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#debug-external-apps">
     Debugging External Applications
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#timeout-detection-recovery">
     Timeout Detection and Recovery (TDR)
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#tesla-compute-cluster">
     Tesla Compute Cluster (TCC)
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#environment-variables">
     Environment Variables
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#direct-compute">
     Direct Compute
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#troubleshooting">
     Troubleshooting NVIDIA Nsight
    </a>
   </p>
  </li>
 </ul>
 <h2>
  Debugging External Applications
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#debugging-external-applications" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Early versions of NVIDIA Nsightâ¢ VSE were only able to debug projects built in Visual C++. However, as of NVIDIA Nsightâ¢ VSEÂ 3.1, CUDA debugging is supported forÂ both C++ and C# projects.
 </p>
 <p>
  If you would like to use NVIDIA Nsightâ¢ VSE to debug an application that is built in an environment other than C++ or C#, use the tutorial outlined below.
 </p>
 <p>
  Using NVIDIA Nsightâ¢ VSE Debugging with Other Project Types
 </p>
 <ol class="arabic">
  <li>
   <p>
    In Visual Studio, create a âdummyâ project by going to File &gt; New &gt; Project.
   </p>
   <p class="admonition-title">
    Note
   </p>
   <p>
    For Visual Studio 2019, the new project setup process is slightly different. Please see
    <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/install-setup/index.html#using-vs-2019">
     Using Visual Studio 2019
    </a>
    .
   </p>
  </li>
  <li>
   <p>
    On the node for Visual C++ templates, select Empty Project.
   </p>
   <p>
    Enter the name for your project and click OK.
   </p>
  </li>
  <li>
   <p>
    Select the projectâs Nsight User Properties to edit the default settings.
   </p>
   <p>
    As an alternative, you can also go to the
    Project
    menu &gt;
    Nsight User Properties
    .
   </p>
  </li>
  <li>
   <p>
    Select Launch external program, and enter the path to the external program for the application that is to be debugged.
   </p>
  </li>
  <li>
   <p>
    Configure any other launch options or file synchronization settings which may be necessary for your particular debugging environment.
   </p>
   <p>
    (For assistance, refer to
    <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/install-setup/index.html#host">
     Host Basics
    </a>
    and
    <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/install-setup/index.html#synchronization">
     Synchronization
    </a>
    .)
   </p>
  </li>
  <li>
   <p>
    Click OK to save your settings.
   </p>
  </li>
  <li>
   <p>
    You can now begin debugging your application with NVIDIA Nsightâ¢ VSE.
   </p>
   <p>
    To do so, go to the Nsight menu or right-click on your project, then select the appropriate activity [Start CUDA Debugging (Legacy/Next-Gen), Start Graphics Debugging, etc.].
   </p>
  </li>
 </ol>
 <h2>
  Timeout Detection &amp; Recovery (TDR)
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#timeout-detection-recovery-tdr" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  TDR stands for Timeout Detection and Recovery. This is a feature of the Windows operating system which detects response problems from a graphics card, and recovers to a functional desktop by resetting the card. If the operating system does not receive a response from a graphics card within a certain amount of time (default is 2 seconds), the operating system resets the graphics card.
 </p>
 <p>
  Before TDR existed, problems of this nature would have resulted in a system freeze and required a reboot of the operating system. If TDR is enabled and you see the TDR error message, âDisplay driver stopped responding and has recovered,â this means that the Windows operating system reset the display driver.
 </p>
 <p>
  There are three different possible debugging configurations:
 </p>
 <ul class="simple">
  <li>
   <p>
    Local debugging with a single GPU,
   </p>
  </li>
  <li>
   <p>
    Local debugging with multiple GPUs, or
   </p>
  </li>
  <li>
   <p>
    Remote debugging.
   </p>
  </li>
 </ul>
 <p>
  Choose the one that most closely reflects your NVIDIA Nsightâ¢ VSE setup:
 </p>
 <h3>
  Local Debugging with a Single GPU
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#local-debugging-with-a-single-gpu" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Disabling TDR removes a valuable layer of protection, so it is generally recommended that you keep it enabled.
 </p>
 <p>
  However, setting the TDR delay too low can cause the debugger to fail for one of two reasons:
 </p>
 <ul class="simple">
  <li>
   <p>
    Debugging on some GPUs will fails with a TDR delay of less than 10 seconds.
   </p>
  </li>
  <li>
   <p>
    Debug builds of CUDA kernels run more slowly and may intrinsically require additional time to complete. With too low of a TDR delay, the kernels may not have enough time to complete.
   </p>
  </li>
 </ul>
 <p>
  Therefore, if you are using local debugging with a single GPU, itâs recommended that you leave TDR enabled, and set the delay to 10 seconds.
 </p>
 <p>
  To enable TDR and change the delay, do the following:
 </p>
 <ol class="arabic">
  <li>
   <p>
    Right-click the Nsight Monitor icon in the system tray.
   </p>
  </li>
  <li>
   <p>
    Select Options.
   </p>
  </li>
  <li>
   <p>
    In the Options window on the
    General
    tab, set WDDM TDR enabled to True.
   </p>
  </li>
  <li>
   <p>
    Change the
    WDDM TDRÂ Delay
    from the default setting to
    10
    .
   </p>
  </li>
 </ol>
 <h3>
  Local Debugging with Multiple GPUs or Remote Debugging
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#local-debugging-with-multiple-gpus-or-remote-debugging" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  When using either a local debugging configuration with multiple GPUs, or a remote debugging configuration, itâs important to disable TDR. This is because with most CUDAÂ applications, a TDR means that any debugging operation after the TDR will fail. You will not be able to step, set breakpoints, view variables, etc. The application will receive a grid launch failure, and the
  <span class="pre">
   CUcontext
  </span>
  will begin to report errors.
 </p>
 <p>
  Having TDR enabled can interfere with GPU debugging because the graphics card is perceived by the operating system as unresponsive when the execution of a target application is paused or when the debugger is performing certain operations.
 </p>
 <p>
  To disable TDR, do the following:
 </p>
 <ol class="arabic">
  <li>
   <p>
    Right-click the Nsight Monitor icon in the system tray.
   </p>
  </li>
  <li>
   <p>
    Select Options.
   </p>
  </li>
  <li>
   <p>
    In the Options window on the
    General
    tab, set
    WDDM TDR enabled
    to
    False
    .
   </p>
  </li>
 </ol>
 <p>
  For more information about TDR, see:
 </p>
 <p>
  <a class="reference external" href="http://www.microsoft.com/whdc/device/display/wddm_timeout.mspx">
   http://www.microsoft.com/whdc/device/display/wddm_timeout.mspx
  </a>
 </p>
 <h2>
  Tesla Compute Cluster (TCC)
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#tesla-compute-cluster" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  For most GPUs, you do not have to do anything specific in NVIDIA Nsightâ¢ VSE to enable debugging on a Tesla Compute Cluster (TCC) device. You donât have to modify your Visual Studio project or enable any specific setting. The TCC device simply shows up as a standard CUDA device. For some GPUs, the default mode is not TCC. See below for more information.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Do not kill a process that is executing code on a TCC device and paused on a breakpoint, except through the normal Stop Debugging command (SHIFT+F5) in Visual Studio. Abnormal termination of a target application that is paused during a debugging session on a TCC device results in unpredictable behavior. It causes future calls to
  <span class="pre">
   cuCtxInit()
  </span>
  to hang indefinitely, even though the killed process seems to terminate normally. The only way to recover is to reboot the target machine. See the latest release notes to learn about any changes in this behavior.
 </p>
 <h3>
  Limitations
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#limitations" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  There are 2 main limitations you will encounter when debugging code running on a TCC device:
 </p>
 <ul class="simple">
  <li>
   <p>
    There is no Vulkan, OpenGL, or Direct3D interop support.
   </p>
  </li>
  <li>
   <p>
    You cannot have a display connected to an adapter when the underlying device is running in TCC mode. Physically connecting a display causes unpredictable behavior. Windows detects the TCC adapter as a âStandard VGAâ device (which it is not), connected to the existing NVIDIA device. The unpredictable behavior results in having to reboot the entire system.
   </p>
  </li>
 </ul>
 <h3>
  Setting TCC Mode for Tesla Products
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#setting-tcc-mode-for-tesla-products" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  NVIDIA GPUs can exist in one of three modes:
  TCC
  ,
  MCDM
  , or
  WDDM
  . TCC and MCDM modes disables Windows graphics and is used in
  <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/install-setup/index.html#setup-local-headless-gpu-debugging">
   headless
  </a>
  configurations, whereas WDDM mode is required for Windows graphics. NVIDIA GPUs also come in three classes:
 </p>
 <ul class="simple">
  <li>
   <p>
    GeForce
    â typically defaults to WDDM mode; used in gaming graphics.
   </p>
  </li>
  <li>
   <p>
    Quadro
    â typically defaults to WDDM mode, but often used as TCC compute devices as well.
   </p>
  </li>
  <li>
   <p>
    Tesla
    â typically defaults to TCC mode, although MCDM is also available. Current drivers require a GRID license to enable WDDM on Tesla devices.
   </p>
  </li>
 </ul>
 <p>
  NVIDIA Nsightâ¢ VSE Â
  <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/cuda-debugger/index.html">
   Compute debugging
  </a>
  does
  not
  require a specific driver mode.
 </p>
 <p>
  The NVIDIA Control Panel will show you what mode your GPUs are in; alternately, you can use the
  <span class="pre">
   nvidia-smi
  </span>
  command to generate a table that will display your GPUs and what mode they are using.
 </p>
 <p>
  To change the TCC mode, use the NVIDIA SMI utility. This is located by default at
  <span class="pre">
   C:\Program
  </span>
  <span class="pre">
   Files\NVIDIA
  </span>
  <span class="pre">
   Corporation\NVSMI
  </span>
  . Use the following syntax to change the TCC mode:
 </p>
 <pre><span class="n">nvidia</span><span class="o">-</span><span class="n">smi</span><span class="o">-</span><span class="n">g</span><span class="p">{</span><span class="n">GPU_ID</span><span class="p">}</span><span class="o">-</span><span class="n">dm</span><span class="p">{</span><span class="mi">0</span><span class="o">|</span><span class="mi">1</span><span class="o">|</span><span class="mi">2</span><span class="p">}</span>
</pre>
 <p>
  0 = WDDM
 </p>
 <p>
  1 = TCC
 </p>
 <p>
  2 = MCDM
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  NVIDIA Nsight Visual Studio Edition supports CUDA debugging in
 </p>
 <ul class="simple">
  <li>
   <p>
    MCDM mode:
   </p>
   <ul>
    <li>
     <p>
      On Pascal Family GPUs (and above)Â using the Next-Gen CUDAÂ Debugger.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    TCC mode:
   </p>
   <ul>
    <li>
     <p>
      On Pascal Family GPUs (and above)Â using the Next-Gen CUDAÂ Debugger.
     </p>
    </li>
    <li>
     <p>
      On Kepler Family GPUs (SM_50 only) using the Legacy CUDAÂ Debugger.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    WDDM mode:
   </p>
   <ul>
    <li>
     <p>
      On Pascal Family GPUs (and above)Â using the Next-Gen CUDAÂ Debugger.
     </p>
    </li>
    <li>
     <p>
      On Kepler Family GPUs (SM_50 only) using the Legacy CUDAÂ Debugger.
     </p>
    </li>
   </ul>
  </li>
 </ul>
 <p>
  See
  <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/install-setup/index.html#system-requirements-compute-debugger-configs">
   Compute Debugger Supported Configurations
  </a>
  for more details.
 </p>
 <h3>
  About TCC
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#about-tcc" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The TCC (Tesla Compute Cluster) driver is a Windows driver that supports CUDA C/C++ applications. The driver enables remote desktop services, and reduces the CUDA kernel launch overhead on Windows. Note that the TCC driver disables graphics on the Tesla products.
 </p>
 <p>
  The main purpose of TCC and the Tesla products is to aid applications that use CUDA to perform simulations, and large scale calculations (especially floating-point calculations), such as image generation for professional use and scientific fields of study.
 </p>
 <p>
  The benefits of using the Tesla Compute Cluster driver package:
 </p>
 <ul class="simple">
  <li>
   <p>
    TCC drivers make it possible to use NVIDIA GPUs in nodes with nonâNVIDIA integrated graphics.
   </p>
  </li>
  <li>
   <p>
    NVIDIA GPUs on systems running the TCC drivers will be available via Remote Desktop, both directly and via cluster management systems that rely on Remote Desktop.
   </p>
  </li>
  <li>
   <p>
    NVIDIA GPUs will be available to applications running as a Windows service (in Session 0) on systems running the TCC drivers.
   </p>
  </li>
 </ul>
 <p>
  The TCC driver was specifically designed to be used with Microsoftâs Windows HPC Server 2008. However, NVIDIAâs TCC driver can be used with operating systems other than Windows HPC Server 2008. The NVIDIA TCC driver does not have the same pinned allocation limits or memory fragmentation behavior as WDDM. You can mix TCC drivers with XP-style display drivers.
 </p>
 <p>
  For more information about supported operating systems, and compatibility with other NVIDIA drivers, refer to the documentation on NVIDIA Tesla:
 </p>
 <p>
  <a class="reference external" href="http://www.nvidia.com/object/tesla_computing_solutions.html">
   http://www.nvidia.com/object/tesla_computing_solutions.html
  </a>
 </p>
 <p>
  For more information about NVIDIA hardware compatibility on Windows HPC Server 2008, see:
 </p>
 <p>
  <a class="reference external" href="http://technet.microsoft.com/en-us/library/ff793340_ws.10_.aspx">
   http://technet.microsoft.com/en-us/library/ff793340_ws.10_.aspx
  </a>
 </p>
 <p>
  To search the NVIDIA web site for Tesla drivers, see:
 </p>
 <p>
  <a class="reference external" href="http://www.nvidia.com/download/index.aspx">
   http://www.nvidia.com/download/index.aspx
  </a>
 </p>
 <h2>
  Environment Variables
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#environment-variables" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  NVIDIA Nsightâ¢ VSE creates certain system environment variables that are useful when defining build properties. These are separate from the build macros defined by Visual Studio.
 </p>
 <p>
  For more information about the list of macros available for build commands such as
  <span class="pre">
   $(OutDir)
  </span>
  and
  <span class="pre">
   $(TargetName)
  </span>
  , see the MSDN article on
  <a class="reference external" href="http://msdn.microsoft.com/en-us/library/c02as0cs.aspx">
   Macros for Build Commands and Properties
  </a>
  .
 </p>
 <p>
  To see the environment variables related to CUDA paths:
 </p>
 <ol class="arabic">
  <li>
   <p>
    Open the Advanced System Settings panel in the Control Panel.
   </p>
   <ol class="loweralpha">
    <li>
     <p>
      Start &gt; Control Panel.
     </p>
    </li>
    <li>
     <p>
      Select System and Maintenance.
     </p>
    </li>
    <li>
     <p>
      Select System.
     </p>
    </li>
    <li>
     <p>
      In the left-hand pane, select Advanced system settings.
     </p>
     <p>
      The System Properties window opens.
     </p>
    </li>
   </ol>
  </li>
  <li>
   <p>
    Click the Environment Variables button.
   </p>
  </li>
  <li>
   <p>
    Under System Variables, scroll to see the variables that have CUDA in the name.
   </p>
  </li>
  <li>
   <p>
    Click OK to close the window.
   </p>
  </li>
 </ol>
 <p>
  The Environment Variables window lists the system environment variables that begin with CUDA. These are variables that you can use in the project Property Pages to control various aspects of the build process.
 </p>
 <h2>
  DirectCompute
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#directcompute" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Microsoft DirectCompute is an application programming interface (API) that supports general-purpose computing on graphics processing units on Microsoft Windows 7 and Windows 8.
 </p>
 <p>
  NVIDIAâs Direct3D 11 GPUs support DirectCompute. This allows developers to harness the massive parallel computing power of NVIDIA GPUs to create compelling computing applications in consumer and professional markets.
 </p>
 <p>
  See Also
 </p>
 <p>
  Reference
 </p>
 <p>
  <a class="reference external" href="https://developer.nvidia.com/directcompute">
   CUDA Zone - DirectCompute
  </a>
 </p>
 <h2>
  Troubleshooting NVIDIA Nsight Visual Studio Edition
  <a class="headerlink" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#troubleshooting-nvidia-nsight-visual-studio-edition" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Problem:
 </p>
 <p>
  When I convert a project to a newer version of Visual Studio, I get build errors.
 </p>
 <p>
  Resolution:
 </p>
 <p>
  For more information on how to convert a project, see the
  <a class="reference external" href="http://devtalk.nvidia.com/">
   NVIDIA Developer Forums
  </a>
  .
 </p>
 <p>
  Problem:
 </p>
 <p>
  How do I get a diagnostic log(s) of the NVIDIA Nsightâ¢ VSE host and monitor for troubleshooting purposes?
 </p>
 <p>
  Resolution:
 </p>
 <ol class="arabic">
  <li>
   <p>
    Close both Visual Studio and the Nsight Monitor.
   </p>
  </li>
  <li>
   <p>
    On both the host and target machines, go to the following locations, and delete any existing files:
   </p>
   <pre>%AppData%\NVIDIA Corporation\Nsight\Vsip\1.0\Logs
%AppData%\NVIDIA Corporation\Nsight\Monitor\1.0\Logs
</pre>
  </li>
  <li>
   <p>
    Edit
    <span class="pre">
     Nvda.Diagnostics.nlog
    </span>
    as follows.
   </p>
   <ol class="loweralpha">
    <li>
     <p>
      On the host machine:
     </p>
     <p>
      C:\Program Files (x86)\NVIDIA Corporation\Nsight Visual Studio EditionÂ 2023.2\Host\Common\Configurations
     </p>
    </li>
    <li>
     <p>
      On the target machine:
     </p>
     <p>
      C:\Program Files (x86)\NVIDIA Corporation\Nsight Visual Studio EditionÂ 2023.2\Monitor\Common\Configurations
     </p>
    </li>
   </ol>
  </li>
  <li>
   <p>
    Go to the last logger at the bottom of the file:
   </p>
   <pre>&lt;logger name="*" minlevel="Error" writeTo="file-high-severity"  /&gt;.
</pre>
  </li>
  <li>
   <p>
    Change the
    <span class="pre">
     minlevel
    </span>
    attribute value from
    <span class="pre">
     "Error"
    </span>
    to
    <span class="pre">
     "Trace"
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    Save the file.
   </p>
  </li>
  <li>
   <p>
    Reproduce the problem, and send the following generated logs:
   </p>
   <pre>%AppData%\NVIDIA Corporation\Nsight\Vsip\1.0\Logs
%AppData%\NVIDIA  Corporation\Nsight\Monitor\1.0\Logs
</pre>
  </li>
 </ol>
 <p>
  Problem:
 </p>
 <p>
  When breakpoints are set in source code, the CUDA Debugger pauses execution at locations unrelated to the breakpoints.
 </p>
 <p>
  This can happen when more than one
  <span class="pre">
   __global__
  </span>
  function (kernel) makes a call to a
  <span class="pre">
   __device__
  </span>
  function within a single module, and both of the following are true:
 </p>
 <ul class="simple">
  <li>
   <p>
    the
    <span class="pre">
     __device__
    </span>
    function is not inlined.
   </p>
  </li>
  <li>
   <p>
    the different kernels call the exact same
    <span class="pre">
     __device__
    </span>
    function.
   </p>
  </li>
 </ul>
 <p>
  Resolution:
 </p>
 <p>
  There are a couple of approaches you can take to work around this issue:
 </p>
 <ol class="arabic">
  <li>
   <p>
    Force the
    <span class="pre">
     __device__
    </span>
    function to be inlined by applying the
    <span class="pre">
     __forceinline__
    </span>
    keyword to the
    <span class="pre">
     __device__
    </span>
    function. Note that using the
    <span class="pre">
     inline
    </span>
    keyword does notÂ force inlining in debug builds.
   </p>
  </li>
  <li>
   <p>
    Reorganize source code so that there is only one
    <span class="pre">
     __global__
    </span>
    function for each instance of the
    <span class="pre">
     __device__
    </span>
    function. This means that each
    <span class="pre">
     .cu
    </span>
    file that is compiled with the NVIDIA CUDA Compiler (nvcc.exe) should contain no more than one
    <span class="pre">
     __global__
    </span>
    function. This works for both Driver API and CUDART applications. Be aware that there are other potential issues with this approach:
   </p>
   <ul>
    <li>
     <p>
      Recommended: move commonly used
      <span class="pre">
       __device__
      </span>
      functions to common header files. Use the
      <span class="pre">
       #include
      </span>
      statement to include the
      <span class="pre">
       __device__
      </span>
      function in each
      <span class="pre">
       .cu
      </span>
      file containing a
      <span class="pre">
       __global__
      </span>
      function.
     </p>
    </li>
    <li>
     <p>
      Potential issue: If your source code contains declarations of a global variable in the following style:
     </p>
     <pre><span class="n">__device__</span><span class="kt">int</span><span class="n">x</span><span class="p">;</span>
</pre>
     <p>
      and that variable is used by multiple
      <span class="pre">
       __global__
      </span>
      functions, then using multiple files to make multiple calls to the
      <span class="pre">
       __global__
      </span>
      function is not a trivial work-around. In this case, we recommend eliminating global variables that are declared in that style from the source code, and making them kernel parameters instead.
     </p>
    </li>
    <li>
     <p>
      Potential issue: Each
      <span class="pre">
       __constant__
      </span>
      variable is associated with one CUDA module (a compiled
      <span class="pre">
       .cu
      </span>
      file).
     </p>
     <p>
      If your source code is written in a way that multiple kernels depend on the same
      <span class="pre">
       __constant__
      </span>
      variable, and the host code side of your application dynamically updates that variable, then you will need some broader changes to your source code:
     </p>
     <ul class="simple">
      <li>
       <p>
        For a CUDART application, when copying the
        <span class="pre">
         __constant__
        </span>
        variable into each
        <span class="pre">
         .cu
        </span>
        file, give each variable a different name.
       </p>
      </li>
      <li>
       <p>
        Any host code that was updating the previously single instance of the variable must now update all the instances.
       </p>
      </li>
     </ul>
    </li>
   </ul>
  </li>
 </ol>
 <p>
  Problem:
 </p>
 <p>
  I get warnings that 64-bit injection and/or 32-bit injection is not present.
 </p>
 <p>
  Resolution:
 </p>
 <p>
  The Nsight Monitor checks for 64-bit versions of the CUDA injection. This means that you can get warnings if 64-bit and/or 32-bit injection is not present. If this happens, re-install the tools.
 </p>
 <p>
  Problem:
 </p>
 <p>
  My machine hangs when I use the CUDA Debugger locally on a single machine with 2 GPUs on it.
 </p>
 <p>
  Resolution:
 </p>
 <p>
  There are several possible issues that can cause a machine to hang when locally debugging on two GPUs with the NVIDIA Nsightâ¢ VSE tools.
 </p>
 <ul>
  <li>
   <p>
    Make sure that your TDR settings have been configured correctly. For more information, see
    <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/reference/index.html#timeout-detection-recovery">
     Timeout Detection and Recovery
    </a>
    .
   </p>
   <p>
    We recommend not having a display attached or a desktop running on the GPU on which you are debugging CUDA code, as having concurrent activities on a GPU can cause machine hangs. See
    <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/install-setup/index.html#setup-local-headless-gpu-debugging">
     How To: Setup Local Headless GPU Debugging
    </a>
    for more information.
   </p>
  </li>
 </ul>
 <p>
  Problem:
 </p>
 <p>
  The GPU debugger hangs when I also use the CPU debugger.
 </p>
 <p>
  Resolution:
 </p>
 <p>
  Never use the same Visual Studio instance to run both the CUDA Debugger and the CPU debugger.
 </p>
 <p>
  In general, make sure you only use either CUDA Debugger or CPU debugger, not both.Â  Attaching the CPU debugger and hitting a CPU breakpoint during a CUDA debugging session will cause the CUDA Debugger to hang (until you resume the CPU process).
 </p>
 <p>
  If you are careful, you can attach two separate Visual Studio instances (one CUDA, one CPU).Â  While you are stopped in CPU code, the CUDA Debugger will hang.Â  Once you resume the CPU code, CUDA Debugger will come back alive.
 </p>
 <p>
  Problem:
 </p>
 <p>
  I am unable to set and hit a breakpoint in my CUDA code.
 </p>
 <p>
  Resolution:
 </p>
 <p>
  Make sure to use the driver version specified in the release notes. This is the most common reason that breakpoints do not work. The driver must be installed on the machine where your application code runs.
 </p>
 <p>
  Also, make sure your project uses a compatible CUDA toolkit. A compatible version of the CUDA toolkit generates symbolics information that allows the CUDA Debugger to properly debug your code when you use the
  <span class="pre">
   -G0
  </span>
  flag on the
  <span class="pre">
   nvcc
  </span>
  command line. If you are using the CUDA Driver API, make sure that there are
  <span class="pre">
   .cubin.elf.o
  </span>
  files alongside each of your compiled
  <span class="pre">
   .cubin
  </span>
  files in the build output directory for your project. Projects using the CUDAÂ Runtime API have the symbolics information embedded in the object file itself.
 </p>
 <p>
  Problem:
 </p>
 <p>
  I get the following error message:
 </p>
 <pre><span class="n">Local</span><span class="n">debugging</span><span class="n">failed</span><span class="p">.</span><span class="n">Nsight</span><span class="n">is</span><span class="n">incompatible</span><span class="n">with</span><span class="n">WPF</span><span class="n">acceleration</span><span class="p">.</span><span class="n">Please</span><span class="n">see</span><span class="n">documentation</span><span class="n">about</span><span class="n">WPF</span><span class="n">acceleration</span><span class="p">.</span><span class="n">Run</span><span class="n">theDisableWpfHardwareAcceleration</span><span class="p">.</span><span class="n">reg</span><span class="n">in</span><span class="n">your</span><span class="n">Nsight</span><span class="n">installation</span><span class="p">.</span>
</pre>
 <p>
  Resolution:
 </p>
 <p>
  Disable WPFÂ D3D acceleration. For more information, see
  <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/install-setup/index.html#setup-local-headless-gpu-debugging">
   Setup Local Debugging
  </a>
  .
 </p>
 <p>
  If one or more applications are running with WPF hardware acceleration and you run the
  <span class="pre">
   .reg
  </span>
  file, you could still have issues until those applications are restarted.Â If you are performing local debugging, this includes the NsightÂ Monitor - you need to restart it seeing as it too is a WPF application.
 </p>
 <p>
  Problem:
 </p>
 <p>
  My program ignores breakpoints set in CPU code when I debug a program by choosing Start CUDA Debugging from the Nsight menu.
 </p>
 <p>
  Resolution:
 </p>
 <p>
  The CUDA Debugger ignores breakpoints set in CPU code as it does not currently support debugging x86 or other CPU code.
 </p>
 <p>
  Problem:
 </p>
 <p>
  When I hit a CUDA breakpoint, I only break once on thread (0, 0, 0) in my CUDA kernel. If I hit Continue (F5), it never breaks again and the entire launch completes.
 </p>
 <p>
  Resolution:
 </p>
 <p>
  The default behavior of the CUDA Debugger is to break unconditionally on the first thread of a kernel. After that, the breakpoints have an implicit conditional based on the CUDA Focus Picker. If you would like to break on a different thread, use the CUDA Focus Picker to switch focus to the desired thread or set a conditional breakpoint so that the debugger stops only on the thread you specify. For more information on setting the conditional breakpoint, see
  <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/cuda-inspect-state/index.html#specify-debugger-context">
   How To: Specify Debugger Context
  </a>
  and
  <a class="reference external" href="https://docs.nvidia.com/nsight-visual-studio-edition/cuda-control-gpu-execution/index.html#breakpoints">
   How To: Set GPU Breakpoints
  </a>
  . After you switch focus, the CUDA Debugger maintains the focus and breaks on breakpoints only in that thread for the duration of the kernel launch.
 </p>
 <p>
  Problem:
 </p>
 <p>
  I encounter an error when trying to copy and paste my shader code.
 </p>
 <p>
  Resolution:
 </p>
 <p>
  This can happen in Visual Studio 2012, when the Productivity Power Tools extension is being used. Disable the HTML Copy option, and you should be able to copy and paste normally in the shader editor.
 </p>
 <p class="rubric-h1 rubric">
  Notices
 </p>
 <p class="rubric-h2 rubric">
  Notice
 </p>
 <p>
  ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE.
 </p>
 <p>
  Information furnished is believed to be accurate and reliable. However, NVIDIA Corporation assumes no responsibility for the consequences of use of such information or for any infringement of patents or other rights of third parties that may result from its use. No license is granted by implication of otherwise under any patent rights of NVIDIA Corporation. Specifications mentioned in this publication are subject to change without notice. This publication supersedes and replaces all other information previously supplied. NVIDIA Corporation products are not authorized as critical components in life support devices or systems without express written approval of NVIDIA Corporation.
 </p>
 <p class="rubric-h2 rubric">
  Trademarks
 </p>
 <p>
  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.
 </p>
 <p>
  © Copyright 2018-2024, NVIDIA Corporation &amp; Affiliates. All rights reserved.
  <span class="lastupdated">
   Last updated on Jun 03, 2024.
  </span>
 </p>
</body>
</body></html>