<html><head><title>CUDA-GDB</title></head><body><body class="wy-body-for-nav">
 <a href="https://docs.nvidia.com/cuda/cuda-gdb/contents.html">
 </a>
 <ul class="current">
  <li class="toctree-l1 current">
   <a class="current reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html">
    1. Introduction
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#what-is-cuda-gdb">
      1.1. What is CUDA-GDB?
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#supported-features">
      1.2. Supported Features
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#about-this-document">
      1.3. About This Document
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#release-notes">
    2. Release Notes
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#getting-started">
    3. Getting Started
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#setting-up-the-debugger-environment">
      3.1. Setting Up the Debugger Environment
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#temporary-directory">
        3.1.1. Temporary Directory
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#using-the-cuda-gdb-debugger-on-jetson-and-drive-tegra-devices">
        3.1.2. Using the CUDA-GDB debugger on Jetson and Drive Tegra devices
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#compiling-the-application">
      3.2. Compiling the Application
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#debug-compilation">
        3.2.1. Debug Compilation
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#compilation-with-linenumber-information">
        3.2.2. Compilation With Linenumber Information
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#compiling-for-specific-gpu-architectures">
        3.2.3. Compiling For Specific GPU architectures
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#using-the-debugger">
      3.3. Using the Debugger
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#single-gpu-debugging-with-the-desktop-manager-running">
        3.3.1. Single-GPU Debugging with the Desktop Manager Running
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#multi-gpu-debugging">
        3.3.2. Multi-GPU Debugging
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#remote-debugging">
        3.3.3. Remote Debugging
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#multiple-debuggers">
        3.3.4. Multiple Debuggers
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#attaching-detaching">
        3.3.5. Attaching/Detaching
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#cuda-gdb-extensions">
    4. CUDA-GDB Extensions
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#command-naming-convention">
      4.1. Command Naming Convention
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#getting-help">
      4.2. Getting Help
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#initialization-file">
      4.3. Initialization File
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#gui-integration">
      4.4. GUI Integration
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#gpu-core-dump-support">
      4.5. GPU core dump support
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#kernel-focus">
    5. Kernel Focus
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#software-coordinates-vs-hardware-coordinates">
      5.1. Software Coordinates vs. Hardware Coordinates
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#current-focus">
      5.2. Current Focus
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#switching-focus">
      5.3. Switching Focus
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#program-execution">
    6. Program Execution
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#interrupting-the-application">
      6.1. Interrupting the Application
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#single-stepping">
      6.2. Single Stepping
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#breakpoints-and-watchpoints">
    7. Breakpoints and Watchpoints
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#symbolic-breakpoints">
      7.1. Symbolic Breakpoints
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#line-breakpoints">
      7.2. Line Breakpoints
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#address-breakpoints">
      7.3. Address Breakpoints
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#kernel-entry-breakpoints">
      7.4. Kernel Entry Breakpoints
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#conditional-breakpoints">
      7.5. Conditional Breakpoints
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#watchpoints">
      7.6. Watchpoints
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#inspecting-program-state">
    8. Inspecting Program State
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#memory-and-variables">
      8.1. Memory and Variables
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#variable-storage-and-accessibility">
      8.2. Variable Storage and Accessibility
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-commands">
      8.3. Info CUDA Commands
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-devices">
        8.3.1. info cuda devices
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-sms">
        8.3.2. info cuda sms
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-warps">
        8.3.3. info cuda warps
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-lanes">
        8.3.4. info cuda lanes
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-kernels">
        8.3.5. info cuda kernels
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-blocks">
        8.3.6. info cuda blocks
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-threads">
        8.3.7. info cuda threads
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-launch-trace">
        8.3.8. info cuda launch trace
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-launch-children">
        8.3.9. info cuda launch children
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-contexts">
        8.3.10. info cuda contexts
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-managed">
        8.3.11. info cuda managed
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#disassembly">
      8.4. Disassembly
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#registers">
      8.5. Registers
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#const-banks">
      8.6. Const banks
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#event-notifications">
    9. Event Notifications
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#context-events">
      9.1. Context Events
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#kernel-events">
      9.2. Kernel Events
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#automatic-error-checking">
    10. Automatic Error Checking
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#checking-api-errors">
      10.1. Checking API Errors
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#gpu-error-reporting">
      10.2. GPU Error Reporting
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#autostep">
      10.3. Autostep
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#walk-through-examples">
    11. Walk-Through Examples
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#example-bitreverse">
      11.1. Example: bitreverse
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#walking-through-the-code">
        11.1.1. Walking through the Code
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#example-autostep">
      11.2. Example: autostep
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#debugging-with-autosteps">
        11.2.1. Debugging with Autosteps
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#example-mpi-cuda-application">
      11.3. Example: MPI CUDA Application
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#tips-and-tricks">
    12. Tips and Tricks
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-break-on-launch">
      12.1. set cuda break_on_launch
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-launch-blocking">
      12.2. set cuda launch_blocking
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-notify">
      12.3. set cuda notify
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-ptx-cache">
      12.4. set cuda ptx_cache
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-single-stepping-optimizations">
      12.5. set cuda single_stepping_optimizations
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-thread-selection">
      12.6. set cuda thread_selection
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-value-extrapolation">
      12.7. set cuda value_extrapolation
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#debugging-docker-containers">
      12.8. Debugging Docker Containers
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#switching-to-classic-debugger-backend">
      12.9. Switching to Classic Debugger Backend
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#thread-block-clusters">
      12.10. Thread Block Clusters
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#debugging-optix-rtcore-applications">
      12.11. Debugging OptiX/RTCore applications
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#debugging-on-windows-subsystem-for-linux">
      12.12. Debugging on Windows Subsystem for Linux
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#supported-platforms">
    13. Supported Platforms
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#common-issues-on-supported-operating-systems">
    14. Common Issues on Supported Operating Systems
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#known-issues">
    15. Known Issues
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#notices">
    16. Notices
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#notice">
      16.1. Notice
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#opencl">
      16.2. OpenCL
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#trademarks">
      16.3. Trademarks
     </a>
    </li>
   </ul>
  </li>
 </ul>
 <a href="https://docs.nvidia.com/cuda/cuda-gdb/contents.html">
  CUDA-GDB
 </a>
 <ul class="wy-breadcrumbs">
  <li>
   <a class="icon icon-home" href="https://docs.nvidia.com/cuda/index.html">
   </a>
   Â»
  </li>
  <li>
   <span class="section-number">
    1.
   </span>
   Introduction
  </li>
  <li class="wy-breadcrumbs-aside">
   <span>
    v12.5 |
   </span>
   <a class="reference external" href="https://docs.nvidia.com/cuda/pdf/cuda-gdb.pdf">
    PDF
   </a>
   <span>
    |
   </span>
   <a class="reference external" href="https://developer.nvidia.com/cuda-toolkit-archive">
    Archive
   </a>
   <span>
    Â
   </span>
  </li>
 </ul>
 <p class="rubric-h1 rubric">
  CUDA-GDB
 </p>
 <p>
  The user manual for CUDA-GDB, the NVIDIA tool for debugging CUDA applications on Linux and QNX systems.
 </p>
 <h1>
  <span class="section-number">
   1.
  </span>
  Introduction
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#introduction" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  This document introduces CUDA-GDB, the NVIDIA
  Â®
  CUDA
  Â®
  debugger for Linux and QNX targets.
 </p>
 <h2>
  <span class="section-number">
   1.1.
  </span>
  What is CUDA-GDB?
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#what-is-cuda-gdb" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA-GDB is the NVIDIA tool for debugging CUDA applications running on Linux and QNX. CUDA-GDB is an extension to GDB, the GNU Project debugger. The tool provides developers with a mechanism for debugging CUDA applications running on actual hardware. This enables developers to debug applications without the potential variations introduced by simulation and emulation environments.
 </p>
 <h2>
  <span class="section-number">
   1.2.
  </span>
  Supported Features
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#supported-features" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA-GDB is designed to present the user with a seamless debugging environment that allows simultaneous debugging of both GPU and CPU code within the same application. Just as programming in CUDA C is an extension to C programming, debugging with CUDA-GDB is a natural extension to debugging with GDB. The existing GDB debugging features are inherently present for debugging the host code, and additional features have been provided to support debugging CUDA device code.
 </p>
 <p>
  CUDA-GDB supports debugging C/C++ and Fortran CUDA applications. Fortran debugging support is limited to 64-bit Linux operating system.
 </p>
 <p>
  CUDA-GDB allows the user to set breakpoints, to single-step CUDA applications, and also to inspect and modify the memory and variables of any given thread running on the hardware.
 </p>
 <p>
  CUDA-GDB supports debugging all CUDA applications, whether they use the CUDA driver API, the CUDA runtime API, or both.
 </p>
 <p>
  CUDA-GDB supports debugging kernels that have been compiled for specific CUDA architectures, such as
  <span class="pre">
   sm_75
  </span>
  or
  <span class="pre">
   sm_80
  </span>
  , but also supports debugging kernels compiled at runtime, referred to as just-in-time compilation, or JIT compilation for short.
 </p>
 <h2>
  <span class="section-number">
   1.3.
  </span>
  About This Document
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#about-this-document" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This document is the main documentation for CUDA-GDB and is organized more as a user manual than a reference manual. The rest of the document will describe how to install and use CUDA-GDB to debug CUDA kernels and how to use the new CUDA commands that have been added to GDB. Some walk-through examples are also provided. It is assumed that the user already knows the basic GDB commands used to debug host applications.
 </p>
 <h1>
  <span class="section-number">
   2.
  </span>
  Release Notes
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#release-notes" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  12.5 Release
 </p>
 Updated GDB version
 <ul class="simple">
  <li>
   <p>
    Moved from GDB 13.1 to 13.2.  See
    <a class="reference external" href="https://sourceware.org/git/gitweb.cgi?p=binutils-gdb.git;a=blob_plain;f=gdb/NEWS;hb=gdb-13.2-release">
     GDB 13.2 changes
    </a>
   </p>
  </li>
 </ul>
 Support removal notice
 <ul class="simple">
  <li>
   <p>
    Support for the macOS host client of CUDA-GDB has been removed.
   </p>
  </li>
  <li>
   <p>
    Support for Android has been removed.
   </p>
  </li>
  <li>
   <p>
    Support for Python 3.6 and 3.7 has been removed.
   </p>
  </li>
 </ul>
 Features
 <ul class="simple">
  <li>
   <p>
    Multi build feature that supports native Python and TUI mode across all supported platforms. The cuda-gdb program is now a wrapper script that calls the appropriate cuda-gdb binary. If no supported Python or libncurses is detected, the wrapper will fallback to a cuda-gdb binary with Python and TUI support disabled.
   </p>
  </li>
  <li>
   <p>
    Added support for TUI mode.
   </p>
  </li>
  <li>
   <p>
    Added support for Python 3.10, 3.11, and 3.12.
   </p>
  </li>
  <li>
   <p>
    Added support for detecting and printing exceptions encountered in exited warps. This can occur when debugging an application with optimizations enabled.
   </p>
  </li>
  <li>
   <p>
    Added new gdb/mi command equivalents for info cuda managed and info cuda line.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Fixed issue with printing reference parameter arguments to CUDA functions.
   </p>
  </li>
  <li>
   <p>
    Fixed issues resulting in crashes/errors when reading/writing from/to CUDA generic memory.
   </p>
  </li>
  <li>
   <p>
    Fixed issue where break_on_launch breakpoints were missed for back to back launches of the same kernel.
   </p>
  </li>
  <li>
   <p>
    Fixed issue with incorrectly reporting breakpoint hit events as SIGTRAP when breakpoint is hit in divergent thread.
   </p>
  </li>
  <li>
   <p>
    Fixed crash on QNX when cuda-gdbserver packets arrive out-of-order.
   </p>
  </li>
  <li>
   <p>
    Better error handling when encountering an error when reading CUDA disassembly.
   </p>
  </li>
  <li>
   <p>
    Better exit handling when resuming execution from a fatal CUDA exception.
   </p>
  </li>
 </ul>
 <p>
  12.4 Release
 </p>
 Updated GDB version
 <ul class="simple">
  <li>
   <p>
    Moved from GDB 12.1 to 13.1.  See
    <a class="reference external" href="https://sourceware.org/git/gitweb.cgi?p=binutils-gdb.git;a=blob_plain;f=gdb/NEWS;hb=gdb-13.1-release">
     GDB 13.1 changes
    </a>
   </p>
  </li>
 </ul>
 Android deprecation notice
 <ul class="simple">
  <li>
   <p>
    Support for Android is deprecated. It will be dropped in an upcoming release.
   </p>
  </li>
 </ul>
 Python 3.6 and 3.7 deprecation notice
 <ul class="simple">
  <li>
   <p>
    Support for end-of-life Python 3.6 and 3.7 versions is deprecated. It will be dropped in an upcoming release.
   </p>
  </li>
 </ul>
 Features
 <ul class="simple">
  <li>
   <p>
    Performance enhancement which reduces the number of overall CUDA Debugger API calls.
   </p>
  </li>
  <li>
   <p>
    Performance enhancement when loading large cubins with device functions using a large number of GPU registers.
   </p>
  </li>
  <li>
   <p>
    Performance enhancement when single stepping over warp wide barriers.
   </p>
  </li>
  <li>
   <p>
    Added support for printing values contained within constant banks from GPU core dumps.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Prevented shell expansion on cloned function names when disassembling.
   </p>
  </li>
  <li>
   <p>
    Fixed crash when setting a conditional breakpoint on an unknown symbol name.
   </p>
  </li>
  <li>
   <p>
    Fixed issue with setting a watchpoint on a global pointer.
   </p>
  </li>
  <li>
   <p>
    Fixed assertion in switch_to_thread_1 during inferior teardown.
   </p>
  </li>
  <li>
   <p>
    Fixed attach failures encountered with newer Intel processors.
   </p>
  </li>
  <li>
   <p>
    Refactored the libpython layer to avoid unnecessary gdb code changes.
   </p>
  </li>
 </ul>
 <p>
  12.3 Release
 </p>
 macOS host client deprecation notice
 <ul class="simple">
  <li>
   <p>
    Support for the macOS host client of CUDA-GDB is deprecated. It will be dropped in an upcoming release.
   </p>
  </li>
 </ul>
 Features
 <ul class="simple">
  <li>
   <p>
    Added support for printing values contained within constant banks. New
    <span class="pre">
     $_cuda_const_bank(bank,
    </span>
    <span class="pre">
     offset)
    </span>
    convenience function to obtain address of offset in constant bank. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#const-banks">
     Const Banks
    </a>
    .
   </p>
  </li>
  <li>
   <p>
    Performance enhancements added which reduce overhead when running applications with many CUDA threads.
   </p>
  </li>
  <li>
   <p>
    Added support for CUDA function pointers.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Fixed issue when detaching from attached process that can result in a crash.
   </p>
  </li>
  <li>
   <p>
    Fixed thread ordering issues present with several
    <span class="pre">
     info
    </span>
    <span class="pre">
     cuda
    </span>
    commands.
   </p>
  </li>
  <li>
   <p>
    Added support for opening of GPU core dumps when no valid warps are present on the device.
   </p>
  </li>
  <li>
   <p>
    Added missing DWARF operators used by OptiX.
   </p>
  </li>
  <li>
   <p>
    Fixed issue with parsing CUDA Fortran pointer types.
   </p>
  </li>
  <li>
   <p>
    Fixed issue where CUDA Cluster coordinates were being displayed when no CUDA Cluster was present.
   </p>
  </li>
 </ul>
 <p>
  12.2 Release
 </p>
 Features
 <ul class="simple">
  <li>
   <p>
    Enabled printing of extended error messages when a CUDA Debugger API error is encountered.
   </p>
  </li>
  <li>
   <p>
    Enabled support for debugging with Confidential Compute mode with devtools mode. See
    Confidential Computing Deployment Guide &lt;https://docs.nvidia.com/confidential-computing-deployment-guide.pdf&gt;
    for more details on how to enable the mode.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Fixed â??â appearing in backtrace in OptiX applications.
   </p>
  </li>
  <li>
   <p>
    Host shadow breakpoints are now handled correctly with CUDA Lazy Loading enabled.
   </p>
  </li>
  <li>
   <p>
    Fixed name mangling issue when debugging LLVM generated cubins.
   </p>
  </li>
  <li>
   <p>
    CUDA Cluster coordinates are now displayed correctly.
   </p>
  </li>
  <li>
   <p>
    Fixed issue with attaching to an application using CUDA Lazy Loading when debugging remotely with cuda-gdbserver.
   </p>
  </li>
 </ul>
 <p>
  12.1 Release
 </p>
 CUDA Driver API added for controlling core dump behavior
 <ul class="simple">
  <li>
   <p>
    CTK 12.1 and the r530 driver adds new APIs that allow developers to enable/configure core dump settings programmatically inside their application instead of using environment variables. See the
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__COREDUMP.html#group__CUDA__COREDUMP/">
     CUDA Driver API
    </a>
    manual for more information.
   </p>
  </li>
 </ul>
 Features
 <ul class="simple">
  <li>
   <p>
    Performance improvements for applications using CUDA Lazy Loading.
   </p>
  </li>
  <li>
   <p>
    Added support for ELF cubins with a large number of sections (more than 32767).
   </p>
  </li>
  <li>
   <p>
    Added
    <span class="pre">
     break_on_launch
    </span>
    support for CUDA Graphs.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Removed unsupported set/show gpu_busy_check command.
   </p>
  </li>
  <li>
   <p>
    On QNX fixed an issue where
    <span class="pre">
     info
    </span>
    <span class="pre">
     threads
    </span>
    incorrectly reported dead host threads.
   </p>
  </li>
  <li>
   <p>
    Performance fixes for stepping/next over inline function calls.
   </p>
  </li>
  <li>
   <p>
    Performance fixes when using the
    <span class="pre">
     info
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     managed
    </span>
    command.
   </p>
  </li>
  <li>
   <p>
    Fixed issue when using
    <span class="pre">
     set
    </span>
    <span class="pre">
     follow-fork-mode
    </span>
    <span class="pre">
     child
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    Fixed issue when parsing DWARF for self referential structures.
   </p>
  </li>
 </ul>
 <p>
  12.0 Release
 </p>
 Updated GDB version
 <ul class="simple">
  <li>
   <p>
    Moved from GDB 10.2 to 12.1.  See
    <a class="reference external" href="https://lists.gnu.org/archive/html/info-gnu/2022-05/msg00000.html">
     GDB 12.1 changes
    </a>
   </p>
  </li>
 </ul>
 Texture and surface reference support removed
 <ul class="simple">
  <li>
   <p>
    CTK 12.0 removed support for the Texture and Surface Reference API. Support for printing texture and surface references has been removed.
   </p>
  </li>
 </ul>
 CUDA Memory Checker integration removed
 <ul class="simple">
  <li>
   <p>
    cuda-memcheck has been deprecated in CUDA 11.x and replaced by Compute Sanitizer. The new memory checking workflow is to use Compute Sanitizer from the CLI. This will support coredumps when issues are detected which can then be opened and inspected with CUDA-GDB, similar to other coredumps. Support for cuda-memcheck has been removed with the CUDA 12.0 release.
   </p>
  </li>
 </ul>
 Debugging of applications using CUDA Dynamic Parallelism
 <ul class="simple">
  <li>
   <p>
    Support for debugging applications using CUDA Dynamic Parallelism with the classic debugger backend or on Maxwell GPUs has been removed by default for applications compiled with the CTK 12.0 or newer. Debugging can be accomplished in these situations by recompiling the application while passing the -DCUDA_FORCE_CDP1_IF_SUPPORTED flag.
   </p>
  </li>
 </ul>
 Features
 <ul class="simple">
  <li>
   <p>
    Moved from base gdb/10.2 to gdb/12.1.
   </p>
  </li>
  <li>
   <p>
    Added initial support for Thread Block Clusters.
   </p>
  </li>
  <li>
   <p>
    Changed the default behavior of
    <span class="pre">
     --cuda-use-lockfile
    </span>
    to
    <span class="pre">
     0
    </span>
    . Lockfiles are no longer created by default.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Addressed a hang that could be encountered when stepping through device system calls.
   </p>
  </li>
  <li>
   <p>
    Fixed an overflow issue with displaying active warp masks in
    <span class="pre">
     info
    </span>
    <span class="pre">
     cuda
    </span>
    commands.
   </p>
  </li>
  <li>
   <p>
    Changed internal CUDA Dynamic Parallelsim detection breakpoint to be set only when
    <span class="pre">
     break_on_launch
    </span>
    is enabled.
   </p>
  </li>
  <li>
   <p>
    Removed unsupported
    <span class="pre">
     gpu_busy_check
    </span>
    <span class="pre">
     setting
    </span>
    .
   </p>
  </li>
 </ul>
 <p>
  11.8 Release
 </p>
 Features
 <ul class="simple">
  <li>
   <p>
    Uses the new Unified Debugger (UD) debugging backend by default.
   </p>
  </li>
  <li>
   <p>
    Added support for debugging applications using CUDA Lazy Loading.
   </p>
  </li>
  <li>
   <p>
    Debugger is now enabled on Windows Subsystem for Linux (WSL).
   </p>
  </li>
  <li>
   <p>
    Added basic type support for printing FP8 values (E4M3 and E5M2).
   </p>
  </li>
 </ul>
 Notes
 <ul class="simple">
  <li>
   <p>
    By default, CUDA-GDB will use the new Unified Debugger (UD) backend. This change is transparent to most users using Pascal or newer cards. For Maxwell debugging, or to force the old classic debugging backend, set
    <span class="pre">
     CUDBG_USE_LEGACY_DEBUGGER
    </span>
    to 1 in your environment.
   </p>
  </li>
  <li>
   <p>
    WSL is not supported on GH100 platforms with this release.
   </p>
  </li>
 </ul>
 <p>
  11.7 Release
 </p>
 Features
 <ul class="simple">
  <li>
   <p>
    Major break_on_launch performance enhancements to use new KERNEL_READY notification mechanism instead of setting manual breakpoints.
   </p>
  </li>
  <li>
   <p>
    Refactored info cuda command output to be more condensed. Omitted printing of inactive messages.
   </p>
  </li>
  <li>
   <p>
    Added new
    <span class="pre">
     --disable-python
    </span>
    commandline option to disable Python interpreter dlopen.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Fixed follow-fork child to avoid hanging behavior when both parent and child processes use CUDA.
   </p>
  </li>
  <li>
   <p>
    Added a missing dlsym of a libpython function that was causing errors with some versions of libpython.
   </p>
  </li>
 </ul>
 <p>
  11.6 Release
 </p>
 Updated GDB version
 <ul class="simple">
  <li>
   <p>
    Moved from GDB 10.1 to 10.2. See
    <a class="reference external" href="https://lists.gnu.org/archive/html/info-gnu/2021-04/msg00006.html">
     GDB 10.2 changes
    </a>
   </p>
  </li>
 </ul>
 Features
 <ul class="simple">
  <li>
   <p>
    Added errorpc instruction prefix to the disassembly view. If an error PC is set, prefix the instruction with
    <span class="pre">
     *&gt;
    </span>
    .
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Fixed lineinfo frames to properly display the source filename.
   </p>
  </li>
  <li>
   <p>
    Fixed writing to gpu global memory that was allocated from the host.
   </p>
  </li>
  <li>
   <p>
    Fixed bug that was preventing reading host variables during certain situations.
   </p>
  </li>
  <li>
   <p>
    Fixed cuda-gdbserver init check that prevented QNX from starting.
   </p>
  </li>
 </ul>
 <p>
  11.5 Release
 </p>
 Python 3 support on Jetson and Drive Tegra devices
 <ul class="simple">
  <li>
   <p>
    Support for Python 2 has been removed. CUDA-GDB now supports Python 3 on Jetson and Drive Tegra devices.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Added robust version checks when dynamic loading the libpython3 library. The loaded libpython3 will match the version of the python3 runtime in PATH.
   </p>
  </li>
  <li>
   <p>
    Added support for checking PEP-3149 flag names when loading libpython3 libraries.
   </p>
  </li>
  <li>
   <p>
    Added support for dynamic loading of Python 3.9.
   </p>
  </li>
  <li>
   <p>
    Fixed overriding PYTHONPATH on certain RHEL distributions.
   </p>
  </li>
 </ul>
 <p>
  11.4 Update 1 Release
 </p>
 Known Issues with Fedora 34
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB has known issues with debugging on Fedora 34 and may not be reliable.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Enabled python integration for ppc64le and aarch64 SBSA.
   </p>
  </li>
  <li>
   <p>
    Fixed a performance regression when debugging CUDA apps.
   </p>
  </li>
  <li>
   <p>
    Fixed an intermitent hang with remote debugging via cuda-gdbserver.
   </p>
  </li>
  <li>
   <p>
    Fixed bug with set cuda api_failures stop not triggering breakpoints on failure.
   </p>
  </li>
  <li>
   <p>
    Changed python behavior to dlopen libpython libraries that match the version of the python3 interpreter in PATH.
   </p>
  </li>
  <li>
   <p>
    OpenMP Fortran: Fixed a crash when setting breakpoints inside an OpenMP parallel region.
   </p>
  </li>
  <li>
   <p>
    OpenMP: Better support for printing local variables within a parallel region.
   </p>
  </li>
  <li>
   <p>
    Fortran: Added updated support for printing assumed shape arrays and array slices.
   </p>
  </li>
  <li>
   <p>
    Fixed selecting between host and device thread focus in cudacore debugging.
   </p>
  </li>
  <li>
   <p>
    Various fixes for QNX remote debugging.
   </p>
  </li>
 </ul>
 <p>
  11.4 Release
 </p>
 Updated GDB version
 <ul class="simple">
  <li>
   <p>
    Moved from GDB 8.3 to 10 (based on GDB 10.1).
    <a class="reference external" href="https://lists.gnu.org/archive/html/info-gnu/2020-10/msg00009.html">
     See GDB 10.1 changes
    </a>
   </p>
  </li>
 </ul>
 Python 3 support
 <ul class="simple">
  <li>
   <p>
    Support for Python 2 has been removed. CUDA-GDB now supports Python 3.
   </p>
  </li>
 </ul>
 GDB TUI mode disabled
 <ul class="simple">
  <li>
   <p>
    Support for GDB TUI mode has been disabled. This avoids cross platform dependency mismatches for OSes that lack ncurses-5.5 support.
   </p>
  </li>
 </ul>
 Kepler deprecation notice
 <ul class="simple">
  <li>
   <p>
    Support for Kepler devices (sm_35 and sm_37) is deprecated. Kepler support will be dropped in an upcoming release.
   </p>
  </li>
 </ul>
 Coredump support
 <ul class="simple">
  <li>
   <p>
    Added support for writing coredumps to named pipe using
    <span class="pre">
     CUDA_COREDUMP_FILE
    </span>
    .
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Added support for displaying SIGTRAP exception in coredumps.
   </p>
  </li>
  <li>
   <p>
    Disabled ability to enable scheduler-locking when debugging CUDA targets.
   </p>
  </li>
  <li>
   <p>
    Fixed cuda_register_name and cuda_special_register_name to avoid returning old cached result on error.
   </p>
  </li>
  <li>
   <p>
    Fixed intermitent race condition when creating the CUDA temporary directory.
   </p>
  </li>
  <li>
   <p>
    Various fixes for QNX remote debugging.
   </p>
  </li>
 </ul>
 <p>
  11.3 Release
 </p>
 Python 2 deprecation notice
 <ul class="simple">
  <li>
   <p>
    Support for Python 2 is being deprecated. CUDA-GDB will move to build with Python 3 support in an upcoming release.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Improvements to late attach for remote debugging.
   </p>
  </li>
 </ul>
 <p>
  11.2 Update 1 Release
 </p>
 GDB TUI deprecation notice
 <ul class="simple">
  <li>
   <p>
    Support for GDB TUI mode is being deprecated. This will avoid cross platform dependency mismatches for OSes that lack ncurses-5.5 support. GDB TUI mode will be disabled in an upcoming release.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Fixed printing of strings in the global GPU memory while running CPU code.
   </p>
  </li>
  <li>
   <p>
    Fixed a bug with extended debug_line handling.
   </p>
  </li>
  <li>
   <p>
    Fixed truncation with builtin gdb variables such as
    <span class="pre">
     gridDim
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    Fixed a segfault during startup for DWARF dies missing names.
   </p>
  </li>
  <li>
   <p>
    Fixed a segfault when a CUDA kernel calls
    <span class="pre">
     assert
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    Fixed a bug that prevented debugging cubins &gt; 2GB.
   </p>
  </li>
  <li>
   <p>
    Added minor usability enhancements for cubins compiled with
    <span class="pre">
     --lineinfo
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    Fixed a segfault cause by a pretty printer when using CUDA-GDB within CLion.
   </p>
  </li>
 </ul>
 <p>
  11.1 Release
 </p>
 Updated GDB version
 <ul class="simple">
  <li>
   <p>
    Moved from GDB 8.2 to 8.3 (based on gdb 8.3.1).
    <a class="reference external" href="https://lists.gnu.org/archive/html/info-gnu/2019-09/msg00006.html">
     See gdb 8.3.1 changes
    </a>
   </p>
  </li>
 </ul>
 Support for SM 8.6
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB now supports Devices with Compute Capability 8.6.
   </p>
  </li>
 </ul>
 Updated DWARF parser
 <ul class="simple">
  <li>
   <p>
    Old binaries might need to be recompiled in order to ensure CUDA-specific DWARF info are up to date.
   </p>
  </li>
 </ul>
 Fixed Issues
 <ul class="simple">
  <li>
   <p>
    Fixed an intermittent deadlock when attaching to a running CUDA process.
   </p>
  </li>
  <li>
   <p>
    Fixed a bug when inspecting the value of half registers.
   </p>
  </li>
 </ul>
 <p>
  11.0 Release
 </p>
 Updated GDB version
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB has been upgraded from GDB/7.12 to GDB/8.2.
   </p>
  </li>
 </ul>
 Support for SM8.0
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB now supports Devices with Compute Capability 8.0.
   </p>
  </li>
 </ul>
 Support for Bfloat16
 <ul class="simple">
  <li>
   <p>
    Support for Bfloat16 (__nv_bfloat16) types have been added.
   </p>
  </li>
 </ul>
 MIG support
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB supports MIG. There can be a separate debugger session on each MIG instance. Refer to
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#multiple-debuggers">
     Multiple Debuggers
    </a>
    in case multiple debuggers are needed.
   </p>
  </li>
 </ul>
 Mac support
 <ul class="simple">
  <li>
   <p>
    Debugging on macOS is no longer supported. However, macOS can still be used as the host system (where CUDA-GDB runs under macOS, using cuda-gdbserver to debug a remote target). The download for the macOS version of CUDA-GDB can be found at the following location:
    <a class="reference external" href="https://developer.nvidia.com/nvidia-cuda-toolkit-developer-tools-mac-hosts">
     Download Here
    </a>
   </p>
  </li>
 </ul>
 <p>
  10.1 Release
 </p>
 Enhanced debugging with only linenumber information
 <ul class="simple">
  <li>
   <p>
    Several enhancements were made to CUDA-GDB support for debugging programs compiled with
    <span class="pre">
     -lineinfo
    </span>
    but not with
    <span class="pre">
     -G
    </span>
    . This is intended primarily for debugging programs built with OptiX/RTCore. See also
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#lineinfo-compilation">
     Compilation With Linenumber Information
    </a>
   </p>
  </li>
 </ul>
 <p>
  10.0 Release
 </p>
 Turing Uniform Register Support
 <ul class="simple">
  <li>
   <p>
    Support added for examining and modifying uniform registers on Turing GPUs.
   </p>
  </li>
 </ul>
 <p>
  9.2 Release
 </p>
 User induced core dump support
 <ul class="simple">
  <li>
   <p>
    For the devices that support compute preemption, user induced core dump support is added. New environment variable:
    <span class="pre">
     CUDA_ENABLE_USER_TRIGGERED_COREDUMP
    </span>
    can be used to enable this feature.
   </p>
  </li>
 </ul>
 <p>
  9.1 Release
 </p>
 Volta-MPS core dump support
 <ul class="simple">
  <li>
   <p>
    GPU core dump generation is supported on Volta-MPS.
   </p>
  </li>
 </ul>
 Lightweight GPU core dump support
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB supports reading lightweight GPU core dump files. New environment variable:
    <span class="pre">
     CUDA_ENABLE_LIGHTWEIGHT_COREDUMP
    </span>
    can be used to enable this feature.
   </p>
  </li>
 </ul>
 <p>
  7.0 Release
 </p>
 GPU core dump support
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB supports reading GPU and GPU+CPU core dump files. New environment variables:
    <span class="pre">
     CUDA_ENABLE_COREDUMP_ON_EXCEPTION
    </span>
    ,
    <span class="pre">
     CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION
    </span>
    and
    <span class="pre">
     CUDA_COREDUMP_FILE
    </span>
    can be used to enable and configure this feature.
   </p>
  </li>
 </ul>
 <p>
  6.5 Release
 </p>
 CUDA Fortran Support
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB supports CUDA Fortran debugging on 64-bit Linux operating systems.
   </p>
  </li>
 </ul>
 GDB 7.6.2 Code Base
 <ul class="simple">
  <li>
   <p>
    The code base for CUDA-GDB was upgraded to GDB 7.6.2.
   </p>
  </li>
 </ul>
 <p>
  6.0 Release
 </p>
 Unified Memory Support
 <ul class="simple">
  <li>
   <p>
    Managed variables can be read and written from either a host thread or a device thread. The debugger also annotates memory addresses that reside in managed memory with
    <span class="pre">
     @managed
    </span>
    . The list of statically allocated managed variables can be accessed through a new
    <span class="pre">
     info
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     managed
    </span>
    command.
   </p>
  </li>
 </ul>
 GDB 7.6 Code Base
 <ul class="simple">
  <li>
   <p>
    The code base for CUDA-GDB was upgraded from GDB 7.2 to GDB 7.6.
   </p>
  </li>
 </ul>
 Android Support
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB can now be used to debug Android native applications either locally or remotely.
   </p>
  </li>
 </ul>
 Single-Stepping Optimizations
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB can now use optimized methods to single-step the program, which accelerate single-stepping most of the time. This feature can be disabled by issuing
    <span class="pre">
     set
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     single_stepping_optimizations
    </span>
    <span class="pre">
     off
    </span>
    .
   </p>
  </li>
 </ul>
 Faster Remote Debugging
 <ul class="simple">
  <li>
   <p>
    A lot of effort has gone into making remote debugging considerably faster, up to 2 orders of magnitude. The effort also made local debugging faster.
   </p>
  </li>
 </ul>
 Kernel Entry Breakpoints
 <ul class="simple">
  <li>
   <p>
    The
    <span class="pre">
     set
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     break_on_launch
    </span>
    option will now break on kernels launched from the GPU. Also, enabling this option does not affect kernel launch notifications.
   </p>
  </li>
 </ul>
 Precise Error Attribution
 <ul class="simple">
  <li>
   <p>
    On Maxwell architecture (SM 5.0), the instruction that triggers an exception will be reported accurately. The application keeps making forward progress and the PC at which the debugger stops may not match that address but an extra output message identifies the origin of the exception.
   </p>
  </li>
 </ul>
 Live Range Optimizations
 <ul class="simple">
  <li>
   <p>
    To mitigate the issue of variables not being accessible at some code addresses, the debugger offers two new options. With
    <span class="pre">
     set
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     value_extrapolation
    </span>
    , the latest known value is displayed with
    <span class="pre">
     (possibly)
    </span>
    prefix. With
    <span class="pre">
     set
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     ptx_cache
    </span>
    , the latest known value of the PTX register associated with a source variable is displayed with the
    <span class="pre">
     (cached)
    </span>
    prefix.
   </p>
  </li>
 </ul>
 Event Notifications
 <ul class="simple">
  <li>
   <p>
    Kernel event notifications are not displayed by default any more.
   </p>
  </li>
  <li>
   <p>
    New kernel events verbosity options have been added:
    <span class="pre">
     set
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     kernel_events
    </span>
    ,
    <span class="pre">
     set
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     kernel_events_depth
    </span>
    . Also
    <span class="pre">
     set
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     defer_kernel_launch_notifications
    </span>
    has been deprecated and has no effect any more.
   </p>
  </li>
 </ul>
 <p>
  5.5 Release
 </p>
 Kernel Launch Trace
 <ul class="simple">
  <li>
   <p>
    Two new commands,
    <span class="pre">
     info
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     launch
    </span>
    <span class="pre">
     trace
    </span>
    and
    <span class="pre">
     info
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     launch
    </span>
    <span class="pre">
     children
    </span>
    , are introduced to display the kernel launch trace and the children kernel of a given kernel when Dynamic Parallelism is used.
   </p>
  </li>
 </ul>
 Single-GPU Debugging (BETA)
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB can now be used to debug a CUDA application on the same GPU that is rendering the desktop GUI. This feature also enables debugging of long-running or indefinite CUDA kernels that would otherwise encounter a launch timeout. In addition, multiple CUDA-GDB sessions can debug CUDA applications context-switching on the same GPU. This feature is available on Linux with SM3.5 devices. For information on enabling this, please see
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#single-gpu-debugging-with-the-desktop-manager-running">
     Single-GPU Debugging with the Desktop Manager Running
    </a>
    and
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#multiple-debuggers">
     Multiple Debuggers
    </a>
    .
   </p>
  </li>
 </ul>
 Remote GPU Debugging
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB in conjunction with CUDA-GDBSERVER can now be used to debug a CUDA application running on the remote host.
   </p>
  </li>
 </ul>
 <p>
  5.0 Release
 </p>
 Dynamic Parallelism Support
 <ul class="simple">
  <li>
   <p>
    CUDA-GDB fully supports Dynamic Parallelism, a new feature introduced with the 5.0 toolkit. The debugger is able to track the kernels launched from another kernel and to inspect and modify variables like any other CPU-launched kernel.
   </p>
  </li>
 </ul>
 Attach/Detach
 <ul class="simple">
  <li>
   <p>
    It is now possible to attach to a CUDA application that is already running. It is also possible to detach from the application before letting it run to completion. When attached, all the usual features of the debugger are available to the user, as if the application had been launched from the debugger. This feature is also supported with applications using Dynamic Parallelism.
   </p>
  </li>
 </ul>
 Attach on exception
 <ul class="simple">
  <li>
   <p>
    Using the environment variable
    <span class="pre">
     CUDA_DEVICE_WAITS_ON_EXCEPTION
    </span>
    , the application will run normally until a device exception occurs. Then the application will wait for the debugger to attach itself to it for further debugging.
   </p>
  </li>
 </ul>
 API Error Reporting
 <ul class="simple">
  <li>
   <p>
    Checking the error code of all the CUDA driver API and CUDA runtime API function calls is vital to ensure the correctness of a CUDA application. Now the debugger is able to report, and even stop, when any API call returns an error. See
    <span class="pre">
     set
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     api_failures
    </span>
    for more information.
   </p>
  </li>
 </ul>
 Inlined Subroutine Support
 <ul class="simple">
  <li>
   <p>
    Inlined subroutines are now accessible from the debugger on SM 2.0 and above. The user can inspect the local variables of those subroutines and visit the call frame stack as if the routines were not inlined.
   </p>
  </li>
 </ul>
 <p>
  4.2 Release
 </p>
 Kepler Support
 <ul class="simple">
  <li>
   <p>
    The primary change in Release 4.2 of CUDA-GDB is the addition of support for the new Kepler architecture. There are no other user-visible changes in this release.
   </p>
  </li>
 </ul>
 <p>
  4.1 Release
 </p>
 Source Base Upgraded to GDB 7.2
 <ul class="simple">
  <li>
   <p>
    Until now, CUDA-GDB was based on GDB 6.6 on Linux, and GDB 6.3.5 on Darwin (the Apple branch). Now, both versions of CUDA-GDB are using the same 7.2 source base.
   </p>
  </li>
  <li>
   <p>
    Now CUDA-GDB supports newer versions of GCC (tested up to GCC 4.5), has better support for DWARF3 debug information, and better C++ debugging support.
   </p>
  </li>
 </ul>
 Simultaneous Sessions Support
 <ul class="simple">
  <li>
   <p>
    With the 4.1 release, the single CUDA-GDB process restriction is lifted. Now, multiple CUDA-GDB sessions are allowed to co-exist as long as the GPUs are not shared between the applications being processed. For instance, one CUDA-GDB process can debug process foo using GPU 0 while another CUDA-GDB process debugs process bar using GPU 1. The exclusive of GPUs can be enforced with the CUDA_VISIBLE_DEVICES environment variable.
   </p>
  </li>
 </ul>
 New Autostep Command
 <ul class="simple">
  <li>
   <p>
    A new âautostepâ command was added. The command increases the precision of CUDA exceptions by automatically single-stepping through portions of code.
   </p>
  </li>
  <li>
   <p>
    Under normal execution, the thread and instruction where an exception occurred may be imprecisely reported. However, the exact instruction that generates the exception can be determined if the program is being single-stepped when the exception occurs.
   </p>
  </li>
  <li>
   <p>
    Manually single-stepping through a program is a slow and tedious process. Therefore âautostepâ aides the user by allowing them to specify sections of code where they suspect an exception could occur. These sections are automatically single-stepped through when the program is running, and any exception that occurs within these sections is precisely reported.
   </p>
  </li>
  <li>
   <p>
    Type âhelp autostepâ from CUDA-GDB for the syntax and usage of the command.
   </p>
  </li>
 </ul>
 Multiple Context Support
 <ul class="simple">
  <li>
   <p>
    On GPUs with compute capability of SM20 or higher, debugging multiple contexts on the same GPU is now supported. It was a known limitation until now.
   </p>
  </li>
 </ul>
 Device Assertions Support
 <ul class="simple">
  <li>
   <p>
    The R285 driver released with the 4.1 version of the toolkit supports device assertions. CUDA_GDB supports the assertion call and stops the execution of the application when the assertion is hit. Then the variables and memory can be inspected as usual. The application can also be resumed past the assertion if needed. Use the âset cuda hide_internal_framesâ option to expose/hide the system call frames (hidden by default).
   </p>
  </li>
 </ul>
 Temporary Directory
 <ul class="simple">
  <li>
   <p>
    By default, the debugger API will use /tmp as the directory to store temporary files. To select a different directory, the $TMPDIR environment variable and the API CUDBG_APICLIENT_PID variable must be set.
   </p>
  </li>
 </ul>
 <h1>
  <span class="section-number">
   3.
  </span>
  Getting Started
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#getting-started" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  The CUDA toolkit can be installed by following instructions in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-quick-start-guide/">
   Quick Start Guide
  </a>
  .
 </p>
 <p>
  Further steps should be taken to set up the debugger environment, build the application, and run the debugger.
 </p>
 <h2>
  <span class="section-number">
   3.1.
  </span>
  Setting Up the Debugger Environment
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#setting-up-the-debugger-environment" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <h3>
  <span class="section-number">
   3.1.1.
  </span>
  Temporary Directory
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#temporary-directory" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  By default, CUDA-GDB uses
  <span class="pre">
   /tmp
  </span>
  as the directory to store temporary files. To select a different directory, set the
  <span class="pre">
   $TMPDIR
  </span>
  environment variable.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The user must have write and execute permission to the temporary directory used by CUDA-GDB. Otherwise, the debugger will fail with an internal error.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The value of
  <span class="pre">
   $TMPDIR
  </span>
  must be the same in the environment of the application and CUDA-GDB. If they do not match, CUDA-GDB will fail to attach onto the application process.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Since
  <span class="pre">
   /tmp
  </span>
  folder does not exist on Android device, the
  <span class="pre">
   $TMPDIR
  </span>
  environment variable must be set and point to a user-writeable folder before launching cuda-gdb.
 </p>
 <h3>
  <span class="section-number">
   3.1.2.
  </span>
  Using the CUDA-GDB debugger on Jetson and Drive Tegra devices
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#using-the-cuda-gdb-debugger-on-jetson-and-drive-tegra-devices" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  By default, on Jetson and Drive Tegra devices, GPU debugging is supported only if
  <span class="pre">
   cuda-gdb
  </span>
  and
  <span class="pre">
   cuda-gdbserver
  </span>
  are launched by a user who is a member of the
  debug
  group.
 </p>
 <p>
  To add the current user to the
  debug
  group run this command:
 </p>
 <pre>sudo usermod -a -G debug $USER
</pre>
 <h2>
  <span class="section-number">
   3.2.
  </span>
  Compiling the Application
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#compiling-the-application" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <h3>
  <span class="section-number">
   3.2.1.
  </span>
  Debug Compilation
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#debug-compilation" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  NVCC, the NVIDIA CUDA compiler driver, provides a mechanism for generating the debugging information necessary for CUDA-GDB to work properly. The
  <span class="pre">
   -g
  </span>
  <span class="pre">
   -G
  </span>
  option pair must be passed to NVCC when an application is compiled for ease of debugging with CUDA-GDB; for example,
 </p>
 <pre>nvcc -g -G foo.cu -o foo
</pre>
 <p>
  Using this line to compile the CUDA application
  <span class="pre">
   foo.cu
  </span>
 </p>
 <ul class="simple">
  <li>
   <p>
    forces
    <span class="pre">
     -O0
    </span>
    compilation, with the exception of very limited dead-code eliminations and register-spilling optimizations.
   </p>
  </li>
  <li>
   <p>
    makes the compiler include debug information in the executable
   </p>
  </li>
 </ul>
 <p>
  To compile your CUDA Fortran code with debgging information necessary for CUDA-GDB to work properly, pgfortran, the PGI CUDA Fortran compiler, must be invoked with
  <span class="pre">
   -g
  </span>
  option. Also, for the ease of debugging and forward compatibility with the future GPU architectures, it is recommended to compile the code with
  <span class="pre">
   -Mcuda=nordc
  </span>
  option; for example,
 </p>
 <pre>pgfortran -g -Mcuda=nordc foo.cuf -o foo
</pre>
 <p>
  For more information about the available compilation flags, please consult the PGI compiler documentation.
 </p>
 <h3>
  <span class="section-number">
   3.2.2.
  </span>
  Compilation With Linenumber Information
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#compilation-with-linenumber-information" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Several enhancements were made to cuda-gdbâs support for debugging programs compiled with
  <span class="pre">
   -lineinfo
  </span>
  but not with
  <span class="pre">
   -G
  </span>
  . This is intended primarily for debugging programs built with OptiX/RTCore.
 </p>
 <p>
  Note that
  <span class="pre">
   -lineinfo
  </span>
  can be used when trying to debug optimized code. In this case, debugger stepping and breakpoint behavior may appear somewhat erratic.
 </p>
 <ul class="simple">
  <li>
   <p>
    The PC may jump forward and backward unexpectedly while stepping.
   </p>
  </li>
  <li>
   <p>
    The user may step into code that has no linenumber information, leading to an inability to determine which source-file/linenumber the code at the PC belongs to.
   </p>
  </li>
  <li>
   <p>
    Breakpoints may break on a different line than they were originally set on.
   </p>
  </li>
 </ul>
 <p>
  When debugging OptiX/RTCore code, the following should be kept in mind:
 </p>
 <ul class="simple">
  <li>
   <p>
    NVIDIA internal code cannot be debugged or examined by the user.
   </p>
  </li>
  <li>
   <p>
    OptiX/RTCode debugging is limited to
    <span class="pre">
     -lineinfo
    </span>
    , and building this code with full debug infomation (
    <span class="pre">
     -G
    </span>
    ) is not supported.
   </p>
  </li>
  <li>
   <p>
    OptiX/RTCode code is highly optimized, and as such the notes above about debugging optimized code apply.
   </p>
  </li>
 </ul>
 <h3>
  <span class="section-number">
   3.2.3.
  </span>
  Compiling For Specific GPU architectures
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#compiling-for-specific-gpu-architectures" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  By default, the compiler will only generate code for the compute_52 PTX and sm_52 cubins. For later GPUs, the kernels are recompiled at runtime from the PTX for the architecture of the target GPU(s). Compiling for a specific virtual architecture guarantees that the application will work for any GPU architecture after that, for a trade-off in performance. This is done for forward-compatibility.
 </p>
 <p>
  It is highly recommended to compile the application once and for all for the GPU architectures targeted by the application, and to generate the PTX code for the latest virtual architecture for forward compatibility.
 </p>
 <p>
  A GPU architecture is defined by its compute capability. The list of GPUs and their respective compute capability, see
  <a class="reference external" href="https://developer.nvidia.com/cuda-gpus">
   https://developer.nvidia.com/cuda-gpus
  </a>
  . The same application can be compiled for multiple GPU architectures. Use the
  <span class="pre">
   -gencode
  </span>
  compilation option to dictate which GPU architecture to compile for. The option can be specified multiple times.
 </p>
 <p>
  For instance, to compile an application for a GPU with compute capability 7.0, add the following flag to the compilation command:
 </p>
 <pre>-gencode arch=compute_70,code=sm_70
</pre>
 <p>
  To compile PTX code for any future architecture past the compute capability 7.0, add the following flag to the compilation command:
 </p>
 <pre>-gencode arch=compute_70,code=compute_70
</pre>
 <p>
  For additional information, please consult the compiler documentation at
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#extended-notation">
   https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#extended-notation
  </a>
 </p>
 <h2>
  <span class="section-number">
   3.3.
  </span>
  Using the Debugger
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#using-the-debugger" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA-GDB can be used in the following system configurations:
 </p>
 <h3>
  <span class="section-number">
   3.3.1.
  </span>
  Single-GPU Debugging with the Desktop Manager Running
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#single-gpu-debugging-with-the-desktop-manager-running" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  For devices with compute capability 6.0 and higher CUDA-GDB can be used to debug CUDA applications on the same GPU that is running the desktop GUI.
 </p>
 <p>
  Additionally for devices with compute capability less than 6.0 software preemption can be used to debug CUDA applications on the same GPU that is running the desktop GUI. There are two ways to enable this functionality:
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  This is a BETA feature available on Linux and is only supported on Maxwell. The options listed below are ignored for GPUs with SM6.0 compute capability and higher.
 </p>
 <ul>
  <li>
   <p>
    Use the following command:
   </p>
   <pre>set cuda software_preemption on
</pre>
  </li>
  <li>
   <p>
    Export the following environment variable:
   </p>
   <pre>CUDA_DEBUGGER_SOFTWARE_PREEMPTION=1
</pre>
  </li>
 </ul>
 <p>
  Either of the options above will activate software preemption. These options must be set
  prior
  to running the application. When the GPU hits a breakpoint or any other event that would normally cause the GPU to freeze, CUDA-GDB releases the GPU for use by the desktop or other applications. This enables CUDA-GDB to debug a CUDA application on the same GPU that is running the desktop GUI, and also enables debugging of multiple CUDA applications context-switching on the same GPU.
 </p>
 <h3>
  <span class="section-number">
   3.3.2.
  </span>
  Multi-GPU Debugging
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#multi-gpu-debugging" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Multi-GPU debugging designates the scenario where the application is running on more than one CUDA-capable device. Multi-GPU debugging is not much different than single-GPU debugging except for a few additional CUDA-GDB commands that let you switch between the GPUs.
 </p>
 <p>
  Any GPU hitting a breakpoint will pause all the GPUs running CUDA on that system. Once paused, you can use
  <span class="pre">
   info
  </span>
  <span class="pre">
   cuda
  </span>
  <span class="pre">
   kernels
  </span>
  to view all the active kernels and the GPUs they are running on. When any GPU is resumed, all the GPUs are resumed.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  If the
  <span class="pre">
   CUDA_VISIBLE_DEVICES
  </span>
  environment is used, only the specified devices are suspended and resumed.
 </p>
 <p>
  All CUDA-capable GPUs may run one or more kernels. To switch to an active kernel, use
  <span class="pre">
   cuda
  </span>
  <span class="pre">
   kernel
  </span>
  <span class="pre">
   &lt;n&gt;
  </span>
  , where
  <span class="pre">
   n
  </span>
  is the ID of the kernel retrieved from
  <span class="pre">
   info
  </span>
  <span class="pre">
   cuda
  </span>
  <span class="pre">
   kernels
  </span>
  .
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The same kernel can be loaded and used by different contexts and devices at the same time. When a breakpoint is set in such a kernel, by either name or file name and line number, it will be resolved arbitrarily to only one instance of that kernel. With the runtime API, the exact instance to which the breakpoint will be resolved cannot be controlled. With the driver API, the user can control the instance to which the breakpoint will be resolved to by setting the breakpoint
  right after
  its module is loaded.
 </p>
 <h3>
  <span class="section-number">
   3.3.3.
  </span>
  Remote Debugging
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#remote-debugging" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  There are multiple methods to remote debug an application with CUDA-GDB. In addition to using SSH or VNC from the host system to connect to the target system, it is also possible to use the
  <span class="pre">
   target
  </span>
  <span class="pre">
   remote
  </span>
  GDB feature. Using this option, the local
  <span class="pre">
   cuda-gdb
  </span>
  (client) connects to the
  <span class="pre">
   cuda-gdbserver
  </span>
  process (the server) running on the target system. This option is supported with a Linux client and a Linux or QNX server.
 </p>
 <p>
  Setting remote debugging that way is a 2-step process:
 </p>
 <p>
  Launch the cuda-gdbserver on the remote host
 </p>
 <p>
  cuda-gdbserver can be launched on the remote host in different operation modes.
 </p>
 <ul>
  <li>
   <p>
    Option 1: Launch a new application in debug mode.
   </p>
   <p>
    To launch a new application in debug mode, invoke cuda-gdb server as follows:
   </p>
   <pre>$ cuda-gdbserver :1234 app_invocation
</pre>
   <p>
    Where
    <span class="pre">
     1234
    </span>
    is the TCP port number that
    <span class="pre">
     cuda-gdbserver
    </span>
    will listen to for incoming connections from
    <span class="pre">
     cuda-gdb
    </span>
    , and
    <span class="pre">
     app-invocation
    </span>
    is the invocation command to launch the application, arguments included.
   </p>
  </li>
  <li>
   <p>
    Option 2: Attach
    <span class="pre">
     cuda-gdbserver
    </span>
    to the running process
   </p>
   <p>
    To attach cuda-gdbserver to an already running process, the
    <span class="pre">
     --attach
    </span>
    option followed by process identification number (PID) must be used:
   </p>
   <pre>$ cuda-gdbserver :1234 --attach 5678
</pre>
   <p>
    Where
    <span class="pre">
     1234
    </span>
    is the TCP port number and
    <span class="pre">
     5678
    </span>
    is process identifier of the application cuda-gdbserver must be attached to.
   </p>
   <p>
    Attaching to an already running process is not supported on QNX platforms.
   </p>
  </li>
 </ul>
 <p>
  Launch cuda-gdb on the client
 </p>
 <p>
  Configure
  <span class="pre">
   cuda-gdb
  </span>
  to connect to the remote target using either:
 </p>
 <pre>(cuda-gdb) target remote
</pre>
 <p>
  or
 </p>
 <pre>(cuda-gdb) target extended-remote
</pre>
 <p>
  It is recommended to use
  <span class="pre">
   set
  </span>
  <span class="pre">
   sysroot
  </span>
  command if libraries installed on the debug target might differ from the ones installed on the debug host. For example, cuda-gdb could be configured to connect to remote target as follows:
 </p>
 <pre>(cuda-gdb) set sysroot remote://
(cuda-gdb) target remote 192.168.0.2:1234
</pre>
 <p>
  Where
  <span class="pre">
   192.168.0.2
  </span>
  is the IP address or domain name of the remote target, and
  <span class="pre">
   1234
  </span>
  is the TCP port previously previously opened by
  <span class="pre">
   cuda-gdbserver
  </span>
  .
 </p>
 <h3>
  <span class="section-number">
   3.3.4.
  </span>
  Multiple Debuggers
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#multiple-debuggers" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  For devices with compute capability 6.0 and higher several debugging sessions may take place simultaneously.
 </p>
 <p>
  For devices with compute capability less than 6.0, several debugging sessions may take place simultaneously as long as the CUDA devices are used exclusively. For instance, one instance of CUDA-GDB can debug a first application that uses the first GPU while another instance of CUDA-GDB debugs a second application that uses the second GPU. The exclusive use of a GPU is achieved by specifying which GPU is visible to the application by using the
  <span class="pre">
   CUDA_VISIBLE_DEVICES
  </span>
  environment variable.
 </p>
 <pre>$ CUDA_VISIBLE_DEVICES=1 cuda-gdb my_app
</pre>
 <p>
  Additionally for devices with compute capability less than 6.0, with software preemption enabled (
  <span class="pre">
   set
  </span>
  <span class="pre">
   cuda
  </span>
  <span class="pre">
   software_preemption
  </span>
  <span class="pre">
   on
  </span>
  ), multiple CUDA-GDB instances can be used to debug CUDA applications context-switching on the same GPU.
 </p>
 <h3>
  <span class="section-number">
   3.3.5.
  </span>
  Attaching/Detaching
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#attaching-detaching" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  CUDA-GDB can attach to and detach from a CUDA application running on GPUs with compute capability 2.0 and beyond, using GDBâs built-in commands for attaching to or detaching from a process.
 </p>
 <p>
  Additionally, if the environment variable CUDA_DEVICE_WAITS_ON_EXCEPTION is set to 1 prior to running the CUDA application, the application will run normally until a device exception occurs. The application will then wait for CUDA-GDB to attach itself to it for further debugging. This feature is not supported on WSL.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  By default on some Linux distributions, the debugger cannot attach to an already running processes due to security settings. In order to enable the attach feature of the CUDA debugger, either cuda-gdb should be launched as root, or
  <span class="pre">
   /proc/sys/kernel/yama/ptrace_scope
  </span>
  should be set to zero, using the following command:
 </p>
 <pre>$ sudo sh -c "echo 0 &gt;/proc/sys/kernel/yama/ptrace_scope"
</pre>
 <p>
  To make the change permanent, edit
  <span class="pre">
   /etc/sysctl.d/10-ptrace.conf
  </span>
  .
 </p>
 <h1>
  <span class="section-number">
   4.
  </span>
  CUDA-GDB Extensions
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#cuda-gdb-extensions" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   4.1.
  </span>
  Command Naming Convention
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#command-naming-convention" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The existing GDB commands are unchanged. Every new CUDA command or option is prefixed with the CUDA keyword. As much as possible, CUDA-GDB command names will be similar to the equivalent GDB commands used for debugging host code. For instance, the GDB command to display the host threads and switch to host thread 1 are, respectively:
 </p>
 <pre>(cuda-gdb) info threads
(cuda-gdb) thread 1
</pre>
 <p>
  To display the CUDA threads and switch to cuda thread 1, the user only has to type:
 </p>
 <pre>(cuda-gdb) info cuda threads
(cuda-gdb) cuda thread 1
</pre>
 <h2>
  <span class="section-number">
   4.2.
  </span>
  Getting Help
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#getting-help" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  As with GDB commands, the built-in help for the CUDA commands is accessible from the
  <span class="pre">
   cuda-gdb
  </span>
  command line by using the help command:
 </p>
 <pre>(cuda-gdb) help cuda name_of_the_cuda_command
(cuda-gdb) help set cuda name_of_the_cuda_option
(cuda-gdb) help info cuda name_of_the_info_cuda_command
</pre>
 <p>
  Moreover, all the CUDA commands can be auto-completed by pressing the TAB key, as with any other GDB command.
 </p>
 <p>
  CUDA commands can also be queried using the
  <span class="pre">
   apropos
  </span>
  command.
 </p>
 <h2>
  <span class="section-number">
   4.3.
  </span>
  Initialization File
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#initialization-file" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The initialization file for CUDA-GDB is named
  <span class="pre">
   .cuda-gdbinit
  </span>
  and follows the same rules as the standard
  <span class="pre">
   .gdbinit
  </span>
  file used by GDB. The initialization file may contain any CUDA- GDB command. Those commands will be processed in order when CUDA-GDB is launched.
 </p>
 <h2>
  <span class="section-number">
   4.4.
  </span>
  GUI Integration
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#gui-integration" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Emacs
 </p>
 <p>
  CUDA-GDB works with GUD in Emacs and XEmacs. No extra step is required other than pointing to the right binary.
 </p>
 <p>
  To use CUDA-GDB, the
  <span class="pre">
   gud-gdb-command-name
  </span>
  variable must be set to
  <span class="pre">
   cuda-gdb
  </span>
  <span class="pre">
   annotate=3
  </span>
  . Use
  <span class="pre">
   M-x
  </span>
  <span class="pre">
   customize-variable
  </span>
  to set the variable.
 </p>
 <p>
  Ensure that
  <span class="pre">
   cuda-gdb
  </span>
  is present in the Emacs/XEmacs
  <span class="pre">
   $PATH
  </span>
  .
 </p>
 <p>
  DDD
 </p>
 <p>
  CUDA-GDB works with DDD. To use DDD with CUDA-GDB, launch DDD with the following command:
 </p>
 <pre>ddd --debugger cuda-gdb
</pre>
 <p>
  <span class="pre">
   cuda-gdb
  </span>
  must be in your
  <span class="pre">
   $PATH
  </span>
  .
 </p>
 <h2>
  <span class="section-number">
   4.5.
  </span>
  GPU core dump support
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#gpu-core-dump-support" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  There are two ways to configure the core dump options for CUDA applications. Environment variables set in the application environment or programmatically from the application with the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__COREDUMP.html#group__CUDA__COREDUMP/">
   CUDA Driver API
  </a>
  .
 </p>
 <p>
  Compilation for GPU core dump generation
 </p>
 <p>
  GPU core dumps will be generated regardless of compilation flags used to generate the GPU application. For the best debugging experience, it is recommended to compile the application with the
  <span class="pre">
   -g
  </span>
  <span class="pre">
   -G
  </span>
  or the
  <span class="pre">
   -lineinfo
  </span>
  option with NVCC. See
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#compiling-the-application">
   Compiling the Application
  </a>
  for more information on passing compilation flags for debugging.
 </p>
 <p>
  Enabling GPU core dump generation on exception with environment variables
 </p>
 <p>
  Set the
  <span class="pre">
   CUDA_ENABLE_COREDUMP_ON_EXCEPTION
  </span>
  environment variable to
  <span class="pre">
   1
  </span>
  in order to enable generating a GPU core dump when a GPU exception is encountered. This option is disabled by default.
 </p>
 <p>
  Set the
  <span class="pre">
   CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION
  </span>
  environment variable to
  <span class="pre">
   0
  </span>
  in order to disable generating a CPU core dump when a GPU exception is encountered. This option is enabled by default when GPU core dump generation is enabled.
 </p>
 <p>
  Set the
  <span class="pre">
   CUDA_ENABLE_LIGHTWEIGHT_COREDUMP
  </span>
  environment variable to
  <span class="pre">
   1
  </span>
  in order to enable generating lightweight corefiles instead of full corefiles. When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. This option is disabled by default.
 </p>
 <p>
  Controlling behavior of GPU core dump generation
 </p>
 <p>
  The
  <span class="pre">
   CUDA_COREDUMP_GENERATION_FLAGS
  </span>
  environment variable can be used when generating GPU core dumps to deviate from default generation behavior. Multiple flags can be provided to this environment variable and are delimited by
  <span class="pre">
   ,
  </span>
  . These flags can be used to accomplish tasks such as reducing the size of the generated GPU core dump or other desired behaviors that deviate from the defaults. The table below lists each flag and the behavior when present.
 </p>
 <table class="table-nostripes colwidths-given docutils align-default" id="id1">
  <span class="caption-text">
   GPU core dump CUDA_COREDUMP_GENERATION_FLAGS
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#id1" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Environment Variable flag
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      skip_nonrelocated_elf_images
     </span>
    </p>
   </td>
   <td>
    <p>
     Disables including copies of nonrelocated elf images in the GPU core dump. Only the relocated images will be present.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      skip_global_memory
     </span>
    </p>
   </td>
   <td>
    <p>
     Disables dumping of GPU global and constbank memory segments.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      skip_shared_memory
     </span>
    </p>
   </td>
   <td>
    <p>
     Disables dumping of GPU shared memory segments.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      skip_local_memory
     </span>
    </p>
   </td>
   <td>
    <p>
     Disables dumping of GPU local memory segments.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      skip_abort
     </span>
    </p>
   </td>
   <td>
    <p>
     Disables calling
     <span class="pre">
      abort()
     </span>
     at the end of the GPU core dump generation process.
    </p>
   </td>
  </tr>
 </table>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Setting the
  <span class="pre">
   CUDA_ENABLE_LIGHTWEIGHT_COREDUMP
  </span>
  environment variable to
  <span class="pre">
   1
  </span>
  is equivalent to
  <span class="pre">
   CUDA_COREDUMP_GENERATION_FLAGS="skip_nonrelocated_elf_images,skip_global_memory,skip_shared_memory,skip_local_memory"
  </span>
  .
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Setting the
  <span class="pre">
   CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION
  </span>
  environment variable to
  <span class="pre">
   0
  </span>
  is equivalent to
  <span class="pre">
   CUDA_COREDUMP_GENERATION_FLAGS="skip_abort"
  </span>
  .
 </p>
 <p>
  Limitations and notes for core dump generation
 </p>
 <p>
  The following limitations apply to core dump support:
 </p>
 <ul class="simple">
  <li>
   <p>
    For Windows WDDM, GPU core dump is only supported on a GPU with compute capability 6.0 or higher. Windows TCC supports GPU core dump on all supported compute capabilities.
   </p>
  </li>
  <li>
   <p>
    GPU core dump is unsupported for the Windows Subsystem for Linux on GPUs running in SLI mode. Multi-GPU setups are supported, but SLI mode cannot be enabled in the Driver Control Panel.
   </p>
  </li>
  <li>
   <p>
    GPU core dump is supported for the Windows Subsystem for Linux only when the
    <a class="reference external" href="https://devblogs.microsoft.com/directx/hardware-accelerated-gpu-scheduling/">
     hardware scheduling mode
    </a>
    is enabled.
   </p>
  </li>
  <li>
   <p>
    Generating a CPU core dump with
    <span class="pre">
     CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION
    </span>
    is currently unsupported on the QNX platform.
   </p>
  </li>
  <li>
   <p>
    GPU core dump is unsupported for the NVIDIA CMP product line.
   </p>
  </li>
  <li>
   <p>
    Per-context core dump can only be enabled on a GPU with compute capability 6.0 or higher. GPUs with compute capability less than 6.0 will return
    <span class="pre">
     CUDA_ERROR_NOT_SUPPORTED
    </span>
    when using the Coredump Attributes Control API.
   </p>
  </li>
  <li>
   <p>
    If an MPS client triggers a core dump, every other client running on the same MPS server will fault. The indirectly faulting clients will also generate a core dump if they have core dump generation enabled.
   </p>
  </li>
  <li>
   <p>
    GPU core dump is unsupported when other developer tools, including CUDA-GDB, are interacting with the application. Unless explicitly documented as a supported use case (e.g
    <span class="pre">
     generate-cuda-core-file
    </span>
    command).
   </p>
  </li>
  <li>
   <p>
    When generating a coredump on exception, if the kernel exits before the exception has been recognized it may result in failure to generate the corefile. See the note in
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#gpu-error-reporting">
     GPU Error Reporting
    </a>
    for strategies on how to work around this issue.
   </p>
  </li>
 </ul>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The user should not send the application process a signal and ensure that the application process does not automatically terminate while the coredump generation is in process. Doing so may cause GPU coredump generation to abort.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Starting from CUDA 11.6, the compute-sanitizer tool can generate a GPU core dump when an error is detected by using the
  <span class="pre">
   --generate-coredump
  </span>
  <span class="pre">
   yes
  </span>
  option. Once the core dump is generated, the target application will abort. See the compute-sanitizer documentation for more information:
  <a class="reference external" href="https://docs.nvidia.com/compute-sanitizer/ComputeSanitizer/index.html#coredump">
   https://docs.nvidia.com/compute-sanitizer/ComputeSanitizer/index.html#coredump
  </a>
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  CPU core dumps will be located in a distribution specific location. Examining the
  <span class="pre">
   /proc/sys/kernel/core_pattern
  </span>
  file will typically hint at the name/location of the CPU core dump.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  NVIDIA vGPU platforms must explicitly enable debugging support to perform GPU core dump generation. Please reference the
  <a class="reference external" href="https://docs.nvidia.com/grid/latest/grid-vgpu-user-guide/index.html#enabling-cuda-toolkit-development-tools-vgpu/">
   Virtual GPU Software User Guide
  </a>
  for information on how to enable debugging on vGPU.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  NVIDIA Jetson and Drive Tegra devices must explicitly enable debugging support to perform GPU core dump generation. Refer to the
  <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#tegra-setup">
   <span class="std std-ref">
    Using the CUDA-GDB debugger on Jetson and Drive Tegra devices
   </span>
  </a>
  section.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  When generating core dumps on NVIDIA Drive Tegra devices running QNX, core dump generation may hang when generating CPU core dumps. If a hang is encountered, set
  <span class="pre">
   CUDA_ENABLE_CPU_COREDUMP_EXCEPTION
  </span>
  to 0.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  If core dumps are not generated when running programs built with OptiX/RTCore, try setting the environment variable
  <span class="pre">
   OPTIX_FORCE_DEPRECATED_LAUNCHER
  </span>
  to 1. Refer to the
  <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#using-optix">
   <span class="std std-ref">
    Debugging OptiX/RTCore applications
   </span>
  </a>
  section.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  If core dumps are not generated when running programs on Windows Subsystem for Linux, ensure the debug interface is enabled via setting the registry key
  <span class="pre">
   &gt;HKEY_LOCAL_MACHINE\SOFTWARE\NVIDIA
  </span>
  <span class="pre">
   Corporation\GPUDebugger\EnableInterface
  </span>
  to
  <span class="pre">
   (DWORD)
  </span>
  <span class="pre">
   1
  </span>
  . Refer to the
  <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#using-wsl">
   <span class="std std-ref">
    Debugging on Windows Subsystem for Linux
   </span>
  </a>
  section.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  GPU core dump is supported on GPUs running with Confidential Compute mode only with devtools mode. See
  Confidential Computing Deployment Guide &lt;https://docs.nvidia.com/confidential-computing-deployment-guide.pdf&gt;
  for more details on how to enable the mode.
 </p>
 <p>
  Naming of GPU core dump files
 </p>
 <p>
  By default, a GPU core dump is created in the current working directory. It is named
  <span class="pre">
   core_TIME_HOSTNAME_PID.nvcudmp
  </span>
  where
  <span class="pre">
   TIME
  </span>
  is the number of seconds since the Epoch,
  <span class="pre">
   HOSTNAME
  </span>
  is the host name of the machine running the CUDA application and
  <span class="pre">
   PID
  </span>
  is the process identifier of the CUDA application.
 </p>
 <p>
  The
  <span class="pre">
   CUDA_COREDUMP_FILE
  </span>
  environment variable can be used to define a template that is used to change the name of a GPU core dump file. The template can either be an absolute path or a relative path to the current working directory. The template can contain
  <span class="pre">
   %
  </span>
  specifiers which are substituted by the following patterns when a GPU core dump is created:
 </p>
 <table class="docutils align-default">
  <tr class="row-odd">
   <th class="head">
    <p>
     Specifier
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      %h
     </span>
    </p>
   </td>
   <td>
    <p>
     Host name of the machine running the CUDA application
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     <span class="pre">
      %p
     </span>
    </p>
   </td>
   <td>
    <p>
     Process identifier of the CUDA application
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     <span class="pre">
      %t
     </span>
    </p>
   </td>
   <td>
    <p>
     Time as the number of seconds since the Epoch,
     <span class="pre">
      1970-01-01
     </span>
     <span class="pre">
      00:00:00
     </span>
     <span class="pre">
      +0000
     </span>
     <span class="pre">
      (UTC)
     </span>
    </p>
   </td>
  </tr>
 </table>
 <p>
  As an example, setting
  <span class="pre">
   CUDA_COREDUMP_FILE
  </span>
  to:
 </p>
 <pre>export CUDA_COREDUMP_FILE=newName.%h.%p
</pre>
 <p>
  Would result in GPU core dumps being written to
  <span class="pre">
   newName.myhost.1234
  </span>
  relative to the current working directory. Here
  <span class="pre">
   myhost
  </span>
  and
  <span class="pre">
   1234
  </span>
  are replaced with the real host name and pid respectively.
 </p>
 <p>
  Setting
  <span class="pre">
   CUDA_COREDUMP_FILE
  </span>
  to:
 </p>
 <pre>export CUDA_COREDUMP_FILE="/home/$USER/newName.%h.%p"
</pre>
 <p>
  Would result in GPU core dumps being written to the userâs home directory with the same name logic as in the above example.
 </p>
 <p>
  If
  <span class="pre">
   CUDA_COREDUMP_FILE
  </span>
  points to an existing file of FIFO type (e.g named pipe), the core dump will be streamed to it.
 </p>
 <p>
  Coredumps may be piped to shell commands via
  <span class="pre">
   CUDA_COREDUMP_FILE
  </span>
  with the following format:
 </p>
 <pre>export CUDA_COREDUMP_FILE='| cmd &gt; file'
</pre>
 <p>
  For example, to pipe a coredump to
  <span class="pre">
   gzip
  </span>
  use:
 </p>
 <pre>export CUDA_COREDUMP_FILE='| gzip -9 &gt; cuda-coredump.gz'
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  When piping a coredump, the
  <span class="pre">
   %
  </span>
  specifiers will not be recognized.
 </p>
 <p>
  Enabling user induced GPU core dump generation
 </p>
 <p>
  For the devices that support compute preemption, the user can interrupt a running CUDA process to generate the GPU core dump.
 </p>
 <p>
  Set the
  <span class="pre">
   CUDA_ENABLE_USER_TRIGGERED_COREDUMP
  </span>
  environment variable to
  <span class="pre">
   1
  </span>
  in order to enable generating a user induced GPU core dump. This option is disabled by default. Setting this environment variable will open a communication pipe for each subsequently running CUDA process. To induce the GPU core dump, the user simply writes to the pipe.
 </p>
 <p>
  To change the default pipe file name, set the
  <span class="pre">
   CUDA_COREDUMP_PIPE
  </span>
  environment variable to a specific pipe name. The default pipe name is in the following format:
  <span class="pre">
   corepipe.cuda.HOSTNAME.PID
  </span>
  where HOSTNAME is the host name of machine running the CUDA application and PID is the process identifier of the CUDA application. This environment variable can take
  <span class="pre">
   %
  </span>
  specifiers as decribed in the above section.
 </p>
 <p>
  Displaying core dump generation progress
 </p>
 <p>
  By default, when an application crashes and generates a GPU core dump, the application may appear to be unresponsive or frozen until fully generated.
 </p>
 <p>
  Set the
  <span class="pre">
   CUDA_COREDUMP_SHOW_PROGRESS
  </span>
  environment variable to
  <span class="pre">
   1
  </span>
  in order to print core dump generation progress messages to
  <span class="pre">
   stderr
  </span>
  . This can be used to determine how far along the coredump generation is:
 </p>
 <pre>coredump: SM 1/14 has finished state collection
coredump: SM 2/14 has finished state collection
coredump: SM 3/14 has finished state collection
coredump: SM 4/14 has finished state collection
coredump: SM 5/14 has finished state collection
coredump: SM 6/14 has finished state collection
coredump: SM 7/14 has finished state collection
coredump: SM 8/14 has finished state collection
coredump: SM 9/14 has finished state collection
coredump: SM 10/14 has finished state collection
coredump: SM 11/14 has finished state collection
coredump: SM 12/14 has finished state collection
coredump: SM 13/14 has finished state collection
coredump: SM 14/14 has finished state collection
coredump: Device 1/1 has finished state collection
coredump: Calculating ELF file layout
coredump: ELF file layout calculated
coredump: Writing ELF file to core_TIME_HOSTNAME_PID.nvcudmp
coredump: Writing out global memory (1073741824 bytes)
coredump: 5%...
coredump: 10%...
coredump: 15%...
coredump: 20%...
coredump: 25%...
coredump: 30%...
coredump: 35%...
coredump: 40%...
coredump: 45%...
coredump: 50%...
coredump: 55%...
coredump: 60%...
coredump: 65%...
coredump: 70%...
coredump: 75%...
coredump: 80%...
coredump: 85%...
coredump: 90%...
coredump: 95%...
coredump: 100%...
coredump: Writing out device table
coredump: Finalizing
coredump: All done
</pre>
 <p>
  Enabling GPU core dump generation with the CUDA Driver API
 </p>
 <p>
  The Driver API has equivalent settings for all of the environment variables, with the added feature of being able to set different core dump settings per-context instead of globally. This API can be called directly inside your application. Use
  <span class="pre">
   cuCoredumpGetAttributeGlobal
  </span>
  and
  <span class="pre">
   cuCoredumpSetAttributeGlobal
  </span>
  to fetch or set the global attribute. Use
  <span class="pre">
   cuCoredumpGetAttribute
  </span>
  and
  <span class="pre">
   cuCoredumpSetAttribute
  </span>
  to fetch or set the per context attribute. See the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__COREDUMP.html#group__CUDA__COREDUMP/">
   Coredump Attributes Control API
  </a>
  manual for more information.
 </p>
 <p>
  The table below lists the environment variables and the equivalent
  <span class="pre">
   CUcoredumpSettings
  </span>
  flags that are available to manage core dump settings with the Coredump Attributes Control API.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The
  <span class="pre">
   CU_COREDUMP_ENABLE_USER_TRIGGER
  </span>
  setting can only be set globally in the driver API and
  <span class="pre">
   CU_COREDUMP_PIPE
  </span>
  must be set (if desired) before user-triggered core dumps are enabled.
 </p>
 <table class="table-nostripes colwidths-given docutils align-default" id="id2">
  <span class="caption-text">
   GPU core dump configuration parameters
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#id2" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Environment Variable
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Environment Variable:
    </p>
    <p>
     <span class="pre">
      CUDA_ENABLE_COREDUMP_ON_EXCEPTION
     </span>
    </p>
    <p>
     CUcoredumpSettings Flag:
    </p>
    <p>
     <span class="pre">
      CU_COREDUMP_ENABLE_ON_EXCEPTION
     </span>
    </p>
   </td>
   <td>
    <p>
     Enables GPU core dump generation for exceptions. Disabled by default.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Environment Variable:
    </p>
    <p>
     <span class="pre">
      CUDA_ENABLE_CPU_COREDUMP_ON_EXCEPTION
     </span>
    </p>
    <p>
     CUcoredumpSettings Flag:
    </p>
    <p>
     <span class="pre">
      CU_COREDUMP_TRIGGER_HOST
     </span>
    </p>
   </td>
   <td>
    <p>
     Triggers host (CPU) core dump after GPU core dump is complete. Enabled by default.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Environment Variable:
    </p>
    <p>
     <span class="pre">
      CUDA_ENABLE_LIGHTWEIGHT_COREDUMP
     </span>
    </p>
    <p>
     CUcoredumpSettings Flag:
    </p>
    <p>
     <span class="pre">
      CU_COREDUMP_LIGHTWEIGHT
     </span>
    </p>
   </td>
   <td>
    <p>
     When enabled, GPU core dumps will not contain the memory dumps (local, shared, global) of the application. Disabled by default.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Environment Variable:
    </p>
    <p>
     <span class="pre">
      CUDA_ENABLE_USER_TRIGGERED_COREDUMP
     </span>
    </p>
    <p>
     CUcoredumpSettings Flag:
    </p>
    <p>
     <span class="pre">
      CU_COREDUMP_ENABLE_USER_TRIGGER
     </span>
    </p>
   </td>
   <td>
    <p>
     Enables user triggerable core dumps by writing to a pipe defined in the
     <span class="pre">
      COREDUMP_PIPE
     </span>
     setting. Disabled by default.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Environment Variable:
    </p>
    <p>
     <span class="pre">
      CUDA_COREDUMP_FILE
     </span>
    </p>
    <p>
     CUcoredumpSettings Flag:
    </p>
    <p>
     <span class="pre">
      CU_COREDUMP_FILE
     </span>
    </p>
   </td>
   <td>
    <p>
     Filename template for the GPU core dump.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Environment Variable:
    </p>
    <p>
     <span class="pre">
      CUDA_COREDUMP_PIPE
     </span>
    </p>
    <p>
     CUcoredumpSettings Flag:
    </p>
    <p>
     <span class="pre">
      CU_COREDUMP_PIPE
     </span>
    </p>
   </td>
   <td>
    <p>
     Filename template for the user pipe trigger.
    </p>
   </td>
  </tr>
 </table>
 <p>
  Inspecting GPU and GPU+CPU core dumps in cuda-gdb
 </p>
 <p>
  Use the following command to load the GPU core dump into the debugger
 </p>
 <ul>
  <li>
   <pre>(cuda-gdb) target cudacore core.cuda.localhost.1234
</pre>
   <p>
    This will open the core dump file and print the exception encountered during program execution. Then, issue standard cuda-gdb commands to further investigate application state on the device at the moment it was aborted.
   </p>
  </li>
 </ul>
 <p>
  Use the following command to load CPU and GPU core dumps into the debugger
 </p>
 <ul>
  <li>
   <pre>(cuda-gdb) target core core.cpu core.cuda
</pre>
   <p>
    This will open the core dump file and print the exception encountered during program execution. Then, issue standard cuda-gdb commands to further investigate application state on the host and the device at the moment it was aborted.
   </p>
  </li>
 </ul>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Coredump inspection does not require that a GPU be installed on the system
 </p>
 <h1>
  <span class="section-number">
   5.
  </span>
  Kernel Focus
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#kernel-focus" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  A CUDA application may be running several host threads and many device threads. To simplify the visualization of information about the state of application, commands are applied to the entity in focus.
 </p>
 <p>
  When the focus is set to a host thread, the commands will apply only to that host thread (unless the application is fully resumed, for instance). On the device side, the focus is always set to the lowest granularity levelâthe device thread.
 </p>
 <h2>
  <span class="section-number">
   5.1.
  </span>
  Software Coordinates vs. Hardware Coordinates
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#software-coordinates-vs-hardware-coordinates" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  A device thread belongs to a block, which in turn belongs to a kernel. Thread, block, and kernel are the software coordinates of the focus. A device thread runs on a lane. A lane belongs to a warp, which belongs to an SM, which in turn belongs to a device. Lane, warp, SM, and device are the hardware coordinates of the focus. Software and hardware coordinates can be used interchangeably and simultaneously as long as they remain coherent.
 </p>
 <p>
  Another software coordinate is sometimes used: the grid. The difference between a grid and a kernel is the scope. The grid ID is unique per GPU whereas the kernel ID is unique across all GPUs. Therefore there is a 1:1 mapping between a kernel and a (grid,device) tuple.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  If software preemption is enabled (
  <span class="pre">
   set
  </span>
  <span class="pre">
   cuda
  </span>
  <span class="pre">
   software_preemption
  </span>
  <span class="pre">
   on
  </span>
  ), hardware coordinates corresponding to a device thread are likely to change upon resuming execution on the device. However, software coordinates will remain intact and will not change for the lifetime of the device thread.
 </p>
 <h2>
  <span class="section-number">
   5.2.
  </span>
  Current Focus
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#current-focus" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To inspect the current focus, use the cuda command followed by the coordinates of interest:
 </p>
 <pre>(cuda-gdb) cuda device sm warp lane block thread
block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0
(cuda-gdb) cuda kernel block thread
kernel 1, block (0,0,0), thread (0,0,0)
(cuda-gdb) cuda kernel
kernel 1
</pre>
 <h2>
  <span class="section-number">
   5.3.
  </span>
  Switching Focus
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#switching-focus" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To switch the current focus, use the cuda command followed by the coordinates to be changed:
 </p>
 <pre>(cuda-gdb) cuda device 0 sm 1 warp 2 lane 3
[Switching focus to CUDA kernel 1, grid 2, block (8,0,0), thread
(67,0,0), device 0, sm 1, warp 2, lane 3]
374 int totalThreads = gridDim.x * blockDim.x;
</pre>
 <p>
  If the specified focus is not fully defined by the command, the debugger will assume that the omitted coordinates are set to the coordinates in the current focus, including the subcoordinates of the block and thread.
 </p>
 <pre>(cuda-gdb) cuda thread (15)
[Switching focus to CUDA kernel 1, grid 2, block (8,0,0), thread
(15,0,0), device 0, sm 1, warp 0, lane 15]
374 int totalThreads = gridDim.x * blockDim.x;
</pre>
 <p>
  The parentheses for the block and thread arguments are optional.
 </p>
 <pre>(cuda-gdb) cuda block 1 thread 3
[Switching focus to CUDA kernel 1, grid 2, block (1,0,0), thread (3,0,0),
device 0, sm 3, warp 0, lane 3]
374 int totalThreads = gridDim.x * blockDim.
</pre>
 <h1>
  <span class="section-number">
   6.
  </span>
  Program Execution
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#program-execution" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  Applications are launched the same way in CUDA-GDB as they are with GDB by using the run command. This chapter describes how to interrupt and single-step CUDA applications
 </p>
 <h2>
  <span class="section-number">
   6.1.
  </span>
  Interrupting the Application
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#interrupting-the-application" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  If the CUDA application appears to be hanging or stuck in an infinite loop, it is possible to manually interrupt the application by pressing CTRL+C. When the signal is received, the GPUs are suspended and the
  <span class="pre">
   cuda-gdb
  </span>
  prompt will appear.
 </p>
 <p>
  At that point, the program can be inspected, modified, single-stepped, resumed, or terminated at the userâs discretion.
 </p>
 <p>
  This feature is limited to applications running within the debugger. It is not possible to break into and debug applications that have been launched outside the debugger.
 </p>
 <h2>
  <span class="section-number">
   6.2.
  </span>
  Single Stepping
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#single-stepping" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Single-stepping device code is supported. However, unlike host code single-stepping, device code single-stepping works at the warp level. This means that single-stepping a device kernel advances all the active threads in the warp currently in focus. The divergent threads in the warp are not single-stepped.
 </p>
 <p>
  In order to advance the execution of more than one warp, a breakpoint must be set at the desired location and then the application must be fully resumed.
 </p>
 <p>
  A special case is single-stepping over thread barrier calls like:
  <span class="pre">
   __syncthreads()
  </span>
  or cluster-wide barriers. In this case, an implicit temporary breakpoint is set immediately after the barrier and all threads are resumed until the temporary breakpoint is hit.
 </p>
 <p>
  You can step in, over, or out of the device functions as long as they are not inlined. To force a function to not be inlined by the compiler, the
  <span class="pre">
   __noinline__
  </span>
  keyword must be added to the function declaration.
 </p>
 <p>
  Asynchronous SASS instructions executed on the device, such as the warpgroup instructions, at prior PCs are not guaranteed to be complete.
 </p>
 <p>
  With Dynamic Parallelism, several CUDA APIs can be called directly from device code. The following list defines single-step behavior when encountering these APIs:
 </p>
 <ul class="simple">
  <li>
   <p>
    When encountering device side kernel launches (denoted by the
    <span class="pre">
     &lt;&lt;&lt;&gt;&gt;&gt;
    </span>
    launch syntax), the
    <span class="pre">
     step
    </span>
    and
    <span class="pre">
     next
    </span>
    commands will have the same behavior, and both will
    step over
    the launch call.
   </p>
  </li>
  <li>
   <p>
    On devices prior to Hopper (SM 9.0), stepping into the deprecated
    <span class="pre">
     cudaDeviceSynchronize()
    </span>
    results in undefined behavior. Users shall step over this call instead.
   </p>
  </li>
  <li>
   <p>
    When stepping a device grid launch to completion, focus will automatically switch back to the CPU. The
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     kernel
    </span>
    focus switching command must be used to switch to another grid of interest (if one is still resident).
   </p>
  </li>
 </ul>
 <p class="admonition-title">
  Note
 </p>
 <p>
  It is not possible to
  step into
  a device launch call (nor the routine launched by the call).
 </p>
 <h1>
  <span class="section-number">
   7.
  </span>
  Breakpoints and Watchpoints
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#breakpoints-and-watchpoints" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  There are multiple ways to set a breakpoint on a CUDA application. These methods are described below. The commands used to set a breakpoint on device code are the same as the commands used to set a breakpoint on host code.
 </p>
 <p>
  If a breakpoint is set on device code, the breakpoint will be marked pending until the ELF image of the kernel is loaded. At that point, the breakpoint will be resolved and its address will be updated.
 </p>
 <p>
  When a breakpoint is set, it forces all resident GPU threads to stop at this location when it reaches the corresponding PC.
 </p>
 <p>
  When a breakpoint is hit by one thread, there is no guarantee that the other threads will hit the breakpoint at the same time. Therefore the same breakpoint may be hit several times, and the user must be careful with checking which thread(s) actually hit(s) the breakpoint. The
  <span class="pre">
   disable
  </span>
  command can be used to prevent hitting the breakpoint by additional threads.
 </p>
 <h2>
  <span class="section-number">
   7.1.
  </span>
  Symbolic Breakpoints
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#symbolic-breakpoints" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To set a breakpoint at the entry of a function, use the
  <span class="pre">
   break
  </span>
  command followed by the name of the function or method:
 </p>
 <pre>(cuda-gdb) break my_function
(cuda-gdb) break my_class::my_method
</pre>
 <p>
  For templatized functions and methods, the full signature must be given:
 </p>
 <pre>(cuda-gdb) break int my_templatized_function&lt;int&gt;(int)
</pre>
 <p>
  The mangled name of the function can also be used. To find the mangled name of a function, you can use the following command:
 </p>
 <pre>(cuda-gdb) set demangle-style none
(cuda-gdb) info function my_function_name
(cuda-gdb) set demangle-style auto
</pre>
 <h2>
  <span class="section-number">
   7.2.
  </span>
  Line Breakpoints
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#line-breakpoints" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To set a breakpoint on a specific line number, use the following syntax:
 </p>
 <pre>(cuda-gdb) break my_file.cu:185
</pre>
 <p>
  If the specified line corresponds to an instruction within templatized code, multiple breakpoints will be created, one for each instance of the templatized code.
 </p>
 <h2>
  <span class="section-number">
   7.3.
  </span>
  Address Breakpoints
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#address-breakpoints" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To set a breakpoint at a specific address, use the
  <span class="pre">
   break
  </span>
  command with the address as argument:
 </p>
 <pre>(cuda-gdb) break *0x1afe34d0
</pre>
 <p>
  The address can be any address on the device or the host.
 </p>
 <h2>
  <span class="section-number">
   7.4.
  </span>
  Kernel Entry Breakpoints
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#kernel-entry-breakpoints" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To break on the first instruction of every launched kernel, set the
  <span class="pre">
   break_on_launch
  </span>
  option to application:
 </p>
 <pre>(cuda-gdb) set cuda break_on_launch application
</pre>
 <p>
  See
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-break-on-launch">
   set cuda break_on_launch
  </a>
  for more information.
 </p>
 <h2>
  <span class="section-number">
   7.5.
  </span>
  Conditional Breakpoints
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#conditional-breakpoints" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To make the breakpoint conditional, use the optional if keyword or the cond command.
 </p>
 <pre>(cuda-gdb) break foo.cu:23 if threadIdx.x == 1 &amp;&amp; i &lt; 5
(cuda-gdb) cond 3 threadIdx.x == 1 &amp;&amp; i &lt; 5
</pre>
 <p>
  Conditional expressions may refer any variable, including built-in variables such as
  <span class="pre">
   threadIdx
  </span>
  and
  <span class="pre">
   blockIdx
  </span>
  . Function calls are not allowed in conditional expressions.
 </p>
 <p>
  Note that conditional breakpoints are always hit and evaluated, but the debugger reports the breakpoint as being hit only if the conditional statement is evaluated to TRUE. The process of hitting the breakpoint and evaluating the corresponding conditional statement is time-consuming. Therefore, running applications while using conditional breakpoints may slow down the debugging session. Moreover, if the conditional statement is always evaluated to FALSE, the debugger may appear to be hanging or stuck, although it is not the case. You can interrupt the application with CTRL-C to verify that progress is being made.
 </p>
 <p>
  Conditional breakpoints can be set on code from CUDA modules that are not already loaded. The verification of the condition will then only take place when the ELF image of that module is loaded. Therefore any error in the conditional expression will be deferred until the CUDA module is loaded. To double check the desired conditional expression, first set an unconditional breakpoint at the desired location and continue. When the breakpoint is hit, evaluate the desired conditional statement by using the
  <span class="pre">
   cond
  </span>
  command.
 </p>
 <h2>
  <span class="section-number">
   7.6.
  </span>
  Watchpoints
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#watchpoints" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Watchpoints on CUDA code are not supported.
 </p>
 <p>
  Watchpoints on host code are supported. The user is invited to read the GDB documentation for a tutorial on how to set watchpoints on host code.
 </p>
 <h1>
  <span class="section-number">
   8.
  </span>
  Inspecting Program State
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#inspecting-program-state" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   8.1.
  </span>
  Memory and Variables
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#memory-and-variables" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The GDB print command has been extended to decipher the location of any program variable and can be used to display the contents of any CUDA program variable including:
 </p>
 <ul class="simple">
  <li>
   <p>
    data allocated via
    <span class="pre">
     cudaMalloc()
    </span>
   </p>
  </li>
  <li>
   <p>
    data that resides in various GPU memory regions, such as shared, local, and global memory
   </p>
  </li>
  <li>
   <p>
    special CUDA runtime variables, such as
    <span class="pre">
     threadIdx
    </span>
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   8.2.
  </span>
  Variable Storage and Accessibility
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#variable-storage-and-accessibility" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Depending on the variable type and usage, variables can be stored either in registers or in
  <span class="pre">
   local
  </span>
  ,
  <span class="pre">
   shared
  </span>
  ,
  <span class="pre">
   const
  </span>
  or
  <span class="pre">
   global
  </span>
  memory. You can print the address of any variable to find out where it is stored and directly access the associated memory.
 </p>
 <p>
  The example below shows how the variable array, which is of type
  <span class="pre">
   shared
  </span>
  <span class="pre">
   int
  </span>
  <span class="pre">
   *
  </span>
  , can be directly accessed in order to see what the stored values are in the array.
 </p>
 <pre>(cuda-gdb) print &amp;array
$1 = (@shared int (*)[0]) 0x20
(cuda-gdb) print array[0]@4
$2 = {0, 128, 64, 192}
</pre>
 <p>
  You can also access the shared memory indexed into the starting offset to see what the stored values are:
 </p>
 <pre>(cuda-gdb) print *(@shared int*)0x20
$3 = 0
(cuda-gdb) print *(@shared int*)0x24
$4 = 128
(cuda-gdb) print *(@shared int*)0x28
$5 = 64
</pre>
 <p>
  The example below shows how to access the starting address of the input parameter to the kernel.
 </p>
 <pre>(cuda-gdb) print &amp;data
$6 = (const @global void * const @parameter *) 0x10
(cuda-gdb) print *(@global void * const @parameter *) 0x10
$7 = (@global void * const @parameter) 0x110000&lt;/&gt;
</pre>
 <h2>
  <span class="section-number">
   8.3.
  </span>
  Info CUDA Commands
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-commands" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  These are commands that display information about the GPU and the applicationâs CUDA state. The available options are:
 </p>
 <span class="pre">
  devices
 </span>
 <p>
  information about all the devices
 </p>
 <span class="pre">
  sms
 </span>
 <p>
  information about all the active SMs in the current device
 </p>
 <span class="pre">
  warps
 </span>
 <p>
  information about all the active warps in the current SM
 </p>
 <span class="pre">
  lanes
 </span>
 <p>
  information about all the active lanes in the current warp
 </p>
 <span class="pre">
  kernels
 </span>
 <p>
  information about all the active kernels
 </p>
 <span class="pre">
  blocks
 </span>
 <p>
  information about all the active blocks in the current kernel
 </p>
 <span class="pre">
  threads
 </span>
 <p>
  information about all the active threads in the current kernel
 </p>
 <span class="pre">
  launch
 </span>
 <span class="pre">
  trace
 </span>
 <p>
  information about the parent kernels of the kernel in focus
 </p>
 <span class="pre">
  launch
 </span>
 <span class="pre">
  children
 </span>
 <p>
  information about the kernels launched by the kernels in focus
 </p>
 <span class="pre">
  contexts
 </span>
 <p>
  information about all the contexts
 </p>
 <p>
  A filter can be applied to every
  <span class="pre">
   info
  </span>
  <span class="pre">
   cuda
  </span>
  command. The filter restricts the scope of the command. A filter is composed of one or more restrictions. A restriction can be any of the following:
 </p>
 <ul class="simple">
  <li>
   <p>
    <span class="pre">
     device
    </span>
    <span class="pre">
     n
    </span>
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     sm
    </span>
    <span class="pre">
     n
    </span>
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     warp
    </span>
    <span class="pre">
     n
    </span>
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     lane
    </span>
    <span class="pre">
     n
    </span>
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     kernel
    </span>
    <span class="pre">
     n
    </span>
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     grid
    </span>
    <span class="pre">
     n
    </span>
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     block
    </span>
    <span class="pre">
     x[,y]
    </span>
    or
    <span class="pre">
     block
    </span>
    <span class="pre">
     (x[,y])
    </span>
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     thread
    </span>
    <span class="pre">
     x[,y[,z]]
    </span>
    or
    <span class="pre">
     thread
    </span>
    <span class="pre">
     (x[,y[,z]])
    </span>
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     breakpoint
    </span>
    <span class="pre">
     all
    </span>
    and
    <span class="pre">
     breakpoint
    </span>
    <span class="pre">
     n
    </span>
   </p>
  </li>
 </ul>
 <p>
  where
  <span class="pre">
   n
  </span>
  ,
  <span class="pre">
   x
  </span>
  ,
  <span class="pre">
   y
  </span>
  ,
  <span class="pre">
   z
  </span>
  are integers, or one of the following special keywords:
  <span class="pre">
   current
  </span>
  ,
  <span class="pre">
   any
  </span>
  , and
  <span class="pre">
   all
  </span>
  .
  <span class="pre">
   current
  </span>
  indicates that the corresponding value in the current focus should be used.
  <span class="pre">
   any
  </span>
  and
  <span class="pre">
   all
  </span>
  indicate that any value is acceptable.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The
  <span class="pre">
   breakpoint
  </span>
  <span class="pre">
   all
  </span>
  and
  <span class="pre">
   breakpoint
  </span>
  <span class="pre">
   n
  </span>
  filter are only effective for the
  <span class="pre">
   info
  </span>
  <span class="pre">
   cuda
  </span>
  <span class="pre">
   threads
  </span>
  command.
 </p>
 <h3>
  <span class="section-number">
   8.3.1.
  </span>
  info cuda devices
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-devices" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  This command enumerates all the GPUs in the system sorted by device index. A
  <span class="pre">
   *
  </span>
  indicates the device currently in focus. This command supports filters. The default is
  <span class="pre">
   device
  </span>
  <span class="pre">
   all
  </span>
  . This command prints
  <span class="pre">
   No
  </span>
  <span class="pre">
   CUDA
  </span>
  <span class="pre">
   Devices
  </span>
  if no active GPUs are found. A device is not considered active until the first kernel launch has been encountered.
 </p>
 <pre>(cuda-gdb) info cuda devices
  Dev PCI Bus/Dev ID                Name Description SM Type SMs Warps/SM Lanes/Warp Max Regs/Lane Active SMs Mask
    0        06:00.0 GeForce GTX TITAN Z      GK110B   sm_35  15       64         32           256 0x00000000
    1        07:00.0 GeForce GTX TITAN Z      GK110B   sm_35  15       64         32           256 0x00000000
</pre>
 <h3>
  <span class="section-number">
   8.3.2.
  </span>
  info cuda sms
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-sms" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  This command shows all the SMs for the device and the associated active warps on the SMs. This command supports filters and the default is
  <span class="pre">
   device
  </span>
  <span class="pre">
   current
  </span>
  <span class="pre">
   sm
  </span>
  <span class="pre">
   all
  </span>
  . A
  <span class="pre">
   *
  </span>
  indicates the SM is focus. The results are grouped per device.
 </p>
 <pre>(cuda-gdb) info cuda sms
 SM Active Warps Mask
Device 0
* 0 0xffffffffffffffff
  1 0xffffffffffffffff
  2 0xffffffffffffffff
  3 0xffffffffffffffff
  4 0xffffffffffffffff
  5 0xffffffffffffffff
  6 0xffffffffffffffff
  7 0xffffffffffffffff
  8 0xffffffffffffffff
...
</pre>
 <h3>
  <span class="section-number">
   8.3.3.
  </span>
  info cuda warps
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-warps" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  This command takes you one level deeper and prints all the warps information for the SM in focus. This command supports filters and the default is
  <span class="pre">
   device
  </span>
  <span class="pre">
   current
  </span>
  <span class="pre">
   sm
  </span>
  <span class="pre">
   current
  </span>
  <span class="pre">
   warp
  </span>
  <span class="pre">
   all
  </span>
  . The command can be used to display which warp executes what block.
 </p>
 <pre>(cuda-gdb) info cuda warps
Wp /Active Lanes Mask/ Divergent Lanes Mask/Active Physical PC/Kernel/BlockIdx
Device 0 SM 0
* 0    0xffffffff    0x00000000 0x000000000000001c    0    (0,0,0)
  1    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)
  2    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)
  3    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)
  4    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)
  5    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)
  6    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)
  7    0xffffffff    0x00000000 0x0000000000000000    0    (0,0,0)
 ...
</pre>
 <h3>
  <span class="section-number">
   8.3.4.
  </span>
  info cuda lanes
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-lanes" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  This command displays all the lanes (threads) for the warp in focus. This command supports filters and the default is
  <span class="pre">
   device
  </span>
  <span class="pre">
   current
  </span>
  <span class="pre">
   sm
  </span>
  <span class="pre">
   current
  </span>
  <span class="pre">
   warp
  </span>
  <span class="pre">
   current
  </span>
  <span class="pre">
   lane
  </span>
  <span class="pre">
   all
  </span>
  . In the example below you can see that all the lanes are at the same physical PC. The command can be used to display which lane executes what thread.
 </p>
 <pre>(cuda-gdb) info cuda lanes
  Ln    State  Physical PC        ThreadIdx
Device 0 SM 0 Warp 0
*  0    active 0x000000000000008c   (0,0,0)
   1    active 0x000000000000008c   (1,0,0)
   2    active 0x000000000000008c   (2,0,0)
   3    active 0x000000000000008c   (3,0,0)
   4    active 0x000000000000008c   (4,0,0)
   5    active 0x000000000000008c   (5,0,0)
   6    active 0x000000000000008c   (6,0,0)
   7    active 0x000000000000008c   (7,0,0)
   8    active 0x000000000000008c   (8,0,0)
   9    active 0x000000000000008c   (9,0,0)
  10    active 0x000000000000008c  (10,0,0)
  11    active 0x000000000000008c  (11,0,0)
  12    active 0x000000000000008c  (12,0,0)
  13    active 0x000000000000008c  (13,0,0)
  14    active 0x000000000000008c  (14,0,0)
  15    active 0x000000000000008c  (15,0,0)
  16    active 0x000000000000008c  (16,0,0)
 ...
</pre>
 <h3>
  <span class="section-number">
   8.3.5.
  </span>
  info cuda kernels
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-kernels" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  This command displays on all the active kernels on the GPU in focus. It prints the SM mask, kernel ID, and the grid ID for each kernel with the associated dimensions and arguments. The kernel ID is unique across all GPUs whereas the grid ID is unique per GPU. The
  <span class="pre">
   Parent
  </span>
  column shows the kernel ID of the parent grid. This command supports filters and the default is
  <span class="pre">
   kernel
  </span>
  <span class="pre">
   all
  </span>
  .
 </p>
 <pre>(cuda-gdb) info cuda kernels
  Kernel Parent Dev Grid Status   SMs Mask   GridDim  BlockDim      Name Args
*      1      -   0    2 Active 0x00ffffff (240,1,1) (128,1,1) acos_main parms=...
</pre>
 <p>
  This command will also show grids that have been launched on the GPU with Dynamic Parallelism. Kernels with a negative grid ID have been launched from the GPU, while kernels with a positive grid ID have been launched from the CPU.
 </p>
 <h3>
  <span class="section-number">
   8.3.6.
  </span>
  info cuda blocks
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-blocks" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  This command displays all the active or running blocks for the kernel in focus. The results are grouped per kernel. This command supports filters and the default is
  <span class="pre">
   kernel
  </span>
  <span class="pre">
   current
  </span>
  <span class="pre">
   block
  </span>
  <span class="pre">
   all
  </span>
  . The outputs are coalesced by default.
 </p>
 <pre>(cuda-gdb) info cuda blocks
   BlockIdx   To BlockIdx  Count  State
Kernel 1
*  (0,0,0)    (191,0,0)    192    running
</pre>
 <p>
  Coalescing can be turned off as follows in which case more information on the Device and the SM get displayed:
 </p>
 <pre>(cuda-gdb) set cuda coalescing off
</pre>
 <p>
  The following is the output of the same command when coalescing is turned off.
 </p>
 <pre>(cuda-gdb) info cuda blocks
  BlockIdx   State    Dev SM
Kernel 1
*   (0,0,0)   running   0   0
    (1,0,0)   running   0   3
    (2,0,0)   running   0   6
    (3,0,0)   running   0   9
    (4,0,0)   running   0  12
    (5,0,0)   running   0  15
    (6,0,0)   running   0  18
    (7,0,0)   running   0  21
    (8,0,0)   running   0   1
 ...
</pre>
 <h3>
  <span class="section-number">
   8.3.7.
  </span>
  info cuda threads
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-threads" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  This command displays the applicationâs active CUDA blocks and threads with the total count of threads in those blocks. Also displayed are the virtual PC and the associated source file and the line number information. The results are grouped per kernel. The command supports filters with default being
  <span class="pre">
   kernel
  </span>
  <span class="pre">
   current
  </span>
  <span class="pre">
   block
  </span>
  <span class="pre">
   all
  </span>
  <span class="pre">
   thread
  </span>
  <span class="pre">
   all
  </span>
  . The outputs are coalesced by default as follows:
 </p>
 <pre>(cuda-gdb) info cuda threads
  BlockIdx ThreadIdx To BlockIdx ThreadIdx Count   Virtual PC    Filename   Line
Device 0 SM 0
* (0,0,0  (0,0,0)    (0,0,0)  (31,0,0)    32  0x000000000088f88c   acos.cu   376
  (0,0,0)(32,0,0)  (191,0,0) (127,0,0) 24544  0x000000000088f800   acos.cu   374
 ...
</pre>
 <p>
  Coalescing can be turned off as follows in which case more information is displayed with the output.
 </p>
 <pre>(cuda-gdb) info cuda threads
   BlockIdx  ThreadIdx  Virtual PC         Dev SM Wp Ln   Filename  Line
Kernel 1
*  (0,0,0)    (0,0,0)  0x000000000088f88c   0  0  0  0    acos.cu    376
   (0,0,0)    (1,0,0)  0x000000000088f88c   0  0  0  1    acos.cu    376
   (0,0,0)    (2,0,0)  0x000000000088f88c   0  0  0  2    acos.cu    376
   (0,0,0)    (3,0,0)  0x000000000088f88c   0  0  0  3    acos.cu    376
   (0,0,0)    (4,0,0)  0x000000000088f88c   0  0  0  4    acos.cu    376
   (0,0,0)    (5,0,0)  0x000000000088f88c   0  0  0  5    acos.cu    376
   (0,0,0)    (6,0,0)  0x000000000088f88c   0  0  0  6    acos.cu    376
   (0,0,0)    (7,0,0)  0x000000000088f88c   0  0  0  7    acos.cu    376
   (0,0,0)    (8,0,0)  0x000000000088f88c   0  0  0  8    acos.cu    376
   (0,0,0)    (9,0,0)  0x000000000088f88c   0  0  0  9    acos.cu    376
 ...
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  In coalesced form, threads must be contiguous in order to be coalesced. If some threads are not currently running on the hardware, they will create
  holes
  in the thread ranges. For instance, if a kernel consist of 2 blocks of 16 threads, and only the 8 lowest threads are active, then 2 coalesced ranges will be printed: one range for block 0 thread 0 to 7, and one range for block 1 thread 0 to 7. Because threads 8-15 in block 0 are not running, the 2 ranges cannot be coalesced.
 </p>
 <p>
  The command also supports
  <span class="pre">
   breakpoint
  </span>
  <span class="pre">
   all
  </span>
  and
  <span class="pre">
   breakpoint
  </span>
  <span class="pre">
   breakpoint_number
  </span>
  as filters. The former displays the threads that hit all CUDA breakpoints set by the user. The latter displays the threads that hit the CUDA breakpoint
  breakpoint_number
  .
 </p>
 <pre>(cuda-gdb) info cuda threads breakpoint all
  BlockIdx ThreadIdx         Virtual PC Dev SM Wp Ln        Filename  Line
Kernel 0
   (1,0,0)   (0,0,0) 0x0000000000948e58   0 11  0  0 infoCommands.cu    12
   (1,0,0)   (1,0,0) 0x0000000000948e58   0 11  0  1 infoCommands.cu    12
   (1,0,0)   (2,0,0) 0x0000000000948e58   0 11  0  2 infoCommands.cu    12
   (1,0,0)   (3,0,0) 0x0000000000948e58   0 11  0  3 infoCommands.cu    12
   (1,0,0)   (4,0,0) 0x0000000000948e58   0 11  0  4 infoCommands.cu    12
   (1,0,0)   (5,0,0) 0x0000000000948e58   0 11  0  5 infoCommands.cu    12

(cuda-gdb) info cuda threads breakpoint 2 lane 1
  BlockIdx ThreadIdx         Virtual PC Dev SM Wp Ln        Filename  Line
Kernel 0
   (1,0,0)   (1,0,0) 0x0000000000948e58   0 11  0  1 infoCommands.cu    12
</pre>
 <h3>
  <span class="section-number">
   8.3.8.
  </span>
  info cuda launch trace
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-launch-trace" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  This command displays the kernel launch trace for the kernel in focus. The first element in the trace is the kernel in focus. The next element is the kernel that launched this kernel. The trace continues until there is no parent kernel. In that case, the kernel is CPU-launched.
 </p>
 <p>
  For each kernel in the trace, the command prints the level of the kernel in the trace, the kernel ID, the device ID, the grid Id, the status, the kernel dimensions, the kernel name, and the kernel arguments.
 </p>
 <pre>(cuda-gdb) info cuda launch trace
  Lvl Kernel Dev Grid     Status   GridDim  BlockDim Invocation
*   0      3   0   -7     Active  (32,1,1)  (16,1,1) kernel3(c=5)
    1      2   0   -5 Terminated (240,1,1) (128,1,1) kernel2(b=3)
    2      1   0    2     Active (240,1,1) (128,1,1) kernel1(a=1)
</pre>
 <p>
  A kernel that has been launched but that is not running on the GPU will have a
  <span class="pre">
   Pending
  </span>
  status. A kernel currently running on the GPU will be marked as
  <span class="pre">
   Active
  </span>
  . A kernel waiting to become active again will be displayed as
  <span class="pre">
   Sleeping
  </span>
  . When a kernel has terminated, it is marked as
  <span class="pre">
   Terminated
  </span>
  . For the few cases, when the debugger cannot determine if a kernel is pending or terminated, the status is set to
  <span class="pre">
   Undetermined
  </span>
  .
 </p>
 <p>
  This command supports filters and the default is
  <span class="pre">
   kernel
  </span>
  <span class="pre">
   all
  </span>
  .
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  With
  <span class="pre">
   set
  </span>
  <span class="pre">
   cuda
  </span>
  <span class="pre">
   software_preemption
  </span>
  <span class="pre">
   on
  </span>
  , no kernel will be reported as active.
 </p>
 <h3>
  <span class="section-number">
   8.3.9.
  </span>
  info cuda launch children
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-launch-children" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  This command displays the list of non-terminated kernels launched by the kernel in focus. For each kernel, the kernel ID, the device ID, the grid Id, the kernel dimensions, the kernel name, and the kernel parameters are displayed.
 </p>
 <pre>(cuda-gdb) info cuda launch children
  Kernel Dev Grid GridDim BlockDim Invocation
*      3   0   -7 (1,1,1)  (1,1,1) kernel5(a=3)
      18   0   -8 (1,1,1) (32,1,1) kernel4(b=5)
</pre>
 <p>
  This command supports filters and the default is
  <span class="pre">
   kernel
  </span>
  <span class="pre">
   all
  </span>
  .
 </p>
 <h3>
  <span class="section-number">
   8.3.10.
  </span>
  info cuda contexts
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-contexts" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  This command enumerates all the CUDA contexts running on all GPUs. A
  <span class="pre">
   *
  </span>
  indicates the context currently in focus. This command shows whether a context is currently active on a device or not.
 </p>
 <pre>(cuda-gdb) info cuda contexts
     Context Dev    State
  0x080b9518   0 inactive
* 0x08067948   0   active
</pre>
 <h3>
  <span class="section-number">
   8.3.11.
  </span>
  info cuda managed
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#info-cuda-managed" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  This command shows all the static managed variables on the device or on the host depending on the focus.
 </p>
 <pre>(cuda-gdb) info cuda managed
Static managed variables on device 0 are:
managed_var = 3
managed_consts = {one = 1, e = 2.71000004, pi = 3.1400000000000001}
</pre>
 <h2>
  <span class="section-number">
   8.4.
  </span>
  Disassembly
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#disassembly" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The device SASS code can be disassembled using the standard GDB disassembly instructions such as
  <span class="pre">
   x/i
  </span>
  and
  <span class="pre">
   display/i
  </span>
  .
 </p>
 <pre>(cuda-gdb) x/4i $pc-32
   0xa689a8 &lt;acos_main(acosParams)+824&gt;: MOV R0, c[0x0][0x34]
   0xa689b8 &lt;acos_main(acosParams)+840&gt;: MOV R3, c[0x0][0x28]
   0xa689c0 &lt;acos_main(acosParams)+848&gt;: IMUL R2, R0, R3
=&gt; 0xa689c8 &lt;acos_main(acosParams)+856&gt;: MOV R0, c[0x0][0x28]
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  For disassembly instruction to work properly,
  <span class="pre">
   cuobjdump
  </span>
  must be installed and present in your
  <span class="pre">
   $PATH
  </span>
  .
 </p>
 <p>
  In the disassembly view, the current pc is prefixed with
  <span class="pre">
   =&gt;
  </span>
  . For Maxwell (SM 5.0) and newer architectures, if an instruction triggers an exception it will be prefixed with
  <span class="pre">
   *&gt;
  </span>
  . If the pc and errorpc are the same instruction it will be prefixed with
  <span class="pre">
   *=&gt;
  </span>
  .
 </p>
 <p>
  For example, consider the following exception:
 </p>
 <pre>CUDA Exception: Warp Illegal Address
The exception was triggered at PC 0x555555c08620 (memexceptions_kernel.cu:17)

Thread 1 "memexceptions" received signal CUDA_EXCEPTION_14, Warp Illegal Address.
[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0]
0x0000555555c08fb0 in exception_kernel&lt;&lt;&lt;(1,1,1),(1,1,1)&gt;&gt;&gt; (data=0x7fffccc00000, exception=MMU_FAULT) at memexceptions_kernel.cu:50
50  }
(cuda-gdb)
</pre>
 <p>
  The
  <span class="pre">
   disas
  </span>
  command can be used to view both the PC and the error PC that triggered the exception.
 </p>
 <pre>(cuda-gdb) disas $pc,+16
Dump of assembler code from 0x555555c08fb0 to 0x555555c08fc0:
=&gt; 0x0000555555c08fb0 &lt;_Z16exception_kernelPv11exception_t+3504&gt;:  ERRBAR
End of assembler dump.
</pre>
 <pre>(cuda-gdb) disas $errorpc,+16
Dump of assembler code from 0x555555c08620 to 0x555555c08630:
*&gt; 0x0000555555c08620 &lt;_Z16exception_kernelPv11exception_t+1056&gt;:  ST.E.U8.STRONG.SYS [R6.64], R5
End of assembler dump.
</pre>
 <h2>
  <span class="section-number">
   8.5.
  </span>
  Registers
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#registers" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The device registers code can be inspected/modified using the standard GDB commands such as
  <span class="pre">
   info
  </span>
  <span class="pre">
   registers
  </span>
  .
 </p>
 <pre>(cuda-gdb) info registers $R0 $R1 $R2 $R3
R0             0xf0 240
R1             0xfffc48 16776264
R2             0x7800   30720
R3             0x80 128
</pre>
 <p>
  The registers are also accessible as
  <span class="pre">
   $R&lt;regnum&gt;
  </span>
  built-in variables, for example:
 </p>
 <pre>(cuda-gdb) printf "%d %d\n", $R0*$R3, $R2
30720 30720
</pre>
 <p>
  Values of predicate and CC registers can be inspecting by printing system registers group or by using their respective pseudo-names:
  <span class="pre">
   $P0
  </span>
  ..
  <span class="pre">
   $P6
  </span>
  and
  <span class="pre">
   $CC
  </span>
  .
 </p>
 <pre>(cuda-gdb) info registers system
P0             0x1  1
P1             0x1  1
P2             0x0  0
P3             0x0  0
P4             0x0  0
P5             0x0  0
P6             0x1  1
CC             0x0  0
</pre>
 <h2>
  <span class="section-number">
   8.6.
  </span>
  Const banks
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#const-banks" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Memory allocated in the constant address space of GPU memory resides in two dimensional arrays called constant banks.
Constant banks are noted
  <span class="pre">
   c[X][Y]
  </span>
  where
  <span class="pre">
   X
  </span>
  is the bank number and
  <span class="pre">
   Y
  </span>
  the offset.
The memory address of a given bank/offset pair is obtained via the convenience function
  <span class="pre">
   $_cuda_const_bank(bank,
  </span>
  <span class="pre">
   offset)
  </span>
  .
 </p>
 <pre>(cuda-gdb) disass $pc,+16
Dump of assembler code from 0x7fffd5043d40 to 0x7fffd5043d50:
=&gt; 0x00007fffd5043d40 &lt;_Z9acos_main10acosParams+1856&gt;:  MOV R0, c[0x0][0xc]
End of assembler dump.
(cuda-gdb) p *$_cuda_const_bank(0x0,0xc)
$1 = 8
</pre>
 <h1>
  <span class="section-number">
   9.
  </span>
  Event Notifications
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#event-notifications" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  As the application is making forward progress, CUDA-GDB notifies the users about kernel events and context events. Within CUDA-GDB,
  kernel
  refers to the device code that executes on the GPU, while
  context
  refers to the virtual address space on the GPU for the kernel. You can enable output of CUDA context and kernel events to review the flow of the active contexts and kernels. By default, only context event messages are displayed.
 </p>
 <h2>
  <span class="section-number">
   9.1.
  </span>
  Context Events
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#context-events" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Any time a CUDA context is created, pushed, popped, or destroyed by the application, CUDA-GDB can optionally display a notification message. The message includes the context id and the device id to which the context belongs.
 </p>
 <pre>[Context Create of context 0xad2fe60 on Device 0]
[Context Destroy of context 0xad2fe60 on Device 0]
</pre>
 <p>
  By default, context event notification is disabled. The context event notification policy is controlled with the
  <span class="pre">
   context_events
  </span>
  option.
 </p>
 <ul>
  <li>
   <pre>(cuda-gdb) set cuda context_events off
</pre>
   <p>
    CUDA-GDB does not display the context event notification messages (default).
   </p>
  </li>
  <li>
   <pre>(cuda-gdb) set cuda context_events on
</pre>
   <p>
    CUDA-GDB displays the context event notification messages.
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   9.2.
  </span>
  Kernel Events
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#kernel-events" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Any time CUDA-GDB is made aware of the launch or the termination of a CUDA kernel, a notification message can be displayed. The message includes the kernel id, the kernel name, and the device to which the kernel belongs.
 </p>
 <pre>[Launch of CUDA Kernel 1 (kernel3) on Device 0]
[Termination of CUDA Kernel 1 (kernel3) on Device 0]
</pre>
 <p>
  The kernel event notification policy is controlled with
  <span class="pre">
   kernel_events
  </span>
  and
  <span class="pre">
   kernel_events_depth
  </span>
  options.
 </p>
 <ul>
  <li>
   <pre>(cuda-gdb) set cuda kernel_events none
</pre>
  </li>
 </ul>
 <p>
  Possible options are:
 </p>
 <span class="pre">
  none
 </span>
 <p>
  no kernel, application or system (default)
 </p>
 <span class="pre">
  application
 </span>
 <p>
  kernel launched by the user application
 </p>
 <span class="pre">
  system
 </span>
 <p>
  any kernel launched by the driver, such as memset
 </p>
 <span class="pre">
  all
 </span>
 <p>
  any kernel, application and system
 </p>
 <ul>
  <li>
   <pre>(cuda-gdb) set cuda kernel_events_depth 0
</pre>
   <p>
    Controls the maximum depth of the kernels after which no kernel event notifications will be displayed. A value of zero means that there is no maximum and that all the kernel notifications are displayed. A value of one means that the debugger will display kernel event notifications only for kernels launched from the CPU (default).
   </p>
  </li>
 </ul>
 <h1>
  <span class="section-number">
   10.
  </span>
  Automatic Error Checking
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#automatic-error-checking" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   10.1.
  </span>
  Checking API Errors
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#checking-api-errors" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA-GDB can automatically check the return code of any driver API or runtime API call. If the return code indicates an error, the debugger will stop or warn the user.
 </p>
 <p>
  The behavior is controlled with the
  <span class="pre">
   set
  </span>
  <span class="pre">
   cuda
  </span>
  <span class="pre">
   api_failures
  </span>
  option. Three modes are supported:
 </p>
 <ul class="simple">
  <li>
   <p>
    <span class="pre">
     hide
    </span>
    CUDA API call failures are not reported
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     ignore
    </span>
    Warning message is printed for every fatal CUDA API call failure (default)
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     stop
    </span>
    The application is stopped when a CUDA API call returns a fatal error
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     ignore_all
    </span>
    Warning message is printed for every CUDA API call failure
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     stop_all
    </span>
    The application is stopped when a CUDA API call returns any error
   </p>
  </li>
 </ul>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The success return code and other non-error return codes are ignored. For the driver API, those are:
  <span class="pre">
   CUDA_SUCCESS
  </span>
  and
  <span class="pre">
   CUDA_ERROR_NOT_READY
  </span>
  . For the runtime API, they are
  <span class="pre">
   cudaSuccess
  </span>
  and
  <span class="pre">
   cudaErrorNotReady
  </span>
  .
 </p>
 <h2>
  <span class="section-number">
   10.2.
  </span>
  GPU Error Reporting
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#gpu-error-reporting" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  With improved GPU error reporting in CUDA-GDB, application bugs are now easier to identify and easy to fix. The following table shows the new errors that are reported on GPUs with compute capability
  <span class="pre">
   sm_20
  </span>
  and higher.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Continuing the execution of your application after these errors are found can lead to application termination or indeterminate results.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Warp errors may result in instructions to continue executing before the exception is recognized and reported. The reported
  <span class="pre">
   $errorpc
  </span>
  shall contain the precise address of the instruction that caused the exception. If the warp exits after the instruction causing exception has executed, but before the exception has been recognized and reported, it may result in the exception not being reported. CUDA-GDB relies on an active warp present on the device in order to report exceptions. To help avoid this scenario of unreported exceptions:
 </p>
 <ul class="simple">
  <li>
   <p>
    For Volta+ architectures, compile the application with
    <span class="pre">
     -G
    </span>
    . See
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#compiling-the-application">
     Compiling the Application
    </a>
    for more information.
   </p>
  </li>
  <li>
   <p>
    Add
    <span class="pre">
     while(1);
    </span>
    before kernel exit. This shall ensure the exception is recognized and reported.
   </p>
  </li>
  <li>
   <p>
    Rely on the compute-sanitizer
    <span class="pre">
     memcheck
    </span>
    tool to catch accesses that can lead to an exception.
   </p>
  </li>
 </ul>
 <table class="small table-no-stripes table-compact longtable colwidths-given docutils align-default" id="id3">
  <span class="caption-text">
   CUDA Exception Codes
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#id3" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Exception Code
    </p>
   </th>
   <th class="head">
    <p>
     Precision of the Error
    </p>
   </th>
   <th class="head">
    <p>
     Scope of the Error
    </p>
   </th>
   <th class="head">
    <p>
     Description
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     CUDA_EXCEPTION_0 : âDevice Unknown Exceptionâ
    </p>
   </td>
   <td>
    <p>
     Unknown
    </p>
   </td>
   <td>
    <p>
     Global error on the GPU
    </p>
   </td>
   <td>
    <p>
     This is a global GPU error caused by the application which does not match any of the listed error codes below. This should be a rare occurrence. Potentially, this may be due to
     <span class="pre">
      Device
     </span>
     <span class="pre">
      Hardware
     </span>
     <span class="pre">
      Stack
     </span>
     overflows or a kernel generating an exception very close to its termination.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     CUDA_EXCEPTION_1 : âDeprecatedâ
    </p>
   </td>
   <td>
    <p>
     Deprecated
    </p>
   </td>
   <td>
    <p>
     Deprecated
    </p>
   </td>
   <td>
    <p>
     This exception is deprecated and should be treated as
     <span class="pre">
      CUDA_EXCEPTION_0
     </span>
     .
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     CUDA_EXCEPTION_2 : âLane User Stack Overflowâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Per lane/thread error
    </p>
   </td>
   <td>
    <p>
     This occurs when a thread exceeds its stack memory limit.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     CUDA_EXCEPTION_3 : âDevice Hardware Stack Overflowâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Global error on the GPU
    </p>
   </td>
   <td>
    <p>
     This occurs when the application triggers a global hardware stack overflow. The main cause of this error is large amounts of divergence in the presence of function calls.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     CUDA_EXCEPTION_4 : âWarp Illegal Instructionâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Warp error
    </p>
   </td>
   <td>
    <p>
     This occurs when any thread within a warp has executed an illegal instruction.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     CUDA_EXCEPTION_5 : âWarp Out-of-range Addressâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Warp error
    </p>
   </td>
   <td>
    <p>
     This occurs when any thread within a warp accesses an address that is outside the valid range of local or shared memory regions.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     CUDA_EXCEPTION_6 : âWarp Misaligned Addressâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Warp error
    </p>
   </td>
   <td>
    <p>
     This occurs when any thread within a warp accesses an address in the local or shared memory segments that is not correctly aligned.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     CUDA_EXCEPTION_7 : âWarp Invalid Address Spaceâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Warp error
    </p>
   </td>
   <td>
    <p>
     This occurs when any thread within a warp executes an instruction that accesses a memory space not permitted for that instruction.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     CUDA_EXCEPTION_8 : âWarp Invalid PCâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Warp error
    </p>
   </td>
   <td>
    <p>
     This occurs when any thread within a warp advances its PC beyond the 40-bit address space.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     CUDA_EXCEPTION_9 : âWarp Hardware Stack Overflowâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Warp error
    </p>
   </td>
   <td>
    <p>
     This occurs when any thread in a warp triggers a hardware stack overflow. This should be a rare occurrence.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     CUDA_EXCEPTION_10 : âDevice Illegal Addressâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Global error
    </p>
   </td>
   <td>
    <p>
     This occurs when a thread accesses an illegal(out of bounds) global address.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     CUDA_EXCEPTION_11 : âDeprecatedâ
    </p>
   </td>
   <td>
    <p>
     Deprecated
    </p>
   </td>
   <td>
    <p>
     Deprecated
    </p>
   </td>
   <td>
    <p>
     This exception is deprecated and should be treated as
     <span class="pre">
      CUDA_EXCEPTION_0
     </span>
     .
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     CUDA_EXCEPTION_12 : âWarp Assertâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Per warp
    </p>
   </td>
   <td>
    <p>
     This occurs when any thread in the warp hits a device side assertion.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     CUDA_EXCEPTION_13 : âDeprecatedâ
    </p>
   </td>
   <td>
    <p>
     Deprecated
    </p>
   </td>
   <td>
    <p>
     Deprecated
    </p>
   </td>
   <td>
    <p>
     This exception is deprecated and should be treated as
     <span class="pre">
      CUDA_EXCEPTION_0
     </span>
     .
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     CUDA_EXCEPTION_14 : âWarp Illegal Addressâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Per warp
    </p>
   </td>
   <td>
    <p>
     This occurs when a thread accesses an illegal(out of bounds) global/local/shared address.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     CUDA_EXCEPTION_15 : âInvalid Managed Memory Accessâ
    </p>
   </td>
   <td>
    <p>
     Precise
    </p>
   </td>
   <td>
    <p>
     Per host thread
    </p>
   </td>
   <td>
    <p>
     This occurs when a host thread attempts to access managed memory currently used by the GPU.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     CUDA_EXCEPTION_13 : âDeprecatedâ
    </p>
   </td>
   <td>
    <p>
     Deprecated
    </p>
   </td>
   <td>
    <p>
     Deprecated
    </p>
   </td>
   <td>
    <p>
     This exception is deprecated and should be treated as
     <span class="pre">
      CUDA_EXCEPTION_0
     </span>
     .
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     CUDA_EXCEPTION_17 : âCluster Out-of-range Addressâ
    </p>
   </td>
   <td>
    <p>
     Not precise
    </p>
   </td>
   <td>
    <p>
     Per Cuda Cluster
    </p>
   </td>
   <td>
    <p>
     This occurs when any thread within a block accesses an address that is outside the valid range of shared memory regions belonging to the cluster.
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     CUDA_EXCEPTION_18 : âCluster Target Block Not Presentâ
    </p>
   </td>
   <td>
    <p>
     Not precise
    </p>
   </td>
   <td>
    <p>
     Per Cuda Cluster
    </p>
   </td>
   <td>
    <p>
     This occurs when any thread within a block accesses another block that is outside the valid range of blocks belonging to the cluster.
    </p>
   </td>
  </tr>
 </table>
 <h2>
  <span class="section-number">
   10.3.
  </span>
  Autostep
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#autostep" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Autostep is a command to increase the precision of CUDA exceptions to the exact lane and instruction, when they would not have been otherwise.
 </p>
 <p>
  Under normal execution, an exception may be reported several instructions after the exception occurred, or the exact thread where an exception occurred may not be known unless the exception is a lane error. However, the precise origin of the exception can be determined if the program is being single-stepped when the exception occurs. Single- stepping manually is a slow and tedious process; stepping takes much longer than normal execution and the user has to single-step each warp individually.
 </p>
 <p>
  Autostep aides the user by allowing them to specify sections of code where they suspect an exception could occur, and these sections are automatically and transparently single- stepped the program is running. The rest of the program is executed normally to minimize the slow-down caused by single-stepping. The precise origin of an exception will be reported if the exception occurs within these sections. Thus the exact instruction and thread where an exception occurred can be found quickly and with much less effort by using autostep.
 </p>
 <p>
  Autostep Usage
 </p>
 <pre>autostep [LOCATION]
autostep [LOCATION] for LENGTH [lines|instructions]
</pre>
 <ul class="simple">
  <li>
   <p>
    <span class="pre">
     LOCATION
    </span>
    may be anything that you use to specify the location of a breakpoint, such as a line number, function name, or an instruction address preceded by an asterisk. If no
    <span class="pre">
     LOCATION
    </span>
    is specified, then the current instruction address is used.
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     LENGTH
    </span>
    specifies the size of the autostep window in number of lines or instructions (
    lines
    and
    instructions
    can be shortened, e.g.,
    l
    or
    i
    ). If the length type is not specified, then
    lines
    is the default. If the
    <span class="pre">
     for
    </span>
    clause is omitted, then the default is 1 line.
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     astep
    </span>
    can be used as an alias for the
    <span class="pre">
     autostep
    </span>
    command.
   </p>
  </li>
  <li>
   <p>
    Calls to functions made during an autostep will be stepped over.
   </p>
  </li>
  <li>
   <p>
    In case of divergence, the length of the autostep window is determined by the number of lines or instructions the first active lane in each warp executes.
    Divergent lanes are also single stepped, but the instructions they execute do not count towards the length of the autostep window.
   </p>
  </li>
  <li>
   <p>
    If a breakpoint occurs while inside an autostep window, the warp where the breakpoint was hit will not continue autostepping when the program is resumed. However, other warps may continue autostepping.
   </p>
  </li>
  <li>
   <p>
    Overlapping autosteps are not supported.
   </p>
  </li>
 </ul>
 <p>
  If an autostep is encountered while another autostep is being executed, then the second autostep is ignored.
 </p>
 <p>
  If an autostep is set before the location of a memory error and no memory error is hit, then it is possible that the chosen window is too small. This may be caused by the presence of function calls between the address of the autostep location and the instruction that triggers the memory error. In that situation, either increase the size of the window to make sure that the faulty instruction is included, or move to the autostep location to an instruction that will be executed closer in time to the faulty instruction.
 </p>
 <p>
  Related Commands
 </p>
 <p>
  Autosteps and breakpoints share the same numbering so most commands that work with breakpoints will also work with autosteps.
 </p>
 <p>
  <span class="pre">
   info
  </span>
  <span class="pre">
   autosteps
  </span>
  shows all breakpoints and autosteps. It is similar to
  <span class="pre">
   info
  </span>
  <span class="pre">
   breakpoints
  </span>
  .
 </p>
 <pre>(cuda-gdb) info autosteps
Num  Type      Disp Enb Address            What
1    autostep  keep y   0x0000000000401234 in merge at sort.cu:30 for 49 instructions
3    autostep  keep y   0x0000000000489913 in bubble at sort.cu:94 for 11 lines
</pre>
 <p>
  <span class="pre">
   disable
  </span>
  <span class="pre">
   autosteps
  </span>
  disables an autostep. It is equivalent to
  <span class="pre">
   disable
  </span>
  <span class="pre">
   breakpoints
  </span>
  <span class="pre">
   n
  </span>
  .
 </p>
 <p>
  <span class="pre">
   delete
  </span>
  <span class="pre">
   autosteps
  </span>
  <span class="pre">
   n
  </span>
  deletes an autostep. It is quivalent to
  <span class="pre">
   delete
  </span>
  <span class="pre">
   breakpoints
  </span>
  <span class="pre">
   n
  </span>
  .
 </p>
 <p>
  <span class="pre">
   ignore
  </span>
  <span class="pre">
   n
  </span>
  <span class="pre">
   i
  </span>
  tells the debugger to not single-step the next
  i
  times the debugger enters the window for autostep
  n
  . This command already exists for breakpoints.
 </p>
 <h1>
  <span class="section-number">
   11.
  </span>
  Walk-Through Examples
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#walk-through-examples" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  The chapter contains two CUDA-GDB walk-through examples:
 </p>
 <ul class="simple">
  <li>
   <p>
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#example-bitreverse">
     Example: bitreverse
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#example-autostep">
     Example: autostep
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#example-mpi">
     Example: MPI CUDA Application
    </a>
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   11.1.
  </span>
  Example: bitreverse
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#example-bitreverse" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This section presents a walk-through of CUDA-GDB by debugging a sample applicationâcalled
  <span class="pre">
   bitreverse
  </span>
  âthat performs a simple 8 bit reversal on a data set.
 </p>
 <p>
  Source Code
 </p>
 <pre>1  #include &lt;stdio.h&gt;
2  #include &lt;stdlib.h&gt;
3
4  // Simple 8-bit bit reversal Compute test
5
6  #define N 256
7
8  __global__ void bitreverse(void *data) {
9     unsigned int *idata = (unsigned int*)data;
10    extern __shared__ int array[];
11
12    array[threadIdx.x] = idata[threadIdx.x];
13
14    array[threadIdx.x] = ((0xf0f0f0f0 &amp; array[threadIdx.x]) &gt;&gt; 4) |
15                        ((0x0f0f0f0f &amp; array[threadIdx.x]) &lt;&lt; 4);
16    array[threadIdx.x] = ((0xcccccccc &amp; array[threadIdx.x]) &gt;&gt; 2) |
17                        ((0x33333333 &amp; array[threadIdx.x]) &lt;&lt; 2);
18    array[threadIdx.x] = ((0xaaaaaaaa &amp; array[threadIdx.x]) &gt;&gt; 1) |
19                         ((0x55555555 &amp; array[threadIdx.x]) &lt;&lt; 1);
20
21    idata[threadIdx.x] = array[threadIdx.x];
22 }
23
24 int main(void) {
25     void *d = NULL; int i;
26     unsigned int idata[N], odata[N];
27
28     for (i = 0; i &lt; N; i++)
29         idata[i] = (unsigned int)i;
30
31     cudaMalloc((void**)&amp;d, sizeof(int)*N);
32     cudaMemcpy(d, idata, sizeof(int)*N,
33                cudaMemcpyHostToDevice);
34
35     bitreverse&lt;&lt;&lt;1, N, N*sizeof(int)&gt;&gt;&gt;(d);
36
37     cudaMemcpy(odata, d, sizeof(int)*N,
38                cudaMemcpyDeviceToHost);
39
40     for (i = 0; i &lt; N; i++)
41        printf("%u -&gt; %u\n", idata[i], odata[i]);
42
43     cudaFree((void*)d);
44     return 0;
45 }
</pre>
 <h3>
  <span class="section-number">
   11.1.1.
  </span>
  Walking through the Code
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#walking-through-the-code" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <ol class="arabic">
  <li>
   <p>
    Begin by compiling the
    <span class="pre">
     bitreverse.cu
    </span>
    CUDA application for debugging by entering the following command at a shell prompt:
   </p>
   <pre>$ nvcc -g -G bitreverse.cu -o bitreverse
</pre>
   <p>
    This command assumes that the source file name is
    <span class="pre">
     bitreverse.cu
    </span>
    and that no additional compiler flags are required for compilation. See also
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#debug-compilation">
     Debug Compilation
    </a>
   </p>
  </li>
  <li>
   <p>
    Start the CUDA debugger by entering the following command at a shell prompt:
   </p>
   <pre>$ cuda-gdb bitreverse
</pre>
  </li>
  <li>
   <p>
    Set breakpoints. Set both the host (
    <span class="pre">
     main
    </span>
    ) and GPU (
    <span class="pre">
     bitreverse
    </span>
    ) breakpoints here. Also, set a breakpoint at a particular line in the device function (
    <span class="pre">
     bitreverse.cu:18
    </span>
    ).
   </p>
   <pre>(cuda-gdb) break main
Breakpoint 1 at 0x18e1: file bitreverse.cu, line 25.
(cuda-gdb) break bitreverse
Breakpoint 2 at 0x18a1: file bitreverse.cu, line 8.
(cuda-gdb) break 21
Breakpoint 3 at 0x18ac: file bitreverse.cu, line 21.
</pre>
  </li>
  <li>
   <p>
    Run the CUDA application, and it executes until it reaches the first breakpoint (
    <span class="pre">
     main
    </span>
    ) set in
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#walking-through-code__set-breakpoints">
     3
    </a>
    .
   </p>
   <pre>(cuda-gdb) run
Starting program: /Users/CUDA_User1/docs/bitreverse
Reading symbols for shared libraries
..++........................................................... done

Breakpoint 1, main () at bitreverse.cu:25
25  void *d = NULL; int i;
</pre>
  </li>
  <li>
   <p>
    At this point, commands can be entered to advance execution or to print the program state. For this walkthrough, letâs continue until the device kernel is launched.
   </p>
   <pre>(cuda-gdb) continue
Continuing.
Reading symbols for shared libraries .. done
Reading symbols for shared libraries .. done
[Context Create of context 0x80f200 on Device 0]
[Launch of CUDA Kernel 0 (bitreverse&lt;&lt;&lt;(1,1,1),(256,1,1)&gt;&gt;&gt;) on Device 0]
Breakpoint 3 at 0x8667b8: file bitreverse.cu, line 21.
[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0]

Breakpoint 2, bitreverse&lt;&lt;&lt;(1,1,1),(256,1,1)&gt;&gt;&gt; (data=0x110000) at bitreverse.cu:9
9   unsigned int *idata = (unsigned int*)data;
</pre>
   <p>
    CUDAâGDB has detected that a CUDA device kernel has been reached. The debugger prints the current CUDA thread of focus.
   </p>
  </li>
  <li>
   <p>
    Verify the CUDA thread of focus with the
    <span class="pre">
     info
    </span>
    <span class="pre">
     cuda
    </span>
    <span class="pre">
     threads
    </span>
    command and switch between host thread and the CUDA threads:
   </p>
   <pre>(cuda-gdb) info cuda threads
  BlockIdx ThreadIdx To BlockIdx ThreadIdx Count            Virtual PC
Filename   Line

Kernel 0
*  (0,0,0)    (0,0,0)    (0,0,0) (255,0,0)    256 0x0000000000866400 bitreverse.cu     9
(cuda-gdb) thread
[Current thread is 1 (process 16738)]
(cuda-gdb) thread 1
[Switching to thread 1 (process 16738)]
#0  0x000019d5 in main () at bitreverse.cu:34
34    bitreverse&lt;&lt;&lt;1, N, N*sizeof(int)&gt;&gt;&gt;(d);
(cuda-gdb) backtrace
#0  0x000019d5 in main () at bitreverse.cu:34
(cuda-gdb) info cuda kernels
Kernel Dev Grid   SMs Mask GridDim  BlockDim        Name Args
     0   0    1 0x00000001 (1,1,1) (256,1,1) bitreverse data=0x110000
(cuda-gdb) cuda kernel 0
[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane 0]
9    unsigned int *idata = (unsigned int*)data;
(cuda-gdb) backtrace
#0   bitreverse&lt;&lt;&lt;(1,1,1),(256,1,1)&gt;&gt;&gt; (data=0x110000) at bitreverse.cu:9
</pre>
  </li>
  <li>
   <p>
    Corroborate this information by printing the block and thread indexes:
   </p>
   <pre>(cuda-gdb) print blockIdx
$1 = {x = 0, y = 0}
(cuda-gdb) print threadIdx
$2 = {x = 0, y = 0, z = 0)
</pre>
  </li>
  <li>
   <p>
    The grid and block dimensions can also be printed:
   </p>
   <pre>(cuda-gdb) print gridDim
$3 = {x = 1, y = 1}
(cuda-gdb) print blockDim
$4 = {x = 256, y = 1, z = 1)
</pre>
  </li>
  <li>
   <p>
    Advance kernel execution and verify some data:
   </p>
   <pre>(cuda-gdb) next
12       array[threadIdx.x] = idata[threadIdx.x];
(cuda-gdb) next
14       array[threadIdx.x] = ((0xf0f0f0f0 &amp; array[threadIdx.x]) &gt;&gt; 4) |
(cuda-gdb) next
16       array[threadIdx.x] = ((0xcccccccc &amp; array[threadIdx.x]) &gt;&gt; 2) |
(cuda-gdb) next
18       array[threadIdx.x] = ((0xaaaaaaaa &amp; array[threadIdx.x]) &gt;&gt; 1) |
(cuda-gdb) next

Breakpoint 3, bitreverse &lt;&lt;&lt;(1,1),(256,1,1)&gt;&gt;&gt; (data=0x100000) at bitreverse.cu:21
21             idata[threadIdx.x] = array[threadIdx.x];
(cuda-gdb) print array[0]@12
$7 = {0, 128, 64, 192, 32, 160, 96, 224, 16, 144, 80, 208}
(cuda-gdb) print/x array[0]@12
$8 = {0x0, 0x80, 0x40, 0xc0, 0x20, 0xa0, 0x60, 0xe0, 0x10, 0x90, 0x50,
0xd0}

(cuda-gdb) print &amp;data
$9 = (@global void * @parameter *) 0x10
(cuda-gdb) print *(@global void * @parameter *) 0x10
$10 = (@global void * @parameter) 0x100000
</pre>
   <p>
    The resulting output depends on the current content of the memory location.
   </p>
  </li>
  <li>
   <p>
    Since thread (
    <span class="pre">
     0,0,0
    </span>
    ) reverses the value of
    <span class="pre">
     0
    </span>
    , switch to a different thread to show more interesting data:
   </p>
   <pre>(cuda-gdb) cuda thread 170
[Switching focus to CUDA kernel 0, grid 1, block (0,0,0), thread
(170,0,0), device 0, sm 0, warp 5, lane 10]
</pre>
  </li>
  <li>
   <p>
    Delete the breakpoints and continue the program to completion:
   </p>
   <pre>(cuda-gdb) delete breakpoints
Delete all breakpoints? (y or n) y
(cuda-gdb) continue
Continuing.

Program exited normally.
(cuda-gdb)
</pre>
  </li>
 </ol>
 <h2>
  <span class="section-number">
   11.2.
  </span>
  Example: autostep
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#example-autostep" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This section shows how to use the autostep command and demonstrates how it helps increase the precision of memory error reporting.
 </p>
 <p>
  Source Code
 </p>
 <pre>1  #define NUM_BLOCKS 8
2  #define THREADS_PER_BLOCK 64
3
4  __global__ void example(int **data) {
5    int value1, value2, value3, value4, value5;
6    int idx1, idx2, idx3;
7
8    idx1 = blockIdx.x * blockDim.x;
9    idx2 = threadIdx.x;
10   idx3 = idx1 + idx2;
11   value1 = *(data[idx1]);
12   value2 = *(data[idx2]);
13   value3 = value1 + value2;
14   value4 = value1 * value2;
15   value5 = value3 + value4;
16   *(data[idx3]) = value5;
17   *(data[idx1]) = value3;
18   *(data[idx2]) = value4;
19   idx1 = idx2 = idx3 = 0;
20 }
21
22 int main(int argc, char *argv[]) {
23   int *host_data[NUM_BLOCKS * THREADS_PER_BLOCK];
24   int **dev_data;
25   const int zero = 0;
26
27   /* Allocate an integer for each thread in each block */
28   for (int block = 0; block &lt; NUM_BLOCKS; block++) {
29     for (int thread = 0; thread &lt; THREADS_PER_BLOCK; thread++) {
30       int idx = thread + block * THREADS_PER_BLOCK;
31       cudaMalloc(&amp;host_data[idx], sizeof(int));
32       cudaMemcpy(host_data[idx], &amp;zero, sizeof(int),
33                  cudaMemcpyHostToDevice);
34     }
35   }
36
37   /* This inserts an error into block 3, thread 39*/
38   host_data[3*THREADS_PER_BLOCK  + 39] = NULL;
39
40   /* Copy the array of pointers to the device */
41   cudaMalloc((void**)&amp;dev_data,  sizeof(host_data));
42   cudaMemcpy(dev_data, host_data, sizeof(host_data), cudaMemcpyHostToDevice);
43
44   /* Execute example */
45   example &lt;&lt;&lt; NUM_BLOCKS, THREADS_PER_BLOCK &gt;&gt;&gt; (dev_data);
46   cudaThreadSynchronize();
47 }
</pre>
 <p>
  In this small example, we have an array of pointers to integers, and we want to do some operations on the integers. Suppose, however, that one of the pointers is NULL as shown in line 38. This will cause
  <span class="pre">
   CUDA_EXCEPTION_10
  </span>
  <span class="pre">
   "Device
  </span>
  <span class="pre">
   Illegal
  </span>
  <span class="pre">
   Address"
  </span>
  to be thrown when we try to access the integer that corresponds with block 3, thread 39. This exception should occur at line 16 when we try to write to that value.
 </p>
 <h3>
  <span class="section-number">
   11.2.1.
  </span>
  Debugging with Autosteps
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#debugging-with-autosteps" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <ol class="arabic">
  <li>
   <p>
    Compile the example and start CUDAâGDB as normal. We begin by running the program:
   </p>
   <pre>(cuda-gdb) run
Starting program: /home/jitud/cudagdb_test/autostep_ex/example
[Thread debugging using libthread_db enabled] [New Thread 0x7ffff5688700 (LWP 9083)]
[Context Create of context 0x617270 on Device 0]
[Launch of CUDA Kernel 0 (example&lt;&lt;&lt;(8,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0]

Program received signal CUDA_EXCEPTION_10, Device Illegal Address.
[Switching focus to CUDA kernel 0, grid 1, block (1,0,0), thread (0,0,0), device 0, sm 1, warp 0, lane 0]
0x0000000000796f60 in example (data=0x200300000) at example.cu:17
17        *(data[idx1]) = value3;
</pre>
   <p>
    As expected, we received a
    <span class="pre">
     CUDA_EXCEPTION_10
    </span>
    . However, the reported thread is block 1, thread 0 and the line is 17. Since
    <span class="pre">
     CUDA_EXCEPTION_10
    </span>
    is a Global error, there is no thread information that is reported, so we would manually have to inspect all 512 threads.
   </p>
  </li>
  <li>
   <p>
    Set
    <span class="pre">
     autosteps
    </span>
    . To get more accurate information, we reason that since
    <span class="pre">
     CUDA_EXCEPTION_10
    </span>
    is a memory access error, it must occur on code that accesses memory. This happens on lines 11, 12, 16, 17, and 18, so we set two autostep windows for those areas:
   </p>
   <pre>(cuda-gdb) autostep 11 for 2 lines
Breakpoint 1 at 0x796d18: file example.cu, line 11.
Created autostep of length 2 lines
(cuda-gdb) autostep 16 for 3 lines
Breakpoint 2 at 0x796e90: file example.cu, line 16.
Created autostep of length 3 lines
</pre>
  </li>
  <li>
   <p>
    Finally, we run the program again with these autosteps:
   </p>
   <pre>(cuda-gdb) run
The program being debugged has been started already.
Start it from the beginning? (y or n) y
[Termination of CUDA Kernel 0 (example&lt;&lt;&lt;(8,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0]
Starting program: /home/jitud/cudagdb_test/autostep_ex/example
[Thread debugging using libthread_db enabled]
[New Thread 0x7ffff5688700 (LWP 9089)]
[Context Create of context 0x617270 on Device 0]
[Launch of CUDA Kernel 1 (example&lt;&lt;&lt;(8,1,1),(64,1,1)&gt;&gt;&gt;) on Device 0]
[Switching focus to CUDA kernel 1, grid 1, block (0,0,0), thread (0,0,0),
device 0, sm 0, warp 0, lane 0]

Program received signal CUDA_EXCEPTION_10, Device Illegal Address.
[Current focus set to CUDA kernel 1, grid 1, block (3,0,0), thread
(32,0,0), device 0, sm 1, warp 3, lane 0]
Autostep precisely caught exception at example.cu:16 (0x796e90)
</pre>
   <p>
    This time we correctly caught the exception at line 16. Even though
    <span class="pre">
     CUDA_EXCEPTION_10
    </span>
    is a global error, we have now narrowed it down to a warp error, so we now know that the thread that threw the exception must have been in the same warp as block 3, thread 32.
   </p>
  </li>
 </ol>
 <p>
  In this example, we have narrowed down the scope of the error from 512 threads down to 32 threads just by setting two
  <span class="pre">
   autosteps
  </span>
  and reârunning the program.
 </p>
 <h2>
  <span class="section-number">
   11.3.
  </span>
  Example: MPI CUDA Application
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#example-mpi-cuda-application" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  For large scale MPI CUDA application debugging, NVIDIA recommends using parallel debuggers supplied by our partners Allinea and Totalview. Both make excellent parallel debuggers with extended support for CUDA. However, for debugging smaller applications, or for debugging just a few processes in a large application, CUDA-GDB can be used.
 </p>
 <p>
  If the cluster nodes have xterm support, launch CUDA-GDB in the same way you would launch gdb with your job launcher. For example:
 </p>
 <pre>$ mpirun -np 4 -host nv1,nv2 xterm -e cuda-gdb a.out
</pre>
 <p>
  You may have to export the
  <span class="pre">
   DISPLAY
  </span>
  variable to make sure that the xterm finds its way back to your display. For example:
 </p>
 <pre>$ mpirun -np 4 -host nv1,nv2 -x DISPLAY=host.nvidia.com:0 xterm -e cuda-gdb a.out
</pre>
 <p>
  Job launchers have different ways of exporting environment variables to the cluster nodes. Consult your job launcher documentation for more details.
 </p>
 <p>
  When xterm is not supported by your cluster environment, you can insert a spin loop inside your program, ssh to the compute node(s), and attach onto the MPI processes. Somewhere near the start of your program, add a code snippet similar to the following:
 </p>
 <pre><span class="p">{</span>
<span class="kt">int</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>
<span class="kt">char</span><span class="n">host</span><span class="p">[</span><span class="mi">256</span><span class="p">];</span>
<span class="n">printf</span><span class="p">(</span><span class="s">"PID %d on node %s is ready for attach</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span>
<span class="n">getpid</span><span class="p">(),</span><span class="n">host</span><span class="p">);</span>
<span class="n">fflush</span><span class="p">(</span><span class="n">stdout</span><span class="p">);</span>
<span class="k">while</span><span class="p">(</span><span class="mi">0</span><span class="o">==</span><span class="n">i</span><span class="p">)</span><span class="p">{</span>
<span class="n">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">);</span>
<span class="p">}</span>
<span class="p">}</span>
</pre>
 <p>
  Recompile and launch the application. After it starts, ssh to the node(s) of interest and attach to the process using CUDA-GDB. Set the variable
  <span class="pre">
   i
  </span>
  to 1 to break out of the loop:
 </p>
 <pre>$ mpirun -np 2 -host nv1,nv2 a.out
PID 20060 on node nv1 is ready for attach
PID 5488 on node nv2 is ready for attach
</pre>
 <pre>$ ssh nv1
[nv1]$ cuda-gdb --pid 5488
</pre>
 <pre>$ ssh nv2
[nv2]$ cuda-gdb --pid 20060
</pre>
 <p>
  For larger applications, you can conditionalize the spin loop based on the MPI rank using the
  <span class="pre">
   MPI_Comm_rank
  </span>
  function.
 </p>
 <p>
  For devices with compute capability below 6.0, the software preemption workaround described in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#multiple-debuggers">
   Multiple Debuggers
  </a>
  does not work with MPI applications. For those GPUs, ensure each MPI rank targets a unique GPU.
 </p>
 <p>
  If
  <span class="pre">
   CUDA_VISIBLE_DEVICES
  </span>
  is set, it may cause problems with the GPU selection logic in the MPI application. It may also prevent CUDA IPC working between GPUs on a node.
 </p>
 <h1>
  <span class="section-number">
   12.
  </span>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#advanced-settings">
   Tips and Tricks
  </a>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#tips-and-tricks" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  This section serves as reference to advanced settings and various tips and tricks users of CUDA-GDB can utilize which are not documented elsewhere.
 </p>
 <h2>
  <span class="section-number">
   12.1.
  </span>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-break-on-launch">
   set cuda break_on_launch
  </a>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-break-on-launch" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To break on the first instruction of every launched kernel, set the
  <span class="pre">
   break_on_launch
  </span>
  option to application:
 </p>
 <pre>(cuda-gdb) set cuda break_on_launch application
</pre>
 <p>
  Possible options are:
 </p>
 <span class="pre">
  none
 </span>
 <p>
  no kernel, application or system (default)
 </p>
 <span class="pre">
  application
 </span>
 <p>
  kernel launched by the user application
 </p>
 <span class="pre">
  system
 </span>
 <p>
  any kernel launched by the driver, such as memset
 </p>
 <span class="pre">
  all
 </span>
 <p>
  any kernel, application and system
 </p>
 <p>
  Those automatic breakpoints are not displayed by the info breakpoints command and are managed separately from individual breakpoints. Turning off the option will not delete other individual breakpoints set to the same address and vice-versa.
 </p>
 <h2>
  <span class="section-number">
   12.2.
  </span>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-launch-blocking">
   set cuda launch_blocking
  </a>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-launch-blocking" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  When enabled, the kernel launches are synchronous as if the environment variable
  <span class="pre">
   CUDA_LAUNCH_BLOCKING
  </span>
  had been set to 1. Once blocking, the launches are effectively serialized and may be easier to debug.
 </p>
 <ul>
  <li>
   <pre>(cuda-gdb) set cuda launch_blocking off
</pre>
   <p>
    The kernel launches are launched synchronously or asynchronously as dictacted by the application. This is the default.
   </p>
  </li>
  <li>
   <pre>(cuda-gdb) set cuda launch_blocking on
</pre>
   <p>
    The kernel launches are synchronous. If the application has already started, the change will only take affect after the current session has terminated.
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   12.3.
  </span>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-notify">
   set cuda notify
  </a>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-notify" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Any time a CUDA event occurs, the debugger needs to be notified. The notification takes place in the form of a signal being sent to a host thread. The host thread to receive that special signal is determined with the
  <span class="pre">
   set
  </span>
  <span class="pre">
   cuda
  </span>
  <span class="pre">
   notify
  </span>
  option.
 </p>
 <ul>
  <li>
   <pre>(cuda-gdb) set cuda notify youngest
</pre>
   <p>
    The host thread with the smallest thread id will receive the notification signal (default).
   </p>
  </li>
  <li>
   <pre>(cuda-gdb) set cuda notify random
</pre>
   <p>
    An arbitrary host thread will receive the notification signal.
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   12.4.
  </span>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-ptx-cache">
   set cuda ptx_cache
  </a>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-ptx-cache" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Before accessing the value of a variable, the debugger checks whether the variable is live or not at the current PC. On CUDA devices, the variables may not be live all the time and will be reported as âOptimized Outâ.
 </p>
 <p>
  CUDA-GDB offers an option to circumvent this limitation by caching the value of the variable at the PTX register level. Each source variable is compiled into a PTX register, which is later mapped to one or more hardware registers. Using the debug information emitted by the compiler, the debugger may be able cache the value of a PTX register based on the latest hardware register it was mapped to at an earlier time.
 </p>
 <p>
  This optimization is always correct. When enabled, the cached value will be displayed as the normal value read from an actual hardware register and indicated with the
  <span class="pre">
   (cached)
  </span>
  prefix. The optimization will only kick in while single-stepping the code.
 </p>
 <ul>
  <li>
   <pre>(cuda-gdb) set cuda ptx_cache off
</pre>
   <p>
    The debugger only read the value of live variables.
   </p>
  </li>
  <li>
   <pre>(cuda-gdb) set cuda ptx_cache on
</pre>
   <p>
    The debugger will use the cached value when possible. This setting is the default and is always safe.
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   12.5.
  </span>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-single-stepping-optimizations">
   set cuda single_stepping_optimizations
  </a>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-single-stepping-optimizations" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Single-stepping can take a lot of time. When enabled, this option tells the debugger to use safe tricks to accelerate single-stepping.
 </p>
 <ul>
  <li>
   <pre>(cuda-gdb) set cuda single_stepping_optimizations off
</pre>
   <p>
    The debugger will not try to accelerate single-stepping. This is the unique and default behavior in the 5.5 release and earlier.
   </p>
  </li>
  <li>
   <pre>(cuda-gdb) set cuda single_stepping_optimizations on
</pre>
   <p>
    The debugger will use safe techniques to accelerate single-stepping. This is the default starting with the 6.0 release.
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   12.6.
  </span>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-thread-selection">
   set cuda thread_selection
  </a>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-thread-selection" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  When the debugger must choose an active thread to focus on, the decision is guided by a heuristics. The
  <span class="pre">
   set
  </span>
  <span class="pre">
   cuda
  </span>
  <span class="pre">
   thread_selection
  </span>
  guides those heuristics.
 </p>
 <ul>
  <li>
   <pre>(cuda-gdb) set cuda thread_selection logical
</pre>
   <p>
    The thread with the lowest blockIdx/threadIdx coordinates is selected.
   </p>
  </li>
  <li>
   <pre>(cuda-gdb) set cuda thread_selection physical
</pre>
   <p>
    The thread with the lowest dev/sm/warp/lane coordinates is selected.
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   12.7.
  </span>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-value-extrapolation">
   set cuda value_extrapolation
  </a>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#set-cuda-value-extrapolation" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Before accessing the value of a variable, the debugger checks whether the variable is live or not at the current PC. On CUDA devices, the variables may not be live all the time and will be reported as âOptimized Outâ.
 </p>
 <p>
  CUDA-GDB offers an option to opportunistically circumvent this limitation by extrapolating the value of a variable when the debugger would otherwise mark it as optimized out. The extrapolation is not guaranteed to be accurate and must be used carefully. If the register that was used to store the value of a variable has been reused since the last time the variable was seen as live, then the reported value will be wrong. Therefore, any value printed using the option will be marked as
  <span class="pre">
   "(possibly)"
  </span>
  .
 </p>
 <ul>
  <li>
   <pre>(cuda-gdb) set cuda value_extrapolation off
</pre>
   <p>
    The debugger only read the value of live variables. This setting is the default and is always safe.
   </p>
  </li>
  <li>
   <pre>(cuda-gdb) set cuda value_extrapolation on
</pre>
   <p>
    The debugger will attempt to extrapolate the value of variables beyound their respecitve live ranges. This setting may report erroneous values.
   </p>
  </li>
 </ul>
 <h2>
  <span class="section-number">
   12.8.
  </span>
  Debugging Docker Containers
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#debugging-docker-containers" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  When debugging an application within a Docker container, the PTRACE capability needs to be enabled. The user needs to also ensure that the root file system has both read/write permissions set.
 </p>
 <p>
  To enable the PTRACE capability, add the following to your Docker run command:
 </p>
 <pre>--cap-add=SYS_PTRACE
</pre>
 <h2>
  <span class="section-number">
   12.9.
  </span>
  Switching to Classic Debugger Backend
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#switching-to-classic-debugger-backend" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  A new debugger backend named the Unified Debugger (UD) has been introduced on Linux platforms with the CTK 11.8 release. UD allows for a unified debugger backend shared with debugging tools such as cuda-gdb and NVIDIAÂ® Nsightâ¢ VSE. UD is supported across multiple platforms including both Windows and Linux. The end user experience with UD is transparent to existing tool use.
 </p>
 <p>
  The previous debugger backend, known as the classic debugger backend, can still be used by setting
  <span class="pre">
   CUDBG_USE_LEGACY_DEBUGGER
  </span>
  to 1 in the environment before starting CUDA-GDB.
 </p>
 <p>
  UD is not supported on Maxwell GPUs. Users must switch to the classic debugger backend to debug their applications on Maxwell GPUs.
 </p>
 <h2>
  <span class="section-number">
   12.10.
  </span>
  Thread Block Clusters
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#thread-block-clusters" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA applications that make use of Thread Block Clusters will see the cluster index displayed in the CUDA focus. Both cluster index and cluster dimension can be queried by printing the convenience variables
  <span class="pre">
   clusterIdx
  </span>
  and
  <span class="pre">
   clusterDim
  </span>
  .
 </p>
 <h2>
  <span class="section-number">
   12.11.
  </span>
  Debugging OptiX/RTCore applications
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#debugging-optix-rtcore-applications" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  When debugging programs built with OptiX/RTCore, it may be necessary to set the environment variable
  <span class="pre">
   OPTIX_FORCE_DEPRECATED_LAUNCHER
  </span>
  to 1. If breakpoints are unable to be hit, try setting this environment variable before starting your application.
 </p>
 <h2>
  <span class="section-number">
   12.12.
  </span>
  Debugging on Windows Subsystem for Linux
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#debugging-on-windows-subsystem-for-linux" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  If you are unable to use the debugger on Windows Subsystem for Linux, make sure the debug interface is enabled by setting the registry key
  <span class="pre">
   &gt;HKEY_LOCAL_MACHINE\SOFTWARE\NVIDIA
  </span>
  <span class="pre">
   Corporation\GPUDebugger\EnableInterface
  </span>
  to
  <span class="pre">
   (DWORD)
  </span>
  <span class="pre">
   1
  </span>
 </p>
 <h1>
  <span class="section-number">
   13.
  </span>
  Supported Platforms
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#supported-platforms" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  Host Platform Requirements
 </p>
 <p>
  CUDA-GDB is supported on all the platforms supported by the CUDA toolkit with which it is shipped. See the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">
   CUDA Toolkit release notes
  </a>
  for more information.
 </p>
 <p>
  GPU Requirements
 </p>
 <p>
  Debugging is supported on all CUDA-capable GPUs supported by the current CUDA release.
 </p>
 <p>
  GDB Python integration
 </p>
 <p>
  GDB Python integration is supported in cuda-gdb with a multiple builds mechanism in order to support multiple python3 interpreters across different platforms. The cuda-gdb program is a shell script that selects the associated supported cuda-gdb binary based on the version of python available on the system. Support exists for the following Python versions:
  <span class="pre">
   Python
  </span>
  <span class="pre">
   3.8
  </span>
  ,
  <span class="pre">
   Python
  </span>
  <span class="pre">
   3.9
  </span>
  ,
  <span class="pre">
   Python
  </span>
  <span class="pre">
   3.10
  </span>
  ,
  <span class="pre">
   Python
  </span>
  <span class="pre">
   3.11
  </span>
  , and
  <span class="pre">
   Python
  </span>
  <span class="pre">
   3.12
  </span>
 </p>
 <p>
  Windows Subsystem for Linux (WSL)
 </p>
 <ul class="simple">
  <li>
   <p>
    cuda-gdb supports debugging CUDA application on WSL2.
   </p>
  </li>
  <li>
   <p>
    Make sure this capability is enabled via the registry key
    <span class="pre">
     &gt;HKEY_LOCAL_MACHINE\SOFTWARE\NVIDIA
    </span>
    <span class="pre">
     Corporation\GPUDebugger\EnableInterface
    </span>
    set to
    <span class="pre">
     (DWORD)
    </span>
    <span class="pre">
     1
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    Debugging compute-intensive apps may require to
    <a class="reference external" href="https://learn.microsoft.com/en-us/windows-hardware/drivers/display/tdr-registry-keys">
     increase or disable TDR
    </a>
    .
   </p>
  </li>
 </ul>
 <h1>
  <span class="section-number">
   14.
  </span>
  Common Issues on Supported Operating Systems
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#common-issues-on-supported-operating-systems" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  The following are known issues with the current release on supported operating systems and how to fix them.
 </p>
 <p>
  Python not initialized
 </p>
 <p>
  This happens due to a missing Python 3.x library on the machine, installing it fixes the issue. This can also be caused by having a mismatched major.minor version of libpython installed with the default python3 interpreter in PATH. A libpython version matching the default python3 interpreter in PATH must be available. The libpython version can be determined with the
  <span class="pre">
   python3
  </span>
  <span class="pre">
   --version
  </span>
  command. For example, the following command would tell us that a libpython3.8.so* needs to be installed in a default library search path:
 </p>
 <pre>$ python3 --version
Python 3.8.10
</pre>
 <p>
  Specific commands to install the proper libpython are below.
 </p>
 RHEL 8/9
 <p>
  <span class="pre">
   $
  </span>
  <span class="pre">
   sudo
  </span>
  <span class="pre">
   yum
  </span>
  <span class="pre">
   -y
  </span>
  <span class="pre">
   install
  </span>
  <span class="pre">
   python3-libs
  </span>
 </p>
 Debian 10/11/12
 <p>
  <span class="pre">
   $
  </span>
  <span class="pre">
   sudo
  </span>
  <span class="pre">
   apt-get
  </span>
  <span class="pre">
   -y
  </span>
  <span class="pre">
   install
  </span>
  <span class="pre">
   libpython3-stdlib
  </span>
 </p>
 Fedora 39
 <p>
  <span class="pre">
   $
  </span>
  <span class="pre">
   sudo
  </span>
  <span class="pre">
   yum
  </span>
  <span class="pre">
   -y
  </span>
  <span class="pre">
   install
  </span>
  <span class="pre">
   python3-libs
  </span>
 </p>
 OpenSUSE 15
 <p>
  <span class="pre">
   $
  </span>
  <span class="pre">
   sudo
  </span>
  <span class="pre">
   zypper
  </span>
  <span class="pre">
   install
  </span>
  <span class="pre">
   -y
  </span>
  <span class="pre">
   libpython3
  </span>
 </p>
 Ubuntu 20.04/22.04
 <p>
  <span class="pre">
   $
  </span>
  <span class="pre">
   sudo
  </span>
  <span class="pre">
   apt-get
  </span>
  <span class="pre">
   -y
  </span>
  <span class="pre">
   install
  </span>
  <span class="pre">
   python3.8
  </span>
  <span class="pre">
   $
  </span>
  <span class="pre">
   sudo
  </span>
  <span class="pre">
   apt-get
  </span>
  <span class="pre">
   -y
  </span>
  <span class="pre">
   install
  </span>
  <span class="pre">
   libpython3.8
  </span>
 </p>
 <h1>
  <span class="section-number">
   15.
  </span>
  Known Issues
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#known-issues" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  The following are known issues with the current release.
 </p>
 <ul class="simple">
  <li>
   <p>
    Debugging on Hopper architecture with MCDM enabled is not supported for WSL.
   </p>
  </li>
  <li>
   <p>
    On Windows Subsystem for Linux (WSL), the r555 WDDM driver released with the CUDA Toolkit 12.5 has a known issue when CUDA_ENABLE_COREDUMP_ON_EXCEPTION=1 that will prevent coredump file generation and may cause a segmentation fault. This is not an issue with earlier drivers (ie. r551) or other driver modes (TCC, MCDM). This WDDM issue is expected to be resolved in an upcoming driver release.
   </p>
  </li>
  <li>
   <p>
    Setting a breakpoint on a line within a
    <span class="pre">
     __device__
    </span>
    or
    <span class="pre">
     __global__
    </span>
    function before its module is loaded may result in the breakpoint being temporarily set on the first line of a function below in the source code. As soon as the module for the targeted function is loaded, the breakpoint will be reset properly. In the meantime, the breakpoint may be hit, depending on the application. In those situations, the breakpoint can be safely ignored, and the application can be resumed.
   </p>
  </li>
  <li>
   <p>
    The
    scheduler-locking
    option cannot be set to
    on
    .
   </p>
  </li>
  <li>
   <p>
    Stepping again after stepping out of a kernel results in undetermined behavior. It is recommended to use the âcontinueâ command instead.
   </p>
  </li>
  <li>
   <p>
    When remotely debugging 32-bit applications on a 64-bit server, the cuda-gdbserver binary used must be 32-bit.
   </p>
  </li>
  <li>
   <p>
    Attaching to a CUDA application with Software Preemption enabled in cuda-gdb is not supported.
   </p>
  </li>
  <li>
   <p>
    Attaching to a CUDA application on QNX is not supported.
   </p>
  </li>
  <li>
   <p>
    Attaching to a CUDA application running in MPS client mode is not supported.
   </p>
  </li>
  <li>
   <p>
    Attaching to the MPS server process (nvidia-cuda-mps-server) using cuda-gdb, or starting the MPS server with cuda-gdb is not supported.
   </p>
  </li>
  <li>
   <p>
    If a CUDA application is started in the MPS client mode with cuda-gdb, the MPS client will wait until all other MPS clients have terminated, and will then run as non-MPS application.
   </p>
  </li>
  <li>
   <p>
    Significant performance degradation will occur when the debugger steps over inlined routines.
   </p>
  </li>
 </ul>
 <p>
  Because inlined code blocks may have multiple exit points, under the hood, the debugger steps every single instruction until an exit point is reached, which incurs considerable cost for large routines. The following actions are recommended to avoid this problem:
 </p>
 <ul class="simple">
  <li>
   <p>
    Avoid using
    <span class="pre">
     __forceinline__
    </span>
    when declaring a function. (For code is compiled with debug information, only routines declared with the
    <span class="pre">
     __forceinline__
    </span>
    keyword are actually inlined)
   </p>
  </li>
  <li>
   <p>
    Use the
    <span class="pre">
     until
    </span>
    <span class="pre">
     &lt;line#&gt;
    </span>
    command to step over inlined subroutines.
   </p>
  </li>
  <li>
   <p>
    On Jetson, calls to the cuda API might result in the debugger jumping to _dl_catch_exception(). A workaround is to continue.
   </p>
  </li>
  <li>
   <p>
    On Jetson and Drive devices GPU debugging works correctly only if the debugger is run with the root permissions. Changes to devfs node permissions are required for the debugger to work without running as root.
   </p>
  </li>
  <li>
   <p>
    Debugger can miss reporting an induced trap(
    <span class="pre">
     __trap()
    </span>
    ) in case it is the next instruction executed after the device resumes from a breakpoint.
   </p>
  </li>
  <li>
   <p>
    Debugger can miss reporting breakpoints or exceptions during resume in case new warps are launched on a previously empty SM.
   </p>
  </li>
  <li>
   <p>
    Debugger uses the libpython installed on the system. Use of Python scripting functionality will expose cuda-gdb to the same vulnerabilities as those in the system libpython version. It is recommended to always keep the system libpython library up-to-date.
   </p>
  </li>
  <li>
   <p>
    Debugger doesnât support accesses to shared memory allocations that are imported from other processes using the CUDA IPC APIs. Attempts to access these shared memory allocations by the debugger will result in an error stating access to memory allocations shared via IPC is not supported.
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     break_on_launch
    </span>
    will not function with OptiX/RTCore programs unless
    <span class="pre">
     OPTIX_FORCE_DEPRECATED_LAUNCHER
    </span>
    is set to
    <span class="pre">
     1
    </span>
    .
   </p>
  </li>
  <li>
   <p>
    It is not possible to generate CUDA coredumps on WSL in WDDM mode.
   </p>
  </li>
 </ul>
 <h1>
  <span class="section-number">
   16.
  </span>
  Notices
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#notices" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   16.1.
  </span>
  Notice
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#notice" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
 </p>
 <p>
  NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
 </p>
 <p>
  Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
 </p>
 <p>
  NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
 </p>
 <p>
  NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
 </p>
 <p>
  NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
 </p>
 <p>
  No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
 </p>
 <p>
  Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
 </p>
 <p>
  THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.
 </p>
 <h2>
  <span class="section-number">
   16.2.
  </span>
  OpenCL
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#opencl" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.
 </p>
 <h2>
  <span class="section-number">
   16.3.
  </span>
  Trademarks
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-gdb/index.html#trademarks" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.
 </p>
 <p class="notices">
  <a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">
   Privacy Policy
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">
   Manage My Privacy
  </a>
  |
  <a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">
   Do Not Sell or Share My Data
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">
   Terms of Service
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">
   Accessibility
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">
   Corporate Policies
  </a>
  |
  <a href="https://www.nvidia.com/en-us/product-security/" target="_blank">
   Product Security
  </a>
  |
  <a href="https://www.nvidia.com/en-us/contact/" target="_blank">
   Contact
  </a>
 </p>
 <p>
  Copyright Â© 2012-2024, NVIDIA Corporation &amp; affiliates. All rights reserved.
 </p>
 <p>
  <span class="lastupdated">
   Last updated on Jul 1, 2024.
  </span>
 </p>
</body>
</body></html>