<html><head><title>NVIDIA GPUDirect Storage Installation and Troubleshooting Guide - NVIDIA Docs</title></head><body><body class="Page-body">
 <!-- Putting icons here, so we don't have to include in a bunch of -body hbs's -->
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <ul class="Navigation-items">
  <li class="Navigation-items-item">
   <a href="https://developer.nvidia.com/" target="_blank">
    NVIDIA Developer
   </a>
  </li>
  <li class="Navigation-items-item">
   <a href="https://developer.nvidia.com/blog/" target="_blank">
    Blog
   </a>
  </li>
  <li class="Navigation-items-item">
   <a href="https://forums.developer.nvidia.com/" target="_blank">
    Forums
   </a>
  </li>
  <li class="Navigation-items-item">
   <a class="Button" data-size="small" data-theme="secondary" href="https://docs.nvidia.com/login" rel="nofollow noopener" style="--button-border-radius: 0px">
    Join
   </a>
  </li>
 </ul>
 <a aria-label="home page" href="https://docs.nvidia.com/">
 </a>
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <ul class="Navigation-items">
  <li class="Navigation-items-item">
   <a data-cms-ai="0" href="https://developer.nvidia.com/" target="_blank">
    NVIDIA Developer
   </a>
  </li>
  <li class="Navigation-items-item">
   <a data-cms-ai="0" href="https://developer.nvidia.com/blog/" target="_blank">
    Blog
   </a>
  </li>
  <li class="Navigation-items-item">
   <a data-cms-ai="0" href="https://forums.developer.nvidia.com/" target="_blank">
    Forums
   </a>
  </li>
  <li class="Navigation-items-item">
   <a class="Button" data-cms-ai="0" data-size="small" data-theme="secondary" href="https://docs.nvidia.com/login" rel="nofollow noopener" style="--button-border-radius: 0px">
    Join
   </a>
  </li>
 </ul>
 <span class="label">
  Menu
 </span>
 <a class="Page-BackToTop" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html">
 </a>
 <h1 class="PageHeading-title">
  NVIDIA GPUDirect Storage Installation and Troubleshooting Guide
 </h1>
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/">
  NVIDIA Docs Hub
 </a>
 <span class="Link">
  NVIDIA GPUDirect Storage (GDS)
 </span>
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/index.html">
  NVIDIA GPUDirect Storage
 </a>
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html">
  NVIDIA GPUDirect Storage Installation and Troubleshooting Guide
 </a>
 <span class="TopicPage-version">
  NVIDIA GPUDirect Storage (GDS) (Latest Release)
 </span>
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/pdf/troubleshooting-guide.pdf">
  Download PDF
 </a>
 <h2>
  <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#abstract" id="abstract" name="abstract" shape="rect">
   NVIDIA GPUDirect Storage Installation and Troubleshooting Guide
  </a>
 </h2>
 <p>
  This guide describes how to install, debug, and isolate the performance and functional problems that are related to GDS and is intended for systems administrators and developers.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#intro">
   1. Introduction
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="intro" name="intro" shape="rect">
  </a>
  This guide describes how to debug and isolate the NVIDIA® Magnum IO GPUDirect® Storage (GDS) related performance and functional problems and is intended for systems administrators and developers.
 </p>
 <p>
  GDS enables a direct data path for direct memory access (DMA) transfers between GPU memory and storage, which avoids a bounce buffer through the CPU. This direct path increases system bandwidth and decreases the latency and utilization load on the CPU.
 </p>
 <p>
  Creating this direct path involves distributed filesystems such as NFSoRDMA, DDN EXAScaler® parallel filesystem solutions (based on the Lustre filesystem) and WekaFS, so the GDS environment is composed of multiple software and hardware components. This guide addresses questions related to the GDS installation and helps you triage functionality and performance issues. For non-GDS issues, contact the respective OEM or filesystems vendor to understand and debug the issue.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-install">
   2. Installing GPUDirect Storage
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="troubleshoot-install" name="troubleshoot-install" shape="rect">
  </a>
  This section includes GDS installation, uninstallation, configuration information, and using experimental repos.
 </p>
 Note:
 <p>
  For NVAIE and vGPU environments, please follow steps from their respective documents.
 </p>
 <h3 id="install-prereqs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-prereqs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-prereqs" id="install-prereqs" name="install-prereqs" shape="rect">
    2.1. Before You Install GDS
   </a>
  </a>
 </h3>
 <p>
  To install GDS on a non-DGX platform, complete the following steps:
 </p>
 <ol>
  <li>
   Run the following command to check the current status of IOMMU.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ dmesg | grep -i iommu</p>
        </pre>
   <p>
    On x86_64 based platforms, if IOMMU is
    enabled
    , complete step 2 to disable it, otherwise continue to step 3.
   </p>
  </li>
  <li>
   Disable IOMMU.
   Note:
   <p>
    In our experience,
    <span>
     iommu=off
    </span>
    works the best in terms of functionality and performance. On certain platforms such as DGX A100 and DGX-2,
    <span>
     iommu=pt
    </span>
    is supported.
    <span>
     iommu=on
    </span>
    is not guaranteed to work functionally or in a performant way.
   </p>
   <ol>
    <li>
     Run the following command:
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p>$ sudo vi /etc/default/grub</p>
        </pre>
    </li>
    <li>
     Add one of the following options to the
     <span>
      GRUB_CMDLINE_LINUX_DEFAULT
     </span>
     option.
     <ul class="ul">
      <li class="li">
       If you have an
       AMD
       CPU, add
       <span>
        amd_iommu=off
       </span>
       .
      </li>
      <li class="li">
       If you have an
       Intel
       CPU, add
       <span>
        intel_iommu=off
       </span>
       .
      </li>
     </ul>
     <p>
      If there are already other options, enter a space to separate the options, for example,
      <span>
       GRUB_CMDLINE_LINUX_DEFAULT="console=tty0 amd_iommu=off
      </span>
     </p>
    </li>
    <li>
     Run the following commands:
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p>$ sudo update-grub
$ sudo reboot</p>
        </pre>
    </li>
    <li>
     After the system reboots, to verify that the change took effect, run the following command:
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/cmdline</p>
        </pre>
     <p>
      It should show the options which have been added to the grub file.
     </p>
    </li>
   </ol>
  </li>
  <li>
   Before you run the instructions, please make sure to read the Notes section. Use the instructions in
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mofed-req-install" shape="rect">
    MLNX_OFED Requirements and Installation
   </a>
   to install MLNX_OFED. Notes:
   <ul class="ul">
    <li class="li">
     This step is required ONLY IF you need to enable support for NVMe, NVMf, NFSoRDMA
    </li>
    <li class="li">
     This step is not required for DGX OS 6.x or later.
    </li>
   </ul>
  </li>
 </ol>
 <h3 id="gds-installing">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-installing">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-installing" id="gds-installing" name="gds-installing" shape="rect">
    2.2. Installing GDS
   </a>
  </a>
 </h3>
 <p>
  GDS installation is supported in two ways:
 </p>
 <ul>
  <li>
   using package managers such as Debian and RPMs
  </li>
 </ul>
 <p>
  For installation on DGX platforms, refer to:
 </p>
 <ul>
  <li>
   <p>
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/dgx/dgx-os-6-user-guide/index.html" shape="rect">
     DGX-OS
    </a>
   </p>
  </li>
  <li>
   <p>
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/dgx/dgx-rhel8-install-guide/index.html" shape="rect">
     RHEL 8
    </a>
   </p>
  </li>
 </ul>
 <p>
  For installation on non-DGX platforms, refer to
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html" shape="rect">
   here
  </a>
  .
 </p>
 Note:
 <p>
  For CUDA 11.5.1 and later, if you plan to use Weka FS or IBM SpectrumScale then you must run:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>modprobe nvidia_peermem</p>
        </pre>
 <p>
  This will load the module that supports PeerDirect capabilities. It is necessary to run this command after reboot of the system.
 </p>
 <p>
  In order to load the module automatically after every reboot, run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>echo "nvidia-peermem" | sudo tee /etc/modules-load.d/nvidia-peermem.conf</p>
        </pre>
 <p>
  Throughout this document, in
  <span>
   cuda-&lt;x&gt;.&lt;y&gt;
  </span>
  ,
  <span>
   x
  </span>
  refers to the CUDA major version and
  <span>
   y
  </span>
  refers to the minor version.
  <a class="Link" data-cms-ai="0" id="config-file-system-settings" name="config-file-system-settings" shape="rect">
  </a>
 </p>
 <h3 id="config-file-system-settings">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#config-file-system-settings">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#config-file-system-settings" id="config-file-system-settings" name="config-file-system-settings" shape="rect">
    2.2.1. Configuring File System Settings for GDS
   </a>
  </a>
 </h3>
 <p>
  Before proceeding, please refer to the File System specific section in this document for necessary configurations needed to support GDS:
 </p>
 <ul>
  <li>
   Lustre-Lnet:
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#multi-osts-peer-sel" shape="rect" title="When there are multiple OSTs (Object Storage Targets), and each OST is dual interface, to need to have one interface on each of the LNets for which the client is configured.">
    Configuring LNet Networks with Multiple OSTs for Optimal Peer Selection
   </a>
  </li>
  <li>
   WekaIO:
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-config-file-changes" shape="rect" title="By default, the configuration for Weka RDMA-based writes is disabled.">
    GDS Configuration File Changes to Support the WekaIO Filesystem
   </a>
  </li>
  <li>
   IBM Spectrum Scale:
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-configs-to-support-spectrum-scale" shape="rect">
    GDS Configuration to Support IBM Spectrum Scale
   </a>
  </li>
  <li>
   BeeGFS:
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-client-config-for-gds" shape="rect">
    BeeGFS Client Configuration for GDS
   </a>
  </li>
  <li>
   VAST:
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#networking-setup" shape="rect" title="This section provides information about how to set up client networking for VAST for GDS.">
    Set Up the Networking
   </a>
  </li>
  <li>
   NVMe:
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#raid-group-config-gpu-aff" shape="rect" title="Creating one RAID group from the available NVMe devices might not be optimal for GDS performance. You might need to create RAID groups that consist of devices that have a pci-affinity with the specified GPU. This is required to prevent and cross-node P2P traffic between the GPU and the NVMe devices.">
    RAID Group Configuration for GPU Affinity
   </a>
  </li>
 </ul>
 Note:
 <p>
  This step can be skipped for local file systems such as Ext4/XFS.
 </p>
 <h3 id="verify-suc-install">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-suc-install">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-suc-install" id="verify-suc-install" name="verify-suc-install" shape="rect">
    2.2.2. Verifying a Successful GDS Installation
   </a>
  </a>
 </h3>
 <p>
  To verify that GDS installation was successful, run
  <span>
   gdscheck
  </span>
  :
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-&lt;x&gt;.&lt;y&gt;/gds/tools/gdscheck.py -p</p>
        </pre>
 Note:
 <p>
  The
  <span>
   gdscheck
  </span>
  command expects python3 to be present on the system. If it fails because of python3 not being available, then you can invoke the command with the explicit path to where python (i.e. python2) is installed. For example:
 </p>
 <p>
  <span>
   $ /usr/bin/python /usr/local/cuda-&lt;x&gt;.&lt;y&gt;/gds/tools/gdscheck.py -p
  </span>
 </p>
 <p>
  The output of this command shows whether a supported filesystem or device installed on the system supports GDS. The output also shows whether PCIe ACS is enabled on any of the PCI switches.
 </p>
 Note:
 <p>
  For best GDS performance, disable PCIe ACS.
 </p>
 <p>
  Sample output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>GDS release version: 1.0.0.80
 nvidia_fs version:  2.7 libcufile version: 2.4 
 ============
 ENVIRONMENT:
 ============
 =====================
 DRIVER CONFIGURATION:
 =====================
 NVMe               : Unsupported
 NVMeOF             : Unsupported
 SCSI               : Unsupported
 ScaleFlux CSD      : Unsupported
 NVMesh             : Unsupported
 DDN EXAScaler      : Unsupported
 IBM Spectrum Scale : Unsupported
 NFS                : Supported
 WekaFS             : Unsupported
 Userspace RDMA     : Unsupported
 --Mellanox PeerDirect : Enabled
 --rdma library        : Not Loaded (libcufile_rdma.so)
 --rdma devices        : Not configured
 --rdma_device_status  : Up: 0 Down: 0
 =====================
 CUFILE CONFIGURATION:
 =====================
 properties.use_compat_mode : false
 properties.gds_rdma_write_support : true
 properties.use_poll_mode : false
 properties.poll_mode_max_size_kb : 4
 properties.max_batch_io_timeout_msecs : 5
 properties.max_direct_io_size_kb : 16384
 properties.max_device_cache_size_kb : 131072
 properties.max_device_pinned_mem_size_kb : 33554432
 properties.posix_pool_slab_size_kb : 4 1024 16384
 properties.posix_pool_slab_count : 128 64 32
 properties.rdma_peer_affinity_policy : RoundRobin
 properties.rdma_dynamic_routing : 0
 fs.generic.posix_unaligned_writes : false
 fs.lustre.posix_gds_min_kb: 0
 fs.weka.rdma_write_support: false
 profile.nvtx : false
 profile.cufile_stats : 0
 miscellaneous.api_check_aggressive : false
 =========
 GPU INFO:
 =========
 GPU index 0 A100-PCIE-40GB bar:1 bar size (MiB):65536 supports GDS
 ==============
 PLATFORM INFO:
 ==============
 IOMMU: disabled
 Platform verification succeeded</p>
        </pre>
 Note:
 <p>
  There are READMEs provided in
  <span>
   /usr/local/cuda-&lt;x&gt;.&lt;y&gt;/gds/tools
  </span>
  and
  <span>
   /usr/local/cuda-&lt;x&gt;.&lt;y&gt;/gds/samples
  </span>
  to show usage.
 </p>
 <h3 id="gds-lib-tools">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-lib-tools">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-lib-tools" id="gds-lib-tools" name="gds-lib-tools" shape="rect">
    2.3. Installed GDS Libraries and Tools
   </a>
  </a>
 </h3>
 <p>
  GPUDirect Storage userspace libraries are located in the /usr/local/cuda-
  .
  /targets/x86_64-linux/lib/
  directory.
 </p>
 Note:
 <p>
  GPUDirect Storage packages are installed at /usr/local/cuda-X.Y/gds, where
  X
  is the major version of the CUDA toolkit, and
  Y
  is the minor version.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ ls -1 /usr/local/cuda-X.Y/targets/x86_64-linux/lib/*cufile*
cufile.h
libcufile.so
libcufile.so.0
libcufile.so.1.0.0
libcufile_rdma.so
libcufile_rdma.so.0
libcufile_rdma.so.1.0.0</p>
        </pre>
 <p>
  GPUDirect Storage tools and samples are located in the /usr/local/cuda-X.Y/gds directory.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ ls -lh /usr/local/cuda-X.Y/gds/
total 20K
-rw-r--r-- 1 root root 8.4K Mar 15 13:01 README
drwxr-xr-x 2 root root 4.0K Mar 19 12:29 samples
drwxr-xr-x 2 root root 4.0K mar 19 10:28 tools</p>
        </pre>
 <p>
  For this release, GPUDirect Storage is providing an additional
  <span>
   libcufile-dev
  </span>
  package (cuFile library developers package) . This is primarily intended for the developer’s environment. Essentially the lincufile-dev package contains a static version of cuFile library (
  <span>
   libcufile_static.a
  </span>
  ,
  <span>
   libcufile_rdma_static.a
  </span>
  ) and
  <span>
   cufile.h
  </span>
  header file which may be required by the applications that use cuFile library APIs.
 </p>
 <h3 id="uninstall-gds">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#uninstall-gds">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#uninstall-gds" id="uninstall-gds" name="uninstall-gds" shape="rect">
    2.4. Uninstalling GPUDirect Storage
   </a>
  </a>
 </h3>
 <p>
  To uninstall GDS from Ubuntu and DGX OS:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo apt-get remove --purge "*libcufile*" "*gds-tools*" "*nvidia-fs*"</p>
        </pre>
 <p>
  To uninstall from RHEL:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo dnf remove "nvidia-gds*"</p>
        </pre>
 <h3 id="environment-variables">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#environment-variables">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#environment-variables" id="environment-variables" name="environment-variables" shape="rect">
    2.5. Environment Variables Used by GPUDirect Storage
   </a>
  </a>
 </h3>
 <p>
  GDS uses the following environment variables.
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" id="environment-variables__table_gps_qsg_rmb" rules="all" summary="">
  Table 1. GDS Environment Variables
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e523" rowspan="1" valign="top">
    CUFILE_ENV Variable
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e527" rowspan="1" valign="top">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_CQ_DEPTH
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Completion queue depth for the DC target.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_ENV_EXPERIMENTAL_FS=
     1
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Controls whether cufile checks for supporting filesystems. When set to 1, allows testing with new filesystems that are not yet officially enabled with cuFile.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_ENV_PATH_JSON=
     /home/user/cufile.json
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Controls the path where the cuFile library reads the configuration variables from. This can be used for container environments and applications that require different configuration settings from system default configuration at
    <span>
     /etc/cufile.json
    </span>
    .
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_ETH_SL
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Sets QOS level on RoCEv2 device QP for userspace RDMA targets (WekaFS and GPFS).
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_IB_SL=[0-15]
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Sets QOS level on IB device QP for userspace RDMA targets (WekaFS and GPFS).
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_LOGFILE_PATH=
     /etc/log/cufile_$$.log
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    <p>
     Controls the path for cuFile log information. Specifies the default log path, which is the current working directory of the application.
    </p>
    <p>
     Useful for containers or logging.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_LOGGING_LEVEL=
     TRACE
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Controls the tracing level and can override the trace level for a specific application without requiring a new configuration file.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_MIN_RNR_TIMER
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Minimum RNR value for QP after which the QP will error out with RNR timeout if no Work Request is posted on the remote end. Default value is 16 (2.56ms).
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_NVTX=
     true
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Enables NVTX tracing for use with Nsight systems.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_RDMA_DC_KEY=
     “0XABABCDEF”
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Controls the DC_KEY for userspace RDMA DC targets for WekaFS and GPFS.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_RDMA_HOP_LIMIT
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Maximum number of hops before the packet is discarded on the network. Prevents indefinite looping of the packet. Default is 64.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_RDMA_PKEY_INDEX
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Partition key index.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_RDMA_SR_MAX_WR
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Maximum number of Work requests supported by the Shared Request Queue.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_RDMA_SR_MAX_SGE
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Maximum number of Scatter Gather Entries supported per Work Request.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_SKIP_TOPOLOGY_DETECTION
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Setting this environment variable to true will skip topology detection in compat mode. This will reduce the high startup latency seen in compat mode on systems with multiple PCI devices.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_FORCE_COMPAT_MODE
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    Overrides cufile.json settings and forces I/O to go through compatible mode instead of GDS mode.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e523" rowspan="1" valign="top">
    <span>
     CUFILE_ALLOW_COMPAT_MODE
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e527" rowspan="1" valign="top">
    This does exactly what the
    <span>
     allow_compat_mode
    </span>
    tag in
    <span>
     cufile.json
    </span>
    file does.
   </td>
  </tr>
 </table>
 <h3 id="json-config-params">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#json-config-params">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#json-config-params" id="json-config-params" name="json-config-params" shape="rect">
    2.6. JSON Config Parameters Used by GPUDirect Storage
   </a>
  </a>
 </h3>
 <p>
  Refer to
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/topics/gds-parameters.html" shape="rect" title="This section describes the JSON configuration parameters used by GDS.">
   GPUDirect Storage Parameters
  </a>
  for details about the JSON Config parameters used by GDS
 </p>
 <p>
  Consider
  <span>
   compat_mode
  </span>
  for systems or mounts that are not yet set up with GDS support. To learn more about Compatibility Mode, refer to
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/topics/cufile-compatibility.html" shape="rect">
   cuFile Compatibility Mode
  </a>
  .
 </p>
 <h3 id="dynamic-routing-config">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-config">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-config" id="dynamic-routing-config" name="dynamic-routing-config" shape="rect">
    2.7. GDS Configuration File Changes to Support Dynamic Routing
   </a>
  </a>
 </h3>
 <p>
  For dynamic routing to support multiple file systems and mount points, configure the global per file system
  <span>
   rdma_dev_addr_list
  </span>
  property for a single mount or the
  <span>
   rdma_dev_addr_list
  </span>
  property for a per file system mount table.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>"fs": {
       "lustre": {
       // if using a single lustre mount, provide the ip addresses    
       // here (use : sudo lnetctl net show)
       //"rdma_dev_addr_list" : []
 
       // if using multiple lustre mounts, provide ip addresses 
       // used by respective mount here
       //"mount_table" : {
       // "/lustre/ai200_01/client" : {
       //    "rdma_dev_addr_list" : ["172.172.1.40", 
                           "172.172.1.42"]
       // },
 
       // "/lustre/ai200_02/client" : {
       //    "rdma_dev_addr_list" : ["172.172.2.40", 
                           "172.172.2.42"]
       //}
      },
 
      "nfs": {
         //"rdma_dev_addr_list" : []
 
         //"mount_table" : {
         //  "/mnt/nfsrdma_01/" : {
         //           "rdma_dev_addr_list" : []
         //},
 
         //  "/mnt/nfsrdma_02/" : {
         //           "rdma_dev_addr_list" : []
         //}
         //}
         },
      },</p>
        </pre>
 <h3 id="det-installed-version">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-installed-version">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-installed-version" id="det-installed-version" name="det-installed-version" shape="rect">
    2.8. Determining Which Version of GDS is Installed
   </a>
  </a>
 </h3>
 <p>
  To determine which version of GDS you have, run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ gdscheck.py -v</p>
        </pre>
 <p>
  Example output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>GDS release version: 1.0.0.78
 nvidia_fs version:  2.7 libcufile version: 2.4</p>
        </pre>
 <h3 id="experimental-repos-dgx">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#experimental-repos-dgx">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#experimental-repos-dgx" id="experimental-repos-dgx" name="experimental-repos-dgx" shape="rect">
    2.9. Experimental Repos for Network Install of GDS Packages for DGX Systems
   </a>
  </a>
 </h3>
 <p>
  GDS 1.0.0 and MLNX_OFED packages can be installed by enabling the preview repository on supported DGX platforms using the following steps.
 </p>
 <p>
  For Ubuntu 18.04/20.04 distributions:
 </p>
 <p>
  GDS 1.0.0, NVSM and MLNX_OFED packages can be installed via network using the preview network repository.
 </p>
 <p>
  For Ubuntu 20.04 distributions:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo apt-key adv --fetch-keys https://repo.download.nvidia.com/baseos/GPG-KEY-dgx-cosmos-support

$ sudo add-apt-repository "deb https://repo.download.nvidia.com/baseos/ubuntu/focal/x86_64/ focal-updates preview"

$ sudo apt update</p>
        </pre>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-errors">
   3. API Errors
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="api-errors" name="api-errors" shape="rect">
  </a>
  This section provides information about the common API errors you might get when using GDS.
 </p>
 <h3 id="api-error-1">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-1">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-1" id="api-error-1" name="api-error-1" shape="rect">
    3.1. CU_FILE_DRIVER_NOT_INITIALIZED
   </a>
  </a>
 </h3>
 <p>
  If the
  <span>
   cuFileDriverOpen
  </span>
  API is not called, errors encountered in the implicit call to driver initialization are reported as cuFile errors encountered when calling
  <span>
   cuFileBufRegister
  </span>
  or
  <span>
   cuFileHandleRegister
  </span>
  .
 </p>
 <h3 id="api-error-2">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-2">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-2" id="api-error-2" name="api-error-2" shape="rect">
    3.2. CU_FILE_DEVICE_NOT_SUPPORTED
   </a>
  </a>
 </h3>
 <p>
  GDS is supported only on NVIDIA graphics processing units (GPU) Tesla® or Quadro® models that support compute mode, and a compute major capability greater than or equal to 6.
 </p>
 Note:
 <p>
  This includes V100 and T4 cards.
 </p>
 <h3 id="api-error-3">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-3">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-3" id="api-error-3" name="api-error-3" shape="rect">
    3.3. CU_FILE_IO_NOT_SUPPORTED
   </a>
  </a>
 </h3>
 <p>
  If the file descriptor is from a local filesystem, or a mount that is not GDS ready, the API returns the
  <span>
   CU_FILE_IO_NOT_SUPPORTED
  </span>
  error.
 </p>
 <p>
  See
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-prereqs" shape="rect" title="To install GDS on a non-DGX platform, complete the following steps:">
   Before You Install GDS
  </a>
  for a list of the supported filesystems. Common reasons for this error include:
 </p>
 <ul>
  <li>
   The file descriptor belongs to an unsupported filesystem.
  </li>
  <li>
   The specified
   <span>
    fd
   </span>
   is not a regular UNIX file.
  </li>
  <li>
   Any combination of encryption, and compression, compliance settings on the
   <span>
    fd
   </span>
   are set.
   <p>
    For example,
    <span>
     FS_COMPR_FL | FS_ENCRYPT_FL | FS_APPEND_FL | FS_IMMUTABLE_FL
    </span>
    .
   </p>
   Note:
   <p>
    These settings are allowed when
    <span>
     compat_mode
    </span>
    is set to
    <span>
     true
    </span>
    .
   </p>
  </li>
  <li>
   Any combination of unsupported file modes are specified in the open call for the
   <span>
    fd
   </span>
   . For example,
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>O_APPEND | O_NOCTTY | O_NONBLOCK | O_DIRECTORY | O_NOFOLLOW | O_TMPFILE</p>
        </pre>
  </li>
 </ul>
 <h3 id="api-error-4">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-4">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-4" id="api-error-4" name="api-error-4" shape="rect">
    3.4. CU_FILE_CUDA_MEMORY_TYPE_INVALID
   </a>
  </a>
 </h3>
 <p>
  Physical memory for
  <span>
   cudaMallocManaged
  </span>
  memory is allocated dynamically at the first use. Currently, it does not provide a mechanism to expose physical memory or Base Address Register (BAR) memory to pin for use in GDS. However, GDS indirectly supports
  <span>
   cudaMallocManaged
  </span>
  memory when the memory is used as an unregistered buffer with
  <span>
   cuFileWrite
  </span>
  and
  <span>
   cuFileRead
  </span>
  .
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-troubleshooting">
   4. Basic Troubleshooting
  </a>
 </h2>
 <h3 id="log-files-gds-lib">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#log-files-gds-lib">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#log-files-gds-lib" id="log-files-gds-lib" name="log-files-gds-lib" shape="rect">
    4.1. Log Files for the GDS Library
   </a>
  </a>
 </h3>
 <p>
  A
  <span>
   cufile.log
  </span>
  file is created in the same location where the application binaries are located. Currently the maximum log file size is 32MB. If the log file size increases to greater than 32MB, the log file is truncated and logging is resumed on the same file.
 </p>
 <h3 id="enable-diff-log-file-app">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-diff-log-file-app">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-diff-log-file-app" id="enable-diff-log-file-app" name="enable-diff-log-file-app" shape="rect">
    4.2. Enabling a Different cufile.log File for Each Application
   </a>
  </a>
 </h3>
 <p>
  You can enable a different
  <span>
   cufile.log
  </span>
  file for each application.
 </p>
 <p>
  There are several relevant cases:
 </p>
 <ul>
  <li>
   If the
   <span>
    logging:dir
   </span>
   property in the default /etc/cufile.json file is not set, by default, the
   <span>
    cufile.log
   </span>
   file is generated in the current working directory of the application.
  </li>
  <li>
   If the
   <span>
    logging:dir
   </span>
   property is set in the default
   <span>
    /etc/cufile.json
   </span>
   file, the log file is created in the specified directory path.
  </li>
 </ul>
 Note:
 <p>
  This is usually not recommended for scenarios where multiple applications use the
  <span>
   libcufile.so
  </span>
  library.
 </p>
 <p>
  For example:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>"logging": {
    // log directory, if not enabled 
    // will create log file under current working      
    // directory
      "dir": "/opt/gdslogs/",
}</p>
        </pre>
 <p>
  The
  <span>
   cufile.log
  </span>
  will be created as a /opt/gdslogs/cufile.log file.
 </p>
 <p>
  If the application needs to enable a different
  <span>
   cufile.log
  </span>
  for different applications, the application can override the default JSON path by doing the following steps:
 </p>
 <ol>
  <li>
   Export CUFILE_ENV_PATH_JSON="/opt/myapp/cufile.json”.
  </li>
  <li>
   Edit the /opt/myapp/cufile.json file.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>"logging": {
    // log directory, if not enabled 
    // will create log file under current working 
    // directory
    "dir": "/opt/myapp",
}</p>
        </pre>
  </li>
  <li>
   Run the application.
  </li>
  <li>
   To check for logs, run:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ ls -l /opt/myapp/cufile.log</p>
        </pre>
  </li>
 </ol>
 <h3 id="enable-trace-lib-calls">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-trace-lib-calls">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-trace-lib-calls" id="enable-trace-lib-calls" name="enable-trace-lib-calls" shape="rect">
    4.3. Enabling Tracing GDS Library API Calls
   </a>
  </a>
 </h3>
 <p>
  There are different logging levels, which can be enabled in the
  <span>
   /etc/cufile.json
  </span>
  file.
 </p>
 <p>
  By default, logging level is set to
  <span>
   ERROR
  </span>
  . Logging will have performance impact as we increase the verbosity levels like
  <span>
   INFO
  </span>
  ,
  <span>
   DEBUG
  </span>
  , and
  <span>
   TRACE
  </span>
  , and should be enabled only to debug field issues.
  Configure tracing and run the following:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>"logging": {
// log directory, if not enabled 
// will create log file under local directory
//"dir": "/home/&lt;xxxx&gt;",

// ERROR|WARN|INFO|DEBUG|TRACE (in decreasing order of priority)
"level": "ERROR"
},</p>
        </pre>
 <h3 id="api-error-5">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-5">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-5" id="api-error-5" name="api-error-5" shape="rect">
    4.4. cuFileHandleRegister Error
   </a>
  </a>
 </h3>
 <p>
  If you see the
  <span>
   cuFileHandleRegister
  </span>
  error on the
  <span>
   cufile.log
  </span>
  file when an IO is issued:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>“cuFileHandleRegister error: GPUDirect Storage not supported on current file.”</p>
        </pre>
 <p>
  Here are some reasons why this error might occur:
 </p>
 <ul>
  <li>
   The filesystem is not supported by GDS.
   <p>
    See
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-2" shape="rect">
     CU_FILE_DEVICE_NOT_SUPPORTED
    </a>
    for more information.
   </p>
  </li>
  <li>
   <span>
    DIRECT_IO
   </span>
   functionality is not supported for the mount on which the file resides.
  </li>
 </ul>
 <p>
  For more information, enable tracing in the
  <span>
   /etc/cufile.json
  </span>
  file.
 </p>
 <h3 id="troubleshoot-cufile-errors">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-cufile-errors">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-cufile-errors" id="troubleshoot-cufile-errors" name="troubleshoot-cufile-errors" shape="rect">
    4.5. Troubleshooting Applications that Return cuFile Errors
   </a>
  </a>
 </h3>
 <p>
  To troubleshoot cuFile errors:
 </p>
 <ol>
  <li>
   See the
   <span>
    cufile.h
   </span>
   file for more information about errors that are returned by the API.
  </li>
  <li>
   If the IO was submitted to the GDS driver, check whether there are any errors in GDS stats.
   <p>
    If the IO fails, the error stats should provide information about the type of error.
   </p>
   <p>
    See
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#find-driver-stats" shape="rect">
     Finding the GDS Driver Statistics
    </a>
    for more information.
   </p>
  </li>
  <li>
   Enable GDS library tracing and monitor the
   <span>
    cufile.log
   </span>
   file.
  </li>
  <li>
   Enable GDS Driver debugging:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ echo 1 &gt;/sys/module/nvidia_fs/parameters/dbg_enabled</p>
        </pre>
  </li>
 </ol>
 <p>
  After the driver debug logs are enabled, you might get more information about the error.
 </p>
 <h3 id="error-no-activity-gds-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#error-no-activity-gds-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#error-no-activity-gds-stats" id="error-no-activity-gds-stats" name="error-no-activity-gds-stats" shape="rect">
    4.6. cuFile-* Errors with No Activity in GPUDirect Storage Statistics
   </a>
  </a>
 </h3>
 <p>
  If there are cuFile errors in the GDS statistics, this means that the API failed in the GDS library. You can enable tracing by setting the appropriate logging level in the /etc/cufile.json file to get more information about the failure in
  <span>
   cufile.log
  </span>
  .
 </p>
 <h3 id="cuda-error-35">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cuda-error-35">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cuda-error-35" id="cuda-error-35" name="cuda-error-35" shape="rect">
    4.7. CUDA Runtime and Driver Mismatch with Error Code 35
   </a>
  </a>
 </h3>
 <p>
  Error code 35 from the CUDA documentation points to
  <span>
   cudaErrorInsufficientDriver
  </span>
  , which indicates that the installed NVIDIA CUDA driver is older than the CUDA runtime library. This is not a supported configuration. For the application to run, you must update the NVIDIA display driver.
 </p>
 Note:
 <p>
  cuFile tools depend on CUDA runtime 10.1 and later. You must ensure that the installed CUDA runtime is compatible with the installed CUDA driver and is at the recommended version.
 </p>
 <h3 id="cuda-api-errors">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cuda-api-errors">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cuda-api-errors" id="cuda-api-errors" name="cuda-api-errors" shape="rect">
    4.8. CUDA API Errors when Running the cuFile-* APIs
   </a>
  </a>
 </h3>
 <p>
  The GDS library uses the CUDA driver APIs.
 </p>
 <p>
  If you observe CUDA API errors, you will observe an error code. Refer to the error codes in the
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/cuda-libraries/index.html" shape="rect">
   CUDA Libraries documentation
  </a>
  for more information.
 </p>
 <h3 id="find-driver-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#find-driver-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#find-driver-stats" id="find-driver-stats" name="find-driver-stats" shape="rect">
    4.9. Finding GDS Driver Statistics
   </a>
  </a>
 </h3>
 <p>
  To find the GDS Driver Statistics, run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/driver/nvidia-fs/stats</p>
        </pre>
 <p>
  GDS Driver kernel statistics for
  <span>
   READ
  </span>
  /
  <span>
   WRITE
  </span>
  are available for all file systems except for Weka. For Weka file system statistics, refer to
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trouble-faq-wekafs" shape="rect" title="This section provides troubleshooting and FAQ information about the WekaIO file system.">
   Troubleshooting and FAQ for the WekaIO Filesystem
  </a>
  for more information about
  <span>
   READ
  </span>
  /
  <span>
   WRITE
  </span>
  .
 </p>
 <h3 id="track-io-activity-gds-driver">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-io-activity-gds-driver">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-io-activity-gds-driver" id="track-io-activity-gds-driver" name="track-io-activity-gds-driver" shape="rect">
    4.10. Tracking IO Activity that Goes Through the GDS Driver
   </a>
  </a>
 </h3>
 <p>
  In GDS Driver statistics, the
  ops
  row shows the active IO operation. The
  <span>
   Read
  </span>
  and
  <span>
   Write
  </span>
  fields show the current active operation in flight. This information should provide an idea of how many total IOs are in flight across all applications in the kernel. If there is a bottleneck in the userspace, the number of active IOs will be less than the number of threads that are submitting the IO. Additionally, to get more details about the
  <span>
   Read
  </span>
  and
  <span>
   Write
  </span>
  bandwidth numbers, look out for counters in the
  <span>
   Read
  </span>
  /
  <span>
   Write
  </span>
  rows.
 </p>
 <h3 id="read-write-bandwidth-latency-nos">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#read-write-bandwidth-latency-nos">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#read-write-bandwidth-latency-nos" id="read-write-bandwidth-latency-nos" name="read-write-bandwidth-latency-nos" shape="rect">
    4.11. Read/Write Bandwidth and Latency Numbers in GDS Stats
   </a>
  </a>
 </h3>
 <p>
  Measured latencies begin when the IO is submitted and end when the IO completion is received by the GDS kernel driver. Userspace latencies are not reported. This should provide an idea whether the user space is bottlenecked or whether the IO is bottlenecked on the backend disks/fabric.
 </p>
 Note:
 <p>
  The WekaIO filesystem reads do not go through the nvidia-fs driver, so
  <span>
   Read
  </span>
  /
  <span>
   Write
  </span>
  bandwidth stats are not available for WekaIO filesystem by using this interface.
 </p>
 <p>
  Refer to the
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trouble-faq-wekafs" shape="rect" title="This section provides troubleshooting and FAQ information about the WekaIO file system.">
   Troubleshooting and FAQ for the WekaIO Filesystem
  </a>
  for more information.
 </p>
 <h3 id="track-regis-deregis-gpu-buffers">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-regis-deregis-gpu-buffers">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-regis-deregis-gpu-buffers" id="track-regis-deregis-gpu-buffers" name="track-regis-deregis-gpu-buffers" shape="rect">
    4.12. Tracking Registration and Deregistration of GPU Buffers
   </a>
  </a>
 </h3>
 <p>
  In GDS Driver stats, look for the active field in BAR1-map stats row.
 </p>
 <p>
  The pinning and unpinning of GPU memory through
  <span>
   cuFileBufRegister
  </span>
  and
  <span>
   cuFileBufDeregister
  </span>
  is an expensive operation. If you notice a large number of
  <span>
   registrations(n)
  </span>
  and
  <span>
   deregistration(free)
  </span>
  in the nvidia-fs stats, it can hurt performance. Refer to the
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html" shape="rect">
   GPUDirect Storage Best Practices Guide
  </a>
  for more information about using the
  <span>
   cuFileBufRegister
  </span>
  API.
 </p>
 <h3 id="enabling-rdma-logging">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enabling-rdma-logging">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enabling-rdma-logging" id="enabling-rdma-logging" name="enabling-rdma-logging" shape="rect">
    4.13. Enabling RDMA-specific Logging for Userspace File Systems
   </a>
  </a>
 </h3>
 <p>
  In order to troubleshoot RDMA related issues for userspace file systems,ensure that the
  <span>
   CUFILE_LOGGING_LEVEL
  </span>
  environment variable is set to
  <span>
   INFO
  </span>
  ,
  <span>
   DEBUG
  </span>
  , or
  <span>
   TRACE
  </span>
  prior to running the application. However, for this to work,
  <span>
   cufile.json
  </span>
  logging level also should be set to
  <span>
   TRACE/DEBUG/INFO
  </span>
  level.
 </p>
 <p>
  For example:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ export CUFILE_LOGGING_LEVEL=INFO This is an example to set log level to INFO via the environment variable. 
$ cat /etc/cufile.json
   ....
   "logging": {
        // log directory, if not enabled will create log file
        // under current working directory
        //"dir": "/home/&lt;xxxx&gt;",
        // ERROR|WARN|INFO|DEBUG|TRACE (in decreasing order of priority)
        "level": "DEBUG"
   },
   ....
   This is an example on how to set log level to DEBUG via cufile.json.</p>
        </pre>
 <h3 id="system-not-ready">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#system-not-ready">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#system-not-ready" id="system-not-ready" name="system-not-ready" shape="rect">
    4.14. CUDA_ERROR_SYSTEM_NOT_READY After Installation
   </a>
  </a>
 </h3>
 <p>
  On systems with NVSwitch, if you notice the CUDA_ERROR_SYSTEM_NOT_READY error being reported, then make sure that you install the same version of Fabric Manager as the CUDA driver.
 </p>
 <p>
  For example, if you use:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo apt install nvidia-driver-460-server -y</p>
        </pre>
 <p>
  then use:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ apt-get install nvidia-fabricmanager-460</p>
        </pre>
 <p>
  Make sure to restart the Fabric Manager service using:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo service nvidia-fabricmanager start</p>
        </pre>
 <h3 id="adding-udev-rules">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#adding-udev-rules">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#adding-udev-rules" id="adding-udev-rules" name="adding-udev-rules" shape="rect">
    4.15. Adding udev Rules for RAID Volumes
   </a>
  </a>
 </h3>
 <p>
  To add udev rules for RAID volumes:
 </p>
 <p>
  As a sudo user, change the following line in
  <span>
   /lib/udev/rules.d/63-md-raid-arrays.rules
  </span>
  :
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>IMPORT{​program}="/usr/sbin/mdadm --detail --export $devnode"</p>
        </pre>
 <p>
  Reboot the node or restart the
  <span>
   mdadm
  </span>
  .
 </p>
 <h3 id="incomplete-write-on-nvme-drives">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#incomplete-write-on-nvme-drives">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#incomplete-write-on-nvme-drives" id="incomplete-write-on-nvme-drives" name="incomplete-write-on-nvme-drives" shape="rect">
    4.16. When You Observe “Incomplete write” on NVME Drives
   </a>
  </a>
 </h3>
 <p>
  During GDS mode writes, you may receive error messages similar to the following:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>Tid: 0 incomplete Write, done = 0 issued = 1048576</p>
        </pre>
 <p>
  GPUDirect storage in P2P mode does not support NVMe end to end data protection features. To support GDS in P2P mode, the NVMe must be formatted with Protection Information -
  <span>
   Metadata Size
  </span>
  is set to zero bytes.
 </p>
 <p>
  Confirm that the drive has data-integrity mode enabled:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo nvme id-ns /dev/nvme0n1 -H
…
LBA Format  0 : Metadata Size: 0   bytes - Data Size: 512 bytes - Relative Performance: 0x1 Better
LBA Format  1 : Metadata Size: 8   bytes - Data Size: 512 bytes - Relative Performance: 0x3 Degraded (in use)
LBA Format  2 : Metadata Size: 0   bytes - Data Size: 4096 bytes - Relative Performance: 0 Best
LBA Format  3 : Metadata Size: 8   bytes - Data Size: 4096 bytes - Relative Performance: 0x2 Good
LBA Format  4 : Metadata Size: 64  bytes - Data Size: 4096 bytes - Relative Performance: 0x3 Degraded</p>
        </pre>
 <p>
  Note in the preceding example, the metadata size of the drive (nvme0n1) is set to non-zero.
 </p>
 <p>
  You can set the LBA format to 0 or 2 to disable the protection feature on the drive:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo nvme format /dev/nvme0n1 -l 2
$ sudo nvme id-ns /dev/nvme0n1 -H
…
LBA Format  0 : Metadata Size: 0   bytes - Data Size: 512 bytes - Relative Performance: 0x1 Better
LBA Format  1 : Metadata Size: 8   bytes - Data Size: 512 bytes - Relative Performance: 0x3 Degraded
LBA Format  2 : Metadata Size: 0   bytes - Data Size: 4096 bytes - Relative Performance: 0 Best (in use)
LBA Format  3 : Metadata Size: 8   bytes - Data Size: 4096 bytes - Relative Performance: 0x2 Good
LBA Format  4 : Metadata Size: 64  bytes - Data Size: 4096 bytes - Relative Performance: 0x3 Degraded</p>
        </pre>
 <h3 id="cufile-async-io-failing">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-async-io-failing">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-async-io-failing" id="cufile-async-io-failing" name="cufile-async-io-failing" shape="rect">
    4.17. CUFILE async I/O is failing
   </a>
  </a>
 </h3>
 <p>
  There could be many reasons for which stream based async I/O can fail. This will be logged in
  <span>
   cufile.log
  </span>
  . One of the common reasons could be that the internal thread pool is not enabled. Refer to
  <span>
   cufile.json
  </span>
  “execution” section on how to enable it.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#advanced-troubleshooting">
   5. Advanced Troubleshooting
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="advanced-troubleshooting" name="advanced-troubleshooting" shape="rect">
  </a>
  This section provides information about troubleshooting some advanced issues.
 </p>
 <h3 id="hung-cufile-no-response">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#hung-cufile-no-response">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#hung-cufile-no-response" id="hung-cufile-no-response" name="hung-cufile-no-response" shape="rect">
    5.1. Resolving Hung cuFile* APIs with No Response
   </a>
  </a>
 </h3>
 <p>
  To resolve hung cuFile APIs:
 </p>
 <ol>
  <li>
   Check whether there are any kernel panics/warnings in
   <span>
    dmesg
   </span>
   :
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ dmesg &gt; warnings.txt. less warnings.txt</p>
        </pre>
  </li>
  <li>
   Check whether the application process is in the
   <span>
    ‘D’
   </span>
   (uninterruptible) state).
  </li>
  <li>
   If the process is in the
   <span>
    ‘D’
   </span>
   state:
   <ol>
    <li>
     Get the PID of the process by running the following command:
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p>$ ps axf | grep ‘ D’</p>
        </pre>
    </li>
    <li>
     As a root user, get the backtrace of the
     <span>
      ‘D’
     </span>
     state process:
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p>$ su root
$ cat /proc/&lt;pid&gt;/stack</p>
        </pre>
    </li>
   </ol>
  </li>
  <li>
   Verify whether the threads are stuck in the kernel or in user space. For more information, review the backtrace of the
   <span>
    ‘D’
   </span>
   state threads.
  </li>
  <li>
   Check whether any threads are showing heavy CPU usage.
   <ol>
    <li>
     The
     <span>
      htop
     </span>
     and
     <span>
      mpstat
     </span>
     tools should show CPU usage per core.
    </li>
    <li>
     Get the call graph of where the CPUs are being used. The following code snippet should narrow down whether the threads are hung in user space or in the kernel:
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p>$ perf top -g</p>
        </pre>
    </li>
   </ol>
  </li>
 </ol>
 <h3 id="kernel-panic-stack-traces">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#kernel-panic-stack-traces">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#kernel-panic-stack-traces" id="kernel-panic-stack-traces" name="kernel-panic-stack-traces" shape="rect">
    5.2. Sending Relevant Data to Customer Support
   </a>
  </a>
 </h3>
 <p>
  This section describes how to resolve a kernel panic with stack traces using NVSM or the GDS Log Collection tool.
 </p>
 <p>
  DGX OS:
 </p>
 <p>
  For DGX BaseOS with the preview network repo enabled and NVSM installed:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo apt-get install nvsm
$ sudo nvsm dump health</p>
        </pre>
 <p>
  For more details on running NVSM commands, refer to
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/datacenter/nvsm/nvsm-user-guide/index.html#topic_3_4_3" shape="rect">
   NVIDIA System Management User Guide
  </a>
  .
 </p>
 <p>
  Non DGX:
 </p>
 <p>
  The GDS Log Collection tool,
  <span>
   gds_log_collection.py
  </span>
  , may be run by GDS users to collect relevant debugging information from the system when issues with GDS IO are seen.
 </p>
 <p>
  Some of the important information that this tool captures is highlighted below:
 </p>
 <ul>
  <li>
   dmesg Output and relevant kernel log files.
  </li>
  <li>
   System map files and vmlinux image
  </li>
  <li>
   modinfo output for relevant modules
  </li>
  <li>
   <span>
    /proc/cmdline
   </span>
   output
  </li>
  <li>
   IB devices info like ibdev2net and ibstatus
  </li>
  <li>
   OS distribution information
  </li>
  <li>
   Cpuinfo, meminfo
  </li>
  <li>
   nvidia-fs stats
  </li>
  <li>
   Per process information like
   <span>
    cufile.log
   </span>
   ,
   <span>
    cufile.json
   </span>
   ,
   <span>
    gds_stats
   </span>
   , stack pointers
  </li>
  <li>
   Any user specified files
  </li>
 </ul>
 <p>
  To use the log collection tool:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo /usr/local/cuda/gds//tools/gdstools/gds_log_collection.py -h</p>
        </pre>
 <p>
  This tool is used to collect logs from the system that are relevant for debugging.
 </p>
 <p>
  It collects logs such as OS and kernel info, nvidia-fs stats, dmesg logs, syslogs, system map files and per process logs such as
  <span>
   cufile.json
  </span>
  ,
  <span>
   cufile.log
  </span>
  , gdsstats, process stack, and so on.
 </p>
 <p>
  Usage:
 </p>
 <p>
  <span>
   ./gds_log_collection.py [options]
  </span>
 </p>
 <p>
  Options:
 </p>
 <p>
  <span>
   -h
  </span>
  help
 </p>
 <p>
  <span>
   -f file1,file2,..
  </span>
  (Note: there should be no spaces between ',’)
 </p>
 <p>
  These files could be any relevant files apart from the one’s being collected (such as crash files).
 </p>
 <p>
  Usage examples:
 </p>
 <p>
  <span>
   sudo ./gds_log_colection.py
  </span>
  - Collects all the relevant logs.
 </p>
 <p>
  <span>
   sudo ./gds_log_colection.py -f file1,file2
  </span>
  - Collects all the relevant files as well as the user specified files.
 </p>
 <h3 id="io-failure-stack-trace-warning">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-failure-stack-trace-warning">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-failure-stack-trace-warning" id="io-failure-stack-trace-warning" name="io-failure-stack-trace-warning" shape="rect">
    5.3. Resolving an IO Failure with EIO and Stack Trace Warning
   </a>
  </a>
 </h3>
 <p>
  You might see an IO failure with EIO and a warning with a stack trace with an
  <span>
   nvfs_mgroup_check_and_set
  </span>
  function in the trace.
 </p>
 <p>
  This could mean that the EXAScaler filesystem did not honor
  <span>
   O_DIRECT
  </span>
  and fell back to page cache mode. GDS tracks this information in the driver and returns EIO.
 </p>
 Note:
 <p>
  The
  WARNING
  stack trace is observed only once during the lifetime of the kernel module. You will get an
  <span>
   Error: Input/Output (EIO)
  </span>
  , but the trace message will be printed only once. If you consistently experience this issue, contact support.
 </p>
 <h3 id="control-gpu-bar-usage">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#control-gpu-bar-usage">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#control-gpu-bar-usage" id="control-gpu-bar-usage" name="control-gpu-bar-usage" shape="rect">
    5.4. Controlling GPU BAR Memory Usage
   </a>
  </a>
 </h3>
 <ol>
  <li>
   To show how much BAR Memory is available per GPU, run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-x.y/gds/tools/gdscheck</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>GPU INFO:
 GPU Index: 0 bar:1 bar size (MB):32768
 GPU Index: 1 bar:1 bar size (MB):32768</p>
        </pre>
   <p>
    GDS uses BAR memory in the following cases:
    <a class="Link" data-cms-ai="0" id="control-gpu-bar-usage__ul_bdc_l5h_rmb" name="control-gpu-bar-usage__ul_bdc_l5h_rmb" shape="rect">
    </a>
   </p>
   <ul class="ul" id="control-gpu-bar-usage__ul_bdc_l5h_rmb">
    <li class="li">
     When the process invokes
     <span>
      cuFileBufRegister
     </span>
     .
    </li>
    <li class="li">
     When GDS uses the cache internally to allocate bounce buffers per GPU.
    </li>
   </ul>
   Note:
   <p>
    There is no per-GPU configuration for cache and BAR memory usage.
   </p>
   <p>
    Each process can control the usage of BAR memory via the configurable property in the
    <span>
     /etc/cufile.json
    </span>
    file:
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>"properties": {

// device memory size for reserving bounce buffers for the entire GPU (in KB)
"max_device_cache_size" : 131072,
// limit on maximum memory that can be pinned for a given process (in KB)
"max_device_pinned_mem_size" : 33554432                            

}</p>
        </pre>
   Note:
   <p>
    This configuration is per process, and the configuration is set across all GPUs.
   </p>
  </li>
 </ol>
 <h3 id="det-how-much-cache">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-how-much-cache">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-how-much-cache" id="det-how-much-cache" name="det-how-much-cache" shape="rect">
    5.5. Determining the Amount of Cache to Set Aside
   </a>
  </a>
 </h3>
 <p>
  By default, 128 MB of cache is set in the configurable
  <span>
   max_device_cache_size
  </span>
  property. However, this does not mean that GDS pre-allocates 128 MB of memory per GPU up front. Memory allocation is done on the fly and is based on need. After the allocation is complete, there is no purging of the cache.
 </p>
 <p>
  By default, since 128 MB is set, the cache can grow up to 128 MB. Setting the cache is application specific and depends on workload. Refer to the
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html" shape="rect">
   GPUDirect Storage Best Practices Guide
  </a>
  to understand the need of cache and how to set the limit based on guidance in the guide.
 </p>
 <h3 id="monitor-bar-mem-usage">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-bar-mem-usage">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-bar-mem-usage" id="monitor-bar-mem-usage" name="monitor-bar-mem-usage" shape="rect">
    5.6. Monitoring BAR Memory Usage
   </a>
  </a>
 </h3>
 <p>
  There is no way to monitor the BAR memory usage per process. However, GDS Stats tracks the global BAR usage across all processes. For more information, see the following stat output from
  <span>
   /proc/driver/nvidia_fs/stats
  </span>
  for the GPU with
  <span>
   B:D:F 0000:34:00.0
  </span>
  :
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>GPU 0000:34:00.0  uuid:12a86a5e-3002-108f-ee49-4b51266cdc07 : Registered_MB=32 Cache_MB=10</p>
        </pre>
 <p>
  <span>
   Registered_MB
  </span>
  tracks how much BAR memory is used when applications are explicitly using the
  <span>
   cuFileBufRegister
  </span>
  API.
 </p>
 <p>
  <span>
   Cache_MB
  </span>
  tracks GDS usage of BAR memory for internal cache.
 </p>
 <h3 id="enomem-error-code">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enomem-error-code">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enomem-error-code" id="enomem-error-code" name="enomem-error-code" shape="rect">
    5.7. Resolving an ENOMEM Error Code
   </a>
  </a>
 </h3>
 <p>
  <span>
   -12
  </span>
  ENOMEM error code.
 </p>
 <p>
  Each GPU has some BAR memory reserved. The
  <span>
   cuFileBufRegister
  </span>
  function makes the pages that underlie a range of GPU virtual memory accessible to a third-party device. This process is completed by pinning the GPU device memory in BAR space by using the
  <span>
   nvidia_p2p_get_pages
  </span>
  API. If the application tries to pin memory beyond the available BAR space, the
  <span>
   nvidia_p2p_get_pages
  </span>
  API returns a
  <span>
   -12
  </span>
  (ENOMEM) error code.
 </p>
 <p>
  To avoid running out of BAR memory, developers should use this output to manage how much memory is pinned by application. Administrators can use this output to investigate how to limit the pinned memory for different applications.
 </p>
 <h3 id="gds-comp-mode">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-comp-mode">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-comp-mode" id="gds-comp-mode" name="gds-comp-mode" shape="rect">
    5.8. GDS and Compatibility Mode
   </a>
  </a>
 </h3>
 <p>
  To determine the GDS compatibility mode, complete the following:
 </p>
 <ol>
  <li>
   In the /etc/cufile.json file, verify that
   <span>
    allow_compat_mode
   </span>
   is set to
   <span>
    true
   </span>
   .
  </li>
  <li>
   <span>
    gdscheck -p
   </span>
   displays whether the
   <span>
    allow_compat_mode
   </span>
   property is set to
   <span>
    true
   </span>
   .
  </li>
  <li>
   Check the
   <span>
    cufile.log
   </span>
   file for the
   <span>
    cufile IO mode: POSIX
   </span>
   message.
   <p>
    This message is in the hot IO path, where logging each instance significantly impacts performance, so the message is only logged when
    <span>
     logging:level
    </span>
    is explicitly set to the
    <span>
     TRACE
    </span>
    mode in the
    <span>
     /etc/cufile.json
    </span>
    file.
   </p>
  </li>
 </ol>
 <h3 id="enable-comp-mode">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-comp-mode">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-comp-mode" id="enable-comp-mode" name="enable-comp-mode" shape="rect">
    5.9. Enabling Compatibility Mode
   </a>
  </a>
 </h3>
 <p>
  Compatibility mode can be used by application developers to test the applications with cuFile-enabled libraries under the following conditions:
  <a class="Link" data-cms-ai="0" id="enable-comp-mode__ul_v22_kzh_rmb" name="enable-comp-mode__ul_v22_kzh_rmb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   When there is no support for GDS for a specific filesystem.
  </li>
  <li>
   The
   <span>
    nvidia-fs.ko
   </span>
   driver is not enabled in the system by the administrator.
  </li>
 </ul>
 <p>
  To enable compatibility mode:
 </p>
 <ol>
  <li>
   Remove the
   <span>
    nvidia-fs
   </span>
   kernel driver:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ rmmod nvidia-fs</p>
        </pre>
  </li>
  <li>
   In the /etc/cufile.json file, set
   <span>
    compat-mode
   </span>
   to
   <span>
    true
   </span>
   .
  </li>
  <li>
   Set the
   <span>
    CUFILE_FORCE_COMPAT_MODE
   </span>
   environment variable to true.
  </li>
 </ol>
 <p>
  <a class="Link" data-cms-ai="0" id="enable-comp-mode__result_rdg_m2h_tmb" name="enable-comp-mode__result_rdg_m2h_tmb" shape="rect">
  </a>
  The IO through
  <span>
   cuFileRead
  </span>
  /
  <span>
   cuFileWrite
  </span>
  will now fall back to the CPU path.
 </p>
 <h3 id="track-enable-comp-mode">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-enable-comp-mode">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-enable-comp-mode" id="track-enable-comp-mode" name="track-enable-comp-mode" shape="rect">
    5.10. Tracking the IO After Enabling Compatibility Mode
   </a>
  </a>
 </h3>
 <p>
  When GDS is used in compatibility mode, and
  <span>
   cufile_stats
  </span>
  is enabled in the
  <span>
   /etc/cufile.json
  </span>
  file, you can use
  <span>
   gds_stats
  </span>
  or another standard Linux tools, such as strace, iostat, iotop, SAR, ftrace, and perf. You can also use the BPF compiler collection tools to track and monitor the IO.
 </p>
 <p>
  When compatibility mode is enabled, internally,
  <span>
   cuFileRead
  </span>
  and
  <span>
   cuFileWrite
  </span>
  use POSIX pread and pwrite system calls, respectively.
 </p>
 <h3 id="bypass-gds">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#bypass-gds">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#bypass-gds" id="bypass-gds" name="bypass-gds" shape="rect">
    5.11. Bypassing GPUDirect Storage
   </a>
  </a>
 </h3>
 <p>
  There are some scenarios in which you can bypass GDS.
 </p>
 <p>
  There are some tunables where GDS IO and POSIX IO can go through simultaneously. The following are cases where GDS can be bypassed without having to remove the GDS driver:
  <a class="Link" data-cms-ai="0" id="bypass-gds__ul_ntl_413_rmb" name="bypass-gds__ul_ntl_413_rmb" shape="rect">
  </a>
 </p>
 <ul class="ul" id="bypass-gds__ul_ntl_413_rmb">
  <li class="li">
   On supported filesystems and block devices.
   <p>
    In the
    <span>
     /etc/cufile.json
    </span>
    file, if the
    <span>
     posix_unaligned_writes
    </span>
    config property is set to
    <span>
     true
    </span>
    , the unaligned writes will fall back to the compatibility mode and will not go through GDS. Refer to
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-prereqs" shape="rect" title="To install GDS on a non-DGX platform, complete the following steps:">
     Before You Install GDS
    </a>
    for a list of supported file systems.
   </p>
  </li>
  <li class="li">
   On an EXAScaler filesystem
   <p>
    In the /etc/cufile.json file, if the
    <span>
     posix_gds_min_kb config
    </span>
    property is set to a certain value (in KB), the IO for which the size is less than or equal to the set value, will fall back to POSIX mode. For example, if
    <span>
     posix_gds_min_kb
    </span>
    is set to 8KB, IOs with a size that is less than or equal to 8KB, will fall back to the POSIX mode.
   </p>
  </li>
  <li class="li">
   On a WekaIO filesystem:
   Note:
   <p>
    Currently,
    <span>
     cuFileWrite
    </span>
    will always fallback to the POSIX mode.
   </p>
   In the
   <span>
    /etc/cufile.json
   </span>
   file, if the
   <span>
    allow-compat-mode config
   </span>
   property is set to
   <span>
    true
   </span>
   :
   <a class="Link" data-cms-ai="0" id="bypass-gds__ul_odw_513_rmb" name="bypass-gds__ul_odw_513_rmb" shape="rect">
   </a>
   <ul>
    <li>
     If RDMA connections and/or memory registrations cannot be established,
     <span>
      cuFileRead
     </span>
     will fall back to the POSIX mode.
    </li>
    <li>
     <span>
      cuFileRead
     </span>
     fails to allocate an internal bounce buffer for non-4K aligned GPU VA addresses.
    </li>
   </ul>
  </li>
 </ul>
 <p>
  Refer to the
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html" shape="rect">
   GPUDirect Storage Best Practices Guide
  </a>
  for more information.
 </p>
 <h3 id="gds-not-working-mount">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-not-working-mount">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-not-working-mount" id="gds-not-working-mount" name="gds-not-working-mount" shape="rect">
    5.12. GDS Does Not Work for a Mount
   </a>
  </a>
 </h3>
 <p>
  GDS will not be used for a mount in the following cases:
 </p>
 <ul>
  <li>
   When the necessary GDS drivers are not loaded on the system.
  </li>
  <li>
   The filesystem associated with that mount is not supported by GDS.
  </li>
  <li>
   The mount point is denylisted in the /etc/cufile.json file.
  </li>
 </ul>
 <h3 id="gds-posix-io-same-file">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-posix-io-same-file">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-posix-io-same-file" id="gds-posix-io-same-file" name="gds-posix-io-same-file" shape="rect">
    5.13. Simultaneously Running the GPUDirect Storage IO and POSIX IO on the Same File
   </a>
  </a>
 </h3>
 <p>
  Since a file is opened in
  <span>
   O_DIRECT
  </span>
  mode for GDS, applications should avoid mixing
  <span>
   O_DIRECT
  </span>
  and normal I/O to the same file and to overlapping byte regions in the same file.
 </p>
 <p>
  Even when the filesystem correctly handles the coherency issues in this situation, overall I/O throughput might be slower than using either mode alone. Similarly, applications should avoid mixing
  <span>
   mmap(2)
  </span>
  of files with direct I/O to the same files. Refer to the filesystem-specific documentation for information about additional
  <span>
   O_DIRECT
  </span>
  limitations.
 </p>
 <h3 id="gds-data-verif-tests">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-data-verif-tests">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-data-verif-tests" id="gds-data-verif-tests" name="gds-data-verif-tests" shape="rect">
    5.14. Running Data Verification Tests Using GPUDirect Storage
   </a>
  </a>
 </h3>
 <p>
  GDS has an internal data verification utility,
  <span>
   gdsio_verify
  </span>
  , which is used to test data integrity of reads and writes. Run
  <span>
   gdsio_verify -h
  </span>
  for detailed usage information.
 </p>
 <p>
  For example:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-11.2/gds/tools/gds_verify -f /mnt/ai200/fio-seq-writes-1 -d 0 -o 0 -s 1G -n 1 -m 1</p>
        </pre>
 <p>
  Sample output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>gpu index :0,file :/mnt/ai200/fio-seq-writes-1, RING buffer size :0, 
gpu buffer alignment :0, gpu buffer offset :0, file offset :0, 
io_requested :1073741824, bufregister :true, sync :1, nr ios :1,
fsync :0,
address = 0x560d32c17000
Data Verification Success</p>
        </pre>
 Note:
 <p>
  This test completes data verification of reads and writes through GDS.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-perf">
   6. Troubleshooting Performance
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="troubleshoot-perf" name="troubleshoot-perf" shape="rect">
  </a>
  This section covers issues related to performance.
 </p>
 <h3 id="perf-benchmark-examples">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#perf-benchmark-examples">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#perf-benchmark-examples" id="perf-benchmark-examples" name="perf-benchmark-examples" shape="rect">
    6.1. Running Performance Benchmarks with GDS
   </a>
  </a>
 </h3>
 <p>
  You can run performance benchmarks with GDS and compare the results with CPU numbers.
 </p>
 <p>
  GDS has a homegrown benchmarking utility, /usr/local/cuda-x.y/gds/tools/gdsio, which helps you compare GDS IO throughput numbers with CPU IO throughput. Run
  <span>
   gdsio -h
  </span>
  for detailed usage information.
 </p>
 <p>
  Here are some examples:
  GDS: Storage --&gt; GPU Memory
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-x.y/tools/gdsio -f /mnt/ai200/fio-seq-writes-1 -d 0 -w 4 -s 10G -i 1M -I 0 -x 0</p>
        </pre>
 <p>
  Storage --&gt; CPU Memory
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-x.y/tools/gdsio -f /mnt/ai200/fio-seq-writes-1 -d 0 -w 4 -s 10G -i 1M -I 0 -x 1</p>
        </pre>
 <p>
  Storage --&gt; CPU Memory --&gt; GPU Memory
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-x.y/tool/gdsio -f /mnt/ai200/fio-seq-writes-1 -d 0 -w 4 -s 10G -i 1M -I 0 -x 2</p>
        </pre>
 <p>
  Storage --&gt; GPU Memory using batch mode
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-x.y/tool/gdsio -f /mnt/ai200/fio-seq-read-1 -d 0 -w 4 -s 10G -i 1M -I 0 -x 6</p>
        </pre>
 <p>
  Storage --&gt; GPU Memory using async stream mode
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-x.y/tool/gdsio -f /mnt/ai200/fio-seq-read-1 -d 0 -w 4 -s 10G -i 1M -I 0 -x 5</p>
        </pre>
 <h3 id="gds-internal-cache">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-internal-cache">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-internal-cache" id="gds-internal-cache" name="gds-internal-cache" shape="rect">
    6.2. Tracking Whether GPUDirect Storage is Using an Internal Cache
   </a>
  </a>
 </h3>
 <p>
  You can determine whether GDS is using an internal cache.
 </p>
 <p>
  Prerequisite
  : Before you start, read the
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html" shape="rect">
   GPUDirect Storage Best Practices Guide
  </a>
  .
 </p>
 <p>
  GDS Stats has per-GPU stats, and each piece of the GPU bus device function (BDF) information is displayed. If the
  <span>
   cache_MB
  </span>
  field is active on a GPU, GDS is using the cache internally to complete the IO. GDS might use the internal cache when one of the following conditions are true:
 </p>
 <ul>
  <li>
   The
   <span>
    file_offset
   </span>
   that was issued in
   <span>
    cuFileRead
   </span>
   /
   <span>
    cuFileWrite
   </span>
   is not 4K aligned.
  </li>
  <li>
   The size in
   <span>
    cuFileRead
   </span>
   /
   <span>
    cuFileWrite
   </span>
   calls are not 4K aligned.
  </li>
  <li>
   The
   <span>
    devPtr_base
   </span>
   that was issued in
   <span>
    cuFileRead
   </span>
   /
   <span>
    cuFileWrite
   </span>
   is not 4K aligned.
  </li>
  <li>
   The
   <span>
    devPtr_base+devPtr_offset
   </span>
   that was issued in
   <span>
    cuFileRead
   </span>
   /
   <span>
    cuFileWrite
   </span>
   is not 4K aligned.
  </li>
 </ul>
 <h3 id="io-cross-pcie-root-complex">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-cross-pcie-root-complex">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-cross-pcie-root-complex" id="io-cross-pcie-root-complex" name="io-cross-pcie-root-complex" shape="rect">
    6.3. Tracking when IO Crosses the PCIe Root Complex and Impacts Performance
   </a>
  </a>
 </h3>
 <p>
  You can track when the IO crosses the PCIe root complex and affects performance.
 </p>
 <p>
  Refer to
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#review-peer-aff-stats" shape="rect" title="The following proc files contain information about peer affinity DMA statistics via nvidia-fs callbacks:">
   Review Peer Affinity Stats for a Kernel Filesystem and Storage Devices
  </a>
  for more information.
 </p>
 <h3 id="monitor-cpu-usage">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-cpu-usage">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-cpu-usage" id="monitor-cpu-usage" name="monitor-cpu-usage" shape="rect">
    6.4. Using GPUDirect Statistics to Monitor CPU Activity
   </a>
  </a>
 </h3>
 <p>
  Although you cannot use GDS statistics to monitor CPU activity, you can use the following Linux tools to complete this task:
 </p>
 <ul>
  <li>
   <span>
    htop
   </span>
  </li>
  <li>
   <span>
    perf
   </span>
  </li>
  <li>
   <span>
    mpstat
   </span>
  </li>
 </ul>
 <h3 id="monit-perf-tracing">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monit-perf-tracing">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monit-perf-tracing" id="monit-perf-tracing" name="monit-perf-tracing" shape="rect">
    6.5. Monitoring Performance and Tracing with cuFile-* APIs
   </a>
  </a>
 </h3>
 <p>
  You can monitor performance and tracing with the cuFile-* APIs.
 </p>
 <p>
  You can use the FTrace, the Perf, or the BCC-BPF tools to monitor performance and tracing. Ensure that you have the symbols that you can use to track and monitor the performance with a standard Linux IO tool.
 </p>
 <h3 id="ex-linux-tools">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#ex-linux-tools">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#ex-linux-tools" id="ex-linux-tools" name="ex-linux-tools" shape="rect">
    6.6. Example: Using Linux Tracing Tools
   </a>
  </a>
 </h3>
 <p>
  The
  <span>
   cuFileBufRegister
  </span>
  function makes the pages that underlie a range of GPU virtual memory accessible to a third-party device. This process is completed by pinning the GPU device memory in the BAR space, which is an expensive operation and can take up to a few milliseconds.
 </p>
 <p>
  You can using the BCC/BPF tool to trace the
  <span>
   cuFileBufRegister
  </span>
  API, understand what is happening in the Linux kernel, and understand why this process is expensive.
 </p>
 <p>
  Scenario
  <a class="Link" data-cms-ai="0" id="ex-linux-tools__ol_fd4_bj3_rmb" name="ex-linux-tools__ol_fd4_bj3_rmb" shape="rect">
  </a>
 </p>
 <ol>
  <li>
   You are running a workload with 8 threads where each thread is issuing
   <span>
    cuFileBufRegister
   </span>
   to pin to the GPU memory.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ ./gdsio -f /mnt/ai200/seq-writes-1 -d 0 -w 8 -s 10G -i 1M -I 0 -x 0</p>
        </pre>
  </li>
  <li>
   When IO is in progress, use a tracing tool to understand what is going on with
   <span>
    cuFileBufRegister
   </span>
   :
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/share/bcc/tools# ./funccount -Ti 1 nvfs_mgroup_pin_shadow_pages</p>
        </pre>
  </li>
  <li>
   Review the sample output:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>15:04:56
FUNC                                    COUNT
nvfs_mgroup_pin_shadow_pages            8</p>
        </pre>
   <p>
    As you can see, the
    <span>
     nvfs_mgroup_pin_shadow_pages
    </span>
    function has been invoked 8 times in one per thread.
   </p>
  </li>
  <li>
   To see the latency for that function, run:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/share/bcc/tools# ./funclatency -i 1 nvfs_mgroup_pin_shadow_pages</p>
        </pre>
  </li>
  <li>
   Review the output:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>Tracing 1 functions for "nvfs_mgroup_pin_shadow_pages"... Hit Ctrl-C to end.

     nsecs               : count     distribution
         0 -&gt; 1          : 0        |                                        |
         2 -&gt; 3          : 0        |                                        |
         4 -&gt; 7          : 0        |                                        |
         8 -&gt; 15         : 0        |                                        |
        16 -&gt; 31         : 0        |                                        |
        32 -&gt; 63         : 0        |                                        |
        64 -&gt; 127        : 0        |                                        |
       128 -&gt; 255        : 0        |                                        |
       256 -&gt; 511        : 0        |                                        |
       512 -&gt; 1023       : 0        |                                        |
      1024 -&gt; 2047       : 0        |                                        |
      2048 -&gt; 4095       : 0        |                                        |
      4096 -&gt; 8191       : 0        |                                        |
      8192 -&gt; 16383      : 1        |*****                                   |
     16384 -&gt; 32767      : 7        |****************************************|</p>
        </pre>
   <p>
    Seven calls of the
    <span>
     nvfs_mgroup_pin_shadow_pages
    </span>
    function took about 16-32 microseconds. This is probably coming from the Linux kernel
    <span>
     get_user_pages_fast
    </span>
    that is used to pin shadow pages.
   </p>
   <p>
    <span>
     cuFileBufRegister
    </span>
    invokes
    <span>
     nvidia_p2p_get_pages
    </span>
    NVIDIA driver function to pin GPU device memory in the BAR space. This information is obtained by running
    <span>
     $ perf top -g
    </span>
    and getting the call graph of
    <span>
     cuFileBufRegister
    </span>
    .
   </p>
  </li>
 </ol>
 <p>
  The following example the overhead of the
  <span>
   nvidia_p2p_get_pages
  </span>
  :
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/share/bcc/tools# ./funclatency -Ti 1 nvidia_p2p_get_pages

15:45:19
nsecs               : count     distribution
  0 -&gt; 1          : 0        |                                        |
  2 -&gt; 3          : 0        |                                        |
  4 -&gt; 7          : 0        |                                        |
  8 -&gt; 15         : 0        |                                        |
 16 -&gt; 31         : 0        |                                        |
 32 -&gt; 63         : 0        |                                        |
 64 -&gt; 127        : 0        |                                        |
128 -&gt; 255        : 0        |                                        |
256 -&gt; 511        : 0        |                                        |
512 -&gt; 1023       : 0        |                                        |
1024 -&gt; 2047       : 0        |                                        |
2048 -&gt; 4095       : 0        |                                        |
4096 -&gt; 8191       : 0        |                                        |
8192 -&gt; 16383      : 0        |                                        |
16384 -&gt; 32767      : 0        |                                        |
32768 -&gt; 65535      : 0        |                                        |
65536 -&gt; 131071     : 0        |                                        |
131072 -&gt; 262143     : 0        |                                        |
262144 -&gt; 524287     : 2        |*************                           |
524288 -&gt; 1048575    : 6        |****************************************|</p>
        </pre>
 <h3 id="trace-cufile-apis">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trace-cufile-apis">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trace-cufile-apis" id="trace-cufile-apis" name="trace-cufile-apis" shape="rect">
    6.7. Tracing the cuFile-* APIs
   </a>
  </a>
 </h3>
 <p>
  You can use nvprof/NVIDIA Nsight to trace the cuFile-* APIs.
 </p>
 <p>
  NVTX static tracepoints are available for public interface in the
  <span>
   libcufile.so
  </span>
  library. After these static tracepoints are enabled, you can view these traces in NVIDIA Nsight just like any other CUDA® symbols. You can enable the NVTX tracing using the JSON configuration at /etc/cufile.json:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>"profile": {
             // nvtx profiling on(true)/off(false)
             "nvtx": true, 
           },</p>
        </pre>
 <h3 id="dynamic-routing-improving-performance">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-improving-performance">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-improving-performance" id="dynamic-routing-improving-performance" name="dynamic-routing-improving-performance" shape="rect">
    6.8. Improving Performance using Dynamic Routing
   </a>
  </a>
 </h3>
 <p>
  On platforms where the IO transfers between GPU(s) and the storage NICs involve PCIe traffic across PCIe-host bridge, GPUDirect Storage IO may not see a great throughput especially for writes. Also, certain chipsets may support only P2P read traffic for host bridge traffic. In such cases, the dynamic routing feature can be enabled to debug and identify what routing policy is deemed best for such platforms. This can be illustrated with a single GPU write test with the
  <span>
   gdsio
  </span>
  tool, where there is one Storage NIC and 10 GPUs with NVLINKs access enabled between the GPUS. With dynamic routing enabled, even though the GPU and NIC might be on different sockets, GDS can still achieve the maximum possible write throughput.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /etc/cufile.json | grep rdma_dev
         "rdma_dev_addr_list": [ "192.168.0.19" ],</p>
        </pre>
 <p>
  Dynamic Routing OFF:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /etc/cufile.json | grep routing 
         "rdma_dynamic_routing": false
$ for i in 0 1 2 3 4 5 6 7 8 9 10;
    do
 ./gdsio -f /mnt/nfs/file1 -d $i -n 0 -w 4 -s 1G -i 1M -x 0 -I 1 -p -T 15 ;
    done</p>
        </pre>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 45792256/4194304(KiB) IOSize: 1024(KiB) Throughput: 2.873560 GiB/sec, Avg_Latency: 1359.280174 usecs ops: 44719 total_time 15.197491 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 45603840/4194304(KiB) IOSize: 1024(KiB) Throughput: 2.867613 GiB/sec, Avg_Latency: 1363.891220 usecs ops: 44535 total_time 15.166344 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 42013696/4194304(KiB) IOSize: 1024(KiB) Throughput: 2.848411 GiB/sec, Avg_Latency: 1373.154082 usecs ops: 41029 total_time 14.066573 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 43517952/4194304(KiB) IOSize: 1024(KiB) Throughput: 2.880763 GiB/sec, Avg_Latency: 1358.207427 usecs ops: 42498 total_time 14.406582 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 34889728/4194304(KiB) IOSize: 1024(KiB) Throughput: 2.341907 GiB/sec, Avg_Latency: 1669.108902 usecs ops: 34072 total_time 14.207836 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 36955136/4194304(KiB) IOSize: 1024(KiB) Throughput: 2.325239 GiB/sec, Avg_Latency: 1680.001220 usecs ops: 36089 total_time 15.156790 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 37075968/4194304(KiB) IOSize: 1024(KiB) Throughput: 2.351491 GiB/sec, Avg_Latency: 1661.198487 usecs ops: 36207 total_time 15.036584 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 35066880/4194304(KiB) IOSize: 1024(KiB) Throughput: 2.235654 GiB/sec, Avg_Latency: 1748.638950 usecs ops: 34245 total_time 14.958656 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 134095872/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.940253 GiB/sec, Avg_Latency: 436.982682 usecs ops: 130953 total_time 14.304269 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 135974912/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.932070 GiB/sec, Avg_Latency: 437.334849 usecs ops: 132788 total_time 14.517998 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 174486528/4194304(KiB) IOSize: 1024(KiB) Throughput: 11.238476 GiB/sec, Avg_Latency: 347.603610 usecs ops: 170397 total_time 14.806573 secs</p>
        </pre>
 <p>
  Dynamic Routing ON (nvlinks enabled):
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /etc/cufile.json | grep routing
         "rdma_dynamic_routing": true
         "rdma_dynamic_routing_order": [ "GPU_MEM_NVLINKS"]

$ for i in 0 1 2 3 4 5 6 7 8 9 10; 
do 
./gdsio -f  /mnt/nfs/file1 -d $i -n 0 -w 4 -s 1G -i 1M -x 0 -I 1 -p -T 15 ;
done</p>
        </pre>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 134479872/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.885214 GiB/sec, Avg_Latency: 437.942083 usecs ops: 131328 total_time 14.434092 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 138331136/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.891407 GiB/sec, Avg_Latency: 437.668104 usecs ops: 135089 total_time 14.837118 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 133800960/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.897250 GiB/sec, Avg_Latency: 437.305565 usecs ops: 130665 total_time 14.341795 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 133990400/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.888714 GiB/sec, Avg_Latency: 437.751327 usecs ops: 130850 total_time 14.375893 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 141934592/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.905190 GiB/sec, Avg_Latency: 437.032919 usecs ops: 138608 total_time 15.200055 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 133379072/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.892493 GiB/sec, Avg_Latency: 437.488259 usecs ops: 130253 total_time 14.304222 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 142271488/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.892426 GiB/sec, Avg_Latency: 437.660016 usecs ops: 138937 total_time 15.258004 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 134951936/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.890496 GiB/sec, Avg_Latency: 437.661177 usecs ops: 131789 total_time 14.476154 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 132667392/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.930203 GiB/sec, Avg_Latency: 437.420830 usecs ops: 129558 total_time 14.167817 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 137982976/4194304(KiB) IOSize: 1024(KiB) Throughput: 8.936189 GiB/sec, Avg_Latency: 437.123356 usecs ops: 134749 total_time 14.725608 secs
    url index :0, urlname :192.168.0.2 urlport :18515
IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 170469376/4194304(KiB) IOSize: 1024(KiB) Throughput: 11.231479 GiB/sec, Avg_Latency: 347.818052 usecs ops: 166474 total_time 14.474698 secs</p>
        </pre>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trouble-io-activity">
   7. Troubleshooting IO Activity
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="trouble-io-activity" name="trouble-io-activity" shape="rect">
  </a>
  This section covers issues that are related to IO activity and the interactions with the rest of Linux.
 </p>
 <h3 id="manage-coherency-page-cache">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#manage-coherency-page-cache">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#manage-coherency-page-cache" id="manage-coherency-page-cache" name="manage-coherency-page-cache" shape="rect">
    7.1. Managing Coherency of Data in the Page Cache and on Disk
   </a>
  </a>
 </h3>
 <p>
  When using GDS, files are often opened with the
  <span>
   O_DIRECT
  </span>
  mode. When IO is complete, in the context of DIRECT IO, it bypasses the page cache.
 </p>
 <p>
  Starting with CUDA toolkit 12.2 (GDS version 1.7.x) files can also be opened with non-O_DIRECT mode. Even in such a case, whenever the library software deems fit, it will follow the GDS enabled O_DIRECT path. This conserves coherency by default.
  <a class="Link" data-cms-ai="0" id="manage-coherency-page-cache__ul_nmw_jl3_rmb" name="manage-coherency-page-cache__ul_nmw_jl3_rmb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   On EXAScaler filesystem:
   <a class="Link" data-cms-ai="0" id="manage-coherency-page-cache__ul_dss_kl3_rmb" name="manage-coherency-page-cache__ul_dss_kl3_rmb" shape="rect">
   </a>
   <ul>
    <li>
     For reads, IO bypasses the page cache and fetches the data directly from backend storage.
    </li>
    <li>
     When writes are issued, the
     <span>
      nvidia-fs
     </span>
     drivers will try to flush the data in the page cache for the range of offset-length before issuing writes to the VFS subsystem.
    </li>
    <li>
     The stats that track this information are:
     <a class="Link" data-cms-ai="0" id="manage-coherency-page-cache__ul_b4x_nl3_rmb" name="manage-coherency-page-cache__ul_b4x_nl3_rmb" shape="rect">
     </a>
     <ul>
      <li>
       <span>
        pg_cache
       </span>
      </li>
      <li>
       <span>
        pg_cache_fail
       </span>
      </li>
      <li>
       <span>
        pg_cache_eio
       </span>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li>
   On WekaIO filesystem:
   <a class="Link" data-cms-ai="0" id="manage-coherency-page-cache__ul_v3c_tl3_rmb" name="manage-coherency-page-cache__ul_v3c_tl3_rmb" shape="rect">
   </a>
   <ul>
    <li>
     For reads, IO bypasses the page cache and fetches the data directly from backend storage.
    </li>
   </ul>
  </li>
 </ul>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-fs-lnet">
   8. EXAScaler Filesystem LNet Troubleshooting
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="exascaler-fs-lnet" name="exascaler-fs-lnet" shape="rect">
  </a>
  This section describes how to troubleshoot issues with the EXAScaler Filesystem.
 </p>
 <h3 id="exascaler-client-mod-version">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-client-mod-version">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-client-mod-version" id="exascaler-client-mod-version" name="exascaler-client-mod-version" shape="rect">
    8.1. Determining the EXAScaler Filesystem Client Module Version
   </a>
  </a>
 </h3>
 <p>
  To check the EXAScaler filesystem Client version, check dmesg after you install the EXAScaler filesystem.
 </p>
 Note:
 <p>
  The EXAScaler server version should be EXA-5.2.
 </p>
 <p>
  This table provides a list of the client kernel module versions that have been tested with DDN AI200 and DDN AI400 systems:
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" id="exascaler-client-mod-version__table_lvd_5cz_gnb" rules="all" summary="">
  Table 2. Tested Kernel Module Versions
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e2961" rowspan="1" valign="top">
    DDN Client Version
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e2964" rowspan="1" valign="top">
    Kernel Version
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e2967" rowspan="1" valign="top">
    MLNX_OFED version
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e2961" rowspan="1" valign="top">
    2.12.3_ddn28
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e2964" rowspan="1" valign="top">
    4.15.0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e2967" rowspan="1" valign="top">
    MLNX_OFED 4.7
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e2961" rowspan="1" valign="top">
    2.12.3_ddn29
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e2964" rowspan="1" valign="top">
    4.15.0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e2967" rowspan="1" valign="top">
    MLNX_OFED 4.7
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e2961" rowspan="1" valign="top">
    2.12.3_ddn39
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e2964" rowspan="1" valign="top">
    4.15.0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e2967" rowspan="1" valign="top">
    MLNX_OFED 5.1
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e2961" rowspan="1" valign="top">
    2.12.5_ddn4
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e2964" rowspan="1" valign="top">
    5.4.0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e2967" rowspan="1" valign="top">
    MLNX_OFED 5.1
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e2961" rowspan="1" valign="top">
    2.12.6_ddn19
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e2964" rowspan="1" valign="top">
    5.4.0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e2967" rowspan="1" valign="top">
    MLNX_OFED 5.3
   </td>
  </tr>
 </table>
 <p>
  To verify the client version, run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo lctl get_param version</p>
        </pre>
 <p>
  Sample output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>Lustre version: 2.12.3_ddn39</p>
        </pre>
 <h3 id="lnet-network-setup">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#lnet-network-setup">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#lnet-network-setup" id="lnet-network-setup" name="lnet-network-setup" shape="rect">
    8.2. Checking the LNet Network Setup on a Client
   </a>
  </a>
 </h3>
 <p>
  To check the LNet network setup on the client:
 </p>
 <ol>
  <li>
   Run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ sudo lnetctl net show:</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>net:
   - net type: lo</p>
        </pre>
  </li>
 </ol>
 <h3 id="check-peer-health">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-peer-health">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-peer-health" id="check-peer-health" name="check-peer-health" shape="rect">
    8.3. Checking the Health of the Peers
   </a>
  </a>
 </h3>
 <p>
  An Lnet health value of 1000 is the best possible value that can be reported for a network interface. Anything less than 1000 indicates that the interface is running in a degraded mode and has encountered some errors.
 </p>
 <ol>
  <li>
   Run the following command ;
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ sudo lnetctl net show -v 3 | grep health</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>health stats:
      health stats:
          health value: 1000
      health stats:
          health value: 1000
      health stats:
          health value: 1000
      health stats:
          health value: 1000
      health stats:
          health value: 1000
      health stats:
          health value: 1000
      health stats:
          health value: 1000
      health stats:
          health value: 1000</p>
        </pre>
  </li>
 </ol>
 <h3 id="multi-rail-support">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#multi-rail-support">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#multi-rail-support" id="multi-rail-support" name="multi-rail-support" shape="rect">
    8.4. Checking for Multi-Rail Support
   </a>
  </a>
 </h3>
 <p>
  To verify whether multi-rail is supported:
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ sudo lnetctl peer show | grep -i Multi-Rail:</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>Multi-Rail: True</p>
        </pre>
  </li>
 </ol>
 <h3 id="check-peer-affinity">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-peer-affinity">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-peer-affinity" id="check-peer-affinity" name="check-peer-affinity" shape="rect">
    8.5. Checking GDS Peer Affinity
   </a>
  </a>
 </h3>
 <p>
  For peer affinity, you need to check whether the expected interfaces are being used for the associated GPUs.
 </p>
 <p>
  The code snippet below is a description of a test that runs load on a specific GPU. The test validates whether the interface that is performing the send and receive is the interface that is the closest, and is correctly mapped, to the GPU. See
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#reset-nvidia-fs-stats" shape="rect" title="To reset the nvidia-fs statistics, run the following commands:">
   Resetting the nvidia-fs Statistics
  </a>
  and
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#review-peer-aff-stats" shape="rect" title="The following proc files contain information about peer affinity DMA statistics via nvidia-fs callbacks:">
   Reviewing Peer Affinity Stats for a Kernel File System and Storage Drivers
  </a>
  for more information about the metrics that are used to check peer affinity.
 </p>
 <p>
  You can run a
  <span>
   gdsio
  </span>
  test for the tools section and monitor the LNET stats. See the readme file for more information. In the
  <span>
   gdsio
  </span>
  test, a write test has been completed on GPU 0. The expected NIC interface for GPU 0 is ib0 on the NVIDIA DGX-2™ platform. The
  <span>
   lnetctl net
  </span>
  show statistics were previously captured, and after the
  <span>
   gdsio
  </span>
  test, you can see that the RPC send and receive have happened over the IB0.
 </p>
 <ol>
  <li>
   Run the
   <span>
    gdsio
   </span>
   test.
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ sudo lustre_rmmod
$ sudo mount -t lustre 192.168.1.61@o2ib,192.168.1.62@o2ib:/ai200 /mnt/ai200/
$ sudo lnetctl net show -v 3 | grep health
          health stats:
              health value: 0
          health stats:
              health value: 1000
          health stats:
              health value: 1000
          health stats:
              health value: 1000
          health stats:
              health value: 1000
          health stats:
              health value: 1000
          health stats:
              health value: 1000
          health stats:
              health value: 1000

$ sudo lnetctl net show -v 3 | grep -B 2 -i 'send_count\|recv_count'           status: up
          statistics:
              send_count: 0
              recv_count: 0
--
              0: ib0
          statistics:
              send_count: 3
              recv_count: 3
--
              0: ib2
          statistics:
              send_count: 3
              recv_count: 3
--
              0: ib3
          statistics:
              send_count: 2
              recv_count: 2
--
              0: ib4
          statistics:
              send_count: 13
              recv_count: 13
--
              0: ib5
          statistics:
              send_count: 12
              recv_count: 12
--
              0: ib6
          statistics:
              send_count: 12
              recv_count: 12
--
              0: ib7
          statistics:
              send_count: 11
              recv_count: 11
 
$ echo 1 &gt; /sys/module/nvidia_fs/parameters/peer_stats_enabled
 
$ /usr/local/cuda-x.y/tools/gdsio -f /mnt/ai200/test -d 0 -n 0 -w 1 -s 1G -i 4K -x 0 -I 1
IoType: WRITE XferType: GPUD Threads: 1  DataSetSize: 1073741824/1073741824 IOSize: 4(KB),Throughput: 0.004727 GB/sec, Avg_Latency: 807.026154 usecs ops: 262144 total_time 211562847.000000 usecs

$ sudo lnetctl net show -v 3 | grep -B 2 -i 'send_count\|recv_count'

          status: up
          statistics:
              send_count: 0
              recv_count: 0
--
              0: ib0
          statistics:
              send_count: 262149
              recv_count: 524293
--
              0: ib2
          statistics:
              send_count: 6
              recv_count: 6
--
              0: ib3
          statistics:
              send_count: 6
              recv_count: 6
--
              0: ib4
          statistics:
              send_count: 33
              recv_count: 33
--
              0: ib5
          statistics:
              send_count: 32
              recv_count: 32
--
              0: ib6
          statistics:
              send_count: 32
              recv_count: 32
--
              0: ib7
          statistics:
              send_count: 32
              recv_count: 32

$ cat /proc/driver/nvidia-fs/peer_affinity                                                                                       
GPU P2P DMA distribution based on pci-distance

(last column indicates p2p via root complex)
GPU :0000:be:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:3b:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e7:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e5:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e0:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:57:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:39:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:36:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e2:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:59:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:b7:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:b9:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:bc:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:34:00.0 :0 0 23872512 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5e:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5c:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</p>
        </pre>
  </li>
 </ol>
 <h3 id="check-lnet-errors">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-lnet-errors">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-lnet-errors" id="check-lnet-errors" name="check-lnet-errors" shape="rect">
    8.6. Checking for LNet-Level Errors
   </a>
  </a>
 </h3>
 <p>
  The errors impact the health of individual NICs and affect how the EXAScaler filesystem selects the best peer, which impacts GDS performance.
 </p>
 Note:
 <p>
  To run these commands, you must have sudo priveleges.
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/driver/nvidia-fs/peer_affinity</p>
        </pre>
  </li>
  <li>
   Review the ouput, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>GPU P2P DMA distribution based on pci-distance
(last column indicates p2p via root complex)
GPU :0000:be:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 GPU :0000:3b:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 GPU :0000:e7:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 GPU :0000:e5:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 GPU :0000:e0:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 GPU :0000:57:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1276417

(Note : if peer traffic goes over Root-Port, one of the reasons might be that health of nearest NIC might be  affected)
GPU :0000:39:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:36:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e2:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:59:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:b7:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:b9:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:bc:00.0 :0 0 7056141 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:34:00.0 :0 0 8356175 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5e:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5c:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0

$ sudo lnetctl stats show
statistics:
    msgs_alloc: 1
    msgs_max: 126
    rst_alloc: 25
    errors: 0
    send_count: 243901
    resend_count: 1
    response_timeout_count: 1935
    local_interrupt_count: 0
    local_dropped_count: 208
    local_aborted_count: 0
    local_no_route_count: 0
    local_timeout_count: 1730
    local_error_count: 0
    remote_dropped_count: 0
    remote_error_count: 0
    remote_timeout_count: 0
    network_timeout_count: 0
    recv_count: 564436
    route_count: 0
    drop_count: 0
    send_length: 336176013248
    recv_length: 95073248
    route_length: 0
    drop_length: 0
 lnetctl net show -v 4

net:
    - net type: o2ib
      local NI(s):
        - nid: 192.168.1.71@o2ib
          status: up
          interfaces:
              0: ib0
          statistics:
              send_count: 171621
              recv_count: 459717
              drop_count: 0
          sent_stats:
              put: 119492
              get: 52129
              reply: 0
              ack: 0
              hello: 0
          received_stats:
              put: 119492
              get: 0
              reply: 340225
              ack: 0
              hello: 0
          dropped_stats:
              put: 0
              get: 0
              reply: 0
              ack: 0
              hello: 0
          health stats:
              health value: 1000
              interrupts: 0
              dropped: 0
              aborted: 0
              no route: 0
              timeouts: 0
              error: 0
          tunables:
              peer_timeout: 180
              peer_credits: 32
              peer_buffer_credits: 0
              credits: 256
              peercredits_hiw: 16
              map_on_demand: 1
              concurrent_sends: 64
              fmr_pool_size: 512
              fmr_flush_trigger: 384
              fmr_cache: 1
              ntx: 512
              conns_per_peer: 1
          lnd tunables:
          dev cpt: 0
          tcp bonding: 0
          CPT: "[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]"
        - nid: 192.168.2.71@o2ib
          status: up
          interfaces:
              0: ib1
          statistics:
              send_count: 79
              recv_count: 79
              drop_count: 0
          sent_stats:
              put: 78
              get: 1
              reply: 0
              ack: 0
              hello: 0
          received_stats:
              put: 78
              get: 0
              reply: 1
              ack: 0
              hello: 0
          dropped_stats:
              put: 0
              get: 0
              reply: 0
              ack: 0
              hello: 0
          health stats:
              health value: 979
              interrupts: 0
              dropped: 0
              aborted: 0
              no route: 0
              timeouts: 1
              error: 0
          tunables:
              peer_timeout: 180
              peer_credits: 32
              peer_buffer_credits: 0
              credits: 256
              peercredits_hiw: 16
              map_on_demand: 1
              concurrent_sends: 64
              fmr_pool_size: 512
              fmr_flush_trigger: 384
              fmr_cache: 1
              ntx: 512
              conns_per_peer: 1
          lnd tunables:
          dev cpt: 0
          tcp bonding: 0
          CPT: "[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]"
        - nid: 192.168.2.72@o2ib
          status: up
          interfaces:
              0: ib3
          statistics:
              send_count: 52154
              recv_count: 52154
              drop_count: 0
          sent_stats:
              put: 25
              get: 52129
              reply: 0
              ack: 0
              hello: 0
          received_stats:
              put: 25
              get: 52129
              reply: 0
              ack: 0
              hello: 0
          dropped_stats:
              put: 0
              get: 0
              reply: 0
              ack: 0
              hello: 0
          health stats:
              health value: 66
              interrupts: 0
              dropped: 208
              aborted: 0
              no route: 0
              timeouts: 1735
              error: 0
          tunables:
              peer_timeout: 180
              peer_credits: 32
              peer_buffer_credits: 0
              credits: 256
              peercredits_hiw: 16
              map_on_demand: 1
              concurrent_sends: 64
              fmr_pool_size: 512
              fmr_flush_trigger: 384
              fmr_cache: 1
              ntx: 512
              conns_per_peer: 1
          lnd tunables:
          dev cpt: 0
          tcp bonding: 0
         CPT: "[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]"</p>
        </pre>
  </li>
 </ol>
 <p>
  If you see incrementing error stats, capture the net logging and provide this information for debugging:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ lctl set_param debug=+net
# reproduce the problem
$ lctl dk &gt; logfile.dk</p>
        </pre>
 <h3 id="lnet-nids-deg-timeouts">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#lnet-nids-deg-timeouts">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#lnet-nids-deg-timeouts" id="lnet-nids-deg-timeouts" name="lnet-nids-deg-timeouts" shape="rect">
    8.7. Resolving LNet NIDs Health Degradation from Timeouts
   </a>
  </a>
 </h3>
 <p>
  With large machines, such as DGX™ that have multiple interfaces, if Linux routing is not correctly set up, there might be connection failures and other unexpected behavior.
 </p>
 <p>
  A typical network setting that is used to resolve local connection timeouts is:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>sysctl -w net.ipv4.conf.all.accept_local=1</p>
        </pre>
 <p>
  <a class="Link" data-cms-ai="0" id="lnet-nids-deg-timeouts__result_wr3_jwc_smb" name="lnet-nids-deg-timeouts__result_wr3_jwc_smb" shape="rect">
  </a>
  There are also generic pointers for resolving LNet Network issues. Refer to
  <a class="xref" data-cms-ai="0" href="https://wiki.whamcloud.com/display/LNet/MR+Cluster+Setup" shape="rect">
   MR Cluster Setup
  </a>
  for more information.
 </p>
 <h3 id="multi-osts-peer-sel">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#multi-osts-peer-sel">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#multi-osts-peer-sel" id="multi-osts-peer-sel" name="multi-osts-peer-sel" shape="rect">
    8.8. Configuring LNet Networks with Multiple OSTs for Optimal Peer Selection
   </a>
  </a>
 </h3>
 <p>
  When there are multiple OSTs (Object Storage Targets), and each OST is dual interface, to need to have one interface on each of the LNets for which the client is configured.
 </p>
 <p>
  For example, you have the following two LNet Subnets on the client side:
 </p>
 <ul>
  <li>
   <span>
    o2ib
   </span>
  </li>
  <li>
   <span>
    o2ib1
   </span>
  </li>
 </ul>
 <p>
  The server has only one Lnet subnet,
  <span>
   o2ib
  </span>
  . In this situation, the routing is not optimal, because you are restricting the ib selection logic to a set of devices, which may not be closest to the GPU. There is no way to reach OST2 except over the LNet to which it is connected.
 </p>
 <p>
  The traffic that goes to this OST will never be optimal, and this configuration might affect overall throughput and latency. If, however, you configure the server to use two networks,
  <span>
   o2ib0
  </span>
  and
  <span>
   o2ib1
  </span>
  , then OST1 and OST2 can be reached over both networks. When the selection algorithm runs, it will determine that the best path is, for example, OST2 over
  <span>
   o2ib1
  </span>
  .
 </p>
 <ol>
  <li>
   To configure the client-side LNET, run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ sudo lnetctl net show</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>net:
    - net type: lo
      local NI(s):
        - nid: 0@lo
          status: up
    - net type: o2ib
      local NI(s):
        - nid: 192.168.1.71@o2ib
          status: up
          interfaces:
              0: ib0
        - nid: 192.168.1.72@o2ib
          status: up
          interfaces:
              0: ib2
        - nid: 192.168.1.73@o2ib
          status: up
          interfaces:
              0: ib4
        - nid: 192.168.1.74@o2ib
          status: up
          interfaces:
              0: ib6
    - net type: o2ib1
      local NI(s):
        - nid: 192.168.2.71@o2ib1
          status: up
          interfaces:
              0: ib1
        - nid: 192.168.2.72@o2ib1
          status: up
          interfaces:
              0: ib3
        - nid: 192.168.2.73@o2ib1
          status: up
          interfaces:
              0: ib5
        - nid: 192.168.2.74@o2ib1
          status: up
          interfaces:
              0: ib7</p>
        </pre>
  </li>
 </ol>
 <p>
  For an optimal configuration, the LNet peer should show two LNet subnets. In this case, the primary
  <span>
   nid
  </span>
  is only one
  <span>
   o2ib
  </span>
  :
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo lnetctl peer show</p>
        </pre>
 <p>
  Sample output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>peer:
    - primary nid: 192.168.1.62@o2ib
      Multi-Rail: True
      peer ni:
        - nid: 192.168.1.62@o2ib
          state: NA
        - nid: 192.168.2.62@o2ib1
          state: NA
       - primary nid: 192.168.1.61@o2ib
      Multi-Rail: True
      peer ni:
        - nid: 192.168.1.61@o2ib
          state: NA
        - nid: 192.168.2.61@o2ib1
          state: NA</p>
        </pre>
 <p>
  From the server side, here is an example of sub-optimal LNet configuration:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>[root@ai200-090a-vm01 ~]# lnetctl net show
net:
    - net type: lo
      local NI(s):
        - nid: 0@lo
          status: up
    - net type: o2ib (o2ib1 is not present)
      local NI(s):
        - nid: 192.168.1.62@o2ib
          status: up
          interfaces:
              0: ib0
        - nid: 192.168.2.62@o2ib
          status: up
          interfaces:
              0: ib1</p>
        </pre>
 <p>
  Here is an example of an IB configuration for a non-optimal case, where a file is stripped over two OSTs, and there are sequential reads:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ ibdev2netdev -v

0000:b8:00.1 mlx5_13 (MT4123 - MCX653106A-ECAT) ConnectX-6 VPI adapter card, 100Gb/s (HDR100, EDR IB and 100GbE), dual-port QSFP56                                                                                                    fw 20.26.4012 port 1 (ACTIVE) ==&gt; ib4 (Up) (o2ib)

ib4: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 2044

        inet 192.168.1.73  netmask 255.255.255.0  broadcast 192.168.1.255

0000:bd:00.1 mlx5_15 (MT4123 - MCX653106A-ECAT) ConnectX-6 VPI adapter card, 100Gb/s (HDR100, EDR IB and 100GbE), dual-port QSFP56                                                                                                     fw 20.26.4012 port 1 (ACTIVE) ==&gt; ib5 (Up) (o2ib1)

 ib5: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 2044

        inet 192.168.2.73  netmask 255.255.255.0  broadcast 192.168.2.255

        

$ cat /proc/driver/nvidia-fs/peer_distance | grep 0000:be:00.0 | grep network

0000:be:00.0    0000:58:00.1    138     0       network
0000:be:00.0    0000:58:00.0    138     0       network
0000:be:00.0    0000:86:00.1    134     0       network
0000:be:00.0    0000:35:00.0    138     0       network
0000:be:00.0    0000:5d:00.0    138     0       network
0000:be:00.0    0000:bd:00.0    3       0       network
0000:be:00.0    0000:b8:00.1    7       30210269        network (ib4) (chosen peer)
0000:be:00.0    0000:06:00.0    134     0       network
0000:be:00.0    0000:0c:00.1    134     0       network
0000:be:00.0    0000:e6:00.0    138     0       network
0000:be:00.0    0000:3a:00.1    138     0       network
0000:be:00.0    0000:e1:00.0    138     0       network
0000:be:00.0    0000:bd:00.1    3       4082933 network (ib5) (best peer)
0000:be:00.0    0000:e6:00.1    138     0       network
0000:be:00.0    0000:86:00.0    134     0       network
0000:be:00.0    0000:35:00.1    138     0       network
0000:be:00.0    0000:e1:00.1    138     0       network
0000:be:00.0    0000:0c:00.0    134     0       network
0000:be:00.0    0000:b8:00.0    7       0       network
0000:be:00.0    0000:5d:00.1    138     0       network
0000:be:00.0    0000:3a:00.0    138     0       network</p>
        </pre>
 <p>
  Here is an example of an optimal LNet configuration:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>[root@ai200-090a-vm00 ~]# lnetctl net show
net:
    - net type: lo
      local NI(s):
        - nid: 0@lo
          status: up
    - net type: o2ib
      local NI(s):
        - nid: 192.168.1.61@o2ib
          status: up
          interfaces:
              0: ib0
    - net type: o2ib1
      local NI(s):
        - nid: 192.168.2.61@o2ib1
          status: up
          interfaces:
              0: ib1</p>
        </pre>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-fs-perf">
   9. Understanding EXAScaler Filesystem Performance
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="exascaler-fs-perf" name="exascaler-fs-perf" shape="rect">
  </a>
  Depending on the type of host channel adapter (HCA), commonly known as a NIC, there are mod parameters that can be tuned for LNet. The NICs that you select should be up and healthy.
 </p>
 <p>
  To verify the health by mounting and running some basic tests, use
  <span>
   lnetctl
  </span>
  health statistics, and run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /etc/modprobe.d/lustre.conf</p>
        </pre>
 <p>
  Example output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>options libcfs cpu_npartitions=24 cpu_pattern=""                        
options lnet networks="o2ib0(ib1,ib2,ib3,ib4,ib6,ib7,ib8,ib9)"                         
options ko2iblnd peer_credits=32 concurrent_sends=64 peer_credits_hiw=16 map_on_demand=0</p>
        </pre>
 <h3 id="osc-tune-perf-param">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#osc-tune-perf-param">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#osc-tune-perf-param" id="osc-tune-perf-param" name="osc-tune-perf-param" shape="rect">
    9.1. osc Tuning Performance Parameters
   </a>
  </a>
 </h3>
 <p>
  The following is information about tuning filesystem parameters.
 </p>
 Note:
 <p>
  To maximize the throughput, you can tune the following EXAScaler® filesystem client parameters, based on the network.
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ lctl get_param osc.*.max* osc.*.checksums</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ lctl get_param osc.*.max* osc.*.checksums

osc.ai400-OST0024-osc-ffff916f6533a000.max_pages_per_rpc=4096
osc.ai400-OST0024-osc-ffff916f6533a000.max_dirty_mb=512
osc.ai400-OST0024-osc-ffff916f6533a000.max_rpcs_in_flight=32
osc.ai400-OST0024-osc-ffff916f6533a000.checksums=0</p>
        </pre>
  </li>
 </ol>
 <p>
  To check llite parameters, run
  <span>
   $ lctl get_param llite.*.*
  </span>
  .
 </p>
 <h3 id="misc-comms">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#misc-comms">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#misc-comms" id="misc-comms" name="misc-comms" shape="rect">
    9.2. Miscellaneous Commands for osc, mdc, and stripesize
   </a>
  </a>
 </h3>
 <p>
  If the tuning parameters are set correctly, you can use these parameters to observe.
 </p>
 <ol>
  <li>
   To get an overall EXAScaler filesystem client side statistics, run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ lctl get_param osc.*.import</p>
        </pre>
   Note:
   <p>
    The command includes
    <span>
     rpc
    </span>
    information.
   </p>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ watch -d 'lctl get_param osc.*.import | grep -B 1 inflight'
    rpcs:
       inflight: 5
    rpcs:
       inflight: 33</p>
        </pre>
  </li>
  <li>
   To get the maximum number of pages that can be transferred per rpc in a EXAScaler filesystem client, run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ lctl get_param osc.*.max_pages_per_rpc</p>
        </pre>
  </li>
  <li>
   To get the overall rpc statistics from a EXAScaler filesystem client, run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ lctl set_param osc.*.rpc_stats=clear (to reset osc stats)
$ lctl get_param osc.*.rpc_stats</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>osc.ai200-OST0000-osc-ffff8e0b47c73800.rpc_stats=
snapshot_time:         1589919461.185215594 (secs.nsecs)
read RPCs in flight:  0
write RPCs in flight: 0
pending write pages:  0
pending read pages:   0

                        read                    write

pages per rpc         rpcs   % cum % |       rpcs   % cum %
1:                14222350  77  77   |          0   0   0
2:                       0   0  77   |          0   0   0
4:                       0   0  77   |          0   0   0
8:                       0   0  77   |          0   0   0
16:                      0   0  77   |          0   0   0
32:                      0   0  77   |          0   0   0
64:                      0   0  77   |          0   0   0
128:                     0   0  77   |          0   0   0
256:               4130365  22 100   |          0   0   0

                        read                    write

rpcs in flight        rpcs   % cum % |       rpcs   % cum %
0:                       0   0   0   |          0   0   0
1:                 3236263  17  17   |          0   0   0
2:                  117001   0  18   |          0   0   0
3:                  168119   0  19   |          0   0   0
4:                  153295   0  20   |          0   0   0
5:                   91598   0  20   |          0   0   0
6:                   42476   0  20   |          0   0   0
7:                   17578   0  20   |          0   0   0
8:                    9454   0  20   |          0   0   0
9:                    7611   0  20   |          0   0   0
10:                   7772   0  20   |          0   0   0
11:                   8914   0  21   |          0   0   0
12:                   9350   0  21   |          0   0   0
13:                   8559   0  21   |          0   0   0
14:                   8734   0  21   |          0   0   0
15:                  10784   0  21   |          0   0   0
16:                  11386   0  21   |          0   0   0
17:                  13148   0  21   |          0   0   0
18:                  15473   0  21   |          0   0   0
19:                  17619   0  21   |          0   0   0
20:                  18851   0  21   |          0   0   0
21:                  21853   0  21   |          0   0   0
22:                  21236   0  21   |          0   0   0
23:                  21588   0  22   |          0   0   0
24:                  23859   0  22   |          0   0   0
25:                  24049   0  22   |          0   0   0
26:                  26232   0  22   |          0   0   0
27:                  29853   0  22   |          0   0   0
28:                  31992   0  22   |          0   0   0
29:                  43626   0  22   |          0   0   0
30:                 116116   0  23   |          0   0   0
31:               14018326  76 100   |          0   0   0</p>
        </pre>
  </li>
 </ol>
 <p>
  To get statistics that are related to client metadata operations, run the following command:
 </p>
 Note:
 <p>
  MetaDataClient (MDC) is the client side counterpart of MetaData Server (MDS).
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ lctl get_param mdc.*.md_stats</p>
        </pre>
 <p>
  To get the stripe layout of the file on the EXAScaler filesystem, run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ lfs getstripe /mnt/ai200</p>
        </pre>
 <h3 id="num-config-disks">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#num-config-disks">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#num-config-disks" id="num-config-disks" name="num-config-disks" shape="rect">
    9.3. Getting the Number of Configured Object-Based Disks
   </a>
  </a>
 </h3>
 <p>
  To get the number of configured object-bsaed disks:
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ lctl get_param lov.*.target_obd</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>0: ai200-OST0000_UUID ACTIVE
1: ai200-OST0001_UUID ACTIVE</p>
        </pre>
  </li>
 </ol>
 <h3 id="addl-stats-exascaler-fs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#addl-stats-exascaler-fs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#addl-stats-exascaler-fs" id="addl-stats-exascaler-fs" name="addl-stats-exascaler-fs" shape="rect">
    9.4. Getting Additional Statistics related to the EXAScaler Filesystem
   </a>
  </a>
 </h3>
 <p>
  You can get additional statistics that are related to the EXAScaler Filesystem.
 </p>
 <p>
  Refer to the
  <a class="xref" data-cms-ai="0" href="http://wiki.lustre.org/Lustre_Monitoring_and_Statistics_Guide" shape="rect">
   Lustre Monitoring and Statistics Guide
  </a>
  for more information.
 </p>
 <h3 id="get-md-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#get-md-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#get-md-stats" id="get-md-stats" name="get-md-stats" shape="rect">
    9.5. Getting Metadata Statistics
   </a>
  </a>
 </h3>
 <p>
  To get metadata statistics:
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ lctl get_param lmv.*.md_stats</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>snapshot_time             1571271931.653827773 secs.nsecs
close                     8 samples [reqs]
create                    1 samples [reqs]
getattr                   1 samples [reqs]
intent_lock               81 samples[reqs]
read_page                 3 samples [reqs]
revalidate_lock           1 samples [reqs]</p>
        </pre>
  </li>
 </ol>
 <h3 id="check-exist-mount-lustre">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-exist-mount-lustre">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-exist-mount-lustre" id="check-exist-mount-lustre" name="check-exist-mount-lustre" shape="rect">
    9.6. Checking for an Existing Mount
   </a>
  </a>
 </h3>
 <p>
  To check for an existing mount in the EXAScaler Filesystem:
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ mount | grep lustre</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>192.168.1.61@o2ib,192.168.1.62@o2ib1:/ai200 on /mnt/ai200 type lustre
(rw,flock,lazystatfs)</p>
        </pre>
  </li>
 </ol>
 <h3 id="unmount-fs-cluster">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-fs-cluster">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-fs-cluster" id="unmount-fs-cluster" name="unmount-fs-cluster" shape="rect">
    9.7. Unmounting an EXAScaler Filesystem Cluster
   </a>
  </a>
 </h3>
 <p>
  To unmount an EXAScaler filesystem cluster:
 </p>
 <p>
  Run the following command.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo umount /mnt/ai200</p>
        </pre>
 <h3 id="sum-exacaler-fs-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#sum-exacaler-fs-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#sum-exacaler-fs-stats" id="sum-exacaler-fs-stats" name="sum-exacaler-fs-stats" shape="rect">
    9.8. Getting a Summary of EXAScaler Filesystem Statistics
   </a>
  </a>
 </h3>
 <p>
  You can get a summary of statistics for the EXAScaler filesystem.
 </p>
 <p>
  Refer to the
  <a class="xref" data-cms-ai="0" href="http://wiki.lustre.org/Lustre_Monitoring_and_Statistics_Guide" shape="rect">
   Lustre Monitoring and Statistics Guide
  </a>
  for more information.
 </p>
 <h3 id="gds-poll-mode">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-poll-mode">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-poll-mode" id="gds-poll-mode" name="gds-poll-mode" shape="rect">
    9.9. Using GPUDirect Storage in Poll Mode
   </a>
  </a>
 </h3>
 <p>
  This section describes how to use GDS in Poll Mode with EXAScaler filesystem files that have a Stripe Count greater than 1.
 </p>
 <p>
  Currently, if poll mode is enabled,
  <span>
   cuFileReads
  </span>
  or
  <span>
   cuFileWrites
  </span>
  might return bytes that are less than the bytes that were requested. This behavior is POSIX compliant and is observed with files that have a stripe count that is greater than the count in their layout. If behavior occurs, we recommend that the application checks for returned bytes and continues until all of the data is consumed. You can also set the corresponding
  <span>
   properties.poll_mode_max_size_kb,(say 1024(KB))
  </span>
  value to the lowest possible stripe size in the directory. This ensures that IO sizes that exceed this limit are not polled.
 </p>
 <ol>
  <li>
   To check EXAScaler filesystem file layout, run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ lfs getstripe &lt;file-path&gt;</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>lfs getstripe /mnt/ai200/single_stripe/md1.0.0
/mnt/ai200/single_stripe/md1.0.0
lmm_stripe_count:  1
lmm_stripe_size:   1048576
lmm_pattern:       raid0
lmm_layout_gen:    0
lmm_stripe_offset: 0
        obdidx           objid           objid           group
             0            6146         0x1802                0</p>
        </pre>
  </li>
 </ol>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trouble-faq-wekafs">
   10. Troubleshooting and FAQ for the WekaIO Filesystem
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="trouble-faq-wekafs" name="trouble-faq-wekafs" shape="rect">
  </a>
  This section provides troubleshooting and FAQ information about the WekaIO file system.
 </p>
 <h3 id="download-wekaio-cp">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#download-wekaio-cp">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#download-wekaio-cp" id="download-wekaio-cp" name="download-wekaio-cp" shape="rect">
    10.1. Downloading the WekaIO Client Package
   </a>
  </a>
 </h3>
 <p>
  To download the WekaIO client package:
 </p>
 <p>
  Run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ curl http://&lt;IP of one of the WekaIO hosts' IB interface&gt;:14000/dist/v1/install | sh</p>
        </pre>
 <p>
  For example,
  <span>
   $ curl http://172.16.8.1:14000/dist/v1/install | sh
  </span>
  .
 </p>
 <h3 id="wekaio-version-gds">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#wekaio-version-gds">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#wekaio-version-gds" id="wekaio-version-gds" name="wekaio-version-gds" shape="rect">
    10.2. Determining Whether the WekaIO Version is Ready for GDS
   </a>
  </a>
 </h3>
 <p>
  To determine whether the WekaIO version is ready for GDS:
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ weka version</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>* 3.6.2.5-rdma-beta</p>
        </pre>
   Note:
   <p>
    Currently, the only WekaIO FS version that supports GDS is
    <span>
     * 3.6.2.5-rdma-beta
    </span>
   </p>
  </li>
 </ol>
 <h3 id="mount-wekaio-fs-cluster">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-wekaio-fs-cluster">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-wekaio-fs-cluster" id="mount-wekaio-fs-cluster" name="mount-wekaio-fs-cluster" shape="rect">
    10.3. Mounting a WekaIO File System Cluster
   </a>
  </a>
 </h3>
 <p>
  The WekaIO filesystem can take a parameter to reserve a fixed number of cores for the user space process.
 </p>
 <ol>
  <li>
   To mount a
   <span>
    server_ip 172.16.8.1
   </span>
   with two dedicated cores, run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ mkdir -p /mnt/weka
$ sudo mount -t wekafs -o num_cores=2 -o net=ib0,net=ib1,net=ib2,net=ib3,net=ib4,net=ib5,net=ib6,net=ib7 
172.16.8.1/fs01 /mnt/weka</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>Mounting 172.16.8.1/fs01 on /mnt/weka
Creating weka container
Starting container
Waiting for container to join cluster
Container "client" is ready (pid = 47740)
Calling the mount command
Mount completed successfully</p>
        </pre>
  </li>
 </ol>
 <h3 id="resolve-fail-mount">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#resolve-fail-mount">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#resolve-fail-mount" id="resolve-fail-mount" name="resolve-fail-mount" shape="rect">
    10.4. Resolving a Failing Mount
   </a>
  </a>
 </h3>
 <ol>
  <li>
   Before you use the IB interfaces in the mount options, verify that the interfaces are set up for
   <span>
    net=&lt;interface&gt;
   </span>
   :
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ sudo mount -t wekafs -o num_cores=2 -o 
net=ib0,net=ib1,net=ib2,net=ib3,net=ib4,net=ib5,net=ib6,net=ib7 
172.16.8.1/fs01 /mnt/weka</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>Mounting 172.16.8.1/fs01 on /mnt/weka
Creating weka container
Starting container
Waiting for container to join cluster
error: Container "client" has run into an error: Resources 
assignment failed: IB/MLNX network devices should have 
pre-configured IPs and ib4 has none</p>
        </pre>
  </li>
  <li>
   Remove interfaces that do not have network connectivity from the mount options.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ ibdev2netdev

mlx5_0 port 1 ==&gt; ib0 (Up)
mlx5_1 port 1 ==&gt; ib1 (Up)
mlx5_2 port 1 ==&gt; ib2 (Up)
mlx5_3 port 1 ==&gt; ib3 (Up)
mlx5_4 port 1 ==&gt; ib4 (Down)
mlx5_5 port 1 ==&gt; ib5 (Down)
mlx5_6 port 1 ==&gt; ib6 (Up)
mlx5_7 port 1 ==&gt; ib7 (Up)
mlx5_8 port 1 ==&gt; ib8 (Up)
mlx5_9 port 1 ==&gt; ib9 (Up)</p>
        </pre>
  </li>
 </ol>
 <h3 id="resolve-wekaio-usage">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#resolve-wekaio-usage">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#resolve-wekaio-usage" id="resolve-wekaio-usage" name="resolve-wekaio-usage" shape="rect">
    10.5. Resolving 100% Usage for WekaIO for Two Cores
   </a>
  </a>
 </h3>
 <p>
  If you have two cores, and you are experiencing 100% CPU usage:
 </p>
 <ol>
  <li>
   Run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ top</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
54816 root      20   0 11.639g 1.452g 392440 R  94.4  0.1 781:06.06 wekanode
54825 root      20   0 11.639g 1.452g 392440 R  94.4  0.1 782:00.32 wekanode</p>
        </pre>
   <p>
    When the
    <span>
     num_cores=2
    </span>
    parameter is specified, two cores are used for the user mode poll driver for WekaIO FE networking. This process improves the latency and performance. Refer to the
    <a class="xref" data-cms-ai="0" href="https://docs.weka.io/" shape="rect">
     WekaIO documentation
    </a>
    for more information.
   </p>
  </li>
 </ol>
 <h3 id="check-existing-mount-wekafs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-existing-mount-wekafs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-existing-mount-wekafs" id="check-existing-mount-wekafs" name="check-existing-mount-wekafs" shape="rect">
    10.6. Checking for an Existing Mount in the Weka File System
   </a>
  </a>
 </h3>
 <p>
  To check for an existing mount in the WekaIO file system:
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ mount | grep wekafs</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>172.16.8.1/fs01 on /mnt/weka type wekafs (
rw,relatime,writecache,inode_bits=auto,dentry_max_age_positive=1000,
dentry_max_age_negative=0)</p>
        </pre>
  </li>
 </ol>
 <h3 id="check-summ-wekaio">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-summ-wekaio">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-summ-wekaio" id="check-summ-wekaio" name="check-summ-wekaio" shape="rect">
    10.7. Checking for a Summary of the WekaIO Filesystem Status
   </a>
  </a>
 </h3>
 <p>
  To check for a summary of the WekaIO file system status.
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ weka status</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>WekaIO v3.6.2.5-rdma-beta (CLI build 3.6.2.5-rdma-beta)
       cluster: Nvidia (e4a4e227-41d0-47e5-aa70-b50688b31f40)
        status: OK (12 backends UP, 72 drives UP)
    protection: 8+2
     hot spare: 2 failure domains (62.84 TiB)
 drive storage: 62.84 TiB total, 819.19 MiB unprovisioned
         cloud: connected
       license: Unlicensed

     io status: STARTED 1 day ago (1584 buckets UP, 228 io-nodes UP)
    link layer: InfiniBand
       clients: 1 connected
         reads: 61.54 GiB/s (63019 IO/s)
        writes: 0 B/s (0 IO/s)
    operations: 63019 ops/s
        alerts: 3 active alerts, use `Wekaalerts` to list them</p>
        </pre>
  </li>
 </ol>
 <h3 id="summ-wekaio-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#summ-wekaio-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#summ-wekaio-stats" id="summ-wekaio-stats" name="summ-wekaio-stats" shape="rect">
    10.8. Displaying the Summary of the WekaIO Filesystem Statistics
   </a>
  </a>
 </h3>
 <p>
  To display a summary of the status of the WekaIO filesystem:
 </p>
 <ol>
  <li>
   Run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/wekafs/stat</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>IO type:      UM Average      UM Longest      KM Average      KM Longest                IO count
--------------------------------------------------------------------------------------------------------------------------------
          total:          812 us       563448 us         9398 ns     10125660 ns               718319292 (63260 IOPS, 0 MB/sec)
         lookup:          117 us         3105 us         6485 ns       436709 ns                4079 (12041)
        readdir:            0 us            0 us            0 ns            0 ns                       0
          mknod:          231 us          453 us         3970 ns         6337 ns                      96
           open:            0 us            0 us            0 ns            0 ns                   0 (3232)
        release:            0 us            0 us            0 ns            0 ns                   0 (2720)
           read:            0 us            0 us            0 ns            0 ns                       0
          write:        18957 us       563448 us       495291 ns       920127 ns              983137 (983041)
        getattr:           10 us           10 us         6771 ns         6771 ns                   1 (9271)
        setattr:          245 us          424 us         4991 ns        48222 ns                      96
          rmdir:            0 us            0 us            0 ns            0 ns                       0
         unlink:            0 us            0 us            0 ns            0 ns                       0
         rename:            0 us            0 us            0 ns            0 ns                       0
        symlink:            0 us            0 us            0 ns            0 ns                       0
       readlink:            0 us            0 us            0 ns            0 ns                       0
       hardlink:            0 us            0 us            0 ns            0 ns                       0
         statfs:         4664 us         5072 us        38947 ns        59618 ns                       7
     SG_release:            0 us            0 us            0 ns            0 ns                       0
    SG_allocate:         1042 us         7118 us         2161 ns       110282 ns                  983072
         falloc:          349 us          472 us         4184 ns        10239 ns                      96
    atomic_open:            0 us            0 us            0 ns            0 ns                       0
          flock:            0 us            0 us            0 ns            0 ns                       0
       backcomm:            0 us            0 us            0 ns            0 ns                       0
        getroot:        19701 us        19701 us        57853 ns        57853 ns                       1
          trace:            0 us            0 us            0 ns            0 ns                       0
    jumbo alloc:            0 us            0 us            0 ns            0 ns                       0
  jumbo release:            0 us            0 us            0 ns            0 ns                       0
    jumbo write:            0 us            0 us            0 ns            0 ns                       0
     jumbo read:            0 us            0 us            0 ns            0 ns                       0
      keepalive:           46 us      1639968 us         1462 ns        38996 ns                  184255
          ioctl:          787 us        50631 us         8732 ns     10125660 ns               717328710
       setxattr:            0 us            0 us            0 ns            0 ns                       0
       getxattr:            0 us            0 us            0 ns            0 ns                       0
      listxattr:            0 us            0 us            0 ns            0 ns                       0
    removexattr:            0 us            0 us            0 ns            0 ns                       0
  setfileaccess:          130 us         3437 us         6440 ns        71036 ns                    3072
        unmount:            0 us            0 us            0 ns            0 ns                       0</p>
        </pre>
  </li>
 </ol>
 <h3 id="wekaio-writes-posix">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#wekaio-writes-posix">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#wekaio-writes-posix" id="wekaio-writes-posix" name="wekaio-writes-posix" shape="rect">
    10.9. Why WekaIO Writes Go Through POSIX
   </a>
  </a>
 </h3>
 <p>
  For the WekaIO filesystem, GDS supports RDMA based reads and writes. You can use the
  <span>
   fs:weka:rdma_write_support
  </span>
  JSON property to enable writes on supported Weka filesystems. This option is disabled by default. If this option is set to false, writes will be internally staged through system memory, and the cuFile library will use pwrite POSIX calls internally for writes.
 </p>
 <h3 id="check-nvdiafsko-support">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-nvdiafsko-support">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-nvdiafsko-support" id="check-nvdiafsko-support" name="check-nvdiafsko-support" shape="rect">
    10.10. Checking for nvidia-fs.ko Support for Memory Peer Direct
   </a>
  </a>
 </h3>
 <p>
  To check for
  <span>
   nvidia-fs.ko
  </span>
  support for memory peer direct:
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ lsmod | grep nvidia_fs | grep ib_core &amp;&amp; echo “Ready for Memory Peer Direct”</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>ib_core               319488  16 
rdma_cm,ib_ipoib,mlx4_ib,ib_srp,iw_cm,nvidia_fs,ib_iser,ib_umad,
rdma_ucm,ib_uverbs,mlx5_ib,ib_cm,ib_ucm
“Ready for Memory Peer Direct”</p>
        </pre>
  </li>
 </ol>
 <h3 id="check-mem-peer-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-mem-peer-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-mem-peer-stats" id="check-mem-peer-stats" name="check-mem-peer-stats" shape="rect">
    10.11. Checking Memory Peer Direct Stats
   </a>
  </a>
 </h3>
 <p>
  To to check memory peer statistics:
 </p>
 <ol>
  <li>
   Run the following script, which shows the counter for memroy peer direct statistics:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>list=`ls /sys/kernel/mm/memory_peers/nvidia-fs/`. for stat in $list . 
do echo  "$stat value: " $(cat /sys/kernel/mm/memory_peers/nvidia-fs/$stat). done</p>
        </pre>
  </li>
  <li>
   Review the output.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>num_alloc_mrs value:  1288
num_dealloc_mrs value:  1288
num_dereg_bytes value:  1350565888
num_dereg_pages value:  329728
num_free_callbacks value:  0
num_reg_bytes value:  1350565888
num_reg_pages value:  329728
version value:  1.0</p>
        </pre>
  </li>
 </ol>
 <h3 id="rel-nvidia-fs-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#rel-nvidia-fs-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#rel-nvidia-fs-stats" id="rel-nvidia-fs-stats" name="rel-nvidia-fs-stats" shape="rect">
    10.12. Checking for Relevant nvidia-fs Statistics for the WekaIO Filesystem
   </a>
  </a>
 </h3>
 <p>
  To check for relevant
  <span>
   nvida-fs
  </span>
  statistics for the WekaIO file system:
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/driver/nvidia-fs/stats | egrep -v 'Reads|Writes|Ops|Error'</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>GDS Version: 1.0.0.80
NVFS statistics(ver: 4.0)
NVFS Driver(version: 2.7.49)
 
Active Shadow-Buffer (MB): 256
Active Process: 1
Mmap            : n=2088 ok=2088 err=0 munmap=1832
Bar1-map        : n=2088 ok=2088 err=0 free=1826 callbacks=6 active=256
GPU 0000:34:00.0  uuid:12a86a5e-3002-108f-ee49-4b51266cdc07 : Registered_MB=32 Cache_MB=0 max_pinned_MB=1977
GPU 0000:e5:00.0  uuid:4c2c6b1c-27ac-8bed-8e88-9e59a5e348b5 : Registered_MB=32 Cache_MB=0 max_pinned_MB=32
GPU 0000:b7:00.0  uuid:b224ba5e-96d2-f793-3dfd-9caf6d4c31d8 : Registered_MB=32 Cache_MB=0 max_pinned_MB=32
GPU 0000:39:00.0  uuid:e8fac7f5-d85d-7353-8d76-330628508052 : Registered_MB=32 Cache_MB=0 max_pinned_MB=32
GPU 0000:5c:00.0  uuid:2b13ed25-f0ab-aedb-1f5c-326745b85176 : Registered_MB=32 Cache_MB=0 max_pinned_MB=32
GPU 0000:e0:00.0  uuid:df46743a-9b22-30ce-6ea0-62562efaf0a2 : Registered_MB=32 Cache_MB=0 max_pinned_MB=32
GPU 0000:bc:00.0  uuid:c4136168-2a1d-1f3f-534c-7dd725fedbff : Registered_MB=32 Cache_MB=0 max_pinned_MB=32
GPU 0000:57:00.0  uuid:54e472f2-e4ee-18dc-f2a1-3595fa8f3d33 : Registered_MB=32 Cache_MB=0 max_pinned_MB=32</p>
        </pre>
   Note:
   <p>
    Reads, Writes, Ops, and Error counters are not available through this interface for the WekaIO filesystem, so the value will be zero. See
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#summ-wekaio-stats" shape="rect" title="To display a summary of the status of the WekaIO filesystem:">
     Displaying the Summary of the WekaIO Filesystem Statistics
    </a>
    about using the Weka status for reads and writes.
   </p>
  </li>
 </ol>
 <h3 id="basic-wekaio-fs-test">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-wekaio-fs-test">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-wekaio-fs-test" id="basic-wekaio-fs-test" name="basic-wekaio-fs-test" shape="rect">
    10.13. Conducting a Basic WekaIO Filesystem Test
   </a>
  </a>
 </h3>
 <p>
  To conduct a basic WekaIO file system test:
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-x.y/tools/gdsio_verify  -f /mnt/weka/gdstest/tests/reg1G 
-n 1 -m 0 -s 1024 -o 0  -d 0 -t 0 -S -g 4K</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>gpu index :0,file :/mnt/weka/gdstest/tests/reg1G, RING buffer size :0, 
gpu buffer alignment :4096, gpu buffer offset :0, file offset :0, 
io_requested :1024, bufregister :false, sync :0, nr ios :1,fsync :0,
address = 0x564ffc5e76c0
Data Verification Success</p>
        </pre>
  </li>
 </ol>
 <h3 id="unmount-wekaio-fs-cluster">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-wekaio-fs-cluster">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-wekaio-fs-cluster" id="unmount-wekaio-fs-cluster" name="unmount-wekaio-fs-cluster" shape="rect">
    10.14. Unmounting a WekaIO File System Cluster
   </a>
  </a>
 </h3>
 <p>
  To unmount a WekaIO file system cluster:
 </p>
 <ol>
  <li>
   Run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ sudo umount /mnt/weka</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>Unmounting /mnt/weka
Calling the umount command
umount successful, stopping and deleting client container
Umount completed successfully</p>
        </pre>
  </li>
 </ol>
 <h3 id="verify-installed-libs-wekaio-fs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-installed-libs-wekaio-fs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-installed-libs-wekaio-fs" id="verify-installed-libs-wekaio-fs" name="verify-installed-libs-wekaio-fs" shape="rect">
    10.15. Verify the Installed Libraries for the WekaIO Filesystem
   </a>
  </a>
 </h3>
 <p>
  The following table summarizes the tasks and command oputput for verifying the installled libraries for the WekaIO filesystems.
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" id="verify-installed-libs-wekaio-fs__table_wxj_22z_gnb" rules="all" summary="">
  Table 3. Verifying the Installed Libraries for WekaIO Filesystems
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e4491" rowspan="1" valign="top">
    Task
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e4494" rowspan="1" valign="top">
    Output
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e4491" rowspan="1" valign="top">
    Check the WekaIO version.
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e4494" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>$ weka status
WekaIO v3.6.2.5-rdma-beta (CLI build 3.6.2.5-rdma-beta)</p>
        </pre>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e4491" rowspan="1" valign="top">
    Check whether GDS support for WekaFS is present.
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e4494" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>$ gdscheck -p
[...]
     WekaFS: Supported
     Userspace RDMA: Supported
[...]</p>
        </pre>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e4491" rowspan="1" valign="top">
    Check for MLNX_OFED information.
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e4494" rowspan="1" valign="top">
    <p>
     Check with
     <span>
      ofed_info -s
     </span>
    </p>
    <p>
     Currently supported with:
    </p>
    <p>
     MLNX_OFED_LINUX-5.1-0.6.6.0
    </p>
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>$ ofed_info -s MLNX_OFED_LINUX-5.1-0.6.6.0:</p>
        </pre>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e4491" rowspan="1" valign="top">
    Check for the
    <span>
     nvidia-fs.ko
    </span>
    driver.
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e4494" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>$ lsmod | grep nvidia_fs | grep ib_core &amp;&amp; echo “Ready for Memory Peer Direct”</p>
        </pre>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e4491" rowspan="1" valign="top">
    Check for
    <span>
     libibverbs.so
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e4494" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>$ dpkg -s libibverbs-dev
Package: libibverbs-dev
Status: install ok installed
Priority: optional
Section: libdevel
Installed-Size: 1151
Maintainer: Linux RDMA Mailing List &lt;linux-rdma@vger.kernel.org&gt;
Architecture: amd64
Multi-Arch: same
Source: rdma-core
Version: 47mlnx1-1.47329</p>
        </pre>
   </td>
  </tr>
 </table>
 <h3 id="gds-config-file-changes">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-config-file-changes">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-config-file-changes" id="gds-config-file-changes" name="gds-config-file-changes" shape="rect">
    10.16. GDS Configuration File Changes to Support the WekaIO Filesystem
   </a>
  </a>
 </h3>
 <p>
  By default, the configuration for Weka RDMA-based writes is disabled.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>"fs": {
    "weka": {
        // enable/disable WekaFs rdma write
        "rdma_write_support" : false
    }
}</p>
        </pre>
 <p>
  To support the WekaIO Filesystem, change the configuration to add a new property,
  <span>
   rdma_dev_addr_list
  </span>
  :
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>"properties": {
    // allow compat mode,
    // this will enable use of cufile posix read/writes
    //"allow_compat_mode": true,

    "rdma_dev_addr_list": [
        "172.16.8.88" , "172.16.8.89", 
        "172.16.8.90" , "172.16.8.91",
        "172.16.8.92" , "172.16.8.93",
        "172.16.8.94", "172.16.8.95"
    ]
}</p>
        </pre>
 <h3 id="relevant-userspace-stats-wekaio-fs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#relevant-userspace-stats-wekaio-fs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#relevant-userspace-stats-wekaio-fs" id="relevant-userspace-stats-wekaio-fs" name="relevant-userspace-stats-wekaio-fs" shape="rect">
    10.17. Check for Relevant User-Space Statistics for the WekaIO Filesystem
   </a>
  </a>
 </h3>
 <p>
  To check for relevant user-space statistics for the WekaIO filesystem, issue the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ ./gds_stats -p &lt;pid&gt; -l 3 | grep GPU</p>
        </pre>
 <p>
  Refer to
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#user-space-rdma-counters" shape="rect" title="The library provides counters to monitor the RDMA traffic at a per-GPU level and requires that cuFile starts verbosity with a value of 3.">
   GDS User-Space RDMA Counters
  </a>
  for more information about statistics.
 </p>
 <h3 id="check-wekafs-support">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-wekafs-support">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-wekafs-support" id="check-wekafs-support" name="check-wekafs-support" shape="rect">
    10.18. Check for WekaFS Support
   </a>
  </a>
 </h3>
 <p>
  If WekaFS support does not exist, the following issues are possible:
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 4. Weka Filesystem Support Issues
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e4666" rowspan="1" valign="top">
    Issue
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e4669" rowspan="1" valign="top">
    Action
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e4666" rowspan="1" valign="top">
    MLNX_OFED peer direct is not enabled.
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e4669" rowspan="1" valign="top">
    <p>
     Check whether MLNX_OFED is installed (
     <span>
      ofed_info -s
     </span>
     ).
    </p>
    <p>
     This issue can occur if the nvidia-fs Debian package was installed before MLNX_OFED was installed. When this issue occurs, uninstall and reinstall the nvidia-fs package.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e4666" rowspan="1" valign="top">
    RDMA devices are not populated in the
    <span>
     /etc/cufile.json
    </span>
    file.
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e4669" rowspan="1" valign="top">
    Add IP addresses to
    <span>
     properties.rdma_dev_addr_list
    </span>
    . Currently only IPv4 addresses are supported.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e4666" rowspan="1" valign="top">
    None of the configured RDMA devices are UP.
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e4669" rowspan="1" valign="top">
    Check IB connectivity for the interfaces.
   </td>
  </tr>
 </table>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#spectrum-scale-intro">
   11. Enabling IBM Spectrum Scale Support with GDS
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="spectrum-scale-intro" name="spectrum-scale-intro" shape="rect">
  </a>
  GDS is supported starting with IBM Spectrum Scale 5.1.2.
 </p>
 <p>
  After reviewing the NVIDIA GDS documentation, refer to IBM Spectrum Scale 5.1.2. Please see especially the GDS sections in the Planning and Installation guides.
 </p>
 <p>
  •Planning:
  <a class="xref" data-cms-ai="0" href="https://www.ibm.com/docs/en/spectrum-scale/5.1.2?topic=considerations-planning-gpudirect-storage" rel="noopener" shape="rect" target="_blank">
   https://www.ibm.com/docs/en/spectrum-scale/5.1.2?topic=considerations-planning-gpudirect-storage
  </a>
 </p>
 <p>
  •Installing:
  <a class="xref" data-cms-ai="0" href="https://www.ibm.com/docs/en/spectrum-scale/5.1.2?topic=installing-gpudirect-storage-spectrum-scale" rel="noopener" shape="rect" target="_blank">
   https://www.ibm.com/docs/en/spectrum-scale/5.1.2?topic=installing-gpudirect-storage-spectrum-scale
  </a>
 </p>
 <p>
  For troubleshooting see
  <a class="xref" data-cms-ai="0" href="https://www.ibm.com/docs/en/spectrum-scale/5.1.2?topic=troubleshooting-gpudirect-storage-issues" rel="noopener" shape="rect" target="_blank">
   https://www.ibm.com/docs/en/spectrum-scale/5.1.2?topic=troubleshooting-gpudirect-storage-issues
  </a>
  .
  <a class="Link" data-cms-ai="0" id="spectrum-scale-limitations-with-gds" name="spectrum-scale-limitations-with-gds" shape="rect">
  </a>
 </p>
 <h3 id="spectrum-scale-limitations-with-gds">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#spectrum-scale-limitations-with-gds">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#spectrum-scale-limitations-with-gds" id="spectrum-scale-limitations-with-gds" name="spectrum-scale-limitations-with-gds" shape="rect">
    11.1. IBM Spectrum Scale Limitations with GDS
   </a>
  </a>
 </h3>
 <p>
  Refer to the following documentation for IBM Spectrum Scale Limitations with GDS:
 </p>
 <p>
  <a class="xref" data-cms-ai="0" href="http://www.ibm.com/support/pages/node/6444075" rel="noopener" shape="rect" target="_blank">
   https://www.ibm.com/docs/en/spectrum-scale/5.1.2?topic=architecture-gpudirect-storage-support-spectrum-scale
  </a>
 </p>
 <h3 id="checking-for-peerdirect-support">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-for-peerdirect-support">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-for-peerdirect-support" id="checking-for-peerdirect-support" name="checking-for-peerdirect-support" shape="rect">
    11.2. Checking nvidia-fs.ko Support for Mellanox PeerDirect
   </a>
  </a>
 </h3>
 <p>
  Use the following command to check support for memory peer direct.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/driver/nvidia-fs/stats | grep -i "Mellanox PeerDirect Supported"</p>
        </pre>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>Mellanox PeerDirect Supported: True</p>
        </pre>
 <p>
  In the above example,
  False
  means that MLNX_OFED was not installed with GPUDirect Storage support prior to installing nvidia-fs.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="checking-for-peerdirect-support__docs-internal-guid-be866dbc-7fff-85a5-44ae-214f6ad79623" name="checking-for-peerdirect-support__docs-internal-guid-be866dbc-7fff-85a5-44ae-214f6ad79623" shape="rect">
  </a>
  The other option to check for Mellanox PeerDirect Support is via “gdscheck -p” output. If it’s enabled, you should be able to see something as below.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>--Mellanox PeerDirect : Enabled</p>
        </pre>
 <h3 id="verifying-libraries-for-spectrum-scale">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verifying-libraries-for-spectrum-scale">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verifying-libraries-for-spectrum-scale" id="verifying-libraries-for-spectrum-scale" name="verifying-libraries-for-spectrum-scale" shape="rect">
    11.3. Verifying Installed Libraries for IBM Spectrum Scale
   </a>
  </a>
 </h3>
 <p>
  The following tasks, shown with sample output, can be peformed to show installed libraries for IBM Spectrum Scale:
 </p>
 <ul>
  <li>
   Check whether GDS support for IBM Spectrum Scale is present:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>[~]# /usr/local/cuda/gds/tools/gdscheck -p | egrep -e "Spectrum Scale|PeerDirect|rdma_device_status"
 IBM Spectrum Scale : Supported
 --Mellanox PeerDirect : Enabled
 --rdma_device_status  : Up: 2 Down: 0</p>
        </pre>
  </li>
  <li>
   Check for MLNX_OFED information:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ ofed_info -s
MLNX_OFED_LINUX-5.4-1.0.3.0:</p>
        </pre>
  </li>
  <li>
   Check for
   <span>
    nvidia-fs.ko
   </span>
   driver:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>[~]# cat /proc/driver/nvidia-fs/stats 	GDS Version: 1.0.0.82
	NVFS statistics(ver: 4.0)
	NVFS Driver(version: 2.7.49)
	Mellanox PeerDirect Supported: True
	IO stats: Disabled, peer IO stats: Disabled
	Logging level: info
	
	Active Shadow-Buffer (MiB): 0
	Active Process: 0
	Reads                           : err=0 io_state_err=0
	Sparse Reads                    : n=230 io=0 holes=0 pages=0
	Writes                          : err=0 io_state_err=237 pg-cache=0 pg-cache-fail=0 pg-cache-eio=0
	Mmap                            : n=27 ok=27 err=0 munmap=27
	Bar1-map                        : n=27 ok=27 err=0 free=27 callbacks=0 active=0
	Error                           : cpu-gpu-pages=0 sg-ext=0 dma-map=0 dma-ref=0
	Ops                             : Read=0 Write=0
	GPU 0000:2f:00.0  uuid:621f7d17-5e7d-8f79-be27-d2f4256ddd88 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=2</p>
        </pre>
  </li>
  <li>
   To check for
   <span>
    libibverbs.so
   </span>
   on Ubuntu:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ dpkg -s libibverbs-dev
root@fscc-sr650-59:~# dpkg -s libibverbs-dev
Package: libibverbs-dev
Status: install ok installed
Priority: optional
Section: libdevel
Installed-Size: 1428
Maintainer: Linux RDMA Mailing List &lt;linux-rdma@vger.kernel.org&gt;
Architecture: amd64
Multi-Arch: same
Source: rdma-core
Version: 54mlnx1-1.54103</p>
        </pre>
   <p>
    To check for
    <span>
     libibverbs.so
    </span>
    on RHEL:
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>[]# rpm -qi libibverbs
	Name        : libibverbs
	Version     : 54mlnx1
	Release     : 1.54103
	Architecture: x86_64
	Install Date: Tue 13 Jul 2021 10:21:18 AM CEST
	Group       : System Environment/Libraries
	Size        : 535489
	License     : GPLv2 or BSD
	Signature   : DSA/SHA1, Fri 02 Jul 2021 08:14:44 PM CEST, Key ID c5ed83e26224c050
	Source RPM  : rdma-core-54mlnx1-1.54103.src.rpm
	Build Date  : Fri 02 Jul 2021 06:59:01 PM CEST
	Build Host  : c-141-24-1-005.mtl.labs.mlnx
	Relocations : (not relocatable)
	URL         : https://github.com/linux-rdma/rdma-core
	Summary     : A library and drivers for direct userspace use of RDMA (InfiniBand/iWARP/RoCE) hardware
	Description :
	libibverbs is a library that allows userspace processes to use RDMA
	"verbs" as described in the InfiniBand Architecture Specification and
	the RDMA Protocol Verbs Specification.  This includes direct hardware
	access from userspace to InfiniBand/iWARP adapters (kernel bypass) for
	fast path operations.
	
	Device-specific plug-in ibverbs userspace drivers are included:
	
- libmlx5: Mellanox ConnectX-4+ InfiniBand HCA</p>
        </pre>
  </li>
 </ul>
 <h3 id="checking-peerdirect-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-peerdirect-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-peerdirect-stats" id="checking-peerdirect-stats" name="checking-peerdirect-stats" shape="rect">
    11.4. Checking PeerDirect Stats
   </a>
  </a>
 </h3>
 <p>
  To check memory peer statistics, run the following script:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>list=`ls /sys/kernel/mm/memory_peers/nvidia-fs/`; for stat in $list;do echo  "$stat value: " $(cat /sys/kernel/mm/memory_peers/nvidia-fs/$stat); done</p>
        </pre>
 <p>
  Sample output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>num_alloc_mrs value:  1288
num_dealloc_mrs value:  1288
num_dereg_bytes value:  1350565888
num_dereg_pages value:  329728
num_free_callbacks value:  0
num_reg_bytes value:  1350565888
num_reg_pages value:  32972
version value:  1.0</p>
        </pre>
 <h3 id="checking-for-relevant-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-for-relevant-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-for-relevant-stats" id="checking-for-relevant-stats" name="checking-for-relevant-stats" shape="rect">
    11.5. Checking for Relevant nvidia-fs Stats with IBM Spectrum Scale
   </a>
  </a>
 </h3>
 <p>
  Use the following steps to check for relevant
  <span>
   nvidia-fs
  </span>
  statistics for the IBM Spectrum Scale file system.
 </p>
 <ol>
  <li>
   Enable
   <span>
    nvidia-fs
   </span>
   statistics:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># echo 1 &gt; /sys/module/nvidia_fs/parameters/rw_stats_enabled</p>
        </pre>
  </li>
  <li>
   <span>
    $ cat /proc/driver/nvidia-fs/stats
   </span>
  </li>
  <li>
   Review the output:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>[~]# cat /proc/driver/nvidia-fs/stats
	GDS Version: 1.0.0.82
	NVFS statistics(ver: 4.0)
	NVFS Driver(version: 2.7.49)
	Mellanox PeerDirect Supported: True
	IO stats: Disabled, peer IO stats: Disabled
	Logging level: info
	
	Active Shadow-Buffer (MiB): 0
	Active Process: 0
	Reads                           : err=0 io_state_err=0
	Sparse Reads                    : n=230 io=0 holes=0 pages=0
	Writes                          : err=0 io_state_err=237 pg-cache=0 pg-cache-fail=0 pg-cache-eio=0
	Mmap                            : n=27 ok=27 err=0 munmap=27
	Bar1-map                        : n=27 ok=27 err=0 free=27 callbacks=0 active=0
	Error                           : cpu-gpu-pages=0 sg-ext=0 dma-map=0 dma-ref=0
	Ops                             : Read=0 Write=0
GPU 0000:2f:00.0  uuid:621f7d17-5e7d-8f79-be27-d2f4256ddd88 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=2</p>
        </pre>
  </li>
 </ol>
 <h3 id="gds-user-space-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-user-space-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-user-space-stats" id="gds-user-space-stats" name="gds-user-space-stats" shape="rect">
    11.6. GDS User Space Stats for IBM Spectrum Scale for Each Process
   </a>
  </a>
 </h3>
 <p>
  To check GDS user space level stats, make sure the “
  <span>
   cufile_stats
  </span>
  ” property in
  <span>
   cufile.json
  </span>
  is set to 3. Run the following command to check the user space stats for a specific process:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-&lt;x&gt;.&lt;y&gt;/gds/tools/gds_stats -p &lt;pid&gt; -l 3 
cuFile STATS VERSION : 4
GLOBAL STATS:
Total Files: 1
Total Read Errors : 0
Total Read Size (MiB): 7302
Read BandWidth (GiB/s): 0.691406
Avg Read Latency (us): 6486
Total Write Errors : 0
Total Write Size (MiB): 0
Write BandWidth (GiB/s): 0
Avg Write Latency (us): 0
READ-WRITE SIZE HISTOGRAM :
0-4(KiB): 0  0
4-8(KiB): 0  0
8-16(KiB): 0  0
16-32(KiB): 0  0
32-64(KiB): 0  0
64-128(KiB): 0  0
128-256(KiB): 0  0
256-512(KiB): 0  0
512-1024(KiB): 0  0
1024-2048(KiB): 0  0
2048-4096(KiB): 3651  0
4096-8192(KiB): 0  0
8192-16384(KiB): 0  0
16384-32768(KiB): 0  0
32768-65536(KiB): 0  0
65536-...(KiB): 0  0
PER_GPU STATS:
GPU 0 Read: bw=0.690716 util(%)=199 n=3651 posix=0 unalign=0 dr=0 r_sparse=0 r_inline=0 err=0 MiB=7302 Write: bw=0 util(%)=0 n=0 posix=0 unalign=0 dr=0 err=0 MiB=0 BufRegister: n=2 err=0 free=0 MiB=4
PER_GPU POOL BUFFER STATS:
PER_GPU POSIX POOL BUFFER STATS:

PER_GPU RDMA STATS:
GPU 0000:43:00.0 :   mlx5_0(130:64):Reads: 3594 Writes: 0  mlx5_1(130:64):Reads: 3708 Writes: 0
RDMA MRSTATS:
peer name   nr_mrs      mr_size(MiB)
mlx5_0      1           2
mlx5_1      1           2</p>
        </pre>
 <p>
  In the example above, 3954 MiB of IBM Spectrum Scale Read-IO went through
  <span>
   mlx5_0
  </span>
  and 3708 MiB MiB of IBM Spectrum Scale Read went through
  <span>
   mlx5_1
  </span>
  . The
  <span>
   RDMA MRSTATS
  </span>
  value shows the number of RDMA memory registrations and size of those registrations.
 </p>
 <h3 id="gds-configs-to-support-spectrum-scale">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-configs-to-support-spectrum-scale">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-configs-to-support-spectrum-scale" id="gds-configs-to-support-spectrum-scale" name="gds-configs-to-support-spectrum-scale" shape="rect">
    11.7. GDS Configuration to Support IBM Spectrum Scale
   </a>
  </a>
 </h3>
 <ol>
  <li>
   Configure the DC key. The DC key for the IBM Spectrum Scale client can be configured in the following ways:
   <ul class="ul">
    <li class="li">
     <p>
      Set the environment variable
      <span>
       CUFILE_RDMA_DC_KEY
      </span>
      . This should be set to a 32-bit hex value. This can be set as shown in the following example.
     </p>
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p>export CUFILE_RDMA_DC_KEY = 0x11223344</p>
        </pre>
    </li>
    <li class="li">
     <p>
      Set the property
      <span>
       rdma_dc_key
      </span>
      in
      <span>
       cufile.json
      </span>
      . This property is a 32-bit value and can be set as shown in the following example.
     </p>
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p>"rdma_dc_key": "0xffeeddcc",</p>
        </pre>
    </li>
   </ul>
   <p>
    In case both the environment variable and the
    <span>
     cufile.json
    </span>
    have the property set, the environment variable
    <span>
     CUFILE_RDMA_DC_KEY
    </span>
    will take precedence over the
    <span>
     rdma_dc_key
    </span>
    property set in
    <span>
     cufile.json
    </span>
    .
   </p>
   <p>
    In case none of the above is set, the default DC Key configured is
    <span>
     0xffeeddcc
    </span>
    .
   </p>
  </li>
  <li>
   Configure the IP addresses in
   <span>
    cufile.json
   </span>
   .
   <p>
    The
    <span>
     &gt;rdma_dev_addr_list
    </span>
    property should be set in
    <span>
     cufile.json
    </span>
    with the IP address of the RDMA devices to be used for IO.
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>"properties": {
    -------
    "rdma_dev_addr_list": [
           "172.16.8.88" , "172.16.8.89", 
           "172.16.8.90" , "172.16.8.91",
           "172.16.8.92" , "172.16.8.93",
           "172.16.8.94", "172.16.8.95" ]
     }
     ---------
}</p>
        </pre>
  </li>
  <li>
   Configure the
   <span>
    max_direct_io_size_kb
   </span>
   property in
   <span>
    cufile.json
   </span>
   .
   <p>
    You can change the IO size with the following property:
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>"properties": {
    -------
    "max_direct_io_size_kb" : 1024
    ---------
}</p>
        </pre>
  </li>
  <li>
   Configure the
   <span>
    rdma_access_mask
   </span>
   property in
   <span>
    cufile.json
   </span>
   .
   <p>
    This property is a performance tunable. Refer to IBM Spectrum Scale documentation in
    <a class="xref" data-cms-ai="0" href="https://www.ibm.com/docs/en/spectrum-scale/5.1.2?topic=configuring-gpudirect-storage-spectrum-scale" rel="noopener" shape="rect" target="_blank">
     https://www.ibm.com/docs/en/spectrum-scale/5.1.2?topic=configuring-gpudirect-storage-spectrum-scale
    </a>
    for optimal configuration of this property.
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>"properties": {
    -------
    "rdma_access_mask": "0x1f",
    ---------
}</p>
        </pre>
  </li>
 </ol>
 <h3 id="scenarios-compat-mode">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#scenarios-compat-mode">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#scenarios-compat-mode" id="scenarios-compat-mode" name="scenarios-compat-mode" shape="rect">
    11.8. Scenarios for Falling Back to Compatibility Mode
   </a>
  </a>
 </h3>
 <p>
  There are couple of scenarios that will cause the IBM Spectrum Scale IOs to go through compatibility mode, irrespective of the
  <span>
   allow_compat_mode
  </span>
  property’s value in
  <span>
   cufile.json
  </span>
  . For a full list of these cases please refer to
  <a class="xref" data-cms-ai="0" href="http://www.ibm.com/support/pages/node/6444075" rel="noopener" shape="rect" target="_blank">
   http://www.ibm.com/support/pages/node/6444075
  </a>
  .
 </p>
 <h3 id="gds-limitations-with-spectrum-scale">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-limitations-with-spectrum-scale">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-limitations-with-spectrum-scale" id="gds-limitations-with-spectrum-scale" name="gds-limitations-with-spectrum-scale" shape="rect">
    11.9. GDS Limitations with IBM Spectrum Scale
   </a>
  </a>
 </h3>
 <p>
  The current maximum of RDMA memory registrations for a GPU buffer is 16. Hence, the maximum size of memory that can be registered with RDMA per GPU buffer is 16 *
  <span>
   max_direct_to_size_kb
  </span>
  (set in
  <span>
   cufile.json
  </span>
  ). Any GDS IO with IBM Spectrum Scale beyond this offset will go through bounce buffers and might have a performance impact.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-intro">
   12. NetApp E-series BeeGFS with GDS Solution Deployment
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="beegfs-intro" name="beegfs-intro" shape="rect">
  </a>
  NetApp supports BeeGFS High Availability.
 </p>
 <p>
  Refer to the BeeGFS with Netapp E-Series Technical Report on how to deploy the BeeGFS parallel file system:
  <a class="xref" data-cms-ai="0" href="https://www.netapp.com/pdf.html?item=/media/17132-tr4755pdf.pdf" rel="noopener" shape="rect" target="_blank">
   Netapp BeeGFS Deployment
  </a>
  . For deployments requiring high availability, refer to
  <a class="xref" data-cms-ai="0" href="https://www.netapp.com/pdf.html?item=/media/19431-tr-4862.pdf" rel="noopener" shape="rect" target="_blank">
   BeeGFS High Availability with NetApp E-Series
  </a>
  .
  <a class="Link" data-cms-ai="0" id="beegfs-package-requirements" name="beegfs-package-requirements" shape="rect">
  </a>
 </p>
 <h3 id="beegfs-package-requirements">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-package-requirements">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-package-requirements" id="beegfs-package-requirements" name="beegfs-package-requirements" shape="rect">
    12.1. Netapp BeeGFS/GPUDirect Storage and Package Requirements
   </a>
  </a>
 </h3>
 <p>
  BeeGFS client and storage with GDS:
 </p>
 <p>
  CUDA and GDS are only required on the beegfs-client hosts. There are no CUDA or GPUDirect Storage requirements for the BeeGFS server hosts.
 </p>
 <h3 id="beegfs-client-config-for-gds">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-client-config-for-gds">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-client-config-for-gds" id="beegfs-client-config-for-gds" name="beegfs-client-config-for-gds" shape="rect">
    12.2. BeeGFS Client Configuration for GDS
   </a>
  </a>
 </h3>
 <p>
  After installing beegfs-client, the client build needs to be configured for RDMA and GDS.
 </p>
 <ol>
  <li>
   <p>
    Edit
    <span>
     /etc/beegfs/beegfs-client-autobuild.conf
    </span>
    . Change line 57 of the file to:
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>buildArgs=-j8 NVFS_H_PATH=/usr/src/mlnx-ofed-kernel-5.4/drivers/nvme/host OFED_INCLUDE_PATH=/usr/src/ofa_kernel/default/include</p>
        </pre>
   <p>
    This should all be on the same line,
   </p>
  </li>
  <li>
   <p>
    Rebuild beegfs-client:
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>sudo /etc/init.d/beegfs-client rebuild</p>
        </pre>
  </li>
 </ol>
 <h3 id="beegfs-gpu-hca-topology">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-gpu-hca-topology">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-gpu-hca-topology" id="beegfs-gpu-hca-topology" name="beegfs-gpu-hca-topology" shape="rect">
    12.3. GPU/HCA Topology on the Client - DGX-A100 and OSS servers Client Server
   </a>
  </a>
 </h3>
 <p>
  Client Server:
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e5232" rowspan="1" valign="top" width="12.5%">
    ibdev
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5235" rowspan="1" valign="top" width="12.5%">
    netdev
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5238" rowspan="1" valign="top" width="12.5%">
    IP
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5241" rowspan="1" valign="top" width="12.5%">
    GPU
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5244" rowspan="1" valign="top" width="12.5%">
    Numa
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5248" rowspan="1" valign="top" width="12.5%">
    OSS
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5251" rowspan="1" valign="top" width="12.5%">
    Target
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5254" rowspan="1" valign="top" width="12.5%">
    Mount Point
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e5232" rowspan="1" valign="top" width="12.5%">
    mlx5_4
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5235" rowspan="1" valign="top" width="12.5%">
    ibp97s0f0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5238" rowspan="1" valign="top" width="12.5%">
    10.10.0.177/24
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5241" rowspan="1" valign="top" width="12.5%">
    0,1,2,3
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5244" rowspan="1" valign="top" width="12.5%">
    0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5248" rowspan="1" valign="top" width="12.5%">
    meta 1
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5251" rowspan="1" valign="top" width="12.5%">
    5,6,7,8
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5254" rowspan="1" valign="top" width="12.5%">
    <span>
     /mnt/beegfs/
    </span>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e5232" rowspan="1" valign="top" width="12.5%">
    mlx5_5
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5235" rowspan="1" valign="top" width="12.5%">
    ibp97s0f1
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5238" rowspan="1" valign="top" width="12.5%">
    10.10.1.177/24
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5241" rowspan="1" valign="top" width="12.5%">
    0,1,2,3
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5244" rowspan="1" valign="top" width="12.5%">
    0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5248" rowspan="1" valign="top" width="12.5%">
    meta 1
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5251" rowspan="1" valign="top" width="12.5%">
    5,6,7,8
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5254" rowspan="1" valign="top" width="12.5%">
    <span>
     /mnt/beegfs/
    </span>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e5232" rowspan="1" valign="top" width="12.5%">
    mlx5_10
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5235" rowspan="1" valign="top" width="12.5%">
    ibp225s0f0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5238" rowspan="1" valign="top" width="12.5%">
    10.10.2.157/24
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5241" rowspan="1" valign="top" width="12.5%">
    4,5,6,7
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5244" rowspan="1" valign="top" width="12.5%">
    4
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5248" rowspan="1" valign="top" width="12.5%">
    meta 2
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5251" rowspan="1" valign="top" width="12.5%">
    1,2,3,4
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5254" rowspan="1" valign="top" width="12.5%">
    <span>
     /mnt/beegfs/
    </span>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e5232" rowspan="1" valign="top" width="12.5%">
    mlx5_11
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5235" rowspan="1" valign="top" width="12.5%">
    ibp225s0f1
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5238" rowspan="1" valign="top" width="12.5%">
    10.10.3.157/24
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5241" rowspan="1" valign="top" width="12.5%">
    4,5,6,7
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5244" rowspan="1" valign="top" width="12.5%">
    4
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5248" rowspan="1" valign="top" width="12.5%">
    meta 2
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5251" rowspan="1" valign="top" width="12.5%">
    1,2,3,4
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5254" rowspan="1" valign="top" width="12.5%">
    <span>
     /mnt/beegfs/
    </span>
   </td>
  </tr>
 </table>
 <p>
  OSS Servers:
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e5403" rowspan="1" valign="top" width="25%">
    OSS
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5406" rowspan="1" valign="top" width="25%">
    ID
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5409" rowspan="1" valign="top" width="25%">
    IP
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5412" rowspan="1" valign="top" width="25%">
    Numa
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e5403" rowspan="1" valign="top" width="25%">
    meta01-numa0-1
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5406" rowspan="1" valign="top" width="25%">
    1001
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5409" rowspan="1" valign="top" width="25%">
    10.10.0.131:8003
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5412" rowspan="1" valign="top" width="25%">
    0
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e5403" rowspan="1" valign="top" width="25%">
    meta01-numa1-2
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5406" rowspan="1" valign="top" width="25%">
    1002
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5409" rowspan="1" valign="top" width="25%">
    10.10.1.131:8004
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5412" rowspan="1" valign="top" width="25%">
    1
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e5403" rowspan="1" valign="top" width="25%">
    meta02-numa0-1
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5406" rowspan="1" valign="top" width="25%">
    2001
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5409" rowspan="1" valign="top" width="25%">
    10.10.2.132:8003
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5412" rowspan="1" valign="top" width="25%">
    0
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e5403" rowspan="1" valign="top" width="25%">
    meta02-numa1-2
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5406" rowspan="1" valign="top" width="25%">
    2002
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5409" rowspan="1" valign="top" width="25%">
    10.10.3.132:8004
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5412" rowspan="1" valign="top" width="25%">
    1
   </td>
  </tr>
 </table>
 <h3 id="beegfs-verify-setup">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-setup">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-setup" id="beegfs-verify-setup" name="beegfs-verify-setup" shape="rect">
    12.4. Verify the Setup
   </a>
  </a>
 </h3>
 <p>
  To verify the setup, run the following commands on any client:
  <a class="Link" data-cms-ai="0" id="beegfs-list-management-node" name="beegfs-list-management-node" shape="rect">
  </a>
 </p>
 <h3 id="beegfs-list-management-node">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-management-node">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-management-node" id="beegfs-list-management-node" name="beegfs-list-management-node" shape="rect">
    12.4.1. List the Management Node
   </a>
  </a>
 </h3>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>root@dgxa100-b:/sys/class# beegfs-ctl --listnodes --nodetype=management --details
meta-02.cpoc.local [ID: 1]
Ports: UDP: 8008; TCP: 8008
Interfaces: em3(TCP)</p>
        </pre>
 <h3 id="beegfs-list-metadata-nodes">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-metadata-nodes">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-metadata-nodes" id="beegfs-list-metadata-nodes" name="beegfs-list-metadata-nodes" shape="rect">
    12.4.2. List the Metadata Nodes
   </a>
  </a>
 </h3>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>root@dgxa100-b:/sys/class# beegfs-ctl --listnodes --nodetype=meta –details

meta01-numa0-1-meta [ID: 1101]
   Ports: UDP: 8005; TCP: 8005
   Interfaces: ib0:net1(RDMA) ib0:net1(TCP)
meta01-numa1-2-meta [ID: 1102]
   Ports: UDP: 8006; TCP: 8006
   Interfaces: ib2:net3(RDMA) ib2:net3(TCP)
meta02-numa0-1-meta [ID: 2101]
   Ports: UDP: 8005; TCP: 8005
   Interfaces: ib0:net0(RDMA) ib0:net0(TCP)
meta02-numa1-2-meta [ID: 2102]
   Ports: UDP: 8006; TCP: 8006
   Interfaces: ib2:net2(RDMA) ib2:net2(TCP)
Number of nodes: 4
Root: 2101</p>
        </pre>
 <h3 id="beegfs-list-storage-nodes">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-storage-nodes">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-storage-nodes" id="beegfs-list-storage-nodes" name="beegfs-list-storage-nodes" shape="rect">
    12.4.3. List the Storage Nodes
   </a>
  </a>
 </h3>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>root@dgxa100-b:/sys/class# beegfs-ctl --listnodes --nodetype=storage –details

meta01-numa0-1 [ID: 1001]
   Ports: UDP: 8003; TCP: 8003
   Interfaces: ib0:net1(RDMA) ib0:net1(TCP)
meta01-numa1-2 [ID: 1002]
   Ports: UDP: 8004; TCP: 8004
   Interfaces: ib2:net3(RDMA) ib2:net3(TCP)
meta02-numa0-1 [ID: 2001]
   Ports: UDP: 8003; TCP: 8003
   Interfaces: ib0:net0(RDMA) ib0:net0(TCP)
meta02-numa1-2 [ID: 2002]
   Ports: UDP: 8004; TCP: 8004
   Interfaces: ib2:net2(RDMA) ib2:net2(TCP)
Number of nodes: 4</p>
        </pre>
 <h3 id="beegfs-list-client-nodes">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-client-nodes">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-client-nodes" id="beegfs-list-client-nodes" name="beegfs-list-client-nodes" shape="rect">
    12.4.4. List the Client Nodes
   </a>
  </a>
 </h3>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>root@dgxa100-b:/sys/class# beegfs-ctl --listnodes --nodetype=client --details
B4330-6161F689-dgxa100-b [ID: 11]
   Ports: UDP: 8004; TCP: 0
 Interfaces: ibp97s0f0(RDMA) ibp97s0f0(TCP) ibp97s0f1(TCP) ibp97s0f1(RDMA) ibp225s0f0(TCP)
        ibp225s0f0(RDMA) ibp225s0f1(TCP) ibp225s0f1(RDMA)</p>
        </pre>
 <h3 id="beegfs-display-client-connections">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-display-client-connections">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-display-client-connections" id="beegfs-display-client-connections" name="beegfs-display-client-connections" shape="rect">
    12.4.5. Display Client Connections
   </a>
  </a>
 </h3>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>root@dgxa100-b:/sys/class# beegfs-net 
mgmt_nodes
=============
meta-02.cpoc.local [ID: 1]
   Connections: TCP: 1 (192.168.0.132:8008);

meta_nodes
=============
meta01-numa0-1-meta [ID: 1101]
   Connections: RDMA: 1 (10.10.1.131:8005);
meta01-numa1-2-meta [ID: 1102]
   Connections: RDMA: 1 (10.10.3.131:8006);
meta02-numa0-1-meta [ID: 2101]
   Connections: RDMA: 1 (10.10.0.132:8005);
meta02-numa1-2-meta [ID: 2102]
   Connections: RDMA: 1 (10.10.2.132:8006);

storage_nodes
=============
meta01-numa0-1 [ID: 1001]
   Connections: RDMA: 8 (10.10.1.131:8003);
meta01-numa1-2 [ID: 1002]
   Connections: RDMA: 8 (10.10.3.131:8004);
meta02-numa0-1 [ID: 2001]
   Connections: RDMA: 16 (10.10.0.132:8003);
meta02-numa1-2 [ID: 2002]
   Connections: RDMA: 8 (10.10.2.132:8004);</p>
        </pre>
 <h3 id="beegfs-verify-connectivity-svcs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-connectivity-svcs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-connectivity-svcs" id="beegfs-verify-connectivity-svcs" name="beegfs-verify-connectivity-svcs" shape="rect">
    12.4.6. Verify Connectivity to the Different Services
   </a>
  </a>
 </h3>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>root@dgxa100-b:/sys/class# beegfs-check-servers

Management
==========
meta-02.cpoc.local [ID: 1]: reachable at 192.168.0.132:8008 (protocol: TCP)

Metadata
==========
meta01-numa0-1-meta [ID: 1101]: reachable at 10.10.1.131:8005 (protocol: TCP)
meta01-numa1-2-meta [ID: 1102]: reachable at 10.10.3.131:8006 (protocol: TCP)
meta02-numa0-1-meta [ID: 2101]: reachable at 10.10.0.132:8005 (protocol: TCP)
meta02-numa1-2-meta [ID: 2102]: reachable at 10.10.2.132:8006 (protocol: TCP)

Storage
==========
meta01-numa0-1 [ID: 1001]: reachable at 10.10.1.131:8003 (protocol: TCP)
meta01-numa1-2 [ID: 1002]: reachable at 10.10.3.131:8004 (protocol: TCP)
meta02-numa0-1 [ID: 2001]: reachable at 10.10.0.132:8003 (protocol: TCP)
meta02-numa1-2 [ID: 2002]: reachable at 10.10.2.132:8004 (protocol: TCP)</p>
        </pre>
 <h3 id="beegfs-list-storage-pools">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-storage-pools">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-storage-pools" id="beegfs-list-storage-pools" name="beegfs-list-storage-pools" shape="rect">
    12.4.7. List Storage Pools
   </a>
  </a>
 </h3>
 <p>
  In this example we used the default mounting point:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>root@dgxa100-b:/sys/class# sudo beegfs-ctl –liststoragepools

Pool ID   Pool Description                      Targets                 Buddy Groups
======= ================== ============================ ============================
      1            Default              1,2,3,4,5,6,7,8</p>
        </pre>
 <h3 id="beegfs-display-free-space-inodes">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-display-free-space-inodes">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-display-free-space-inodes" id="beegfs-display-free-space-inodes" name="beegfs-display-free-space-inodes" shape="rect">
    12.4.8. Display the Free Space and inodes on the Storage and Metadata Targets
   </a>
  </a>
 </h3>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>root@dgxa100-b:/sys/class# beegfs-df

METADATA SERVERS:
TargetID   Cap. Pool        Total         Free    %      ITotal       IFree    %
========   =========        =====         ====    =      ======       =====    =
    1101      normal     573.3GiB     572.9GiB 100%      401.1M      401.0M 100%
    1102      normal     573.3GiB     572.9GiB 100%      401.1M      401.0M 100%
    2101      normal     573.3GiB     572.9GiB 100%      401.1M      401.0M 100%
    2102      normal     573.3GiB     572.9GiB 100%      401.1M      401.0M 100%

STORAGE TARGETS:
TargetID   Cap. Pool        Total         Free    %      ITotal       IFree    %
========   =========        =====         ====    =      ======       =====    =
       1      normal    2574.7GiB    1470.8GiB  57%      270.1M      270.1M 100%
       2      normal    2574.7GiB    1404.0GiB  55%      270.1M      270.1M 100%
       3      normal    2574.7GiB    1265.5GiB  49%      270.1M      270.1M 100%
       4      normal    2574.7GiB    1278.5GiB  50%      270.1M      270.1M 100%
       5      normal    2574.7GiB    1274.0GiB  49%      270.1M      270.1M 100%
       6      normal    2574.7GiB    1342.6GiB  52%      270.1M      270.1M 100%
       7      normal    2574.7GiB    1485.3GiB  58%      270.1M      270.1M 100%
       8      normal    2574.7GiB    1481.7GiB  58%      270.1M      270.1M 100%</p>
        </pre>
 <h3 id="beegfs-testing">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-testing">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-testing" id="beegfs-testing" name="beegfs-testing" shape="rect">
    12.5. Testing
   </a>
  </a>
 </h3>
 <h3 id="beegfs-verify-integration">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-integration">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-integration" id="beegfs-verify-integration" name="beegfs-verify-integration" shape="rect">
    12.5.1. Verifying Integration is Working
   </a>
  </a>
 </h3>
 <p>
  Once beegfs-client has been started with GDS support, a basic test can be performed to verify that the integration is working:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>root@dgxa100-b:/usr/local/cuda-11.4/gds/tools# ./gdscheck.py -p
GDS release version: 1.1.1.14
nvidia_fs version:  2.7 libcufile version: 2.9
============
ENVIRONMENT:
============
=====================
DRIVER CONFIGURATION:
=====================
NVMe               : Supported
NVMeOF             : Unsupported
SCSI               : Unsupported
ScaleFlux CSD      : Unsupported
NVMesh             : Unsupported
DDN EXAScaler      : Unsupported
IBM Spectrum Scale : Unsupported
NFS                : Unsupported
BEEGFS      : Supported
WekaFS             : Unsupported
Userspace RDMA     : Unsupported
--Mellanox PeerDirect : Disabled
--rdma library        : Not Loaded (libcufile_rdma.so)
--rdma devices        : Not configured
--rdma_device_status  : Up: 0 Down: 0
=====================
CUFILE CONFIGURATION:
=====================
properties.use_compat_mode : true
properties.gds_rdma_write_support : false
properties.use_poll_mode : false
properties.poll_mode_max_size_kb : 4
properties.max_batch_io_timeout_msecs : 5
properties.max_direct_io_size_kb : 16384
properties.max_device_cache_size_kb : 131072
properties.max_device_pinned_mem_size_kb : 33554432
properties.posix_pool_slab_size_kb : 4 1024 16384 
properties.posix_pool_slab_count : 128 64 32 
properties.rdma_peer_affinity_policy : RoundRobin
properties.rdma_dynamic_routing : 0
fs.generic.posix_unaligned_writes : false
fs.lustre.posix_gds_min_kb: 0
fs.beegfs.posix_gds_min_kb: 0
fs.weka.rdma_write_support: false
profile.nvtx : false
profile.cufile_stats : 0
miscellaneous.api_check_aggressive : false
=========
IOMMU: disabled
Platform verification succeeded</p>
        </pre>
 <h3 id="beegfs-conducting-basic-fs-test">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-conducting-basic-fs-test">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-conducting-basic-fs-test" id="beegfs-conducting-basic-fs-test" name="beegfs-conducting-basic-fs-test" shape="rect">
    12.5.2. Conducting a Basic NetApp BeeGFS Filesystem Test
   </a>
  </a>
 </h3>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>/usr/local/cuda/gds/tools/gdsio_verify -f /mnt/beegfs/file 1g -d 0 -o 0 -s 1G -n 1 -m 1
gpu index :0, file :/mnt/beegfs/file 1g, gpu buffer alignment :0, gpu buffer offset :0, gpu devptr offset :0, file offset :0, io_requested :1073741824, io_chunk_size :1073741824, bufregister :true, sync :1, nr ios :1, fsync :0,
Data Verification Success</p>
        </pre>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-set-up-vast-data">
   13. Setting Up and Troubleshooting VAST Data (NFSoRDMA+MultiPath)
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="troubleshoot-set-up-vast-data" name="troubleshoot-set-up-vast-data" shape="rect">
  </a>
  This section provides information about how to set up and troubleshoot VAST data (NFSoRDMA+MultiPath).
 </p>
 <h3 id="install-mofed-vast">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-mofed-vast">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-mofed-vast" id="install-mofed-vast" name="install-mofed-vast" shape="rect">
    13.1. Installing MLNX_OFED and VAST NFSoRDMA+Multipath Packages
   </a>
  </a>
 </h3>
 <h3 id="client-software-reqs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#client-software-reqs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#client-software-reqs" id="client-software-reqs" name="client-software-reqs" shape="rect">
    13.1.1. Client Software Requirements
   </a>
  </a>
 </h3>
 <p>
  The following table lists the
  minimum
  client software requirements for using MLNX_OFED and VAST NFSoRDMA+Multipath packages.
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 5. Minimum Client Requirements
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e5716" rowspan="1" valign="top">
    NFS Connection Type
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5719" rowspan="1" valign="top">
    Linux Kernel
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e5722" rowspan="1" valign="top">
    MLNX_OFED
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e5716" rowspan="1" valign="top">
    NFSoRDMA + Multipath
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5719" rowspan="1" valign="top">
    The following kernel versions are supported:
    <ul>
     <li>
      4.15
     </li>
     <li>
      4.18
     </li>
     <li>
      5.4
     </li>
    </ul>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e5722" rowspan="1" valign="top">
    The following MLNX_OFED versions are supported:
    <ul>
     <li>
      4.6
     </li>
     <li>
      4.7
     </li>
     <li>
      5.0
     </li>
     <li>
      5.1
     </li>
     <li>
      5.3
     </li>
    </ul>
   </td>
  </tr>
 </table>
 <p>
  For the most up to date supportability matrix and client configuration steps and package downloads, refer to:
  <a class="xref" data-cms-ai="0" href="https://support.vastdata.com/hc/en-us/articles/360016813140-NFSoRDMA-with-Multipath" shape="rect">
   https://support.vastdata.com/hc/en-us/articles/360016813140-NFSoRDMA-with-Multipath
  </a>
  .
 </p>
 <p>
  MLNX_OFED must be installed for the VAST NFSoRDMA+Multipath package to function optimally. It is also important to download the correct VAST software packages to match your kernel+MLNX_OFED version combination. Refer to
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nvme-nvmeof-support" shape="rect" title="This section provides troubleshooting information for NVME and NVMeOF support.">
   Troubleshooting and FAQ for NVMe and NVMeOF
  </a>
  support for information about how to install MLNX_OFED with GDS support.
 </p>
 <ul>
  <li>
   To verify the current version of MLNX_OFED, issue the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ ofed_info -s
MLNX_OFED_LINUX-5.3-0.6.6.01:</p>
        </pre>
  </li>
  <li>
   To verify the currently installed Linux kernel version, issue the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ uname -r -v</p>
        </pre>
  </li>
 </ul>
 <p>
  After you verify that your system has the correct combination of kernel and MLNX_OFED, you can install the VAST Multipath package.
 </p>
 <h3 id="install-vast">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-vast">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-vast" id="install-vast" name="install-vast" shape="rect">
    13.1.2. Install the VAST Multipath Package
   </a>
  </a>
 </h3>
 <p>
  Although the VAST Multipath with NFSoRDMA package has been submitted upstream for inclusion in a future kernel release, it is currently only available as a download from:
  <a class="xref" data-cms-ai="0" href="https://support.vastdata.com/hc/en-us/articles/360016813140-NFSoRDMA-with-Multipath" shape="rect">
   https://support.vastdata.com/hc/en-us/articles/360016813140-NFSoRDMA-with-Multipath
  </a>
  .
 </p>
 <p>
  Be sure to download the correct
  <span>
   .deb
  </span>
  file that is based on your kernel and MLNX_OFED. version.
 </p>
 <ol>
  <li>
   Install the VAST NFSoRDMA+Multipath package.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ sudo apt-get install mlnx-nfsrdma-*.deb</p>
        </pre>
  </li>
  <li>
   Generate a new initramfs image.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ sudo update-initramfs -u -k `uname -r`</p>
        </pre>
  </li>
  <li>
   Verify that the package is installed, and the version is the number that you expected.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ dpkg -l | grep mlnx-nfsrdma
ii  mlnx-nfsrdma-dkms         5.3-OFED.5.1.0.6.6.0      all  DKMS support for NFS RDMA kernel module</p>
        </pre>
  </li>
  <li>
   Reboot the host and run the following commands to verify that the correct version is loaded.
   Note:
   <p>
    The versions shown by each command should match.
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ cat /sys/module/sunrpc/srcversion
4CC8389C7889F82F5A59269
$ modinfo sunrpc | grep srcversion
srcversion:     4CC8389C7889F82F5A59269</p>
        </pre>
  </li>
 </ol>
 <h3 id="networking-setup">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#networking-setup">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#networking-setup" id="networking-setup" name="networking-setup" shape="rect">
    13.2. Set Up the Networking
   </a>
  </a>
 </h3>
 <p>
  This section provides information about how to set up client networking for VAST for GDS.
 </p>
 <p>
  To ensure optimal GPU-to-storage performance while leveraging GDS, you need to configure VAST and client networking in a balanced manner.
  <a class="Link" data-cms-ai="0" id="vast-network-config" name="vast-network-config" shape="rect">
  </a>
 </p>
 <h3 id="vast-network-config">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#vast-network-config">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#vast-network-config" id="vast-network-config" name="vast-network-config" shape="rect">
    13.2.1. VAST Network Configuration
   </a>
  </a>
 </h3>
 <p>
  VAST is a multi-node architecture. Each node has multiple high-speed (IB-HDR100 or 100GbE) interfaces, which can host-client-facing Virtual IPs. Refer to
  <a class="xref" data-cms-ai="0" href="https://support.vastdata.com/hc/en-us/articles/360016231794-Managing-Virtual-IP-VIP-Pools" shape="rect">
   VAST-Managing Virtual IP (VIP) Pools
  </a>
  for more information. Here is the typical workflow:
  <a class="Link" data-cms-ai="0" id="vast-network-config__ol_xr5_ck5_hnb" name="vast-network-config__ol_xr5_ck5_hnb" shape="rect">
  </a>
 </p>
 <ol>
  <li>
   Multiply the number of VAST-Nodes * 2 (one per Interface).
  </li>
  <li>
   Create a VIP Pool with the resulting IP count.
  </li>
  <li>
   Place the VAST-VIP Pool on the same IP-subnet as the client.
  </li>
 </ol>
 <h3 id="cllent-network-config">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cllent-network-config">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cllent-network-config" id="cllent-network-config" name="cllent-network-config" shape="rect">
    13.2.2. Client Network Configuration
   </a>
  </a>
 </h3>
 <p>
  The following is information about client network configuration.
 </p>
 <p>
  Typically, GPU optimized clients (such as the NVIDIA DGX-2 and DGX-A100) are configured with multiple high speed network interface cards (NICs). In the following example, the system contains 8 separate NICs that were selected for optimal balance for NIC --&gt;GPU and NIC --&gt;CPU bandwidth.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo ibdev2netdev
mlx5_0 port 1 ==&gt; ibp12s0 (Up)
mlx5_1 port 1 ==&gt; ibp18s0 (Up)
mlx5_10 port 1 ==&gt; ibp225s0f0 (Down)
mlx5_11 port 1 ==&gt; ibp225s0f1 (Down)
mlx5_2 port 1 ==&gt; ibp75s0 (Up)
mlx5_3 port 1 ==&gt; ibp84s0 (Up)
mlx5_4 port 1 ==&gt; ibp97s0f0 (Down)
mlx5_5 port 1 ==&gt; ibp97s0f1 (Down)
mlx5_6 port 1 ==&gt; ibp141s0 (Up)
mlx5_7 port 1 ==&gt; ibp148s0 (Up)
mlx5_8 port 1 ==&gt; ibp186s0 (Up)
mlx5_9 port 1 ==&gt; ibp202s0 (Up)</p>
        </pre>
 <p>
  Not all interfaces are connected, and this is to ensure optimal bandwidth.
 </p>
 <p>
  When using the aforementioned VAST NFSoRDAM+Multipath package, it is recommended to assign static IP’s to each interface on the same subnet, which should also match the subnet configured on the VAST VIP Pool. If using GDS with NVIDIA DGX-A100’s, a simplistic netplan is all that is required, for example:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>ibp12s0:
  addresses: [172.16.0.17/24]
  dhcp4: no
ibp141s0:
  addresses: [172.16.0.18/24]
  dhcp4: no
ibp148s0:
  addresses: [172.16.0.19/24]
  dhcp4: no</p>
        </pre>
 <p>
  However, if you are using other systems, or non-GDS code, you need to apply the following code to ensure that the proper interfaces are used to traverse from Client--&gt;VAST.
 </p>
 Note:
 <p>
  See the
  <span>
   routes
  </span>
  section for each interface in the following sample.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /etc/netplan/01-netcfg.yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    enp226s0:
      dhcp4: yes
    ibp12s0:
      addresses: [172.16.0.25/24]
      dhcp6: no
      routes:
         - to: 172.16.0.0/24
           via: 172.16.0.25
           table: 101
      routing-policy:
          - from: 172.16.0.25
           table: 101
    ibp18s0:
      addresses: [172.16.0.26/24]
      dhcp4: no
      routes:
         - to: 172.16.0.0/24
           via: 172.16.0.26
           table: 102
      routing-policy:
          - from: 172.16.0.26
           table: 102
    ibp75s0:
      addresses: [172.16.0.27/24]
      dhcp4: no
      routes:
         - to: 172.16.0.0/24
           via: 172.16.0.27
           table: 103
      routing-policy:
          - from: 172.16.0.27
           table: 103
    ibp84s0:
      addresses: [172.16.0.28/24]
      dhcp4: no
      routes:
         - to: 172.16.0.0/24
           via: 172.16.0.28
           table: 104
      routing-policy:
          - from: 172.16.0.28
           table: 104
    ibp141s0:
      addresses: [172.16.0.29/24]
      dhcp4: no
      routes:
         - to: 172.16.0.0/24
           via: 172.16.0.29
           table: 105
      routing-policy:
          - from: 172.16.0.29
           table: 105
    ibp148s0:
      addresses: [172.16.0.30/24]
      dhcp4: no
      routes:
         - to: 172.16.0.0/24
           via: 172.16.0.30
           table: 106
      routing-policy:
          - from: 172.16.0.30
           table: 106
    ibp186s0:
      addresses: [172.16.0.31/24]
      dhcp4: no
      routes:
         - to: 172.16.0.0/24
           via: 172.16.0.31
           table: 107
      routing-policy:
          - from: 172.16.0.31
           table: 107
    ibp202s0:
      addresses: [172.16.0.32/24]
      dhcp4: no
      routes:
         - to: 172.16.0.0/24
           via: 172.16.0.32
           table: 108
      routing-policy:
          - from: 172.16.0.32
           table: 108</p>
        </pre>
 <p>
  After making changes to the netplan, before issuing the following command, ensure that you have a IPMI/console connection to the client:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo netplan apply</p>
        </pre>
 <h3 id="verify-network-connect">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-network-connect">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-network-connect" id="verify-network-connect" name="verify-network-connect" shape="rect">
    13.2.3. Verify Network Connectivity
   </a>
  </a>
 </h3>
 <p>
  Once the proper netplan is applied, verify connectivity between all client interfaces and all VAST-VIPs with a ping loop:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p># Replace with appropriate interface names

$ export IFACES="ibp12s0 ibp18s0 ibp75s0 ibp84s0 ibp141s0 ibp148s0 ibp186s0 ibp202s0"
# replace with appropriate VAST-VIPs

$ export VIPS=$(echo 172.16.0.{101..116})

$ echo "starting pingtest" &gt; pingtest.log

$ for i in $IFACES;do for v in $VIPS; do echo $i &gt;&gt; pingtest.log; ping -c 1 $v -W 0.2 -I $i|grep loss &gt;&gt; pingtest.log;done;done;

# Verify no failures:
$ grep '100%' pingtest.log</p>
        </pre>
 <p>
  You should also verify that one of the following conditions are met:
 </p>
 <ul>
  <li>
   All client interfaces are directly cabled to the same IB switches as VAST.
  </li>
  <li>
   There are sufficient InterSwitch Links (ISLs) between client-switches, and switches to which VAST is connected.
  </li>
 </ul>
 <p>
  To verify the current IB switch topology, issue the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo ibnetdiscover 
&lt;output trimmed&gt;

[37] "H-b8599f0300c3f4cb"[1](b8599f0300c3f4cb) # "vastraplab-cn1 HCA-2" lid 55 2xHDR  # &lt;-- example of Vast-Node

[43] "S-b8599f0300e361f2"[43]           # "MF0;RL-QM87-C20-U33:MQM8700/U1" lid 1 4xHDR # &lt;-- example of ISL

[67] "H-1c34da030073c27e"[1](1c34da030073c27e) # "rl-dgxa-c21-u19 mlx5_9" lid 23 4xHDR # &lt;-- example of client</p>
        </pre>
 <h3 id="mount-vast-nfs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-vast-nfs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-vast-nfs" id="mount-vast-nfs" name="mount-vast-nfs" shape="rect">
    13.3. Mount VAST NFS
   </a>
  </a>
 </h3>
 <p>
  To fully utilize available VAST VIPs, you must mount the filesystem by issuing the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo mount -o proto=rdma,port=20049,vers=3 \

-o noidlexprt,nconnect=40 \
-o localports=172.16.0.25-172.16.0.32 \
-o remoteports=172.16.0.101-172.16.0.140 \
172.16.0.101:/ /mnt/vast</p>
        </pre>
 <p>
  The options are:
 </p>
 proto
 RDMA must be specified.
 port=20049
 Must be specified, this is RDMA control port.
 noidlexprt
 Do not disconnect idle connections. This is to detect and recover failing connections when there are no pending I/O’s.
 nconnect
 Number of concurrent connections. Should be divisible evenly by the number of remoteports specified below for best balance.
 localports
 A list of IPv4 addresses for the local ports to bind.
 Remoteports
 A list of NFS server IPv4 ports to bind.
 <p>
  For both
  localports
  and
  remoteports
  you can specify an inclusive range with the
  -
  delimiter, for example,
  FIRST-LAST
  . Multiple ranges or individual IP addresses can be separated by
  ~
  (a tilde)
 </p>
 <h3 id="debug-monitor">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#debug-monitor">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#debug-monitor" id="debug-monitor" name="debug-monitor" shape="rect">
    13.4. Debugging and Monitoring VAST Data
   </a>
  </a>
 </h3>
 <p>
  Typically,
  <span>
   mountstats
  </span>
  under
  <span>
   /proc
  </span>
  shows
  <span>
   xprt
  </span>
  statistics. However, instead of modifying it in a non-compatible way with the
  <span>
   nfsstat
  </span>
  utility, the VAST Multipath package extends
  <span>
   mountstats
  </span>
  with extra state reporting, to be exclusively accessed from
  <span>
   /sys/kernel/debug
  </span>
  .
 </p>
 <p>
  The
  <span>
   stats
  </span>
  node was added for each RPC client, and the RPC client 0 shows the mount that is completed:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo cat /sys/kernel/debug/sunrpc/rpc_clnt/0/stats</p>
        </pre>
 <p>
  The added information is multipath IP address information per xprt and xprt state in string format. For example:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>xprt:   rdma 0 0 1 0 24 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11 0 0 0
        172.25.1.101 -&gt; 172.25.1.1, state: CONNECTED BOUND
xprt:   rdma 0 0 1 0 24 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11 0 0 0
        172.25.1.102 -&gt; 172.25.1.2, state: CONNECTED BOUND
xprt:   rdma 0 0 1 0 23 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11 0 0 0
        172.25.1.103 -&gt; 172.25.1.3, state: CONNECTED BOUND
xprt:   rdma 0 0 1 0 22 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 11 0 0 0
        172.25.1.104 -&gt; 172.25.1.4, state: CONNECTED BOUND
xprt:   rdma 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        172.25.1.101 -&gt; 172.25.1.5, state: BOUND
xprt:   rdma 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        172.25.1.102 -&gt; 172.25.1.6, state: BOUND
xprt:   rdma 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        172.25.1.103 -&gt; 172.25.1.7, state: BOUND
xprt:   rdma 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
        172.25.1.104 -&gt; 172.25.1.8, state: BOUND</p>
        </pre>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nvme-nvmeof-support">
   14. Troubleshooting and FAQ for NVMe and NVMeOF Support
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="nvme-nvmeof-support" name="nvme-nvmeof-support" shape="rect">
  </a>
  This section provides troubleshooting information for NVME and NVMeOF support.
 </p>
 <h3 id="mofed-req-install">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mofed-req-install">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mofed-req-install" id="mofed-req-install" name="mofed-req-install" shape="rect">
    14.1. MLNX_OFED Requirements and Installation
   </a>
  </a>
 </h3>
 <ul>
  <li>
   To enable GDS support for NVMe and NVMeOF, you need to install at least MLNX_OFED 5.3 or later.
  </li>
  <li>
   You must install MLNX_OFED with support for GDS.
  </li>
 </ul>
 <p>
  After installation is complete, for the changes to take effect, use
  <span>
   update -initramfs
  </span>
  and reboot. The Linux kernel version that was tested with MLNX_OFED 5.3-1.0.5.01 is 4.15.0-x and 5.4.0-x. Issue the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo ./mlnxofedinstall --with-nvmf --with-nfsrdma --enable-gds --add-kernel-support --dkms</p>
        </pre>
 Note:
 <p>
  With MLNX_OFED 5.3 onwards, the
  <span>
   --enable-gds
  </span>
  flag is no longer necessary.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo update-initramfs -u -k `uname -r`
$ reboot</p>
        </pre>
 <h3 id="det-nvme-support-gds">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-nvme-support-gds">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-nvme-support-gds" id="det-nvme-support-gds" name="det-nvme-support-gds" shape="rect">
    14.2. Determining Whether the NVMe device is Supported for GDS
   </a>
  </a>
 </h3>
 <p>
  NVMe devices must be compatible with GDS; the device cannot have the block device integrity capability.
 </p>
 <p>
  For device integrity, the Linux block layer completes the metadata processing based on the payload in the host memory. This is a deviation from the standard GDS IO path and, as a result, cannot accommodate these devices. The cuFile file registration will fail when this type of underlying device is detected with appropriate error log in the
  <span>
   cufile.log
  </span>
  file.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /sys/block/&lt;nvme&gt;/integrity/device_is_integrity_capable</p>
        </pre>
 <h3 id="det-raid-level">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-raid-level">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-raid-level" id="det-raid-level" name="det-raid-level" shape="rect">
    14.3. RAID Support in GDS
   </a>
  </a>
 </h3>
 <p>
  Currently, GDS only supports RAID 0.
 </p>
 <h3 id="mount-local-fs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-local-fs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-local-fs" id="mount-local-fs" name="mount-local-fs" shape="rect">
    14.4. Mounting a Local Filesystem for GDS
   </a>
  </a>
 </h3>
 <p>
  Currently, EXT4 and XFS are the only block device based filesystem that GDS supports. Because of Direct IO semantics, the ext4 file system must be mounted with the journaling mode set to
  <span>
   data=ordered
  </span>
  . This has to be explicitly part of the mount options so that the library can recognize it:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo mount -o data=ordered /dev/nvme0n1 /mnt</p>
        </pre>
 <p>
  If the EXT4 journaling mode is not in the expected mode, the
  <span>
   cuFileHandleRegister
  </span>
  will fail, and an appropriate error message will be logged in the log file. For instance, in the following case,
  <span>
   /mnt1
  </span>
  is mounted with writeback, and GDS returns an error:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ mount | grep /mnt1
/dev/nvme0n1p2 on /mnt1 type ext4 (rw,relatime,data=writeback)

$ ./cufile_sample_001 /mnt1/foo 0
opening file /mnt1/foo
file register error:GPUDirect Storage not supported on current file</p>
        </pre>
 <h3 id="check-exist-mount">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-exist-mount">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-exist-mount" id="check-exist-mount" name="check-exist-mount" shape="rect">
    14.5. Check for an Existing EXT4 Mount
   </a>
  </a>
 </h3>
 <p>
  To check for an existing EXT4 mount:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ mount | grep ext4
/dev/sda2 on / type ext4 (rw,relatime,errors=remount-ro,data=ordered)
/dev/nvme1n1 on /mnt type ext4 (rw,relatime,data=ordered)
/dev/nvme0n1p2 on /mnt1 type ext4 (rw,relatime,data=writeback)</p>
        </pre>
 Note:
 <p>
  A similar check can be used to check for an existing XFS mount, for example:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>mount | grep xfs</p>
        </pre>
 <h3 id="check-io-stats-block-devmt">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-io-stats-block-devmt">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-io-stats-block-devmt" id="check-io-stats-block-devmt" name="check-io-stats-block-devmt" shape="rect">
    14.6. Check for IO Statistics with Block Device Mount
   </a>
  </a>
 </h3>
 <p>
  The following command and partial log show you how to obtain the IO statistics:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo iotop
Actual DISK READ:       0.00 B/s | Actual DISK WRITE:     193.98 K/s
  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO&gt;    COMMAND
  881  be/3  root      0.00 B/s   15.52 K/s  0.00 %  0.01 % [jbd2/sda2-8]
    1  be/4  root      0.00 B/s    0.00 B/s  0.00 %  0.00 % init splash</p>
        </pre>
 <h3 id="raid-group-config-gpu-aff">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#raid-group-config-gpu-aff">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#raid-group-config-gpu-aff" id="raid-group-config-gpu-aff" name="raid-group-config-gpu-aff" shape="rect">
    14.7. RAID Group Configuration for GPU Affinity
   </a>
  </a>
 </h3>
 <p>
  Creating one RAID group from the available NVMe devices might not be optimal for GDS performance. You might need to create RAID groups that consist of devices that have a pci-affinity with the specified GPU. This is required to prevent and cross-node P2P traffic between the GPU and the NVMe devices.
 </p>
 <p>
  If affinity is not enforced, GDS will use an internal mechanism of device bounce buffers to copy data from the NVMe devices to an intermediate device that is closest to the drives and copy the data back to the actual GPU. If NVLink is enabled, this will speed up these transfers.
 </p>
 <h3 id="basic-ext4-fs-test">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-ext4-fs-test">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-ext4-fs-test" id="basic-ext4-fs-test" name="basic-ext4-fs-test" shape="rect">
    14.8. Conduct a Basic EXT4 Filesystem Test
   </a>
  </a>
 </h3>
 <p>
  To conduct a basic EXT4 filesystem test, issue the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-x.y/gds/tools/gdsio_verify  -f /mnt/nvme/gdstest/tests/reg1G -n 1 -m 0 -s 1024 -o 0  -d 0 -t 0 -S -g 4K</p>
        </pre>
 <p>
  Sample output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>gpu index :0,file :/mnt/weka/gdstest/tests/reg1G, RING buffer size :0, gpu buffer alignment :4096, gpu buffer offset :0, file offset :0, io_requested :1024, bufregister :false, sync :0, nr ios :1,fsync :0,
address = 0x564ffc5e76c0
Data Verification Success</p>
        </pre>
 <h3 id="unmount-ext4-fs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-ext4-fs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-ext4-fs" id="unmount-ext4-fs" name="unmount-ext4-fs" shape="rect">
    14.9. Unmount a EXT4 Filesystem
   </a>
  </a>
 </h3>
 <p>
  To unmount an EXT4 filesystem, issue the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo umount /mnt/</p>
        </pre>
 <h3 id="udev-name-conv-block-device">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#udev-name-conv-block-device">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#udev-name-conv-block-device" id="udev-name-conv-block-device" name="udev-name-conv-block-device" shape="rect">
    14.10. Udev Device Naming for a Block Device
   </a>
  </a>
 </h3>
 <p>
  The library has a limitation when identifying the NVMe-based block devices in that it expects device names to have the
  <span>
   nvme
  </span>
  prefix as part of the naming convention.
 </p>
 <h3 id="batch-io-performance">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#batch-io-performance">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#batch-io-performance" id="batch-io-performance" name="batch-io-performance" shape="rect">
    14.11. BATCH I/O Performance
   </a>
  </a>
 </h3>
 <p>
  It has been observed that m separate batches with n/m entries each, showed better performance than 1 batch with n entries especially in case of NVMe based storage.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#display-gds-driver-stats">
   15. Displaying GDS NVIDIA FS Driver Statistics
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="display-gds-driver-stats" name="display-gds-driver-stats" shape="rect">
  </a>
  GDS exposes the IO statistics information on the
  <span>
   procfs
  </span>
  filesystem.
 </p>
 <ol>
  <li>
   To display driver statistics, run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/driver/nvidia-fs/stat</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>GDS Version: 1.0.0.71
NVFS statistics(ver: 4.0)
NVFS Driver(version: 2:7:47)
Mellanox PeerDirect Supported: True
IO stats: Enabled, peer IO stats: Enabled
Logging level: info
                     
Active Shadow-Buffer (MiB): 0
Active Process: 0
Reads                           : n=0 ok=0 err=0 readMiB=0 io_state_err=0
Reads                           : Bandwidth(MiB/s)=0 Avg-Latency(usec)=0
Sparse Reads : n=6 io=0 holes=0 pages=0
Writes                          : n=0 ok=0 err=0 writeMiB=0 io_state_err=0
        pg-cache=0 pg-cache-fail=0 pg-cache-eio=0
Writes                          : Bandwidth(MiB/s)=0 Avg-Latency(usec)=0
Mmap : n=183 ok=183 err=0 munmap=183
Bar1-map                        : n=183 ok=183 err=0 free=165 callbacks=18
        active=0
Error                           : cpu-gpu-pages=0 sg-ext=0 dma-map=0 dma-ref=0
Ops : Read=0 Write=0
GPU 0000:be:00.0  uuid:87e5c586-88ed-583b-df45-fcee0f1e7917 : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:e7:00.0  uuid:029faa3b-cb0d-2718-259c-6dc650c636eb : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:5e:00.0  uuid:39eeb04b-1c52-81cc-d76e-53d03eb6ed32 : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:57:00.0  uuid:a99a7a93-7801-5711-258b-c6aca4fe6d85 : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:39:00.0  uuid:d22b0bc4-cdb1-65ac-7495-3570e5860fda : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:34:00.0  uuid:e11b33d9-60f7-a721-220a-d14e5b15a52c : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=128 cross_root_port(%)=0
GPU 0000:b7:00.0  uuid:e8630cd2-5cb7-cab7-ef2e-66c25507c119 : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:e5:00.0  uuid:b3d46477-d54f-c23f-dc12-4eb5ea172af6 : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:e0:00.0  uuid:7a10c7bd-07e0-971b-a19c-61e7c185a82c : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:bc:00.0  uuid:bb96783c-5a46-233a-cbce-071aeb308083 : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:e2:00.0  uuid:b6565ee8-2100-7009-bcc6-a3809905620d : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=2 cross_root_port(%)=0
GPU 0000:5c:00.0  uuid:5527d7fb-a560-ab42-d027-20aeb5512197 : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:59:00.0  uuid:bb734f6b-24ad-2f83-86c3-6ab179bce131 : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:3b:00.0  uuid:0ef0b9ee-bb8f-cdae-4535-c0d790b2c663 : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:b9:00.0  uuid:ad59f685-5836-c2ea-2c79-3c95bea23f0d : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0          
GPU 0000:36:00.0  uuid:fda65234-707b-960a-d577-18c519301848 : Registered_MiB=0 Cache_MiB=0
        max_pinned_MiB=1 cross_root_port(%)=0</p>
        </pre>
  </li>
 </ol>
 <h3 id="nvidia-fs-stats2">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nvidia-fs-stats2">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nvidia-fs-stats2" id="nvidia-fs-stats2" name="nvidia-fs-stats2" shape="rect">
    15.1. nvidia-fs Statistics
   </a>
  </a>
 </h3>
 <p>
  The following table describes
  <span>
   nvidia-fs
  </span>
  statistics.
 </p>
 <a class="Link" data-cms-ai="0" id="nvidia-fs-stats2__section_g22_lmj_smb" name="nvidia-fs-stats2__section_g22_lmj_smb" shape="rect">
 </a>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" id="nvidia-fs-stats2__table_qlm_nmj_smb" rules="all" summary="">
  Table 6. NVIDIA-FS Statistics
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e6536" rowspan="1" valign="top">
    Type
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e6539" rowspan="1" valign="top">
    Statistics
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e6542" rowspan="1" valign="top">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6536" rowspan="1" valign="top">
    Reads
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     n
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of read requests.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     ok
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of successful read requests.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     err
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of read errors.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    Readmb (mb)
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total data read into the GPUs.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     io_state_err
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    <p>
     Read errors that were seen.
    </p>
    <p>
     Some pages might have been in the page cache.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6536" rowspan="1" valign="top">
    Reads
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    Bandwidth (MB/s)
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    <p>
     Active Read Bandwidth when IO is in flight. This is the period from when IO was submitted to the GDS kernel driver until the IO completion was received by the GDS kernel driver.
    </p>
    <p>
     There was no userspace involved.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    Avg-Latency (usec)
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    <p>
     Active Read latency when IO is in flight. This is from the period from when IO was submitted to the GDS kernel driver until the IO completion is received by the GDS kernel driver.
    </p>
    <p>
     There was no userspace involved.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6536" rowspan="1" valign="top">
    Sparse Reads
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     n
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of sparse read requests.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     holes
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of holes that were observed during reads.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     pages
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of pages that span the holes.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6536" rowspan="1" valign="top">
    Writes
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     n
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of write requests.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     ok
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of successful write requests.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     err
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of write errors.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    Writemb (mb)
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total data that was written from the GPUs to the disk.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     io_state_err
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    <p>
     Write errors that were seen.
    </p>
    <p>
     Some pages might have been in the page cache.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     pg-cache
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of write requests that were found in the page cache.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     pg-cache-fail
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of write requests that were found in the page cache but could not be flushed.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     pg-cache-eio
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of write requests that were found in the
    <span>
     page-cache
    </span>
    , but could not be flushed after multiple retries, and IO failed with EIO.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6536" rowspan="1" valign="top">
    Writes
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    Bandwidth (MB/s)
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    <p>
     Active Write Bandwidth when IO is in flight. This is the period from when IO is submitted to the GDS kernel driver until the IO completion is received by the GDS kernel driver.
    </p>
    <p>
     There was no userspace involved.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    Avg-Latency (usec)
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    <p>
     Active Write latency when IO is in flight. This is the period from when IO is submitted to the GDS kernel driver until the IO completion is received by the GDS kernel driver.
    </p>
    <p>
     There was no userspace involved.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6536" rowspan="1" valign="top">
    Mmap
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>n</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of mmap system calls that were issued.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     ok
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of successful mmap system calls.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     err
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Errors that were observed through the mmap system call.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     munmap
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of munmap that were issued.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6536" rowspan="1" valign="top">
    Bar-map
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     n
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of times the GPU BAR memory was pinned.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     ok
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of times the successful GPU BAR memory was pinned.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     err
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total errors that were observed during the BAR1 pinning.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     free
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of times the BAR1 memory was unpinned.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     callbacks
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of times the NVIDIA kernel driver invoked callback to the GDS driver. This is invoked on the following instances:
    <a class="Link" data-cms-ai="0" id="nvidia-fs-stats2__ul_k2k_2pj_smb" name="nvidia-fs-stats2__ul_k2k_2pj_smb" shape="rect">
    </a>
    <ul>
     <li>
      When the process crashes or was abruptly killed.
     </li>
     <li>
      When
      <span>
       cudaFree
      </span>
      is invoked on memory, which is pinned through
      <span>
       cuFileBufRegister
      </span>
      , but
      <span>
       cuFileBufDeregister
      </span>
      is not invoked.
     </li>
    </ul>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     active
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    <p>
     Active number of BAR1 memory that was pinned.
    </p>
    <p>
     (This value is the total number and not the total memory.)
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6536" rowspan="1" valign="top">
    Error
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     cpu-gpu-pages
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Number of IO requests that had a mix of CPU-GPU pages when
    <span>
     nvfs_dma_map_sg_attrs
    </span>
    is invoked.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     sg-ext
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Scatterlist that could not be expanded because the number of GPU pages is greater than
    <span>
     blk_nq_nr_phys_segments
    </span>
    .
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    <span>
     dma-map
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    A DMA map error.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6536" rowspan="1" valign="top">
    ops
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    Read
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of Active Read IO in flight.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e6539" rowspan="1" valign="top">
    Write
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e6542" rowspan="1" valign="top">
    Total number of Active Write iO in flight.
   </td>
  </tr>
 </table>
 <h3 id="analyze-per-gpu-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#analyze-per-gpu-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#analyze-per-gpu-stats" id="analyze-per-gpu-stats" name="analyze-per-gpu-stats" shape="rect">
    15.2. Analyze Statistics for each GPU
   </a>
  </a>
 </h3>
 <p>
  You can analyze the statistics for each GPU to better understand what is happening in that GPU.
 </p>
 <p>
  Consider the following example output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>GPU 0000:5e:00:0  uuid:dc87fe99-4d68-247b-b5d2-63f96d2adab1 : pinned_MB=0 cache_MB=0 max_pinned_MB=79
GPU 0000:b7:00:0  uuid:b3a6a195-d08c-09d1-bf8f-a5423c277c04 : pinned_MB=0 cache_MB=0 max_pinned_MB=76
GPU 0000:e7:00:0  uuid:7c432aed-a612-5b18-76e7-402bb48f21db : pinned_MB=0 cache_MB=0 max_pinned_MB=80
GPU 0000:57:00:0  uuid:aa871613-ee53-9a0c-a546-851d1afe4140 : pinned_MB=0 cache_MB=0 max_pinned_MB=80</p>
        </pre>
 <p>
  In this sample output,
  <span>
   0000:5e:00:0
  </span>
  , is the PCI BDF of the GPU with the
  <span>
   Dc87fe99-4d68-247b-b5d2-63f96d2adab1
  </span>
  UUID. This is the same UUID that can be used to observe
  <span>
   nvidia-smi
  </span>
  statistics for this GPU. Here is some additional information about the statistics:
 </p>
 <ul>
  <li>
   <span>
    pinned-MB
   </span>
   shows the active GPU memory that is pinned by using
   <span>
    nvidia_p2p_get_pages
   </span>
   from the GDS driver in MB across all active processes.
  </li>
  <li>
   <span>
    cache_MB
   </span>
   shows the active GPU memory that is pinned by using
   <span>
    nvidia_p2p_get_pages
   </span>
   , but this memory is used as the internal cache by GDS across all active processes.
  </li>
  <li>
   <span>
    max_pinned_MB
   </span>
   shows the max GPU memory that is pinned by GDS at any point in time on this GPU across multiple processes.
   <p>
    This value indicates that the max BAR size and administrator can be used for system sizing purposes.
   </p>
  </li>
 </ul>
 <h3 id="reset-nvidia-fs-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#reset-nvidia-fs-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#reset-nvidia-fs-stats" id="reset-nvidia-fs-stats" name="reset-nvidia-fs-stats" shape="rect">
    15.3. Resetting the nvidia-fs Statistics
   </a>
  </a>
 </h3>
 <p>
  To reset the
  <span>
   nvidia-fs
  </span>
  statistics, run the following commands:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo bash
$ echo 1 &gt;/proc/driver/nvidia-fs/stats</p>
        </pre>
 <h3 id="review-peer-aff-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#review-peer-aff-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#review-peer-aff-stats" id="review-peer-aff-stats" name="review-peer-aff-stats" shape="rect">
    15.4. Checking Peer Affinity Stats for a Kernel Filesystem and Storage Drivers
   </a>
  </a>
 </h3>
 <p>
  The following
  <span>
   proc
  </span>
  files contain information about peer affinity DMA statistics via
  <span>
   nvidia-fs
  </span>
  callbacks:
 </p>
 <ul>
  <li>
   <span>
    nvidia-fs
   </span>
   /
   <span>
    stats
   </span>
  </li>
  <li>
   <span>
    nvidia-fs
   </span>
   /
   <span>
    peer_affinity
   </span>
  </li>
  <li>
   <span>
    nvidia-fs
   </span>
   /
   <span>
    peer_distance
   </span>
  </li>
 </ul>
 <p>
  To enable the statistics, run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ sudo bash
$ echo 1 &gt; /sys/module/nvidia_fs/parameters/peer_stats_enabled</p>
        </pre>
 <p>
  To view consolidated statistics as a regular user, run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/driver/nvidia-fs/stats</p>
        </pre>
 <p>
  Sample output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>GDS Version: 1.0.0.71
NVFS statistics(ver: 4.0)
NVFS Driver(version: 2:7:47)
Mellanox PeerDirect Supported: True
IO stats: Enabled, peer IO stats: Enabled
Logging level: info
 
Active Shadow-Buffer (MiB): 0
Active Process: 0
Reads                           : n=0 ok=0 err=0 readMiB=0 io_state_err=0
Reads                           : Bandwidth(MiB/s)=0 Avg-Latency(usec)=0
Sparse Reads                    : n=6 io=0 holes=0 pages=0
Writes                          : n=0 ok=0 err=0 writeMiB=0 io_state_err=0 pg-cache=0 pg-cache-fail=0 pg-cache-eio=0
Writes                          : Bandwidth(MiB/s)=0 Avg-Latency(usec)=0
Mmap                            : n=183 ok=183 err=0 munmap=183
Bar1-map                        : n=183 ok=183 err=0 free=165 callbacks=18 active=0
Error                           : cpu-gpu-pages=0 sg-ext=0 dma-map=0 dma-ref=0
Ops                             : Read=0 Write=0
GPU 0000:be:00.0  uuid:87e5c586-88ed-583b-df45-fcee0f1e7917 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:e7:00.0  uuid:029faa3b-cb0d-2718-259c-6dc650c636eb : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:5e:00.0  uuid:39eeb04b-1c52-81cc-d76e-53d03eb6ed32 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:57:00.0  uuid:a99a7a93-7801-5711-258b-c6aca4fe6d85 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:39:00.0  uuid:d22b0bc4-cdb1-65ac-7495-3570e5860fda : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:34:00.0  uuid:e11b33d9-60f7-a721-220a-d14e5b15a52c : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=128 cross_root_port(%)=0
GPU 0000:b7:00.0  uuid:e8630cd2-5cb7-cab7-ef2e-66c25507c119 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:e5:00.0  uuid:b3d46477-d54f-c23f-dc12-4eb5ea172af6 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:e0:00.0  uuid:7a10c7bd-07e0-971b-a19c-61e7c185a82c : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:bc:00.0  uuid:bb96783c-5a46-233a-cbce-071aeb308083 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:e2:00.0  uuid:b6565ee8-2100-7009-bcc6-a3809905620d : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=2 cross_root_port(%)=0
GPU 0000:5c:00.0  uuid:5527d7fb-a560-ab42-d027-20aeb5512197 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:59:00.0  uuid:bb734f6b-24ad-2f83-86c3-6ab179bce131 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:3b:00.0  uuid:0ef0b9ee-bb8f-cdae-4535-c0d790b2c663 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:b9:00.0  uuid:ad59f685-5836-c2ea-2c79-3c95bea23f0d : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0
GPU 0000:36:00.0  uuid:fda65234-707b-960a-d577-18c519301848 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=1 cross_root_port(%)=0</p>
        </pre>
 <p>
  The
  <span>
   cross_root_port (%)
  </span>
  port is the percentage of total DMA traffic through
  <span>
   nvidia-fs
  </span>
  callbacks, and this value spans across PCIe root ports between GPU and its peers such as HCA.
 </p>
 <ul>
  <li>
   This can be a major reason for low throughput on certain platforms.
  </li>
  <li>
   This does not consider the DMA traffic that is initiated via
   <span>
    cudaMemcpyDeviceToDevice
   </span>
   or
   <span>
    cuMemcpyPeer
   </span>
   with the specified GPU.
  </li>
 </ul>
 <h3 id="peer-affinity-usage-kernel-fs-sd">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#peer-affinity-usage-kernel-fs-sd">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#peer-affinity-usage-kernel-fs-sd" id="peer-affinity-usage-kernel-fs-sd" name="peer-affinity-usage-kernel-fs-sd" shape="rect">
    15.5. Checking the Peer Affinity Usage for a Kernel File System and Storage Drivers
   </a>
  </a>
 </h3>
 <ol>
  <li>
   To get the peer affinity usage for a kernel file system and storage drivers, run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/driver/nvidia-fs/peer_affinity</p>
        </pre>
  </li>
  <li>
   Review the sample output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>GPU P2P DMA distribution based on pci-distance
 
(last column indicates p2p via root complex)
GPU :0000:bc:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e0:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e5:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:57:00.0 :0 0 524288 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:59:00.0 :0 0 524288 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:be:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:34:00.0 :0 0 1274489 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e7:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:b7:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:36:00.0 :0 0 524288 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:3b:00.0 :0 0 524288 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:39:00.0 :0 0 524288 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:b9:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5c:00.0 :0 0 524288 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e2:00.0 :0 0 39434 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5e:00.0 :0 0 513889 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</p>
        </pre>
  </li>
 </ol>
 <p>
  Each row represents a GPU entry, and the columns indicate the peer ranks in ascending order. The lower the rank, the better the affinity. Each column entry is the total number of DMA transactions that occurred between the specified GPU and the peers that belong to the same rank.
 </p>
 <p>
  For example, the row with
  <span>
   GPU 0000:34:00.0
  </span>
  has
  <span>
   2621440
  </span>
  IO operations through the peer with rank 3. Non-zero values in the last column indicate that the IO is routed through the root complex.
 </p>
 <p>
  Here are some examples:
 </p>
 <p>
  Run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-x.y/gds/samples /mnt/lustre/test 0
$ cat /proc/driver/nvidia-fs/stats</p>
        </pre>
 <p>
  Here is the output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>GDS Version: 1.0.0.71
NVFS statistics(ver: 4.0)
NVFS Driver(version: 2:7:47)
Mellanox PeerDirect Supported: True
IO stats: Enabled, peer IO stats: Enabled
Logging level: info


Active Shadow-Buffer (MB): 0
Active Process: 0
Reads                           : n=0 ok=0 err=0 readmb=0 io_state_err=0
Reads                           : Bandwidth(MB/s)=0 Avg-Latency(usec)=0
Sparse Reads                : n=0 io=0 holes=0 pages=0
Writes                          : n=1 ok=1 err=0 writemb=0 io_state_err=0 pg-cache=0 pg-cache-fail=0 
pg-cache-eio=0
Writes                          : Bandwidth(MB/s)=0 Avg-Latency(usec)=0 
Mmap                            : n=1 ok=1 err=0 munmap=1
Bar1-map                       : n=1 ok=1 err=0 free=1 callbacks=0 active=0
Error                : cpu-gpu-pages=0 sg-ext=0 dma-map=0
Ops                             : Read=0 Write=0
GPU 0000:34:00:0  uuid:98bb4b5c-4576-b996-3d84-4a5d778fa970 : pinned_MB=0 cache_MB=0 max_pinned_MB=0 cross_root_port(%)=100</p>
        </pre>
 <p>
  Run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/driver/nvidia-fs/peer_affinity</p>
        </pre>
 <p>
  Here is the output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>GPU P2P DMA distribution based on pci-distance
 
(last column indicates p2p via root complex)
 
GPU :0000:b7:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:b9:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:bc:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:be:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e0:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e2:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e5:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e7:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:34:00:0 :0 0 0 0 0 0 0 0 0 0 0 2
GPU :0000:36:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:39:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:3b:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:57:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:59:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5c:00:0 :0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5e:00:0 :0 0 0 0 0 0 0 0 0 0 0 0</p>
        </pre>
 <p>
  In the above example, there are DMA transactions between the GPU (
  <span>
   34:00.0
  </span>
  ) and one of its peers. The peer device has the highest possible rank which indicates it is farthest away from the respective GPU pci-distance wise.
 </p>
 <p>
  To check the percentage of traffic, check the
  <span>
   cross_root_ port %
  </span>
  in
  <span>
   /proc/driver/nvidia-fs/stats
  </span>
  . In the third example above, this value is
  <span>
   100%
  </span>
  , which means that the peer-to peer-traffic is happening over QPI links.
 </p>
 <h3 id="gpu-peer-dist-table">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gpu-peer-dist-table">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gpu-peer-dist-table" id="gpu-peer-dist-table" name="gpu-peer-dist-table" shape="rect">
    15.6. Display the GPU-to-Peer Distance Table
   </a>
  </a>
 </h3>
 <p>
  The
  <span>
   peer_distance
  </span>
  table displays the device-wise IO distribution for each peer with its rank for the specified GPU, and it complements the rank-based stats.
 </p>
 <p>
  The peer_distance table displays the device-wise IO distribution for each peer with its rank for the specified GPU. It complements the rank-based stats. The ranking is done in the following order:
 </p>
 <ol>
  <li>
   Primary priority given to p2p distance (upper 2 bytes).
  </li>
  <li>
   Secondary priority is given to the device bandwidth (lower 2 bytes)
  </li>
 </ol>
 <p>
  For peer paths that cross the root port, a fixed cost for p2p distance (127) is added. This is done to induce a preference for paths under one CPU root port relative to paths that cross the CPU root ports. Issue the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/driver/nvidia-fs/peer_distance</p>
        </pre>
 <p>
  Sample output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>gpu             peer            peerrank                p2pdist np2p    link    gen     class
0000:af:00.0    0000:86:00.0    0x820088                0x82    0       0x8     0x3     network
0000:af:00.0    0000:18:00.0    0x820088                0x82    0       0x8     0x3     nvme
0000:af:00.0    0000:86:00.1    0x820088                0x82    0       0x8     0x3     network
0000:af:00.0    0000:19:00.1    0x820088                0x82    0       0x8     0x3     network
0000:af:00.0    0000:87:00.0    0x820088                0x82    0       0x8     0x3     nvme
0000:af:00.0    0000:19:00.0    0x820088                0x82    0       0x8     0x3     network
0000:3b:00.0    0000:86:00.0    0x820088                0x82    0       0x8     0x3     network
0000:3b:00.0    0000:18:00.0    0x820088                0x82    0       0x8     0x3     nvme
0000:3b:00.0    0000:86:00.1    0x820088                0x82    0       0x8     0x3     network
0000:3b:00.0    0000:19:00.1    0x820088                0x82    0       0x8     0x3     network
0000:3b:00.0    0000:87:00.0    0x820088                0x82    0       0x8     0x3     nvme
0000:3b:00.0    0000:19:00.0    0x820088                0x82    0       0x8     0x3     network
0000:5e:00.0    0000:86:00.0    0x820088                0x82    0       0x8     0x3     network
0000:5e:00.0    0000:18:00.0    0x820088                0x82    0       0x8     0x3     nvme
0000:5e:00.0    0000:86:00.1    0x820088                0x82    0       0x8     0x3     network
0000:5e:00.0    0000:19:00.1    0x820088                0x82    0       0x8     0x3     network
0000:5e:00.0    0000:87:00.0    0x820088                0x82    0       0x8     0x3     nvme
0000:5e:00.0    0000:19:00.0    0x820088                0x82    0       0x8     0x3     network
0000:d8:00.0    0000:86:00.0    0x820088                0x82    0       0x8     0x3     network
0000:d8:00.0    0000:18:00.0    0x820088                0x82    0       0x8     0x3     nvme
0000:d8:00.0    0000:86:00.1    0x820088                0x82    0       0x8     0x3     network
0000:d8:00.0    0000:19:00.1    0x820088                0x82    0       0x8     0x3     network
0000:d8:00.0    0000:87:00.0    0x820088                0x82    0       0x8     0x3     nvme
0000:d8:00.0    0000:19:00.0    0x820088                0x82    0       0x8     0x3     network</p>
        </pre>
 <h3 id="gdsio-tool">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gdsio-tool">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gdsio-tool" id="gdsio-tool" name="gdsio-tool" shape="rect">
    15.7. The GDSIO Tool
   </a>
  </a>
 </h3>
 <p>
  GDSIO is a synthetic IO benchmarking tool that uses cufile APIs for IO. The tool can be found in the /usr/local/cuda-x.y/tools directory. For more information about how to use this tool, run $ /usr/local/cuda-x.y/tools/gdsio -h or review the
  gdsio
  section in the/usr/local/cuda-x.y/tools/README file. In the examples below, the files are created on an ext4 file system.
 </p>
 <p>
  Issue the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p># ./gdsio -f /root/sg/test -d 0 -w 4 -s 1M -x 0 -i 4K:32K:1K -I 1</p>
        </pre>
 <p>
  Sample output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 671/1024(KiB) 
IOSize: 4-32-1(KiB) Throughput: 0.044269 GiB/sec, Avg_Latency: 
996.094925 usecs ops: 60 total_time 0.014455 secs</p>
        </pre>
 <p>
  This command does a write IO (-I 1) on a file named test of size 1MiB (-s 1M) with an IO size that varies between 4KiB to 32 KiB in steps of 1KiB (-i 4K:32K:1K). The transfer is performed using GDS (-x 0) using 4 threads (-w 4) on GPU 0 (-d 0). Some additional features of the tool are:
 </p>
 <ul class="ul">
  <li class="li">
   Support for read/write at random offsets in a file.
   <p>
    The gdsio tool provides options to perform a read and write to a file at random offsets.
   </p>
   <ul>
    <li>
     Using -I 2 and -I 3 options does a file read and write operation at random offset respectively but the random offsets are always 4KiB aligned.
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p># ./gdsio -f /root/sg/test -d 0 -w 4 -s 1M -x 0 -i 4K:32K:1K -I 3
IoType: RANDWRITE XferType: GPUD Threads: 4 DataSetSize: 706/1024(KiB) IOSize: 4-32-1(KiB) Throughput: 0.079718 GiB/sec, Avg_Latency: 590.853274 usecs ops: 44 total_time 0.008446 secs</p>
        </pre>
    </li>
    <li>
     To perform a random read and write at unaligned 4KiB offsets, the -U option can be used with -I 0 or -I 1 for read and write, respectively.
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p># ./gdsio -f /root/sg/test -d 0 -w 4 -s 1M -x 0 -i 4K:32K:1K -I 1 -U

IoType: RANDWRITE XferType: GPUD Threads: 4 DataSetSize: 825/1024(KiB) IOSize: 4-32-1(KiB) Throughput: 0.055666 GiB/sec, Avg_Latency: 919.112500 usecs ops: 49 total_time 0.014134 secs</p>
        </pre>
    </li>
    <li>
     Random buffer fill for dedupe and compression.
     <p>
      Using the ‘-R’ option fills the io size buffer (-i) with random data. This random data is then written to the file onto different file offsets.
     </p>
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p># ./gdsio -f /root/sg/test -d 0 -w 4 -s 1M -x 0 -i 4K:32K:1K -I 1 -R

IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 841/1024(KiB) IOSize: 4-32-1(KiB) Throughput: 0.059126 GiB/sec, Avg_Latency: 788.884580 usecs ops: 69 total_time 0.013565 secs</p>
        </pre>
    </li>
    <li>
     Using the
     <span>
      -F
     </span>
     option will fill the entire file with random data.
     Copy
     Copied!
     <pre class="language-" data-line="" id="play">
            
            <p># ./gdsio -f /root/sg/test -d 0 -w 4 -s 1M -x 0 -i 4K:32K:1K -I 1 -F

IoType: WRITE XferType: GPUD Threads: 4 DataSetSize: 922/1024(KiB) IOSize: 4-32-1(KiB) Throughput: 0.024376 GiB/sec, Avg_Latency: 1321.104532 usecs ops: 73 total_time 0.036072 secs</p>
        </pre>
     <p>
      This is useful for file systems that use dedupe and compression algorithms to minimize disk access. Using random data increases the probability that these file systems will hit the backend disk more often.
     </p>
    </li>
   </ul>
  </li>
  <li class="li">
   Variable block size.
   <p>
    To perform a read or a write on a file, you can specify the block size (- i), which says that IO would be performed in chunks of block sized lengths. To check the stats for what block sizes are used use the gds_stats tool. Ensure the the /etc/cufile.json file has cufile_stats is set to 3:
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./gds_stats -p &lt;pid of the gdsio process&gt; -l 3</p>
        </pre>
   <p>
    Sample output:
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>0-4(KiB): 0  0 
4-8(KiB): 0 17205
8-16(KiB): 0 45859 16-32(KiB): 0 40125
32-64(KiB): 0  0
64-128(KiB): 0  0
128-256(KiB): 0  0
256-512(KiB): 0  0
512-1024(KiB): 0  0
1024-2048(KiB): 0  0
2048-4096(KiB): 0  0
4096-8192(KiB): 0  0
8192-16384(KiB): 0  0
16384-32768(KiB): 0  0
32768-65536(KiB): 0  0
65536-...(KiB): 0  0</p>
        </pre>
   <p>
    The highlighted counters show that, for the command above, the block sizes that are used for file IO are in the 4-32 KiB range.
   </p>
  </li>
  <li class="li">
   Verification mode usage and limitations. To ensure data integrity, there is an option to perform IO in a Write and Read in verify mode using the
   <span>
    -V
   </span>
   option. Here is an example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./gdsio -V -f /root/sg/test -d 0 -w 1 -s 2G -o 0 -x 0 -k 0 -i 4K:32K:1K -I 1
IoType: WRITE XferType: GPUD Threads: 1 DataSetSize: 2097144/2097152(KiB) IOSize: 4-32-1(KiB) Throughput: 0.074048 GiB/sec, Avg_Latency: 231.812570 usecs ops: 116513 total_time 27.009349 secs
Verifying data
IoType: READ XferType: GPUD Threads: 1 DataSetSize: 2097144/2097152(KiB) IOSize: 4-32-1(KiB) Throughput: 0.103465 GiB/sec, Avg_Latency: 165.900663 usecs ops: 116513 total_time 19.330184 secs</p>
        </pre>
   <p>
    The command mentioned above will perform a write followed by a read verify test.
   </p>
   <p>
    While using the verify mode, remember the following points:
   </p>
   <ul>
    <li>
     read test (-I 0) with verify option (
     <span>
      -V
     </span>
     ) should be used with files written (-I 1) with the
     <span>
      -V
     </span>
     option
    </li>
    <li>
     read test (-I 2) with verify option (
     <span>
      -V
     </span>
     ) should be used with files written (-I 3) with the
     <span>
      -V
     </span>
     option and using same random seed (
     <span>
      -k
     </span>
     ) using same number of threads, offset, and data size
    </li>
    <li>
     write test (-I 1/3) with verify option (
     <span>
      -V
     </span>
     ) will perform writes followed by read.
    </li>
    <li>
     Verify mode cannot be used in timed mode (
     <span>
      -T
     </span>
     option).
     <p>
      If Verify mode is used in a timed mode, it will be ignored.
     </p>
    </li>
   </ul>
  </li>
  <li class="li">
   The configuration file
   <p>
    GDSIO has an option to configure the parameters that are needed to perform an IO in a configuration file and run the IO using those configurations. The configuration file gives the option of performing multiple jobs, where each job has some different configurations.
   </p>
   <p>
    The configuration file has global parameters and job specific parameter support. For example, with a configuration file, you can configure each job to perform on a GPU and with a different number of threads. The global parameters, such as IO Size and transfer mode, remain the same for each job. For more information, refer to /usr/local/cuda-x.y/tools/README and /usr/local/cuda-x.y/tools/rw-sample.gdsio files. After configuring the parameters, to perform the IO operation using the configuration file, run the following command:
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./gdsio &lt;config file name&gt;</p>
        </pre>
  </li>
 </ul>
 <p>
  See
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tab-fields" shape="rect" title="The following table describes the tabulated fields in the output of the #./gdsio &lt;config file name&gt; command.">
   Tabulated Fields
  </a>
  for a list of the tabulated fields.
 </p>
 <h3 id="tab-fields">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tab-fields">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tab-fields" id="tab-fields" name="tab-fields" shape="rect">
    15.8. Tabulated Fields
   </a>
  </a>
 </h3>
 <p>
  The following table describes the tabulated fields in the output of the
  <span>
   #./gdsio &lt;config file name&gt;
  </span>
  command.
 </p>
 <a class="Link" data-cms-ai="0" id="tab-fields__section_bcx_d5z_gnb" name="tab-fields__section_bcx_d5z_gnb" shape="rect">
 </a>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" id="tab-fields__table_xx1_g5z_gnb" rules="all" summary="">
  Table 7. Tabulated Fields
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e7591" rowspan="1" valign="top">
    Global Option
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e7594" rowspan="1" valign="top">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     xfer_type
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    GDSIO Transfer types:
    <a class="Link" data-cms-ai="0" id="tab-fields__ul_p2l_j5z_gnb" name="tab-fields__ul_p2l_j5z_gnb" shape="rect">
    </a>
    <ul>
     <li>
      0 : Storage-&gt;GPU
     </li>
     <li>
      1 : Storage-&gt;CPU
     </li>
     <li>
      2 : Storage-&gt;CPU-&gt;GPU
     </li>
     <li>
      3 : Storage-&gt;CPU-&gt;GPU_ASYNC
     </li>
     <li>
      4 : Storage-&gt;PAGE_CACHE-&gt;CPU-&gt;GPU
     </li>
     <li>
      5 : Storage-&gt;GPU_ASYNC_STREAM
     </li>
     <li>
      6: Storage-&gt;GPU_BATCH
     </li>
     <li>
      7: Storage-&gt;GPU_BATCH_STREAM
     </li>
    </ul>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     rw
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    IO type, rw=read, rw=write, rw=randread, rw=randwrite
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     bs
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    block size, for example, bs=1M, for variable block size can specify range, for example, bs=1M:4M:1M, (1M : start block size, 4M : end block size, 1M :steps in which size is varied).
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     size
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    File-size, for example, size=2G.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     runtime
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Duration in seconds.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     do_verify
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Use 1 for enabling verification
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     skip_bufregister
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Skip cufile buffer registration, ignored in cpu mode.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     enable_nvlinks
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    <p>
     Set up NVlinks.
    </p>
    <p>
     This field is recommended if p2p traffic is cross node.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     random_seed
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Use random seed, for example, 1234.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     refill_buffer
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Refill io buffer after every write.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     fill_random
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Fill request buffer with random data.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     unaligned_random
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Use random offsets which are not page-aligned.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     start_offset
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    File offset to start read/write from.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    Per-Job Options
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Description
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     numa_node
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    NUMA node.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     gpu_dev_id
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    GPU device index (check nvidia-smi).
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     num_threads
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Number of IO Threads per job.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     directory
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Directory name where files are present. Each thread will work on a per file basis.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     filename
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Filename for single file mode, where threads share the same file. (Note: directory mode and filemode should not be used in a mixed manner across jobs).
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     mem_type
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    Memory types to be used. Supported values: 0 - (
    <span>
     cudaMalloc
    </span>
    ), 1 - (
    <span>
     cuMem
    </span>
    ), 2 - (
    <span>
     cudaMallocHost
    </span>
    ) 3 - malloc 4 - mmap.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e7591" rowspan="1" valign="top">
    <span>
     fd_type
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e7594" rowspan="1" valign="top">
    File Descriptor mode.
    <p>
     0 - O_DIRECT (default)
    </p>
    <p>
     1 - non-O_DIRECT
    </p>
   </td>
  </tr>
 </table>
 <h3 id="gdscheck">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gdscheck">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gdscheck" id="gdscheck" name="gdscheck" shape="rect">
    15.9. The GDSCHECK Tool
   </a>
  </a>
 </h3>
 <p>
  The
  <span>
   /usr/local/cuda-x.y/tools/gdscheck.py
  </span>
  tool is used to perform a GDS platform check and has other options that can be found by using
  <span>
   -h
  </span>
  option.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ ./gdscheck.py -h
usage: gdscheck.py [-h] [-p] [-f FILE] [-v] [-V]
GPUDirectStorage platform checker
optional arguments:
  -h, --help  show this help message and exit
  -p          gds platform check
  -f FILE     gds file check
  -v          gds version checks
  -V          gds fs checks</p>
        </pre>
 <p>
  To perform a GDS platform check, issue the following command and expect the output in the following format:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p># ./gdscheck.py -p
GDS release version: 1.0.0.78
 nvidia_fs version:  2.7 libcufile version: 2.4
 ============
 ENVIRONMENT:
 ============
 =====================
 DRIVER CONFIGURATION:
 =====================
 NVMe               : Supported
 NVMeOF             : Unsupported
 SCSI               : Unsupported
 ScaleFlux CSD      : Unsupported
 NVMesh             : Unsupported
 DDN EXAScaler      : Supported
 IBM Spectrum Scale : Unsupported
 NFS                : Unsupported
 WekaFS             : Unsupported
 Userspace RDMA     : Unsupported
 --Mellanox PeerDirect : Enabled
 --rdma library        : Not Loaded (libcufile_rdma.so)
 --rdma devices        : Not configured
 --rdma_device_status  : Up: 0 Down: 0
 =====================
 CUFILE CONFIGURATION:
 =====================
 properties.use_compat_mode : true
 properties.gds_rdma_write_support : true
 properties.use_poll_mode : false
 properties.poll_mode_max_size_kb : 4
 properties.max_batch_io_timeout_msecs : 5
 properties.max_direct_io_size_kb : 16384
 properties.max_device_cache_size_kb : 131072
 properties.max_device_pinned_mem_size_kb : 33554432
 properties.posix_pool_slab_size_kb : 4 1024 16384
 properties.posix_pool_slab_count : 128 64 32
 properties.rdma_peer_affinity_policy : RoundRobin
 properties.rdma_dynamic_routing : 0
 fs.generic.posix_unaligned_writes : false
 fs.lustre.posix_gds_min_kb: 0
 fs.weka.rdma_write_support: false
 profile.nvtx : false
 profile.cufile_stats : 0
 miscellaneous.api_check_aggressive : false
 =========
 GPU INFO:
 =========
 GPU index 0 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 1 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 2 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 3 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 4 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 5 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 6 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 7 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 8 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 9 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 10 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 11 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 12 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 13 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 14 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 GPU index 15 Tesla V100-SXM3-32GB bar:1 bar size (MiB):32768 supports GDS
 ==============
 PLATFORM INFO:
 ==============
 IOMMU: disabled
 Platform verification succeeded</p>
        </pre>
 <h3 id="nfs-suppport-gds">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nfs-suppport-gds">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nfs-suppport-gds" id="nfs-suppport-gds" name="nfs-suppport-gds" shape="rect">
    15.10. NFS Support with GPUDirect Storage
   </a>
  </a>
 </h3>
 <p>
  This section provides information about NFS support with GDS.
 </p>
 <h3 id="install-nfs-server-rdma-mofed_5-1">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-nfs-server-rdma-mofed_5-1">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-nfs-server-rdma-mofed_5-1" id="install-nfs-server-rdma-mofed_5-1" name="install-nfs-server-rdma-mofed_5-1" shape="rect">
    15.10.1. Install Linux NFS server with RDMA Support on MLNX_OFED 5.3 or Later
   </a>
  </a>
 </h3>
 <p>
  To install a standard Linux kernel-based NFS server with RDMA support, complete the following steps:
 </p>
 Note:
 <p>
  The server must have a Mellanox connect-X4/5 NIC with MLNX_OFED 5.3 or later installed.
 </p>
 <ol>
  <li>
   Issue the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ ofed_info -s MLNX_OFED_LINUX-5.3-1.0.5.1:</p>
        </pre>
  </li>
  <li>
   Review the output to ensure that the server was installed.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ sudo apt-get install nfs-kernel-server
$ mkfs.ext4 /dev/nvme0n1
$ mount -o data=ordered /dev/nvme0n1 /mnt/nvme
$ cat /etc/exports
  /mnt/nvme *(rw,async,insecure,no_root_squash,no_subtree_check)
$ service nfs-kernel-server restart
$ modprobe rpcrdma
$ echo rdma 20049 &gt; /proc/fs/nfsd/portlist</p>
        </pre>
  </li>
 </ol>
 <h3 id="install-gds-supp-nfs-client">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-gds-supp-nfs-client">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-gds-supp-nfs-client" id="install-gds-supp-nfs-client" name="install-gds-supp-nfs-client" shape="rect">
    15.10.2. Install GPUDirect Storage Support for the NFS Client
   </a>
  </a>
 </h3>
 <p>
  To install a NFS client with GDS support complete the following steps:
 </p>
 Note:
 <p>
  The client must have a Mellanox connect-X4/5 NIC with MLNX_OFED 5.3 or later installed.
 </p>
 <ol>
  <li>
   Issue the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ ofed_info -s MLNX_OFED_LINUX-5.3-1.0.5.0:</p>
        </pre>
  </li>
  <li>
   Review the output to ensure that the support exists.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ sudo apt-get install nfs-common
$ modprobe rpcrdma
$ mkdir -p /mnt/nfs_rdma_gds
$ sudo mount -v -o proto=rdma,port=20049,vers=3 172.16.0.101:/ /mnt/nfs_rdma_gds
To mount with nconnect using VAST nfs client package:
Eg: client IB interfaces 172.16.0.17 , 172.16.0.18, 172.16.0.19, 172.16.0.20, 172.16.0.21,172.16.0.22,172.16.0.23 172.16.0.24
$ sudo mount -v -o proto=rdma,port=20049,vers=3,nconnect=20,localports=172.16.0.17-172.16.0.24,remoteports=172.16.0.101-172.16.0.120 172.16.0.101:/ /mnt/nfs_rdma_gds</p>
        </pre>
  </li>
 </ol>
 <h3 id="nfs-gds-stas-debug">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nfs-gds-stas-debug">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nfs-gds-stas-debug" id="nfs-gds-stas-debug" name="nfs-gds-stas-debug" shape="rect">
    15.11. NFS GPUDirect Storage Statistics and Debugging
   </a>
  </a>
 </h3>
 <p>
  NFS IO can be observed using regular Linux tools that are used for monitoring IO, such as
  <span>
   iotop
  </span>
  and
  <span>
   nfsstat
  </span>
  .
 </p>
 <ul>
  <li>
   To enable NFS RPC stats debugging, run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ rpcdebug -v</p>
        </pre>
  </li>
  <li>
   To observer GDS-related IO stats, run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/driver/nvidia-fs/stats</p>
        </pre>
  </li>
  <li>
   To determine GDS statistics per process, run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-x.y/tools/gds_stats -p &lt;PID&gt; -l 3</p>
        </pre>
  </li>
 </ul>
 <h3 id="gds-io-behavior">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-io-behavior">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-io-behavior" id="gds-io-behavior" name="gds-io-behavior" shape="rect">
    15.12. GPUDirect Storage IO Behavior
   </a>
  </a>
 </h3>
 <p>
  This section provides information about IO behavior in GDS.
 </p>
 <h3 id="read-write-atomiticity-cons">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#read-write-atomiticity-cons">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#read-write-atomiticity-cons" id="read-write-atomiticity-cons" name="read-write-atomiticity-cons" shape="rect">
    15.12.1. Read/Write Atomicity Consistency with GPUDirect Storage Direct IO
   </a>
  </a>
 </h3>
 <p>
  In GDS, the
  <span>
   max_direct_io_size_kb
  </span>
  property controls the IO unit size in which the limitation is issued to the underlying file system. By default, this value is 16MB. This implies that from a Linux VFS perspective, the atomicity of size is limited to the
  <span>
   max_direct_io_size_kb
  </span>
  size and not the original request size. This limitation exists in the standard GDS path and in compatible mode.
 </p>
 <h3 id="write-file-in-o-append-mode">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#write-file-in-o-append-mode">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#write-file-in-o-append-mode" id="write-file-in-o-append-mode" name="write-file-in-o-append-mode" shape="rect">
    15.12.2. Write with File a Opened in O_APPEND Mode (cuFileWrite)
   </a>
  </a>
 </h3>
 <p>
  For a file that is opened in O_APPEND mode with concurrent writers, if the IO size that is used is larger than the
  <span>
   max_direct_io_size_kb
  </span>
  property, because of the write atomicity limitations, the file might have interleaved data from multiple writers. This cannot be prevented even if the underlying file-system has locking guarantees.
 </p>
 <h3 id="gpu-nic-peer-aff">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gpu-nic-peer-aff">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gpu-nic-peer-aff" id="gpu-nic-peer-aff" name="gpu-nic-peer-aff" shape="rect">
    15.12.3. GPU to NIC Peer Affinity
   </a>
  </a>
 </h3>
 <p>
  The library maintains a peer affinity table that is a pci-distance-based ranking for a GPU and the available NICs in the platform for RDMA. Currently, the limitation in the ranking does not consider NUMA attributes for the NICs. For a NIC that does not share a common root port with a GPU, the P2P traffic might get routed cross socket over QPI links even if there is a NIC that resides on the same CPU socket as the GPU.
 </p>
 <h3 id="comp-mode-unreg-buffers">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#comp-mode-unreg-buffers">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#comp-mode-unreg-buffers" id="comp-mode-unreg-buffers" name="comp-mode-unreg-buffers" shape="rect">
    15.12.4. Compatible Mode with Unregistered Buffers
   </a>
  </a>
 </h3>
 <p>
  Currently, in compatible mode, the IO path with non-registered buffers does not have optimal performance and does buffer allocation and deallocation in every cuFileRead or cuFileWrite.
 </p>
 <h3 id="unaligned-writes-non-reg-buff">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unaligned-writes-non-reg-buff">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unaligned-writes-non-reg-buff" id="unaligned-writes-non-reg-buff" name="unaligned-writes-non-reg-buff" shape="rect">
    15.12.5. Unaligned writes with Non-Registered Buffers
   </a>
  </a>
 </h3>
 <p>
  For unaligned writes, using unregistered buffers performance may not be optimal as compared to registered buffers.
 </p>
 <h3 id="process-hang-nfs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#process-hang-nfs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#process-hang-nfs" id="process-hang-nfs" name="process-hang-nfs" shape="rect">
    15.12.6. Process Hang with NFS
   </a>
  </a>
 </h3>
 <p>
  A process hang is observed in NFS environments when the application crashes.
 </p>
 <h3 id="tool-limit-cuda-9">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tool-limit-cuda-9">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tool-limit-cuda-9" id="tool-limit-cuda-9" name="tool-limit-cuda-9" shape="rect">
    15.12.7. Tools Support Limitations for CUDA 9 and Earlier
   </a>
  </a>
 </h3>
 <p>
  The gdsio binary has been built against CUDA runtime 10.1 and has a dependency on the CUDA runtime environment to be equal to version 10.1 or later. Otherwise, a driver dependency error will be reported by the tool.
 </p>
 <h3 id="dynamic-routing-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-stats" id="dynamic-routing-stats" name="dynamic-routing-stats" shape="rect">
    15.13. GDS Statistics for Dynamic Routing
   </a>
  </a>
 </h3>
 <p>
  Dynamic Routing decisions are performed at I/O operation granularity. The GDS User-space Statistics contain a per-GPU counter to indicate the number of I/Os that have been routed using Dynamic Routing.
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 8. cuFile Dynamic Routing Counter
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e8195" rowspan="1" valign="top" width="22.727272727272727%">
    Entry
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e8198" rowspan="1" valign="top" width="77.27272727272727%">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e8195" rowspan="1" valign="top" width="22.727272727272727%">
    <span>
     dr
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e8198" rowspan="1" valign="top" width="77.27272727272727%">
    Number of
    <span>
     cuFileRead
    </span>
    /
    <span>
     cuFileWrite
    </span>
    for which I/O was routed using Dynamic Routing for a given GPU.
   </td>
  </tr>
 </table>
 <p>
  There are existing counters in the
  <span>
   PER_GPU POOL BUFFER STATS
  </span>
  and
  <span>
   PER_GPU POSIX POOL BUFFER STATS
  </span>
  from which a user can infer the GPUs that are chosen by dynamic routing for use as the bounce buffers.
 </p>
 <p>
  a) Platform has GPUs (0 and 1) not sharing the same PCIe host bridge as the NICs:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>"rdma_dev_addr_list": [ "192.168.0.12", "192.168.1.12" ],
"rdma_dynamic_routing": true,
"rdma_dynamic_routing_order": [ "GPU_MEM_NVLINKS", "GPU_MEM", "SYS_MEM" ]</p>
        </pre>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ gds_stats -p &lt;process id&gt; -l 3</p>
        </pre>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>GPU 0 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 dr=0 r_sparse=0 r_inline=0 err=0 MiB=0 Write: bw=3.37598 util(%)=532 n=6629 posix=0 unalign=0 dr=6629 err=0 MiB=6629 BufRegister: n=4 err=0 free=0 MiB=4
GPU 1 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 dr=0 r_sparse=0 r_inline=0 err=0 MiB=0 Write: bw=3.29297 util(%)=523 n=6637 posix=0 unalign=0 dr=6637 err=0 MiB=6637 BufRegister: n=4 err=0 free=0 MiB=4

PER_GPU POOL BUFFER STATS:
GPU : 6 pool_size_MiB : 7 usage : 1/7 used_MiB : 1
GPU : 7 pool_size_MiB : 7 usage : 0/7 used_MiB : 0
GPU : 8 pool_size_MiB : 7 usage : 2/7 used_MiB : 2
GPU : 9 pool_size_MiB : 7 usage : 2/7 used_MiB : 2

PER_GPU POSIX POOL BUFFER STATS:

PER_GPU RDMA STATS:
GPU 0000:34:00.0 :   mlx5_3(138:48):0  mlx5_6(265:48):0
GPU 0000:36:00.0 :   mlx5_3(138:48):0  mlx5_6(265:48):0
GPU 0000:39:00.0 :   mlx5_3(138:48):0  mlx5_6(265:48):0
GPU 0000:3b:00.0 :   mlx5_3(138:48):0  mlx5_6(265:48):0
GPU 0000:57:00.0 :   mlx5_3(7:48):0  mlx5_6(265:48):0
GPU 0000:59:00.0 :   mlx5_3(7:48):0  mlx5_6(265:48):0
GPU 0000:5c:00.0 :   mlx5_3(3:48):3318  mlx5_6(265:48):0
GPU 0000:5e:00.0 :   mlx5_3(3:48):3318  mlx5_6(265:48):0
GPU 0000:b7:00.0 :   mlx5_6(3:48):3316  mlx5_3(265:48):0
GPU 0000:b9:00.0 :   mlx5_6(3:48):3317  mlx5_3(265:48):0
GPU 0000:bc:00.0 :   mlx5_6(7:48):0  mlx5_3(265:48):0
GPU 0000:be:00.0 :   mlx5_6(7:48):0  mlx5_3(265:48):0
GPU 0000:e0:00.0 :   mlx5_6(138:48):0  mlx5_3(265:48):0
GPU 0000:e2:00.0 :   mlx5_6(138:48):0  mlx5_3(265:48):0
GPU 0000:e5:00.0 :   mlx5_6(138:48):0  mlx5_3(265:48):0
GPU 0000:e7:00.0 :   mlx5_6(138:48):0  mlx5_3(265:48):0</p>
        </pre>
 <p>
  b) Platform configuration that has no GPUs sharing the same PCIe host bridge as the NICs and no NVLinks between the GPUs. For such configurations, an admin can set a policy to use system memory other than the default P2P policy.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>"rdma_dev_addr_list": [ "192.168.0.12", "192.168.1.12" ],
"rdma_dynamic_routing": true,
"rdma_dynamic_routing_order": [ "SYS_MEM" ]</p>
        </pre>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>PER_GPU STATS:
GPU 4 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 r_sparse=0 r_inline=0 err=0 MiB=0 Write: bw=1.11GiB util(%)=0 n=1023 posix=1023 unalign=1023 dr=1023 err=0 MiB=1023 BufRegister: n=0 err=0 free=0 MiB=0
GPU 8 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 r_sparse=0 r_inline=0 err=0 MiB=0 Write: bw=1.11GiB util(%)=0 n=1023 posix=1023 unalign=1023 dr=1023 err=0 MiB=1023 BufRegister: n=0 err=0 free=0 MiB=0
PER_GPU POSIX POOL BUFFER STATS:
GPU 4 4(KiB) :0/0 1024(KiB) :0/1 16384(KiB) :0/0
GPU 8 4(KiB) :0/0 1024(KiB) :1/1 16384(KiB) :0/0</p>
        </pre>
 <h3 id="dynamic-routing-peer-affinity">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-peer-affinity">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-peer-affinity" id="dynamic-routing-peer-affinity" name="dynamic-routing-peer-affinity" shape="rect">
    15.13.1. Peer Affinity Dynamic Routing
   </a>
  </a>
 </h3>
 <p>
  Dynamic Routing decisions are performed at I/O operation granularity. The GDS User-space Statistics contain a per-GPU counter to indicate the number of I/Os that have been routed using Dynamic Routing.
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 9. cuFile Dynamic Routing Counter
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e8295" rowspan="1" valign="top" width="22.727272727272727%">
    Entry
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e8298" rowspan="1" valign="top" width="77.27272727272727%">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e8295" rowspan="1" valign="top" width="22.727272727272727%">
    <span>
     dr
    </span>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e8298" rowspan="1" valign="top" width="77.27272727272727%">
    Number of
    <span>
     cuFileRead
    </span>
    /
    <span>
     cuFileWrite
    </span>
    for which I/O was routed using Dynamic Routing for a given GPU.
   </td>
  </tr>
 </table>
 <p>
  There are existing counters in the
  <span>
   PER_GPU POOL BUFFER STATS
  </span>
  and
  <span>
   PER_GPU POSIX POOL BUFFER STATS
  </span>
  from which a user can infer the GPUs that are chosen by dynamic routing for use as the bounce buffers.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>// "rdma_dev_addr_list": [ "192.168.4.12", "192.168.5.12", "192.168.6.12", "192.168.7.12" ],

cufile.log:
--------------
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:133 Computing GPU-&gt;NIC affinity table:
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:34:00.0  RDMA dev: mlx5_6 mlx5_8 mlx5_7 mlx5_9
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:36:00.0  RDMA dev: mlx5_6 mlx5_8 mlx5_7 mlx5_9
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:39:00.0  RDMA dev: mlx5_6 mlx5_8 mlx5_7 mlx5_9
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:3b:00.0  RDMA dev: mlx5_6 mlx5_8 mlx5_7 mlx5_9
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:57:00.0  RDMA dev: mlx5_6 mlx5_8 mlx5_7 mlx5_9
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:59:00.0  RDMA dev: mlx5_6 mlx5_8 mlx5_7 mlx5_9
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:5c:00.0  RDMA dev: mlx5_6 mlx5_8 mlx5_7 mlx5_9
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:5e:00.0  RDMA dev: mlx5_6 mlx5_8 mlx5_7 mlx5_9
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:b7:00.0  RDMA dev: mlx5_6
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:b9:00.0  RDMA dev: mlx5_6
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:bc:00.0  RDMA dev: mlx5_7
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:be:00.0  RDMA dev: mlx5_7
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:e0:00.0  RDMA dev: mlx5_8
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:e2:00.0  RDMA dev: mlx5_8
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:e5:00.0  RDMA dev: mlx5_9
 23-02-2021 10:17:49:641 [pid=22436 tid=22436] INFO   curdma-ldbal:139 GPU: 0000:e7:00.0  RDMA dev: mlx5_9</p>
        </pre>
 <p>
  A sample from gds_stats showing the GPU to NIC binding during a sample IO test:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>PER_GPU RDMA STATS:
   GPU 0000:34:00.0 :   mlx5_6(265:48):0  mlx5_8(265:48):0  mlx5_7(265:48):0  mlx5_9(265:48):0
   GPU 0000:36:00.0 :   mlx5_6(265:48):0  mlx5_8(265:48):0  mlx5_7(265:48):0  mlx5_9(265:48):0
   GPU 0000:39:00.0 :   mlx5_6(265:48):0  mlx5_8(265:48):0  mlx5_7(265:48):0  mlx5_9(265:48):0
   GPU 0000:3b:00.0 :   mlx5_6(265:48):0  mlx5_8(265:48):0  mlx5_7(265:48):0  mlx5_9(265:48):0
   GPU 0000:57:00.0 :   mlx5_6(265:48):0  mlx5_8(265:48):0  mlx5_7(265:48):0  mlx5_9(265:48):0
   GPU 0000:59:00.0 :   mlx5_6(265:48):0  mlx5_8(265:48):0  mlx5_7(265:48):0  mlx5_9(265:48):0
   GPU 0000:5c:00.0 :   mlx5_6(265:48):0  mlx5_8(265:48):0  mlx5_7(265:48):0  mlx5_9(265:48):0
   GPU 0000:5e:00.0 :   mlx5_6(265:48):0  mlx5_8(265:48):0  mlx5_7(265:48):0  mlx5_9(265:48):0
   GPU 0000:b7:00.0 :   mlx5_6(3:48):22918  mlx5_7(7:48):0  mlx5_8(138:48):0  mlx5_9(138:48):0
   GPU 0000:b9:00.0 :   mlx5_6(3:48):22949  mlx5_7(7:48):0  mlx5_8(138:48):0  mlx5_9(138:48):0
   GPU 0000:bc:00.0 :   mlx5_7(3:48):22945  mlx5_6(7:48):0  mlx5_8(138:48):0  mlx5_9(138:48):0
   GPU 0000:be:00.0 :   mlx5_7(3:48):22942  mlx5_6(7:48):0  mlx5_8(138:48):0  mlx5_9(138:48):0
   GPU 0000:e0:00.0 :   mlx5_8(3:48):22937  mlx5_9(7:48):0  mlx5_6(138:48):0  mlx5_7(138:48):0
   GPU 0000:e2:00.0 :   mlx5_8(3:48):22930  mlx5_9(7:48):0  mlx5_6(138:48):0  mlx5_7(138:48):0
   GPU 0000:e5:00.0 :   mlx5_9(3:48):22922  mlx5_8(7:48):0  mlx5_6(138:48):0  mlx5_7(138:48):0
   GPU 0000:e7:00.0 :   mlx5_9(3:48):22920  mlx5_8(7:48):0  mlx5_6(138:48):0  mlx5_7(138:48):0</p>
        </pre>
 <ul>
  <li>
   For kernel-based DFS, DDN-Lustre and VAST-NFS, nvidia-fs driver provides a callback to determine the best NIC given a target GPU. The nvidia-fs peer_affinity can be used to track end-to-end IO affinity behavior.
   <p>
    For example, with a routing policy of
    “GPU_MEM_NVLINK”
    , one should not see cross-port traffic as shown in the statistics snippet below:
   </p>
  </li>
 </ul>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ cat /proc/driver/nvidia-fs/peer_affinity
GPU P2P DMA distribution based on pci-distance

(last column indicates p2p via root complex)
GPU :0000:bc:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e0:00.0 :0 0 205305577 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e5:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:57:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:59:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:be:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:34:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e7:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:b7:00.0 :0 0 205279892 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:36:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:3b:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:39:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:b9:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5c:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e2:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5e:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</p>
        </pre>
 <p>
  With routing policy of
  P2P
  , one can expect to see cross-port traffic as shown in the following statistics snippet:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgxuser@e155j-dgx2-c6-u04:~/ssen$ cat /proc/driver/nvidia-fs/peer_affinity
GPU P2P DMA distribution based on pci-distance

(last column indicates p2p via root complex)
GPU :0000:bc:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e0:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e5:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:57:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:59:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:be:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:34:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9186359
GPU :0000:e7:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:b7:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:36:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9191164
GPU :0000:3b:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9194318
GPU :0000:39:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 9188836
GPU :0000:b9:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5c:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:e2:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
GPU :0000:5e:00.0 :0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0</p>
        </pre>
 <h3 id="dynamic-routing-cufile-log">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-cufile-log">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-cufile-log" id="dynamic-routing-cufile-log" name="dynamic-routing-cufile-log" shape="rect">
    15.13.2. cuFile Log Related to Dynamic Routing
   </a>
  </a>
 </h3>
 <p>
  The following log shows the routing table with possible GPUS to be used for IP addresses:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>/"rdma_dev_addr_list": [ "192.168.0.12", "192.168.1.12", "192.168.2.12", "192.168.3.12", "192.168.4.12", "192.168.5.12", "192.168.6.12", "192.168.7.12" ],

 22-02-2021 21:16:27:776 [pid=90794 tid=90794] INFO   cufio-route:141 Computing NIC-&gt;GPU affinity table for rdma devices available in config:
 22-02-2021 21:16:27:776 [pid=90794 tid=90794] INFO   cufio-route:156 netdev:ib3 bdf:0000:5d:00.0 ip: 192.168.3.12 best gpus: 6 7 4 5
 22-02-2021 21:16:27:776 [pid=90794 tid=90794] INFO   cufio-route:156 netdev:ib9 bdf:0000:e6:00.0 ip: 192.168.5.12 best gpus: 14 15 12 13
 22-02-2021 21:16:27:776 [pid=90794 tid=90794] INFO   cufio-route:156 netdev:ib2 bdf:0000:58:00.0 ip: 192.168.2.12 best gpus: 4 5 6 7
 22-02-2021 21:16:27:776 [pid=90794 tid=90794] INFO   cufio-route:156 netdev:ib6 bdf:0000:b8:00.0 ip: 192.168.6.12 best gpus: 8 9 10 11
 22-02-2021 21:16:27:776 [pid=90794 tid=90794] INFO   cufio-route:156 netdev:ib1 bdf:0000:3a:00.0 ip: 192.168.1.12 best gpus: 3 2 0 1
 22-02-2021 21:16:27:776 [pid=90794 tid=90794] INFO   cufio-route:156 netdev:ib8 bdf:0000:e1:00.0 ip: 192.168.4.12 best gpus: 12 13 14 15
 22-02-2021 21:16:27:776 [pid=90794 tid=90794] INFO   cufio-route:156 netdev:ib0 bdf:0000:35:00.0 ip: 192.168.0.12 best gpus: 0 1 3 2
 22-02-2021 21:16:27:776 [pid=90794 tid=90794] INFO   cufio-route:156 netdev:ib7 bdf:0000:bd:00.0 ip: 192.168.7.12 best gpus: 10 11 8 9

22-02-2021 21:16:27:776 [pid=90794 tid=90794] DEBUG  cufio:1218 target gpu: 4 best gpu: 4 selected based on dynamic routing</p>
        </pre>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-lib-tracing">
   16. GDS Library Tracing
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="gds-lib-tracing" name="gds-lib-tracing" shape="rect">
  </a>
  The GDS Library has USDT (static tracepoints), which can be used with Linux tools such as
  <span>
   lttng
  </span>
  ,
  <span>
   bcc
  </span>
  /
  <span>
   bpf
  </span>
  ,
  <span>
   perf
  </span>
  . This section assumes familiarity with these tools.
 </p>
 <p>
  The examples in this section show tracing by using the
  <span>
   <a class="Link" data-cms-ai="0" href="https://github.com/iovisor/bcc">
    bcc
   </a>
  </span>
  <a class="Link" data-cms-ai="0" href="https://github.com/iovisor/bcc">
   /
  </a>
  <span>
   <a class="Link" data-cms-ai="0" href="https://github.com/iovisor/bcc">
    bpf
   </a>
  </span>
  tracing facility. GDS does not ship these tracing tools. Refer to
  <a class="xref" data-cms-ai="0" href="https://github.com/iovisor/bcc/blob/master/INSTALL.md" shape="rect">
   Installing BCC
  </a>
  for more information about installing bcc/bpf tools. Users must have root privileges to install.
 </p>
 Note:
 <p>
  The user must also have
  <span>
   sudo
  </span>
  access to use these tools.
 </p>
 <h3 id="display-tracepoints">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#display-tracepoints">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#display-tracepoints" id="display-tracepoints" name="display-tracepoints" shape="rect">
    16.1. Example: Display Tracepoints
   </a>
  </a>
 </h3>
 <ol>
  <li>
   To display tracepoints, run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./tplist -l /usr/local/gds/lib/libcufile.so</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>/usr/local/cuda-x.y/lib/libcufile.so cufio:cufio_px_read
/usr/local/cuda-x.y/lib/libcufile.so cufio:cufio_rdma_read
/usr/local/cuda-x.y/lib/libcufile.so cufio:cufio_gds_read
/usr/local/cuda-x.y/lib/libcufile.so cufio:cufio_gds_read_async
/usr/local/cuda-x.y/lib/libcufile.so cufio:cufio_px_write
/usr/local/cuda-x.y/lib/libcufile.so cufio:cufio_gds_write
/usr/local/cuda-x.y/lib/libcufile.so cufio:cufio_gds_write_async
/usr/local/cuda-x.y/lib/libcufile.so cufio-internal:cufio-internal-write-bb
/usr/local/cuda-x.y/lib/libcufile.so cufio-internal:cufio-internal-read-bb
/usr/local/cuda-x.y/lib/libcufile.so cufio-internal:cufio-internal-bb-done
/usr/local/cuda-x.y/lib/libcufile.so cufio-internal:cufio-internal-io-done
/usr/local/cuda-x.y/lib/libcufile.so cufio-internal:cufio-internal-map</p>
        </pre>
  </li>
 </ol>
 <h3 id="tracepoint-arguments">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tracepoint-arguments">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tracepoint-arguments" id="tracepoint-arguments" name="tracepoint-arguments" shape="rect">
    16.1.1. Example: Tracepoint Arguments
   </a>
  </a>
 </h3>
 <p>
  Here are examples of tracepoint arguments.
 </p>
 <h4>
  <span>
   cufio_px_read
  </span>
 </h4>
 <p>
  This tracepoint tracks POSIX IO reads and takes the following arguments:
 </p>
 <ul>
  <li>
   Arg1: File descriptor
  </li>
  <li>
   Arg 2: File offset
  </li>
  <li>
   Arg 3: Read size
  </li>
  <li>
   Arg 4: GPU Buffer offset
  </li>
  <li>
   Arg 5: Return value
  </li>
  <li>
   Arg 6: GPU ID for which IO is done
  </li>
 </ul>
 <h4>
  <span>
   cufio_rdma_read
  </span>
 </h4>
 <p>
  This tracepoint tracks IO reads for through WEKA filesystem and takes the following arguments:
 </p>
 <ul>
  <li>
   Arg1: File descriptor
  </li>
  <li>
   Arg2: File offset
  </li>
  <li>
   Arg3: Read size
  </li>
  <li>
   Arg4: GPU Buffer offset
  </li>
  <li>
   Arg5: Return value
  </li>
  <li>
   Arg6: GPU ID for which IO is done
  </li>
  <li>
   Arg7: Is the IO done to GPU Bounce buffer
  </li>
 </ul>
 <h4>
  <span>
   cufio_rdma_write
  </span>
 </h4>
 <p>
  This tracepoint tracks IO reads for through WEKA filesystem and takes the following arguments:
 </p>
 <ul>
  <li>
   Arg1: File descriptor
  </li>
  <li>
   Arg2: File offset
  </li>
  <li>
   Arg3: Write size
  </li>
  <li>
   Arg4: GPU Buffer offset
  </li>
  <li>
   Arg5: Return value
  </li>
  <li>
   Arg6: GPU ID for which IO is done
  </li>
  <li>
   Arg7: Is the IO done to GPU Bounce buffer
  </li>
 </ul>
 <h4>
  <span>
   cufio_gds_read
  </span>
 </h4>
 <p>
  This tracepoint tracks IO reads going through the GDS kernel drive and takes the following arguments:
 </p>
 <ul>
  <li>
   Arg1: File descriptor
  </li>
  <li>
   Arg2: File offset
  </li>
  <li>
   Arg3: Read size
  </li>
  <li>
   Arg4: GPU Buffer offset
  </li>
  <li>
   Arg5: Return value
  </li>
  <li>
   Arg6: GPU ID for which IO is done
  </li>
  <li>
   Arg7: Is the IO done to GPU Bounce buffer
  </li>
 </ul>
 <h4>
  <span>
   cufio_gds_read_async
  </span>
 </h4>
 <p>
  This tracepoint tracks iO reads going through the GDS kernel driver and poll mode is set and takes the following arguments:
 </p>
 <ul>
  <li>
   Arg1: File descriptor
  </li>
  <li>
   Arg2: File offset
  </li>
  <li>
   Arg3: Read size
  </li>
  <li>
   Arg4: GPU Buffer offset
  </li>
  <li>
   Arg5: Return value
  </li>
  <li>
   Arg6: GPU ID for which IO is done
  </li>
  <li>
   Arg7: Is the IO done to GPU Bounce buffer
  </li>
 </ul>
 <h4>
  <span>
   cufio_px_write
  </span>
 </h4>
 <p>
  This tracepoint tracks POSIX IO writes and takes the following arguments:
 </p>
 <ul>
  <li>
   Arg1: File descriptor
  </li>
  <li>
   Arg 2: File offset
  </li>
  <li>
   Arg 3: Write size
  </li>
  <li>
   Arg 4: GPU Buffer offset
  </li>
  <li>
   Arg 5: Return value
  </li>
  <li>
   Arg 6: GPU ID for which IO is done
  </li>
 </ul>
 <h4>
  <span>
   cufio_gds_write
  </span>
 </h4>
 <p>
  This tracepoint tracks IO writes going through the GDS kernel driver and takes the following arguments:
 </p>
 <ul>
  <li>
   Arg1: File descriptor
  </li>
  <li>
   Arg2: File offset
  </li>
  <li>
   Arg3: Write size
  </li>
  <li>
   Arg4: GPU Buffer offset
  </li>
  <li>
   Arg5: Return value
  </li>
  <li>
   Arg6: GPU ID for which IO is done
  </li>
  <li>
   Arg7: Is the IO done to GPU Bounce buffer
  </li>
 </ul>
 <h4>
  <span>
   cufio_gds_unaligned_write
  </span>
 </h4>
 <p>
  This tracepoint tracks IO writes going through the GDS kernel driver if the IO was unaligned and takes the following arguments:
 </p>
 <ul>
  <li>
   Arg1: File descriptor
  </li>
  <li>
   Arg2: File offset
  </li>
  <li>
   Arg3: Write size
  </li>
  <li>
   Arg4: GPU Buffer offset
  </li>
  <li>
   Arg5: Return value
  </li>
  <li>
   Arg6: GPU ID for which IO is done
  </li>
  <li>
   Arg7: Is the IO done to GPU Bounce buffer
  </li>
 </ul>
 <h4>
  <span>
   cufio_gds_write_async
  </span>
 </h4>
 <p>
  This tracepoint tracks IO writes going through the GDS kernel driver, and poll mode is set and takes the following arguments:
 </p>
 <ul>
  <li>
   Arg1: File descriptor
  </li>
  <li>
   Arg2: File offset
  </li>
  <li>
   Arg3: Write size
  </li>
  <li>
   Arg4: GPU Buffer offset
  </li>
  <li>
   Arg5: Return value
  </li>
  <li>
   Arg6: GPU ID for which IO is done
  </li>
  <li>
   Arg7: Is the IO done to GPU Bounce buffer
  </li>
 </ul>
 <h4>
  <span>
   cufio-internal-write-bb
  </span>
 </h4>
 <p>
  This tracepoint tracks IO writes going through internal GPU Bounce buffers and is specific to the EXAScaler® filesystem and block device-based filesystems. This tracepoint is in hot IO-path tracking in every IO and takes the following arguments:
  <a class="Link" data-cms-ai="0" id="tracepoint-arguments__ul_iwh_y1k_smb" name="tracepoint-arguments__ul_iwh_y1k_smb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   Arg1: Application GPU (GPU ID)
  </li>
  <li>
   Arg2: GPU Bounce buffer (GPU ID)
  </li>
  <li>
   Arg3: File descriptor
  </li>
  <li>
   Arg4: File offset
  </li>
  <li>
   Arg5: Write size
  </li>
  <li>
   Arg6: Application GPU Buffer offset
  </li>
  <li>
   Arg7: Size is bytes transferred from application GPU buffer to target GPU bounce buffer.
  </li>
  <li>
   Arg8: Total Size in bytes transferred so far through bounce buffer.
  </li>
  <li>
   Arg9: Pending IO count in this transaction
  </li>
 </ul>
 <h4>
  <span>
   cufio-internal-read-bb
  </span>
 </h4>
 <p>
  This tracepoint tracks IO reads going through internal GPU Bounce buffers and is specific to the EXAScaler® filesystem and block device-based filesystems. This tracepoint is in hot IO-path tracking every IO and takes the following arguments:
  <a class="Link" data-cms-ai="0" id="tracepoint-arguments__ul_tlg_3bk_smb" name="tracepoint-arguments__ul_tlg_3bk_smb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   Arg1: Application GPU (GPU ID)
  </li>
  <li>
   Arg2: GPU bounce buffer (GPU ID)
  </li>
  <li>
   Arg3: File descriptor
  </li>
  <li>
   Arg4: File offset
  </li>
  <li>
   Arg5: Read size
  </li>
  <li>
   Arg6: Application GPU Buffer offset
  </li>
  <li>
   Arg7: Size is bytes transferred from the GPU bounce buffer to application GPU buffer.
  </li>
  <li>
   Arg8: Total Size in bytes transferred so far through bounce buffer.
  </li>
  <li>
   Arg9: Pending IO count in this transaction.
  </li>
 </ul>
 <h4>
  <span>
   cufio-internal-bb-done
  </span>
 </h4>
 <p>
  This tracepoint tracks all IO going through bounce buffers and is invoked when IO is completed through bounce buffers. The tracepoint can be used to track all IO going through bounce buffers and takes the following arguments:
  <a class="Link" data-cms-ai="0" id="tracepoint-arguments__ul_j5p_rbk_smb" name="tracepoint-arguments__ul_j5p_rbk_smb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   Arg1: IO-type READ - 0, WRITE - 1
  </li>
  <li>
   Arg2: Application GPU (GPU ID)
  </li>
  <li>
   Arg3: GPU Bounce buffer (GPU ID)
  </li>
  <li>
   Arg4: File descriptor
  </li>
  <li>
   Arg5: File offset
  </li>
  <li>
   Arg6: Read/Write size
  </li>
  <li>
   Arg7: GPU buffer offset
  </li>
  <li>
   Arg8: IO is unaligned (1 - True, 0 - False)
  </li>
  <li>
   Arg9: Buffer is registered (1 - True, 0 - False)
  </li>
 </ul>
 <h4>
  <span>
   cufio-internal-io-done
  </span>
 </h4>
 <p>
  This tracepoint tracks all IO going through the GDS kernel driver. This tracepoint is invoked when the IO is completed and takes the following arguments:
  <a class="Link" data-cms-ai="0" id="tracepoint-arguments__ul_rbd_zbk_smb" name="tracepoint-arguments__ul_rbd_zbk_smb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   Arg1: IO-type READ - 0, WRITE - 1
  </li>
  <li>
   Arg2: GPU ID for which IO is done
  </li>
  <li>
   Arg3: File descriptor
  </li>
  <li>
   Arg4: File offset
  </li>
  <li>
   Arg5: Total bytes transferred
  </li>
 </ul>
 <h4>
  <span>
   cufio-internal-map
  </span>
 </h4>
 <p>
  This tracepoint tracks GPU buffer registration using
  <span>
   cuFileBufRegister
  </span>
  and takes the following arguments:
  <a class="Link" data-cms-ai="0" id="tracepoint-arguments__ul_dzz_2ck_smb" name="tracepoint-arguments__ul_dzz_2ck_smb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   Arg1: GPU ID
  </li>
  <li>
   Arg2: GPU Buffer size for which registration is done
  </li>
  <li>
   Arg3: max_direct_io_size that was used for this buffer.
   <p>
    The shadow memory size is set in the /etc/cufile.json file.
   </p>
  </li>
  <li>
   Arg4: boolean value indicating whether buffer is pinned.
  </li>
  <li>
   Arg5: boolean value indicating whether this buffer is a GPU bounce buffer.
  </li>
  <li>
   Arg6: GPU offset.
  </li>
 </ul>
 <p>
  The data type of each argument in these tracepoints can be found by running the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p># ./tplist -l /usr/local/cuda-x.y/lib/libcufile.so -vvv | grep cufio_px_read -A 7
cufio:cufio_px_read [sema 0x0]</p>
        </pre>
 <p>
  Here is the output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p># ./tplist -l /usr/local/cuda-x.y/lib/libcufile.so -vvv | grep cufio_px_read -A 7
cufio:cufio_px_read [sema 0x0]
  location #1 /usr/local/cuda-x.y/lib/libcufile.so 0x16437c
    argument #1 4 signed   bytes @ dx
    argument #2 8 signed   bytes @ cx
    argument #3 8 unsigned bytes @ si
    argument #4 8 signed   bytes @ di
    argument #5 8 signed   bytes @ r8
    argument #6 4 signed   bytes @ ax</p>
        </pre>
 <h3 id="track-io-issue-cuapis">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-io-issue-cuapis">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-io-issue-cuapis" id="track-io-issue-cuapis" name="track-io-issue-cuapis" shape="rect">
    16.2. Example: Track the IO Activity of a Process that Issues cuFileRead/ cuFileWrite
   </a>
  </a>
 </h3>
 <p>
  This example provides information about how you can track the IO activity of a process that issues the
  <span>
   cuFileRead
  </span>
  or the
  <span>
   cuFileWrite
  </span>
  API.
 </p>
 <ol>
  <li>
   Run the folloiwng command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./funccount u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_* -i 1 -T -p 59467
Tracing 7 functions for "u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_*"... Hit Ctrl-C to end.</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>cufio_gds_write                          1891

16:21:13
FUNC                                    COUNT
cufio_gds_write                          1852

16:21:14
FUNC                                    COUNT
cufio_gds_write                          1865
^C
16:21:14
FUNC                                    COUNT
cufio_gds_write                          1138
Detaching…</p>
        </pre>
  </li>
 </ol>
 <h3 id="disp-io-patters-through-gds">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#disp-io-patters-through-gds">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#disp-io-patters-through-gds" id="disp-io-patters-through-gds" name="disp-io-patters-through-gds" shape="rect">
    16.3. Example: Display the IO Pattern of all the IOs that Go Through GDS
   </a>
  </a>
 </h3>
 <p>
  This example provides information about how you can display and understand the IO pattern of all IOs that go through GDS.
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./argdist -C 'u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_gds_read():size_t:arg3# Size Distribution'</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>[16:38:22]
IO Size Distribution
    COUNT      EVENT
    4654       arg3 = 1048576
    7480       arg3 = 131072
    9029       arg3 = 65536
    13561      arg3 = 8192
    14200      arg3 = 4096
[16:38:23]
IO Size Distribution
    COUNT      EVENT
    4682       arg3 = 1048576
    7459       arg3 = 131072
    9049       arg3 = 65536
    13556      arg3 = 8192
    14085      arg3 = 4096
[16:38:24]
IO Size Distribution
    COUNT      EVENT
    4678       arg3 = 1048576
    7416       arg3 = 131072
    9018       arg3 = 65536
    13536      arg3 = 8192
    14082      arg3 = 4096</p>
        </pre>
  </li>
 </ol>
 <p>
  The 1M, 128K, 64K, 8K, and 4K IOs are all completing reads through GDS.
 </p>
 <h3 id="io-pattern-process">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-pattern-process">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-pattern-process" id="io-pattern-process" name="io-pattern-process" shape="rect">
    16.4. Understand the IO Pattern of a Process
   </a>
  </a>
 </h3>
 <p>
  You can review the output to understand the IO pattern of a process.
 </p>
 <ol>
  <li>
   Run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./argdist -C 'u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_gds_read():size_t:arg3#IO 
Size Distribution' -p 59702</p>
        </pre>
  </li>
  <li>
   Review the output.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>[16:40:46]
IO Size Distribution
    COUNT      EVENT
    20774      arg3 = 4096
[16:40:47]
IO Size Distribution
    COUNT      EVENT
    20727      arg3 = 4096
[16:40:48]
IO Size Distribution
    COUNT      EVENT
    20713      arg3 = 4096</p>
        </pre>
  </li>
 </ol>
 <p>
  Process 59702 issues 4K IOs.
 </p>
 <h3 id="io-pattern-file-desc">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-pattern-file-desc">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-pattern-file-desc" id="io-pattern-file-desc" name="io-pattern-file-desc" shape="rect">
    16.5. IO Pattern of a Process with the File Descriptor on Different GPUs
   </a>
  </a>
 </h3>
 <ol>
  <li>
   Run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./argdist -C 
'u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_gds_read():int,int,size:arg1,
arg6,arg3#IO Size Distribution arg1=fd, arg6=GPU# arg3=IOSize' -p `pgrep -n gdsio`</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>[17:00:03]
u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_gds_read():int,int,size_t:arg1,arg6,arg3#IO Size Distribution arg1=fd, arg6=GPU# arg3=IOSize
    COUNT      EVENT
    5482       arg1 = 87, arg6 = 2, arg3 = 131072
    7361       arg1 = 88, arg6 = 1, arg3 = 65536
    9797       arg1 = 89, arg6 = 0, arg3 = 8192
    11145      arg1 = 74, arg6 = 3, arg3 = 4096
[17:00:04]
u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_gds_read():int,int,size_t:arg1,arg6,arg3#IO Size Distribution arg1=fd, arg6=GPU# arg3=IOSize
    COUNT      EVENT
    5471       arg1 = 87, arg6 = 2, arg3 = 131072
    7409       arg1 = 88, arg6 = 1, arg3 = 65536
    9862       arg1 = 89, arg6 = 0, arg3 = 8192
    11079      arg1 = 74, arg6 = 3, arg3 = 4096
[17:00:05]
u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_gds_read():int,int,size_t:arg1,arg6,arg3#IO Size Distribution arg1=fd, arg6=GPU# arg3=IOSize
    COUNT      EVENT
    5490       arg1 = 87, arg6 = 2, arg3 = 131072
    7402       arg1 = 88, arg6 = 1, arg3 = 65536
    9827       arg1 = 89, arg6 = 0, arg3 = 8192
    11131      arg1 = 74, arg6 = 3, arg3 = 4096</p>
        </pre>
  </li>
 </ol>
 <p>
  <a class="Link" data-cms-ai="0" id="io-pattern-file-desc__result_wkg_wqk_smb" name="io-pattern-file-desc__result_wkg_wqk_smb" shape="rect">
  </a>
  <span>
   gdsio
  </span>
  issues
  <span>
   READS
  </span>
  to 4 files with fd=87, 88,89, 74 to GPU 2, 1, 0, and 3 and with
  <span>
   IO-SIZE
  </span>
  of 128K, 64K, 8K, and 4K.
 </p>
 <h3 id="iops-bandwidth-process-gpu">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#iops-bandwidth-process-gpu">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#iops-bandwidth-process-gpu" id="iops-bandwidth-process-gpu" name="iops-bandwidth-process-gpu" shape="rect">
    16.6. Determine the IOPS and Bandwidth for a Process in a GPU
   </a>
  </a>
 </h3>
 <p>
  You can determine the IOPS and bandwidth for each process in a GPU.
 </p>
 <ol>
  <li>
   Run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>#./argdist -C 
'u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_gds_read():int,int,size_t:arg1,
arg6,arg3:arg6==0||arg6==3#IO Size Distribution arg1=fd, arg6=GPU# 
arg3=IOSize' -p `pgrep -n gdsio`</p>
        </pre>
  </li>
  <li>
   Review the output.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>[17:49:33]
u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_gds_read():int,int,size_t:arg1,arg6,arg3:arg6==0||arg6==3#IO Size Distribution arg1=fd, arg6=GPU# arg3=IOSize
    COUNT      EVENT
    9826       arg1 = 89, arg6 = 0, arg3 = 8192
    11168      arg1 = 86, arg6 = 3, arg3 = 4096
[17:49:34]
u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_gds_read():int,int,size_t:arg1,arg6,arg3:arg6==0||arg6==3#IO Size Distribution arg1=fd, arg6=GPU# arg3=IOSize
    COUNT      EVENT
    9815       arg1 = 89, arg6 = 0, arg3 = 8192
    11141      arg1 = 86, arg6 = 3, arg3 = 4096
[17:49:35]
u:/usr/local/cuda-x.y/lib/libcufile.so:cufio_gds_read():int,int,size_t:arg1,arg6,arg3:arg6==0||arg6==3#IO Size Distribution arg1=fd, arg6=GPU# arg3=IOSize
    COUNT      EVENT
    9914       arg1 = 89, arg6 = 0, arg3 = 8192
    11194      arg1 = 86, arg6 = 3, arg3 = 4096</p>
        </pre>
  </li>
 </ol>
 <ul>
  <li>
   gdsio is doing IO on all 4 GPUs, and the output is filtered for GPU 0 and GPU 3.
  </li>
  <li>
   Bandwidth per GPU is GPU 0 - 9826 IOPS of 8K block size, and the bandwidth = ~80MB/s .
  </li>
 </ul>
 <h3 id="freq-reads-cufileread-api">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#freq-reads-cufileread-api">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#freq-reads-cufileread-api" id="freq-reads-cufileread-api" name="freq-reads-cufileread-api" shape="rect">
    16.7. Display the Frequency of Reads by Processes that Issue cuFileRead
   </a>
  </a>
 </h3>
 <p>
  You can display information about the frequency of reads by process that issue the
  <span>
   cuFileRead
  </span>
  API.
 </p>
 <ol>
  <li>
   Run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>#./argdist -C 'r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead():u32:$PID'</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>[17:58:01]
r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead():u32:$PID
    COUNT      EVENT
    31191      $PID = 60492
    31281      $PID = 60593
[17:58:02]
r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead():u32:$PID
    COUNT      EVENT
    11741      $PID = 60669
    30447      $PID = 60593
    30670      $PID = 60492
[17:58:03]
r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead():u32:$PID
    COUNT      EVENT
    29887      $PID = 60593
    29974      $PID = 60669
    30017      $PID = 60492
[17:58:04]
r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead():u32:$PID
    COUNT      EVENT
    29972      $PID = 60593
    30062      $PID = 60492
    30068      $PID = 60669</p>
        </pre>
  </li>
 </ol>
 <h3 id="freq-reads-cufileread-0.1-ms">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#freq-reads-cufileread-0.1-ms">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#freq-reads-cufileread-0.1-ms" id="freq-reads-cufileread-0.1-ms" name="freq-reads-cufileread-0.1-ms" shape="rect">
    16.8. Display the Frequency of Reads when cuFileRead Takes More than 0.1 ms
   </a>
  </a>
 </h3>
 <p>
  You can display the frequency of reads when the
  <span>
   cuFileRead
  </span>
  API takes more than 0.1 ms.
 </p>
 <ol>
  <li>
   Run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>#./argdist -C 'r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead():u32:$PID:$latency &gt; 100000'</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>[18:07:35]
r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead():u32:$PID:$latency &gt; 100000
    COUNT      EVENT
    17755      $PID = 60772
[18:07:36]
r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead():u32:$PID:$latency &gt; 100000
    COUNT      EVENT
    17884      $PID = 60772
[18:07:37]
r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead():u32:$PID:$latency &gt; 100000
    COUNT      EVENT
    17748      $PID = 60772
[18:07:38]
r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead():u32:$PID:$latency &gt; 100000
    COUNT      EVENT
    17898      $PID = 60772
[18:07:39]
r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead():u32:$PID:$latency &gt; 100000
    COUNT      EVENT
    17811      $PID = 60772</p>
        </pre>
  </li>
 </ol>
 <h3 id="disp-latency-cufileread">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#disp-latency-cufileread">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#disp-latency-cufileread" id="disp-latency-cufileread" name="disp-latency-cufileread" shape="rect">
    16.9. Displaying the Latency of cuFileRead for Each Process
   </a>
  </a>
 </h3>
 <p>
  You can display the latency of the the
  <span>
   cuFileRead
  </span>
  API for each process.
 </p>
 <ol>
  <li>
   Run the following command.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>#./funclatency /usr/local/cuda-x.y/lib/libcufile.so:cuFileRead -i 1 -T -u</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>Tracing 1 functions for 
"/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead"... Hit Ctrl-C to end.</p>
        </pre>
   <p>
    Here are two process with PID 60999 and PID 60894 that are issuing cuFileRead:
   </p>
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>18:12:11
Function = cuFileRead [60999]
     usecs               : count     distribution
         0 -&gt; 1          : 0        |                                        |
         2 -&gt; 3          : 0        |                                        |
         4 -&gt; 7          : 0        |                                        |
         8 -&gt; 15         : 0        |                                        |
        16 -&gt; 31         : 0        |                                        |
        32 -&gt; 63         : 0        |                                        |
        64 -&gt; 127        : 17973    |****************************************|
       128 -&gt; 255        : 13383    |*****************************           |
       256 -&gt; 511        : 27       |                                        |
Function = cuFileRead [60894]
     usecs               : count     distribution
         0 -&gt; 1          : 0        |                                        |
         2 -&gt; 3          : 0        |                                        |
         4 -&gt; 7          : 0        |                                        |
         8 -&gt; 15         : 0        |                                        |
        16 -&gt; 31         : 0        |                                        |
        32 -&gt; 63         : 0        |                                        |
        64 -&gt; 127        : 17990    |****************************************|
       128 -&gt; 255        : 13329    |*****************************           |
       256 -&gt; 511        : 19       |                                        |

18:12:12
Function = cuFileRead [60999]
     usecs               : count     distribution
         0 -&gt; 1          : 0        |                                        |
         2 -&gt; 3          : 0        |                                        |
         4 -&gt; 7          : 0        |                                        |
         8 -&gt; 15         : 0        |                                        |
        16 -&gt; 31         : 0        |                                        |
        32 -&gt; 63         : 0        |                                        |
        64 -&gt; 127        : 18209    |****************************************|
       128 -&gt; 255        : 13047    |****************************            |
       256 -&gt; 511        : 58       |                                        |

Function = cuFileRead [60894]
     usecs               : count     distribution
         0 -&gt; 1          : 0        |                                        |
         2 -&gt; 3          : 0        |                                        |
         4 -&gt; 7          : 0        |                                        |
         8 -&gt; 15         : 0        |                                        |
        16 -&gt; 31         : 0        |                                        |
        32 -&gt; 63         : 0        |                                        |
        64 -&gt; 127        : 18199    |****************************************|
       128 -&gt; 255        : 13015    |****************************            |
       256 -&gt; 511        : 46       |                                        |
       512 -&gt; 1023       : 1        |</p>
        </pre>
  </li>
 </ol>
 <h3 id="track-process-cufilebufregister">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-process-cufilebufregister">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-process-cufilebufregister" id="track-process-cufilebufregister" name="track-process-cufilebufregister" shape="rect">
    16.10. Example: Tracking the Processes that Issue cuFileBufRegister
   </a>
  </a>
 </h3>
 <p>
  This example shows you can track processes that issue the
  <span>
   cuFileBufRegister
  </span>
  API.
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./trace 'u:/usr/local/cuda-x.y/lib/libcufile.so:cufio-internal-map "GPU 
%d Size %d Bounce-Buffer %d",arg1,arg2,arg5'</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>PID     TID     COMM     FUNC             -
62624   62624   gdsio_verify    cufio-internal-map GPU 0 Size 1048576 Bounce-Buffer 1
62659   62726   fio             cufio-internal-map GPU 0 Size 8192 Bounce-Buffer 0
62659   62728   fio             cufio-internal-map GPU 2 Size 131072 Bounce-Buffer 0
62659   62727   fio             cufio-internal-map GPU 1 Size 65536 Bounce-Buffer 0
62659   62725   fio             cufio-internal-map GPU 3 Size 4096 Bounce-Buffer 0</p>
        </pre>
  </li>
 </ol>
 <p>
  <span>
   gdsio_verify
  </span>
  issued an IO, but it did not register GPU memory using
  <span>
   cuFileBufRegister
  </span>
  . As a result, the GDS library pinned 1M of a bounce buffer on GPU 0. FIO, on the other hand, issued a
  <span>
   cuFileBufRegister
  </span>
  of 128K on GPU 2.
 </p>
 <h3 id="proc-constant-cufilebufregister">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#proc-constant-cufilebufregister">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#proc-constant-cufilebufregister" id="proc-constant-cufilebufregister" name="proc-constant-cufilebufregister" shape="rect">
    16.11. Example: Tracking Whether the Process is Constant when Invoking cuFileBufRegister
   </a>
  </a>
 </h3>
 <p>
  You can track whether the process is constant when invoking the
  <span>
   cuFileBufRegister
  </span>
  API.
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./trace 'u:/usr/local/cuda-x.y/lib/libcufile.so:cufio-internal-map (arg5 == 0) 
"GPU %d Size %d",arg1,arg2'</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>PID     TID     COMM            FUNC             -
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576
444     472     cufile_sample_0 cufio-internal-map GPU 0 Size 1048576</p>
        </pre>
  </li>
 </ol>
 <p>
  As seen in this example, there is one thread in a process that continuously issues 1M of
  <span>
   cuFileBufRegister
  </span>
  on GPU 0. This might mean that the API is called in a loop and might impact performance.
 </p>
 Note:
 <p>
  <span>
   cuFileBufRegister
  </span>
  involves pinning GPU memory, which is an expensive operation.
 </p>
 <h3 id="monitor-io-bb">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-io-bb">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-io-bb" id="monitor-io-bb" name="monitor-io-bb" shape="rect">
    16.12. Example: Monitoring IOs that are Going Through the Bounce Buffer
   </a>
  </a>
 </h3>
 <p>
  This example shows how you can monitor whether IOs are going through the bounce buffer.
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./trace 'u:/usr/local/cuda-x.y/lib/libcufile.so:cufio-internal-bb-done 
"Application GPU %d Bounce-Buffer GPU %d Transfer Size %d Unaligned %d Registered %d", 
arg2,arg3,arg8,arg9,arg10'</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>PID TID COMM  FUNC             -
1013 1041  gdsio App-GPU 0 Bounce-Buffer GPU 0 Transfer Size 1048576 Unaligned 1 Registered 0
1013 1042  gdsio App-GPU 3 Bounce-Buffer GPU 3 Transfer Size 1048576 Unaligned 1 Registered 0
1013 1041  gdsio App-GPU 0 Bounce-Buffer GPU 0 Transfer Size 1048576 Unaligned 1 Registered 0
1013 1042  gdsio App-GPU 3 Bounce-Buffer GPU 3 Transfer Size 1048576 Unaligned 1 Registered 0</p>
        </pre>
   <p>
    The
    <span>
     gdsio
    </span>
    app has 2 threads and both are doing unaligned IO on GPU 0 and GPU 3. Since the IO is unaligned, bounce buffers are also from the same application GPU.
   </p>
  </li>
 </ol>
 <h3 id="trace-cufileread-cufilewrite-issues">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trace-cufileread-cufilewrite-issues">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trace-cufileread-cufilewrite-issues" id="trace-cufileread-cufilewrite-issues" name="trace-cufileread-cufilewrite-issues" shape="rect">
    16.13. Example: Tracing cuFileRead and cuFileWrite Failures, Print, Error Codes, and Time of Failure
   </a>
  </a>
 </h3>
 <p>
  This example shows you how to trace the
  <span>
   cuFileRead
  </span>
  and
  <span>
   cuFileWrite
  </span>
  failures, print, error codes, and time of failure.
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p># ./trace 'r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileRead ((int)retval &lt; 0) 
"cuFileRead failed: %d", retval' 'r:/usr/local/cuda-x.y/lib/libcufile.so:cuFileWrite ((int)retval &lt; 0) 
"cuFileWrite failed: %d", retval' -T</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>TIME      PID      TID               COMM                     FUNC             -
23:22:16 4201    4229    gdsio           cuFileRead       cuFileRead failed: -5
23:22:42 4237    4265    gdsio           cuFileWrite      cuFileWrite failed: -5</p>
        </pre>
  </li>
 </ol>
 <p>
  In this example, two failures were observed with EIO (-5) as the return code with the timestamp.
 </p>
 <h3 id="per-process-user-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#per-process-user-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#per-process-user-stats" id="per-process-user-stats" name="per-process-user-stats" shape="rect">
    16.14. Example: User-Space Statistics for Each GDS Process
   </a>
  </a>
 </h3>
 <p>
  The
  <span>
   cuFile
  </span>
  library exports user-level statistics in the form of API level counters for each process. In addition to the regular GDS IO path, there are paths for user-space file-systems and IO compatibility modes that use POSIX read/writes, which do not go through the nvidia-fs driver. The user-level statistics are more useful in these scenarios.
 </p>
 There is a verbosity level for the counters which users can specify using JSON configuration file to enable and set the level. The following describes various verbosity levels.
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 10. User-Space Statistics for Each GDS Process
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e9566" rowspan="1" valign="top">
    Level
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e9569" rowspan="1" valign="top">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9566" rowspan="1" valign="top">
    Level 0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9569" rowspan="1" valign="top">
    cuFile stats will be disabled.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9566" rowspan="1" valign="top">
    Level 1
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9569" rowspan="1" valign="top">
    cuFile stats will report only Global Counters like overall throughput, average latency and error counts.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9566" rowspan="1" valign="top">
    Level 2
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9569" rowspan="1" valign="top">
    With the Global Counters, an IO Size histogram will be reported for information on access patterns.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9566" rowspan="1" valign="top">
    Level 3
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9569" rowspan="1" valign="top">
    <p>
     At this level, per GPU counters are reported and also live usage
    </p>
    <p>
     of cuFile internal pool buffers.
    </p>
   </td>
  </tr>
 </table>
 <a class="Link" data-cms-ai="0" id="per-process-user-stats__section_q1t_wfl_smb" name="per-process-user-stats__section_q1t_wfl_smb" shape="rect">
 </a>
 The following is the JSON configuration key to enable GDS statistics by using the /etc/cufile.json file:
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>"profile": {
                // cufile stats level(0-3)
                "cufile_stats": 3
            },</p>
        </pre>
 <h3 id="gds-stats-tool">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-stats-tool">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-stats-tool" id="gds-stats-tool" name="gds-stats-tool" shape="rect">
    16.15. Example: Viewing GDS User-Level Statistics for a Process
   </a>
  </a>
 </h3>
 <p>
  This example shows how you can use the
  <span>
   gds_stats
  </span>
  tool to display user-level statistics for a process.
 </p>
 <p>
  Prerequisite
  : Before you run the tool, ensure that the IO application is active, and the
  <span>
   gds_stats
  </span>
  has the same user permissions as the application.
 </p>
 <p>
  The
  <span>
   gds_stats
  </span>
  tool can be used to read statistics that are exported by
  <span>
   libcufile.so
  </span>
  .
 </p>
 <p>
  The output of the statistics is displayed in the standard output. If the user permissions are not the same, there might not be sufficient privilege to view the stats. A future version of
  <span>
   gds_stats
  </span>
  will integrate nvidia-fs kernel level statistics into this tool. To use the tool, run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ /usr/local/cuda-x.y/tools/gds_stats -p &lt;pidof application&gt; -l &lt;stats_level(1-3)&gt;</p>
        </pre>
 <p>
  When specifying the statistics level, ensure that the corresponding level (
  <span>
   profile.cufile_stats
  </span>
  ) is also enabled in the
  <span>
   /etc/cufile.json
  </span>
  file.
 </p>
 <p>
  The GDS user level statistics are logged once to
  <span>
   cufile.log
  </span>
  file when the library is shut down, or the
  <span>
   cuFileDriverClose
  </span>
  API is run. To view statistics in the log file, set the log level to INFO.
 </p>
 <h3 id="sample-ul-stats-per-process">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#sample-ul-stats-per-process">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#sample-ul-stats-per-process" id="sample-ul-stats-per-process" name="sample-ul-stats-per-process" shape="rect">
    16.16. Example: Displaying Sample User-Level Statistics for each GDS Process
   </a>
  </a>
 </h3>
 <p>
  This example shows how to display sample user-level statistics for each GDS process.
 </p>
 <ol>
  <li>
   Run the following command:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>$ ./gds_stats -p 23198 -l 3</p>
        </pre>
  </li>
  <li>
   Review the output, for example:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>cuFile STATS VERSION : 4
GLOBAL STATS:
Total Files: 1
Total Read Errors : 0
Total Read Size (MiB): 7302
Read BandWidth (GiB/s): 0.691406
Avg Read Latency (us): 6486
Total Write Errors : 0
Total Write Size (MiB): 0
Write BandWidth (GiB/s): 0
Avg Write Latency (us): 0
READ-WRITE SIZE HISTOGRAM :
0-4(KiB): 0  0
4-8(KiB): 0  0
8-16(KiB): 0  0
16-32(KiB): 0  0
32-64(KiB): 0  0
64-128(KiB): 0  0
128-256(KiB): 0  0
256-512(KiB): 0  0
512-1024(KiB): 0  0
1024-2048(KiB): 0  0
2048-4096(KiB): 3651  0
4096-8192(KiB): 0  0
8192-16384(KiB): 0  0
16384-32768(KiB): 0  0
32768-65536(KiB): 0  0
65536-...(KiB): 0  0
PER_GPU STATS:
GPU 0 Read: bw=0.690716 util(%)=199 n=3651 posix=0 unalign=0 dr=0 r_sparse=0 r_inline=0 err=0 MiB=7302 Write: bw=0 util(%)=0 n=0 posix=0 unalign=0 dr=0 err=0 MiB=0 BufRegister: n=2 err=0 free=0 MiB=4
PER_GPU POOL BUFFER STATS:
PER_GPU POSIX POOL BUFFER STATS:

PER_GPU RDMA STATS:
GPU 0000:43:00.0 :   mlx5_0(130:64):Reads: 3594 Writes: 0  mlx5_1(130:64):Reads: 3708 Writes: 0
RDMA MRSTATS:
peer name   nr_mrs      mr_size(MiB)
mlx5_0      1           2
mlx5_1      1           2</p>
        </pre>
  </li>
 </ol>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-space-counters">
   17. User-Space Counters in GPUDirect Storage
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="gds-space-counters" name="gds-space-counters" shape="rect">
  </a>
  The following tables provide information about user-space counters in GDS.
 </p>
 <a class="Link" data-cms-ai="0" id="gds-space-counters__section_z4t_t3l_smb" name="gds-space-counters__section_z4t_t3l_smb" shape="rect">
 </a>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 11. Global cuFile Counters
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e9764" rowspan="1" valign="top">
    Counter Name
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e9767" rowspan="1" valign="top">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9764" rowspan="1" valign="top">
    Total Files
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9767" rowspan="1" valign="top">
    <p>
     Total number of files registered successfully with
     <span>
      cuFileHandleRegister
     </span>
     . This is a cumulative counter.
    </p>
    <p>
     <span>
      cuFileHandleDeregister
     </span>
     does not change this counter.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9764" rowspan="1" valign="top">
    Total Read Errors
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9767" rowspan="1" valign="top">
    Total number of
    <span>
     cuFileRead
    </span>
    errors.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9764" rowspan="1" valign="top">
    Total Read Size
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9767" rowspan="1" valign="top">
    Total number of bytes read in MB using cuFileRead.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9764" rowspan="1" valign="top">
    Read Bandwidth
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9767" rowspan="1" valign="top">
    Average overall read throughput in GiB/s over one second time period.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9764" rowspan="1" valign="top">
    Avg Read Latency
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9767" rowspan="1" valign="top">
    Overall average read latency in microseconds over one second time period.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9764" rowspan="1" valign="top">
    Total Write Errors
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9767" rowspan="1" valign="top">
    Total number of cuFileWrite errors.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9764" rowspan="1" valign="top">
    Total Write Size
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9767" rowspan="1" valign="top">
    Total number of bytes written in MB using cuFileWrite.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9764" rowspan="1" valign="top">
    Write Bandwidth
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9767" rowspan="1" valign="top">
    Overall average write throughput in GiB/s over one second time period.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9764" rowspan="1" valign="top">
    Avg Write Latency
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9767" rowspan="1" valign="top">
    Overall average read latency in microseconds over one second time period.
   </td>
  </tr>
 </table>
 <a class="Link" data-cms-ai="0" id="gds-space-counters__section_lww_ljl_smb" name="gds-space-counters__section_lww_ljl_smb" shape="rect">
 </a>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 12. IO-Size Histogram
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e9891" rowspan="1" valign="top">
    Counter Name
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e9894" rowspan="1" valign="top">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9891" rowspan="1" valign="top">
    Read
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9894" rowspan="1" valign="top">
    Distribution of number of cuFileRead requests based on IO size. Bin Size uses a 4K log scale.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9891" rowspan="1" valign="top">
    Write
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9894" rowspan="1" valign="top">
    Distribution of number of cuFileWrite requests based on IO size. Bin Size uses a 4K log scale.
   </td>
  </tr>
 </table>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 13. Per-GPU Counters
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e9937" rowspan="1" valign="top">
    Counter Name
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e9940" rowspan="1" valign="top">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    Read.bw/Write.bw
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    Average GPU read/write bandwidth in GiB/s per GPU.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    Read.util/Write.util
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    Average per GPU read/write utilization in %. If A is the total length of time the resource was busy in a time interval T, then utilization is defined as A/T. Here the utilization is reported over one second period.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    Read.n/Write.n
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    Number of
    <span>
     cuFileRead
    </span>
    /
    <span>
     cuFileWrite
    </span>
    requests per GPU.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    Read.posix/Write.posix
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    Number of
    <span>
     cuFileRead
    </span>
    /
    <span>
     cuFileWrite
    </span>
    using POSIX read/write APIs per GPU.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    Read.dr/Write.dr
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    <p>
     <a class="Link" data-cms-ai="0" id="gds-space-counters__docs-internal-guid-01cbe535-7fff-cc02-c368-2bb8a3db3a25" name="gds-space-counters__docs-internal-guid-01cbe535-7fff-cc02-c368-2bb8a3db3a25" shape="rect">
     </a>
     Number of
     <span>
      cuFileRead
     </span>
     /
     <span>
      cuFileWrites
     </span>
     for a GPU have been issued using dynamic routing.
    </p>
    <p>
     If the routing policy uses SYS_MEM, GPU posix counters for read/writes will be incrementing in addition to the
     <span>
      dr
     </span>
     counter. Note: This counter does not tell which GPU was actually being used for routing the IO. For the latter information, one needs to observe the PER_GPU POOL BUFFER STATS/PER_GPU POSIX POOL BUFFER STATS.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    Read.unalign/Write.unalign
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    Number of
    <span>
     cuFileRead
    </span>
    /
    <span>
     cuFileWrite
    </span>
    per GPU which have at least one IO parameter not 4K aligned. This can be either size, file offset or device pointer.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    Read.error/Write.error
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    Number of
    <span>
     cuFileRead
    </span>
    /
    <span>
     cuFileWrite
    </span>
    errors per GPU.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    Read.mb/Write.mb
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    Total number of bytes in MB read/written using
    <span>
     cuFileRead
    </span>
    /
    <span>
     cuFileWrite
    </span>
    per GPU.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    BufRegister.n
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    Total number of
    <span>
     cuFileBufRegister
    </span>
    calls per GPU.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    BufRegister.err
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    Total number of errors per GPU seen with
    <span>
     cuFileBufRegister
    </span>
    .
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    BufRegister.free
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    Total number of
    <span>
     cuFileBufRegister
    </span>
    calls per GPU.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e9937" rowspan="1" valign="top">
    BufRegister.mb
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e9940" rowspan="1" valign="top">
    Total number of bytes in MB currently registered per GPU.
   </td>
  </tr>
 </table>
 <a class="Link" data-cms-ai="0" id="gds-space-counters__section_q2f_tjl_smb" name="gds-space-counters__section_q2f_tjl_smb" shape="rect">
 </a>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 14. Bounce Buffer Counters Per GPU
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e10132" rowspan="1" valign="top">
    Counter Name
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e10135" rowspan="1" valign="top">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10132" rowspan="1" valign="top">
    pool_size_mb
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10135" rowspan="1" valign="top">
    Total size of buffers allocated for per GPU bounce buffers in MB.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10132" rowspan="1" valign="top">
    used_mb
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10135" rowspan="1" valign="top">
    Total size of buffers currently used per GPU for bounce buffer based IO.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10132" rowspan="1" valign="top">
    usage
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10135" rowspan="1" valign="top">
    Fraction of bounce buffers used currently.
   </td>
  </tr>
 </table>
 <h3 id="dist-io-util-gpu">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dist-io-util-gpu">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dist-io-util-gpu" id="dist-io-util-gpu" name="dist-io-util-gpu" shape="rect">
    17.1. Distribution of IO Usage in Each GPU
   </a>
  </a>
 </h3>
 <p>
  The cuFile library has a metric for IO utilization per GPU by application. This metric indicates the amount of time, in percentage, that the cuFile resource was busy in IO.
 </p>
 <p>
  To run a single-threaded
  <span>
   gdsio
  </span>
  test, run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$./gdsio -f /mnt/md1/test -d 0 -n 0 -w 1 -s 10G -i 4K -x 0 -I 1</p>
        </pre>
 <p>
  Here is the sample output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>PER_GPU STATS
GPU 0 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 err=0 mb=0 Write: bw=0.154598 
util(%)=89 n=510588 posix=0 unalign=0 err=0 mb=1994 BufRegister: n=1 err=0 free=0 mb=0</p>
        </pre>
 <p>
  The
  <span>
   util
  </span>
  metric says that the application was completing IO on GPU 0 89% of the time. To run a
  <span>
   gdsio
  </span>
  test using two-threads, run the following command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$./gdsio -f /mnt/md1/test -d 0 -n 0 -w 2 -s 10G -i 4K -x 0 -I 1</p>
        </pre>
 <p>
  Here is the sample output:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>PER_GPU STATS
GPU 0 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 err=0 mb=0 Write: bw=0.164967 util(%)=186 n=140854 posix=0 unalign=0 err=0 mb=550 BufRegister: n=2 err=0 free=0 mb=0</p>
        </pre>
 <p>
  Now the utilization is ~186%, which indicates the amount of parallelism in the way each GPU is used for IO.
 </p>
 <h3 id="dynamic-routing-user-space-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-user-space-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-user-space-stats" id="dynamic-routing-user-space-stats" name="dynamic-routing-user-space-stats" shape="rect">
    17.2. User-space Statistics for Dynamic Routing
   </a>
  </a>
 </h3>
 <p>
  The
  <span>
   PER_GPU
  </span>
  section of
  <span>
   gds_stats
  </span>
  has a
  <span>
   dr
  </span>
  counter which indicates how many
  <span>
   cuFileRead
  </span>
  /
  <span>
   cuFileWrite
  </span>
  s for a GPU have been issued using dynamic routing.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ ./gds_stats -p &lt;pidof application&gt; -l 3
 
    GPU 0 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 dr=0 r_sparse=0 r_inline=0
 err=0 MiB=0 Write: bw=3.37598 util(%)=532 n=6629 posix=0 unalign=0 dr=6629 err=0
 MiB=6629 BufRegister: n=4 err=0 free=0 MiB=4
    GPU 1 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 dr=0 r_sparse=0 r_inline=0
 err=0 MiB=0 Write: bw=3.29297 util(%)=523 n=6637 posix=0 unalign=0 dr=6637 err=0
 MiB=6637 BufRegister: n=4 err=0 free=0 MiB=4</p>
        </pre>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#user-space-rdma-counters">
   18. User-Space RDMA Counters in GPUDirect Storage
  </a>
 </h2>
 <p>
  <a class="Link" data-cms-ai="0" id="user-space-rdma-counters" name="user-space-rdma-counters" shape="rect">
  </a>
  The library provides counters to monitor the RDMA traffic at a per-GPU level and requires that cuFile starts verbosity with a value of 3.
 </p>
 <p>
  Table
  <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-io-counters" shape="rect" title="The following tables list cuFile RDMA IO counters.">
   14-1
  </a>
  provides the following information:
 </p>
 <ul>
  <li>
   Each column stores the total number of bytes that are sent/received between a GPU and a NIC.
  </li>
  <li>
   Each row shows the distribution of RDMA load with regards to a GPU across all NICS.
  </li>
  <li>
   Each row reflects the order of affinity that a GPU has with a NIC.
   <p>
    Ideally, all traffic should be routed through the NIC with the best affinity or is closest to the GPU as shown in
    Example 1
    in
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-io-counters" shape="rect" title="The following tables list cuFile RDMA IO counters.">
     cuFile RDMA IO Counters (PER_GPU RDMA STATS)
    </a>
    .
   </p>
  </li>
 </ul>
 <p>
  In the annotation of each NIC entry in the table, the major number is the pci-distance in terms of the number of hops between the GPU and the NIC, and the minor number indicates the current bandwidth of the NIC (link_width multiplied by pci-generation). The NICs that the GPUs use for RDMA are loaded from the
  <span>
   rdma_dev_addr_list cufile.json
  </span>
  property:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>"rdma_dev_addr_list": [ 
      "172.172.1.240",
      "172.172.1.241",
      "172.172.1.242",
      "172.172.1.243",
      "172.172.1.244",
      "172.172.1.245",
      "172.172.1.246",
      "172.172.1.247" ],</p>
        </pre>
 <p>
  Each IP address corresponds to an IB device that appear as column entries in the RDMA counter table.
  <a class="Link" data-cms-ai="0" id="cufile-rdma-io-counters" name="cufile-rdma-io-counters" shape="rect">
  </a>
 </p>
 <h3 id="cufile-rdma-io-counters">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-io-counters">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-io-counters" id="cufile-rdma-io-counters" name="cufile-rdma-io-counters" shape="rect">
    18.1. cuFile RDMA IO Counters (PER_GPU RDMA STATS)
   </a>
  </a>
 </h3>
 <p>
  The following tables list cuFile RDMA IO counters.
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 15. cuFile RDMA IO Counters (PER_GPU RDMA STATS)
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e10352" rowspan="1" valign="top">
    Entry
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e10355" rowspan="1" valign="top">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10352" rowspan="1" valign="top">
    GPU
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10355" rowspan="1" valign="top">
    Bus device function
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10352" rowspan="1" valign="top">
    NIC
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10355" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>+)Bus device function
+)Device Attributes
   ++)pci-distance between GPU and NIC
   ++)device bandwidth indicator
+)Send/Receive bytes</p>
        </pre>
   </td>
  </tr>
 </table>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 16. Example 1
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>GPU 0000:34:00.0 :</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_3 (3:48):6293</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_5 (7:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_15 (138:48:0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_15 (138:48:0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_17 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_9 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_13 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_7 (138:12):0</p>
        </pre>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>GPU 0000:36:00.0 :</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_3 (3:48):6077</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_5 (7:48):1858</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_15 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_19 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_17 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_9 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_13 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_7 (138:12):0</p>
        </pre>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>GPU 0000:3b:00.0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_5 (3:48):5467</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_3 (7:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_15 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_19 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_17 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_9 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_13 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_7(138:12):0</p>
        </pre>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>GPU 0000:57:00.0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_7 (3:12):4741</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_9 (7:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_15 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_19 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_5 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_17 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_13 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_3 (138:48):0</p>
        </pre>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>GPU 0000:59:00.0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_7 (3:12):4744</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_9 (7:48):1473</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_15 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_19 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_5 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_17 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_13 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_3 (138:48):0</p>
        </pre>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>GPU 0000:5c:00.0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_9 (3:48):4707</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_7 (7:12):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_15 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_19 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_5 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_17 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_13 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_3 (138:48):0</p>
        </pre>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>GPU 0000:5e:00.0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_9 (3:48):4700</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_7 (7:12):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_15 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_19 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_5 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_17 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_13 (138:48):0</p>
        </pre>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top">
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>mlx5_3 (138:48):0</p>
        </pre>
   </td>
  </tr>
 </table>
 <h3 id="cufile-rdma-mem-reg-counters">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-mem-reg-counters">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-mem-reg-counters" id="cufile-rdma-mem-reg-counters" name="cufile-rdma-mem-reg-counters" shape="rect">
    18.2. cuFile RDMA Memory Registration Counters (RDMA MRSTATS)
   </a>
  </a>
 </h3>
 <p>
  The following tables list cuFile RDMA memory registeration counters.
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 17. cuFile RDMA IO Counters (PER_GPU RDMA STATS)
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e10851" rowspan="1" valign="top">
    Entry
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e10854" rowspan="1" valign="top">
    Description
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10851" rowspan="1" valign="top">
    peer name
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10854" rowspan="1" valign="top">
    System name of the NIC.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10851" rowspan="1" valign="top">
    nr_mrs
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10854" rowspan="1" valign="top">
    Count of active memory registration per NIC.
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10851" rowspan="1" valign="top">
    mr_size(mb)
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10854" rowspan="1" valign="top">
    Total size
   </td>
  </tr>
 </table>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 18. Example 2
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e10908" rowspan="1" valign="top">
    peer name
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e10911" rowspan="1" valign="top">
    nr_ms
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e10914" rowspan="1" valign="top">
    mr_size (mb)
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10908" rowspan="1" valign="top">
    mlx5_3
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10911" rowspan="1" valign="top">
    128
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10914" rowspan="1" valign="top">
    128
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10908" rowspan="1" valign="top">
    mlx5_5
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10911" rowspan="1" valign="top">
    128
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10914" rowspan="1" valign="top">
    128
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10908" rowspan="1" valign="top">
    mlx5_11
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10911" rowspan="1" valign="top">
    0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10914" rowspan="1" valign="top">
    0
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10908" rowspan="1" valign="top">
    mlx5_1
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10911" rowspan="1" valign="top">
    0
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10914" rowspan="1" valign="top">
    0
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10908" rowspan="1" valign="top">
    mlx5_15
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10911" rowspan="1" valign="top">
    128
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10914" rowspan="1" valign="top">
    128
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10908" rowspan="1" valign="top">
    mlx5_19
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10911" rowspan="1" valign="top">
    128
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10914" rowspan="1" valign="top">
    128
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10908" rowspan="1" valign="top">
    mlx5_17
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10911" rowspan="1" valign="top">
    128
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10914" rowspan="1" valign="top">
    128
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10908" rowspan="1" valign="top">
    mlx5_9
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10911" rowspan="1" valign="top">
    128
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10914" rowspan="1" valign="top">
    128
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10908" rowspan="1" valign="top">
    mlx5_13
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10911" rowspan="1" valign="top">
    128
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10914" rowspan="1" valign="top">
    128
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e10908" rowspan="1" valign="top">
    mlx5_7
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10911" rowspan="1" valign="top">
    128
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e10914" rowspan="1" valign="top">
    128
   </td>
  </tr>
 </table>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cheat-sheet">
   19. Cheat Sheet for Diagnosing Problems
  </a>
 </h2>
 <p>
  The following tables can help users diagnose GDS problems.
 </p>
 Make sure to go through following variables and observe if performance is where it needs to be.
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d7005e11076" rowspan="1" valign="top" width="33.33333333333333%">
    Variable impacting performance
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e11080" rowspan="1" valign="top" width="33.33333333333333%">
    Description
   </th>
   <th align="left" class="entry" colspan="1" id="d7005e11083" rowspan="1" valign="top" width="33.33333333333333%">
    Steps to take enable/disable the functionality (“How to”)
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e11076" rowspan="1" valign="top" width="33.33333333333333%">
    Compat mode
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11080" rowspan="1" valign="top" width="33.33333333333333%">
    Disable compat mode in cufile.json
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11083" rowspan="1" valign="top" width="33.33333333333333%">
    <p>
     Set
     allow_compat_mode: false
     in cufile.json.
    </p>
    <p>
     Or
    </p>
    <p>
     Set CUFILE_FORCE_COMPAT_MODE environment variable to false.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e11076" rowspan="1" valign="top" width="33.33333333333333%">
    Log level
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11080" rowspan="1" valign="top" width="33.33333333333333%">
    Set log level to ERROR in cufile.json
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11083" rowspan="1" valign="top" width="33.33333333333333%">
    <p>
     Following setting in cufile.json will set the logging level to ERROR.
    </p>
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>"logging": {
         // log</p>
        </pre>
    <p>
     directory, if not enabled will create log file under current working directory
    </p>
    Copy
    Copied!
    <pre class="language-" data-line="" id="play">
            
            <p>//"dir": "/home/&lt;xxxx&gt;",
         // NOTICE|ERROR|WARN|INFO|DEBUG|TRACE (in decreasing order of severity)
        "level": "ERROR"
         },</p>
        </pre>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e11076" rowspan="1" valign="top" width="33.33333333333333%">
    Nvidia-fs stats
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11080" rowspan="1" valign="top" width="33.33333333333333%">
    Make sure nvidia-fs read/write stats are disabled. These can have a performance impact for small IO sizes. By default these are disabled.
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11083" rowspan="1" valign="top" width="33.33333333333333%">
    <p>
     To check the current state of the stats, use the following command.
    </p>
    <p>
     <span>
      #cat /sys/module/nvidia_fs/parameters/rw_stats_enabled
     </span>
    </p>
    <p>
     0 - Disabled
    </p>
    <p>
     1 - Enabled
    </p>
    <p>
     To disable them,
    </p>
    <p>
     <span>
      echo 0 &gt; /sys/module/nvidia_fs/parameters/rw_stats_enabled
     </span>
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e11076" rowspan="1" valign="top" width="33.33333333333333%">
    GDR stats/RDMA stats (CQE errors)
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e11076" rowspan="1" valign="top" width="33.33333333333333%">
    Relax Ordering
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11080" rowspan="1" valign="top" width="33.33333333333333%">
    <p>
     For distributed file systems, make sure NICs have relax ordering is enabled
    </p>
    <p>
     set “MAX_ACC_OUT_READ=44" on CX-6.
    </p>
    <p>
     Set “MAX_ACC_OUT_READ=128" for CX-7.
    </p>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11083" rowspan="1" valign="top" width="33.33333333333333%">
    <p>
     <span>
      sudo mlxconfig -y -d &lt;NIC&gt; set ADVANCED_PCI_SETTINGS=1
     </span>
    </p>
    <p>
     <span>
      sudo mlxconfig -y -d &lt;NIC&gt; set MAX_ACC_OUT_READ=44|128
     </span>
    </p>
    <p>
     <span>
      sudo reboot
     </span>
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e11076" rowspan="1" valign="top" width="33.33333333333333%">
    Persistent mode
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11080" rowspan="1" valign="top" width="33.33333333333333%">
    Enable persistent mode
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e11076" rowspan="1" valign="top" width="33.33333333333333%">
    Clock speed
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11080" rowspan="1" valign="top" width="33.33333333333333%">
    Set clock speed to maximum
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e11076" rowspan="1" valign="top" width="33.33333333333333%">
    BAR size
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11080" rowspan="1" valign="top" width="33.33333333333333%">
    Make sure BAR size is enabled to maximum possible value
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e11076" rowspan="1" valign="top" width="33.33333333333333%">
    Numa affinity
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11080" rowspan="1" valign="top" width="33.33333333333333%">
    Set numa affinity of the process where NIC-GPU are in the same switch
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d7005e11076" rowspan="1" valign="top" width="33.33333333333333%">
    MRRS
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11080" rowspan="1" valign="top" width="33.33333333333333%">
    Sets the PCIe Max Read Request Size for the NIC/NVMe
    <p>
     Specifying Max Read request size enables the Requestor (NIC/NVME) to read data from the GPU memory upto the specified size to improve Writes from GPU to storage performance.
    </p>
   </td>
   <td align="left" class="entry" colspan="1" headers="d7005e11083" rowspan="1" valign="top" width="33.33333333333333%">
    <p>
     Check the setting using
    </p>
    <p>
     <span>
      # lspci -vvv -s &lt;B:D.F&gt; | grep -i MaxReadReq
     </span>
    </p>
    <p>
     Read the current value
    </p>
    <p>
     <span>
      #setpci -v -s &lt;B:D.F&gt; cap_exp+8.w
     </span>
    </p>
    <p>
     To set to 4K
    </p>
    <p>
     <span>
      #setpci -v -s &lt;B:D.F&gt; cap_exp+8.w=
      5
      000:7000
     </span>
    </p>
    <p>
     To set to 512 bytes
    </p>
    <p>
     <span>
      #setpci -v -s &lt;B:D.F&gt; cap_exp+8.w=
      2
      000:7000
     </span>
    </p>
    <p>
     The acceptable values are: 0 - 128B, 1 - 256B, 2 - 512B, 3 - 1024B, 4 - 2048B and 5 - 4096B.
    </p>
    Note:
    <p>
     Specifying selector indexes outside this range might cause the system to crash.
    </p>
   </td>
  </tr>
 </table>
 <p>
  For ROCE setups, consider additional following items:
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="17.33102253032929%">
    CPU Governor
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="26.51646447140381%">
    Performance
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="56.1525129982669%">
    <span>
     #cpupower frequency-set -g performance
    </span>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="17.33102253032929%">
    RX/TX ring
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="26.51646447140381%">
    Set them to maximum
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="56.1525129982669%">
    <span>
     #ethtool -G $adapter rx $(ethtool -g $adapter | awk '/RX:/ {print $NF; exit}')#ethtool -G $adapter tx $(ethtool -g $adapter | awk '/TX:/ {print $NF; exit}')
    </span>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="17.33102253032929%">
    RX/TX channels
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="26.51646447140381%">
    Set to max allowed
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="56.1525129982669%">
    <span>
     #ethtool -L $adapter combined 15
    </span>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="17.33102253032929%">
    LRO
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="26.51646447140381%">
    Turn on large receive offload
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="56.1525129982669%">
    <span>
     #ethtool -K $adapter lro on
    </span>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="17.33102253032929%">
    IRQ affinity
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="26.51646447140381%">
    Set IRQ affinity to the affine NUMA node
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="17.33102253032929%">
    IRQ balance
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="26.51646447140381%">
    Turn off IRQ balancer
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="56.1525129982669%">
    <span>
     #systemctl stop irqbalance
    </span>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="17.33102253032929%">
    TX queue length
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="26.51646447140381%">
    Increase TX queue length
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="56.1525129982669%">
    <span>
     #ifconfig $adapter $addr/$netmask mtu 9000 txqueuelen 20000 up
    </span>
   </td>
  </tr>
 </table>
 If the above steps do not help, collect the following information and share it with us.
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="17.035775127768314%">
    Measure GDR performance
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="41.39693356047701%">
    For RDMA connectivity and performance issues run ib_read and ib_write tests with cuda and GPU enabled
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="41.56729131175469%">
    <p>
     Please follow instructions at
     <a class="xref" data-cms-ai="0" href="https://github.com/linux-rdma/perftest" shape="rect">
      https://github.com/linux-rdma/perftest
     </a>
    </p>
    <p>
     In tests use the option
    </p>
    <p>
     <span>
      --use_cuda=&lt;gpu index&gt;
     </span>
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="17.035775127768314%">
    Use Nsight systems
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="41.39693356047701%">
    For RDMA connectivity and performance issues run ib_read and ib_write tests with cuda and GPU enabled
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="41.56729131175469%">
    <p>
     <span>
      cat /etc/cufile.json | grep nvtx
     </span>
    </p>
    <p>
     <span>
      // nvtx profiling on/off
     </span>
    </p>
    <p>
     <span>
      “nvtx":
      true
     </span>
     ,
    </p>
    <p>
     <span>
      /usr/local/cuda/bin/nsys profile &lt;command&gt;
     </span>
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="17.035775127768314%">
    Describe env
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="41.39693356047701%">
    Virtual (Docker or actual VM) or BM
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="41.56729131175469%">
    <p>
     <span>
      # printenv
     </span>
    </p>
    <p>
     <span>
      # lsb_release -a
     </span>
    </p>
    <p>
     <span>
      # dmidecode
     </span>
    </p>
    <p>
     <span>
      # docker info
     </span>
    </p>
    <p>
     <span>
      # docker container inspect &lt;container id&gt;
     </span>
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="17.035775127768314%">
    Collect gds logs
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="41.39693356047701%">
    <p>
     Collect cufile.log
    </p>
    <p>
     <span>
      /usr/local/cuda/gds/tools/gds_log_collection.py
     </span>
    </p>
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="41.56729131175469%">
    <span>
     # /usr/local/cuda/gds/tools/gds_log_collection.py
    </span>
   </td>
  </tr>
 </table>
 Debugging:
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="19.72386587771203%">
    Dmesg errors?
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="35.700197238658774%">
    Check for kernel errors
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="44.57593688362918%">
    <p>
     <span>
      # dmesg
     </span>
    </p>
    <p>
     <span>
      # /var/log/kern.log
     </span>
    </p>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="19.72386587771203%">
    MiG mode enabled?
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="35.700197238658774%">
    Check if MIG is enabled
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="44.57593688362918%">
    <span>
     # nvidia-smi mig -lgi
    </span>
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="19.72386587771203%">
    FM enabled or not
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="35.700197238658774%">
    For NVSwitch based systems. Check if fabric manager is running and active without any errors
   </td>
   <td align="left" class="entry" colspan="1" rowspan="1" valign="top" width="44.57593688362918%">
    <p>
     <span>
      # systemctl status nvidia fabricmanager
     </span>
    </p>
   </td>
  </tr>
 </table>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#notices-header">
   Notices
  </a>
 </h2>
 <h3>
  Notice
 </h3>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-1" name="notice__notice-para-1" shape="rect">
  </a>
  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-2" name="notice__notice-para-2" shape="rect">
  </a>
  NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-3" name="notice__notice-para-3" shape="rect">
  </a>
  Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-4" name="notice__notice-para-4" shape="rect">
  </a>
  NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-5" name="notice__notice-para-5" shape="rect">
  </a>
  NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-6" name="notice__notice-para-6" shape="rect">
  </a>
  NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-7" name="notice__notice-para-7" shape="rect">
  </a>
  No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-8" name="notice__notice-para-8" shape="rect">
  </a>
  Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-9" name="notice__notice-para-9" shape="rect">
  </a>
  THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.
 </p>
 <h3>
  OpenCL
 </h3>
 <p>
  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.
 </p>
 <h3>
  Trademarks
 </h3>
 <p>
  NVIDIA, the NVIDIA logo, DGX, DGX-1, DGX-2, DGX-A100, Tesla, and Quadro are trademarks and/or registered trademarks of NVIDIA Corporation in the United States and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.
 </p>
 <span class="Page-copyright-text">
  © 2020-2024 NVIDIA Corporation and affiliates. All rights reserved.
 </span>
 <span class="Page-copyright-update">
  Last updated on Jun 14, 2024.
 </span>
 Topics
 <ul class="Book-items">
  <li class="Book-items-item">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/index.html">
    NVIDIA GPUDirect Storage
   </a>
  </li>
  <li class="Book-items-item">
   <span class="Link">
    NVIDIA GPUDirect Storage
   </span>
   <ul class="Chapter-chapters">
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html">
      NVIDIA GPUDirect Storage Design Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#design-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#data-transfer-gpu-storage">
        2. Data Transfer Issues for GPU and Storage
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#gds-storage-benefits">
        3. GPUDirect Storage Benefits
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#app-sustain">
        4. Application Suitability
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#transfers-to-from-gpu">
          4.1. Transfers To and From the GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#io-bottleneck">
          4.2. Understanding IO Bottlenecks
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#explicit">
          4.3. Explicit GDS APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#pinned">
          4.4. Pinned Memory for DMA Transfers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#cufile-apis">
          4.5. cuFile APIs
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#plat-perf-stable">
        5. Platform Performance Suitability
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#bandw-from-storage">
          5.1. Bandwidth from Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#path-storage-gpu">
          5.2. Paths from Storage to GPUs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#gpu-bar1-size">
          5.3. GPU BAR1 Size
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#call-to-action">
        6. Call to Action
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html">
      NVIDIA GPUDirect Storage Overview Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#overview-intro">
        1. Introduction
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#related-docs">
          1.1. Related Documents
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dev-benefits">
          1.2. Benefits for a Developer
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#intended-uses">
          1.3. Intended Uses
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#func-overview">
        2. Functional Overview
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#explicit-and-direct">
          2.1. Explicit and Direct
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#perf-optimize">
          2.2. Performance Optimizations
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#imp-perf-enhance">
            2.2.1. Implementation Performance Enhancements
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#concurrency-across-threads">
            2.2.2. Concurrency Across Threads
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#asychrony">
            2.2.3. Asynchrony
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#batching">
            2.2.4. Batching
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#streams">
            2.2.5. Use of CUDA Streams in cuFile
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#comp-and-gen">
          2.3. Compatibility and Generality
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#monitoring">
          2.4. Monitoring
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#solution-scope">
          2.5. Scope of the Solutions in GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dynamic-routing-overview">
          2.6. Dynamic Routing
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#cufile-config-dynamic-routing">
            2.6.1. cuFile Configuration for Dynamic Routing
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#cufile-config-for-dfs-mount">
            2.6.2. cuFile Configuration for DFS Mount
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dynamic-routing-cufile-config-validation">
            2.6.3. cuFile Configuration Validation for Dynamic Routing
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#software-arch">
        3. Software Architecture
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#software-comp">
          3.1. Software Components
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#prim-comp">
          3.2. Primary Components
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#gds-workflows">
            3.2.1. Workflows for GDS Functionality
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-1">
            3.2.2. Workflow 1
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-2">
            3.2.3. Workflow 2
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#align-linux-initiatives">
          3.3. Aligning with Other Linux Initiatives
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#deployment">
        4. Deployment
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#deploy-software-comp">
          4.1. Software Components for Deployment
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#using-gds-containers">
          4.2. Using GPUDirect Storage in Containers
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html">
      cuFile API Reference Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#introduction">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#usage">
        2. Usage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#dynamic-interactions">
          2.1. Dynamic Interactions
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#driver-file-buffer">
          2.2. Driver, File, and Buffer Management
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-compatibility-mode">
          2.3. cuFile Compatibility Mode
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-api-specification">
        3. cuFile API Specification
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#data-types">
          3.1. Data Types
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#declarations-and-definitions">
            3.1.1. Declarations and Definitions
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#typedefs">
            3.1.2. Typedefs
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#enumerations">
            3.1.3. Enumerations
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-driver-api">
          3.2. cuFile Driver APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-io-api">
          3.3. cuFile Synchronous IO APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-file-handle-api">
          3.4. cuFile File Handle APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-buffer-api">
          3.5. cuFile Buffer APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-stream-api">
          3.6. cuFile Stream APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-batch-api">
          3.7. cuFile Batch APIs
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-api-functional-specification">
        4. cuFile API Functional Specification
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriver-api-functional-specification">
          4.1. cuFileDriver API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriveropen">
            4.1.1. cuFileDriverOpen
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriverclose">
            4.1.2. cuFileDriverClose
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledrivergetproperties">
            4.1.3. cuFileDriverGetProperties
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetpollmode">
            4.1.4. cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetmaxdirectiosize">
            4.1.5. cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetmaxcachesize">
            4.1.6. cuFileDriverSetMaxCacheSize(size_t max_cache_size)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetmaxpinnedmemsize">
            4.1.7. cuFileDriverSetMaxPinnedMemSize(size_t max_pinned_memory_size)
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-io-api-functional-specification">
          4.2. cuFile IO API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilehandleregister">
            4.2.1. cuFileHandleRegister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#unique_1423451733">
            4.2.2. cuFileHandleDeregister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufileread">
            4.2.3. cuFileRead
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilewrite">
            4.2.4. cuFileWrite
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-memory-mgmt-functional-specification">
          4.3. cuFile Memory Management Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebufregister">
            4.3.1. cuFileBufRegister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebufderegister">
            4.3.2. cuFileBufDeregister
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-stream-api-functional-specification">
          4.4. cuFile Stream API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilestreamregister">
            4.4.1. cuFileStreamRegister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilestreamderegister">
            4.4.2. cuFileStreamDeregister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilereadasync">
            4.4.3. cuFileReadAsync
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilewriteasync">
            4.4.4. cuFileWriteAsync
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-batch-api-functional-specification">
          4.5. cuFile Batch API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiosetup">
            4.5.1. cuFileBatchIOSetUp
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiosubmit">
            4.5.2. cuFileBatchIOSubmit
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiogetstatus">
            4.5.3. cuFileBatchIOGetStatus
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiocancel">
            4.5.4. cuFileBatchIOCancel
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiodestroy">
            4.5.5. cuFileBatchIODestroy
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#sample-program">
        5. Sample Program with cuFile APIs
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#batch-api-known-limitations">
        6. Known Limitations of cuFile Batch APIs
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html">
      NVIDIA GPUDirect Storage Release Notes
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#rn-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#new-features">
        2. New Features and Changes
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#mofed-fs-req">
        3. MLNX_OFED and Filesystem Requirements
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#support-matrix">
        4. Support Matrix
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#gds-enabled-libraries">
        5. GDS Enabled Libraries/Frameworks
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#included-packages">
        6. Included Packages
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#updates-bug-fixes">
        7. Minor Updates and Bug Fixes
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#known-issues">
        8. Known Issues
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#known-limitations">
        9. Known Limitations
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html">
      Getting Started with NVIDIA GPUDirect Storage
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#gs-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#sys-admin">
        2. If you are a system administrator or a performance engineer
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#developer">
        3. If you are a developer
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#oem-odm-csp">
        4. If you are OEM, ODM, CSP
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#troubleshoting-issues">
        5. Troubleshooting GDS issues
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="Book-items-item">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/index.html#understanding-gpudirect-storage">
    Understanding GPUDirect Storage
   </a>
   <ul class="Chapter-chapters">
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html">
      NVIDIA GPUDirect Storage Best Practices Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#gds-bp-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#settings">
        2. Software Settings
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#system-settings">
          2.1. System Settings
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cuda-context-in-gpu-kernels">
          2.2. Use of CUDA Context in GPU Kernels and Storage IO
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-config">
          2.3. cuFile Configuration Settings
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#api-usage">
        3. API Usage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-driveropen">
          3.1. cuFileDriverOpen
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-handle-register">
          3.2. cuFileHandleRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-bufregister-fileread-filewrite">
          3.3. cuFileBufRegister, cuFileRead, cuFileWrite, cuFileBatchIOSubmit, cuFileBatchIOGetStatus, cuFileReadAsync, cuFileWriteAsync, and cuFileStreamRegister
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-1">
            3.3.1. IO Pattern 1
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-2">
            3.3.2. IO Pattern 2
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-3">
            3.3.3. IO Pattern 3
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-4">
            3.3.4. IO Pattern 4
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-5">
            3.3.5. IO Pattern 5
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-6">
            3.3.6. IO Pattern 6
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-7">
            3.3.7. IO Pattern 7
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-handle-deregister">
          3.4. cuFileHandleDeregister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-buf-deregister">
          3.5. cuFileBufDeregister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-stream-register">
          3.6. cuFileStreamRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#unique_880314822">
          3.7. cuFileStreamDeregister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-driver-close">
          3.8. cuFileDriverClose
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html">
      NVIDIA GPUDirect Storage Benchmarking and Configuration Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#introduction">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#about-this-guide">
        2. About this Guide
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-gds">
        3. Benchmarking GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#determining-pcie-device-affinity">
          3.1. Determining PCIe Device Affinity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-configuration-parameters">
          3.2. GPUDirect Storage Configuration Parameters
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#system-parameters">
            3.2.1. System Parameters
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-parameters">
            3.2.2. GPUDirect Storage Parameters
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-tools">
          3.3. GPUDirect Storage Benchmarking Tools
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gdsio">
            3.3.1. gdsio Utility
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-stats">
            3.3.2. gds-stats Tool
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-direct-attached-storage-das">
        4. GPUDirect Storage Benchmarking on Direct Attached Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-perf-dgx2">
          4.1. GPUDirect Storage Performance on DGX-2 System
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-perf-dgx-a100">
          4.2. GPUDirect Storage Performance on a DGX A100 System
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-nas">
        5. GPUDirect Storage Benchmarking on Network Attached Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-nfs">
          5.1. GPUDirect Storage Benchmarking on NFS
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#summary">
        6. Summary
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-and-performance">
        A. Benchmarking and Performance
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#the-language-of-performance">
          A.1. The Language of Performance
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-storage-performance">
          A.2. Benchmarking Storage Performance
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html">
      NVIDIA GPUDirect Storage Installation and Troubleshooting Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-install">
        2. Installing GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-prereqs">
          2.1. Before You Install GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-installing">
          2.2. Installing GDS
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#config-file-system-settings">
            2.2.1. Configuring File System Settings for GDS
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-suc-install">
            2.2.2. Verifying a Successful GDS Installation
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-lib-tools">
          2.3. Installed GDS Libraries and Tools
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#uninstall-gds">
          2.4. Uninstalling GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#environment-variables">
          2.5. Environment Variables Used by GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#json-config-params">
          2.6. JSON Config Parameters Used by GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-config">
          2.7. GDS Configuration File Changes to Support Dynamic Routing
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-installed-version">
          2.8. Determining Which Version of GDS is Installed
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#experimental-repos-dgx">
          2.9. Experimental Repos for Network Install of GDS Packages for DGX Systems
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-errors">
        3. API Errors
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-1">
          3.1. CU_FILE_DRIVER_NOT_INITIALIZED
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-2">
          3.2. CU_FILE_DEVICE_NOT_SUPPORTED
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-3">
          3.3. CU_FILE_IO_NOT_SUPPORTED
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-4">
          3.4. CU_FILE_CUDA_MEMORY_TYPE_INVALID
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-troubleshooting">
        4. Basic Troubleshooting
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#log-files-gds-lib">
          4.1. Log Files for the GDS Library
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-diff-log-file-app">
          4.2. Enabling a Different cufile.log File for Each Application
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-trace-lib-calls">
          4.3. Enabling Tracing GDS Library API Calls
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-5">
          4.4. cuFileHandleRegister Error
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-cufile-errors">
          4.5. Troubleshooting Applications that Return cuFile Errors
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#error-no-activity-gds-stats">
          4.6. cuFile-* Errors with No Activity in GPUDirect Storage Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cuda-error-35">
          4.7. CUDA Runtime and Driver Mismatch with Error Code 35
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cuda-api-errors">
          4.8. CUDA API Errors when Running the cuFile-* APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#find-driver-stats">
          4.9. Finding GDS Driver Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-io-activity-gds-driver">
          4.10. Tracking IO Activity that Goes Through the GDS Driver
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#read-write-bandwidth-latency-nos">
          4.11. Read/Write Bandwidth and Latency Numbers in GDS Stats
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-regis-deregis-gpu-buffers">
          4.12. Tracking Registration and Deregistration of GPU Buffers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enabling-rdma-logging">
          4.13. Enabling RDMA-specific Logging for Userspace File Systems
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#system-not-ready">
          4.14. CUDA_ERROR_SYSTEM_NOT_READY After Installation
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#adding-udev-rules">
          4.15. Adding udev Rules for RAID Volumes
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#incomplete-write-on-nvme-drives">
          4.16. When You Observe “Incomplete write” on NVME Drives
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-async-io-failing">
          4.17. CUFILE async I/O is failing
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#advanced-troubleshooting">
        5. Advanced Troubleshooting
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#hung-cufile-no-response">
          5.1. Resolving Hung cuFile* APIs with No Response
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#kernel-panic-stack-traces">
          5.2. Sending Relevant Data to Customer Support
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-failure-stack-trace-warning">
          5.3. Resolving an IO Failure with EIO and Stack Trace Warning
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#control-gpu-bar-usage">
          5.4. Controlling GPU BAR Memory Usage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-how-much-cache">
          5.5. Determining the Amount of Cache to Set Aside
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-bar-mem-usage">
          5.6. Monitoring BAR Memory Usage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enomem-error-code">
          5.7. Resolving an ENOMEM Error Code
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-comp-mode">
          5.8. GDS and Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-comp-mode">
          5.9. Enabling Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-enable-comp-mode">
          5.10. Tracking the IO After Enabling Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#bypass-gds">
          5.11. Bypassing GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-not-working-mount">
          5.12. GDS Does Not Work for a Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-posix-io-same-file">
          5.13. Simultaneously Running the GPUDirect Storage IO and POSIX IO on the Same File
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-data-verif-tests">
          5.14. Running Data Verification Tests Using GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html">
          NVIDIA GPUDirect Storage Installation and Troubleshooting Guide
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-perf">
        6. Troubleshooting Performance
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#perf-benchmark-examples">
          6.1. Running Performance Benchmarks with GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-internal-cache">
          6.2. Tracking Whether GPUDirect Storage is Using an Internal Cache
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-cross-pcie-root-complex">
          6.3. Tracking when IO Crosses the PCIe Root Complex and Impacts Performance
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-cpu-usage">
          6.4. Using GPUDirect Statistics to Monitor CPU Activity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monit-perf-tracing">
          6.5. Monitoring Performance and Tracing with cuFile-* APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#ex-linux-tools">
          6.6. Example: Using Linux Tracing Tools
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trace-cufile-apis">
          6.7. Tracing the cuFile-* APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-improving-performance">
          6.8. Improving Performance using Dynamic Routing
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trouble-io-activity">
        7. Troubleshooting IO Activity
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#manage-coherency-page-cache">
          7.1. Managing Coherency of Data in the Page Cache and on Disk
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-fs-lnet">
        8. EXAScaler Filesystem LNet Troubleshooting
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-client-mod-version">
          8.1. Determining the EXAScaler Filesystem Client Module Version
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#lnet-network-setup">
          8.2. Checking the LNet Network Setup on a Client
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-peer-health">
          8.3. Checking the Health of the Peers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#multi-rail-support">
          8.4. Checking for Multi-Rail Support
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-peer-affinity">
          8.5. Checking GDS Peer Affinity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-lnet-errors">
          8.6. Checking for LNet-Level Errors
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#lnet-nids-deg-timeouts">
          8.7. Resolving LNet NIDs Health Degradation from Timeouts
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#multi-osts-peer-sel">
          8.8. Configuring LNet Networks with Multiple OSTs for Optimal Peer Selection
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-fs-perf">
        9. Understanding EXAScaler Filesystem Performance
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#osc-tune-perf-param">
          9.1. osc Tuning Performance Parameters
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#misc-comms">
          9.2. Miscellaneous Commands for osc, mdc, and stripesize
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#num-config-disks">
          9.3. Getting the Number of Configured Object-Based Disks
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#addl-stats-exascaler-fs">
          9.4. Getting Additional Statistics related to the EXAScaler Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#get-md-stats">
          9.5. Getting Metadata Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-exist-mount-lustre">
          9.6. Checking for an Existing Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-fs-cluster">
          9.7. Unmounting an EXAScaler Filesystem Cluster
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#sum-exacaler-fs-stats">
          9.8. Getting a Summary of EXAScaler Filesystem Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-poll-mode">
          9.9. Using GPUDirect Storage in Poll Mode
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trouble-faq-wekafs">
        10. Troubleshooting and FAQ for the WekaIO Filesystem
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#download-wekaio-cp">
          10.1. Downloading the WekaIO Client Package
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#wekaio-version-gds">
          10.2. Determining Whether the WekaIO Version is Ready for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-wekaio-fs-cluster">
          10.3. Mounting a WekaIO File System Cluster
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#resolve-fail-mount">
          10.4. Resolving a Failing Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#resolve-wekaio-usage">
          10.5. Resolving 100% Usage for WekaIO for Two Cores
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-existing-mount-wekafs">
          10.6. Checking for an Existing Mount in the Weka File System
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-summ-wekaio">
          10.7. Checking for a Summary of the WekaIO Filesystem Status
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#summ-wekaio-stats">
          10.8. Displaying the Summary of the WekaIO Filesystem Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#wekaio-writes-posix">
          10.9. Why WekaIO Writes Go Through POSIX
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-nvdiafsko-support">
          10.10. Checking for nvidia-fs.ko Support for Memory Peer Direct
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-mem-peer-stats">
          10.11. Checking Memory Peer Direct Stats
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#rel-nvidia-fs-stats">
          10.12. Checking for Relevant nvidia-fs Statistics for the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-wekaio-fs-test">
          10.13. Conducting a Basic WekaIO Filesystem Test
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-wekaio-fs-cluster">
          10.14. Unmounting a WekaIO File System Cluster
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-installed-libs-wekaio-fs">
          10.15. Verify the Installed Libraries for the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-config-file-changes">
          10.16. GDS Configuration File Changes to Support the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#relevant-userspace-stats-wekaio-fs">
          10.17. Check for Relevant User-Space Statistics for the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-wekafs-support">
          10.18. Check for WekaFS Support
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#spectrum-scale-intro">
        11. Enabling IBM Spectrum Scale Support with GDS
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#spectrum-scale-limitations-with-gds">
          11.1. IBM Spectrum Scale Limitations with GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-for-peerdirect-support">
          11.2. Checking nvidia-fs.ko Support for Mellanox PeerDirect
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verifying-libraries-for-spectrum-scale">
          11.3. Verifying Installed Libraries for IBM Spectrum Scale
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-peerdirect-stats">
          11.4. Checking PeerDirect Stats
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-for-relevant-stats">
          11.5. Checking for Relevant nvidia-fs Stats with IBM Spectrum Scale
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-user-space-stats">
          11.6. GDS User Space Stats for IBM Spectrum Scale for Each Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-configs-to-support-spectrum-scale">
          11.7. GDS Configuration to Support IBM Spectrum Scale
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#scenarios-compat-mode">
          11.8. Scenarios for Falling Back to Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-limitations-with-spectrum-scale">
          11.9. GDS Limitations with IBM Spectrum Scale
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-intro">
        12. NetApp E-series BeeGFS with GDS Solution Deployment
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-package-requirements">
          12.1. Netapp BeeGFS/GPUDirect Storage and Package Requirements
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-client-config-for-gds">
          12.2. BeeGFS Client Configuration for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-gpu-hca-topology">
          12.3. GPU/HCA Topology on the Client - DGX-A100 and OSS servers Client Server
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-setup">
          12.4. Verify the Setup
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-management-node">
            12.4.1. List the Management Node
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-metadata-nodes">
            12.4.2. List the Metadata Nodes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-storage-nodes">
            12.4.3. List the Storage Nodes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-client-nodes">
            12.4.4. List the Client Nodes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-display-client-connections">
            12.4.5. Display Client Connections
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-connectivity-svcs">
            12.4.6. Verify Connectivity to the Different Services
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-storage-pools">
            12.4.7. List Storage Pools
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-display-free-space-inodes">
            12.4.8. Display the Free Space and inodes on the Storage and Metadata Targets
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-testing">
          12.5. Testing
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-integration">
            12.5.1. Verifying Integration is Working
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-conducting-basic-fs-test">
            12.5.2. Conducting a Basic NetApp BeeGFS Filesystem Test
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-set-up-vast-data">
        13. Setting Up and Troubleshooting VAST Data (NFSoRDMA+MultiPath)
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-mofed-vast">
          13.1. Installing MLNX_OFED and VAST NFSoRDMA+Multipath Packages
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#client-software-reqs">
            13.1.1. Client Software Requirements
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-vast">
            13.1.2. Install the VAST Multipath Package
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#networking-setup">
          13.2. Set Up the Networking
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#vast-network-config">
            13.2.1. VAST Network Configuration
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cllent-network-config">
            13.2.2. Client Network Configuration
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-network-connect">
            13.2.3. Verify Network Connectivity
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-vast-nfs">
          13.3. Mount VAST NFS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#debug-monitor">
          13.4. Debugging and Monitoring VAST Data
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nvme-nvmeof-support">
        14. Troubleshooting and FAQ for NVMe and NVMeOF Support
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mofed-req-install">
          14.1. MLNX_OFED Requirements and Installation
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-nvme-support-gds">
          14.2. Determining Whether the NVMe device is Supported for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-raid-level">
          14.3. RAID Support in GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-local-fs">
          14.4. Mounting a Local Filesystem for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-exist-mount">
          14.5. Check for an Existing EXT4 Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-io-stats-block-devmt">
          14.6. Check for IO Statistics with Block Device Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#raid-group-config-gpu-aff">
          14.7. RAID Group Configuration for GPU Affinity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-ext4-fs-test">
          14.8. Conduct a Basic EXT4 Filesystem Test
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-ext4-fs">
          14.9. Unmount a EXT4 Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#udev-name-conv-block-device">
          14.10. Udev Device Naming for a Block Device
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#batch-io-performance">
          14.11. BATCH I/O Performance
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#display-gds-driver-stats">
        15. Displaying GDS NVIDIA FS Driver Statistics
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nvidia-fs-stats2">
          15.1. nvidia-fs Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#analyze-per-gpu-stats">
          15.2. Analyze Statistics for each GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#reset-nvidia-fs-stats">
          15.3. Resetting the nvidia-fs Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#review-peer-aff-stats">
          15.4. Checking Peer Affinity Stats for a Kernel Filesystem and Storage Drivers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#peer-affinity-usage-kernel-fs-sd">
          15.5. Checking the Peer Affinity Usage for a Kernel File System and Storage Drivers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gpu-peer-dist-table">
          15.6. Display the GPU-to-Peer Distance Table
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gdsio-tool">
          15.7. The GDSIO Tool
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tab-fields">
          15.8. Tabulated Fields
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gdscheck">
          15.9. The GDSCHECK Tool
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nfs-suppport-gds">
          15.10. NFS Support with GPUDirect Storage
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-nfs-server-rdma-mofed_5-1">
            15.10.1. Install Linux NFS server with RDMA Support on MLNX_OFED 5.3 or Later
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-gds-supp-nfs-client">
            15.10.2. Install GPUDirect Storage Support for the NFS Client
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nfs-gds-stas-debug">
          15.11. NFS GPUDirect Storage Statistics and Debugging
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-io-behavior">
          15.12. GPUDirect Storage IO Behavior
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#read-write-atomiticity-cons">
            15.12.1. Read/Write Atomicity Consistency with GPUDirect Storage Direct IO
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#write-file-in-o-append-mode">
            15.12.2. Write with File a Opened in O_APPEND Mode (cuFileWrite)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gpu-nic-peer-aff">
            15.12.3. GPU to NIC Peer Affinity
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#comp-mode-unreg-buffers">
            15.12.4. Compatible Mode with Unregistered Buffers
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unaligned-writes-non-reg-buff">
            15.12.5. Unaligned writes with Non-Registered Buffers
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#process-hang-nfs">
            15.12.6. Process Hang with NFS
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tool-limit-cuda-9">
            15.12.7. Tools Support Limitations for CUDA 9 and Earlier
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-stats">
          15.13. GDS Statistics for Dynamic Routing
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-peer-affinity">
            15.13.1. Peer Affinity Dynamic Routing
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-cufile-log">
            15.13.2. cuFile Log Related to Dynamic Routing
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-lib-tracing">
        16. GDS Library Tracing
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#display-tracepoints">
          16.1. Example: Display Tracepoints
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tracepoint-arguments">
            16.1.1. Example: Tracepoint Arguments
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-io-issue-cuapis">
          16.2. Example: Track the IO Activity of a Process that Issues cuFileRead/ cuFileWrite
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#disp-io-patters-through-gds">
          16.3. Example: Display the IO Pattern of all the IOs that Go Through GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-pattern-process">
          16.4. Understand the IO Pattern of a Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-pattern-file-desc">
          16.5. IO Pattern of a Process with the File Descriptor on Different GPUs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#iops-bandwidth-process-gpu">
          16.6. Determine the IOPS and Bandwidth for a Process in a GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#freq-reads-cufileread-api">
          16.7. Display the Frequency of Reads by Processes that Issue cuFileRead
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#freq-reads-cufileread-0.1-ms">
          16.8. Display the Frequency of Reads when cuFileRead Takes More than 0.1 ms
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#disp-latency-cufileread">
          16.9. Displaying the Latency of cuFileRead for Each Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-process-cufilebufregister">
          16.10. Example: Tracking the Processes that Issue cuFileBufRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#proc-constant-cufilebufregister">
          16.11. Example: Tracking Whether the Process is Constant when Invoking cuFileBufRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-io-bb">
          16.12. Example: Monitoring IOs that are Going Through the Bounce Buffer
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trace-cufileread-cufilewrite-issues">
          16.13. Example: Tracing cuFileRead and cuFileWrite Failures, Print, Error Codes, and Time of Failure
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#per-process-user-stats">
          16.14. Example: User-Space Statistics for Each GDS Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-stats-tool">
          16.15. Example: Viewing GDS User-Level Statistics for a Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#sample-ul-stats-per-process">
          16.16. Example: Displaying Sample User-Level Statistics for each GDS Process
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-space-counters">
        17. User-Space Counters in GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dist-io-util-gpu">
          17.1. Distribution of IO Usage in Each GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-user-space-stats">
          17.2. User-space Statistics for Dynamic Routing
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#user-space-rdma-counters">
        18. User-Space RDMA Counters in GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-io-counters">
          18.1. cuFile RDMA IO Counters (PER_GPU RDMA STATS)
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-mem-reg-counters">
          18.2. cuFile RDMA Memory Registration Counters (RDMA MRSTATS)
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cheat-sheet">
        19. Cheat Sheet for Diagnosing Problems
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html">
      NVIDIA GPUDirect Storage O_DIRECT Requirements Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#intro">
        1. Introduction
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#rel-docs">
          1.1. Related Documents
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#gds-req">
        2. GPUDirect Storage Requirements
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#summ-basic-req">
          2.1. Summary of Basic Requirements
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#client-server">
          2.2. Client and Server
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#odirect-not-a-fit">
          2.3. Cases Where O_DIRECT is Not a Fit
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#buffered-io">
            2.3.1. Buffered IO
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#inline-files">
            2.3.2. Inline Files
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#block-alloc-writes">
            2.3.3. Block Allocation For Writes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#examine-transform-data">
            2.3.4. Examining or Transforming User Data
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#summary">
            2.3.5. Summary
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
     </ul>
    </li>
   </ul>
  </li>
 </ul>
 <ul class="FooterNavigation-items" data-column-count="3">
  <li class="FooterNavigation-items-item">
   <span>
    Corporate Info
   </span>
   <ul class="FooterNavigationItem-items">
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://www.nvidia.com/en-us/" target="_blank">
      <span class="NavigationLink-text">
       NVIDIA.com Home
      </span>
     </a>
    </li>
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/" target="_blank">
      <span class="NavigationLink-text">
       About NVIDIA
      </span>
     </a>
    </li>
   </ul>
  </li>
  <li class="FooterNavigation-items-item">
   <span>
    ‎NVIDIA Developer
   </span>
   <ul class="FooterNavigationItem-items">
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://developer.nvidia.com/" target="_blank">
      <span class="NavigationLink-text">
       Developer Home
      </span>
     </a>
    </li>
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://blogs.nvidia.com/" target="_blank">
      <span class="NavigationLink-text">
       Blog
      </span>
     </a>
    </li>
   </ul>
  </li>
  <li class="FooterNavigation-items-item">
   <span>
    Resources
   </span>
   <ul class="FooterNavigationItem-items">
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://www.nvidia.com/en-us/contact/" target="_blank">
      <span class="NavigationLink-text">
       Contact Us
      </span>
     </a>
    </li>
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://developer.nvidia.com/developer-program" target="_blank">
      <span class="NavigationLink-text">
       Developer Program
      </span>
     </a>
    </li>
   </ul>
  </li>
 </ul>
 <p>
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" rel="noopener" target="_blank">
   Privacy Policy
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" rel="noopener" target="_blank">
   Manage My Privacy
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/preferences/start/" rel="noopener" target="_blank">
   Do Not Sell or Share My Data
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" rel="noopener" target="_blank">
   Terms of Service
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" rel="noopener" target="_blank">
   Accessibility
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" rel="noopener" target="_blank">
   Corporate Policies
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/product-security/" rel="noopener" target="_blank">
   Product Security
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/contact/" rel="noopener" target="_blank">
   Contact
  </a>
 </p>
 <p>
  Copyright © 2024 NVIDIA Corporation
 </p>
</body>
</body></html>