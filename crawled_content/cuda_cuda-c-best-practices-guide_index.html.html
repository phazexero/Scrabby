<html><head><title>CUDA C++ Best Practices Guide</title></head><body><body class="wy-body-for-nav">
 <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/contents.html">
 </a>
 <ul class="current">
  <li class="toctree-l1 current">
   <a class="current reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html">
    1. Preface
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#what-is-this-document">
      1.1. What Is This Document?
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#who-should-read-this-guide">
      1.2. Who Should Read This Guide?
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#assess-parallelize-optimize-deploy">
      1.3. Assess, Parallelize, Optimize, Deploy
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#assess">
        1.3.1. Assess
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#parallelize">
        1.3.2. Parallelize
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#optimize">
        1.3.3. Optimize
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#deploy">
        1.3.4. Deploy
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#recommendations-and-best-practices">
      1.4. Recommendations and Best Practices
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#assessing-your-application">
      1.5. Assessing Your Application
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#heterogeneous-computing">
    2. Heterogeneous Computing
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#differences-between-host-and-device">
      2.1. Differences between Host and Device
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#what-runs-on-a-cuda-enabled-device">
      2.2. What Runs on a CUDA-Enabled Device?
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#application-profiling">
    3. Application Profiling
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#profile">
      3.1. Profile
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#creating-the-profile">
        3.1.1. Creating the Profile
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#identifying-hotspots">
        3.1.2. Identifying Hotspots
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#understanding-scaling">
        3.1.3. Understanding Scaling
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strong-scaling-and-amdahl-s-law">
          3.1.3.1. Strong Scaling and Amdahlâs Law
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#weak-scaling-and-gustafson-s-law">
          3.1.3.2. Weak Scaling and Gustafsonâs Law
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#applying-strong-and-weak-scaling">
          3.1.3.3. Applying Strong and Weak Scaling
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#parallelizing-your-application">
    4. Parallelizing Your Application
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#getting-started">
    5. Getting Started
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#parallel-libraries">
      5.1. Parallel Libraries
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#parallelizing-compilers">
      5.2. Parallelizing Compilers
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#coding-to-expose-parallelism">
      5.3. Coding to Expose Parallelism
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#getting-the-right-answer">
    6. Getting the Right Answer
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#verification">
      6.1. Verification
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#reference-comparison">
        6.1.1. Reference Comparison
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#unit-testing">
        6.1.2. Unit Testing
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#debugging">
      6.2. Debugging
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#numerical-accuracy-and-precision">
      6.3. Numerical Accuracy and Precision
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#single-vs-double-precision">
        6.3.1. Single vs. Double Precision
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#floating-point-math-is-not-associative">
        6.3.2. Floating Point Math Is not Associative
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#ieee-754-compliance">
        6.3.3. IEEE 754 Compliance
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#x86-80-bit-computations">
        6.3.4. x86 80-bit Computations
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#optimizing-cuda-applications">
    7. Optimizing CUDA Applications
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#performance-metrics">
    8. Performance Metrics
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#timing">
      8.1. Timing
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#using-cpu-timers">
        8.1.1. Using CPU Timers
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#using-cuda-gpu-timers">
        8.1.2. Using CUDA GPU Timers
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#bandwidth">
      8.2. Bandwidth
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#theoretical-bandwidth-calculation">
        8.2.1. Theoretical Bandwidth Calculation
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#effective-bandwidth-calculation">
        8.2.2. Effective Bandwidth Calculation
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#throughput-reported-by-visual-profiler">
        8.2.3. Throughput Reported by Visual Profiler
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations">
    9. Memory Optimizations
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#data-transfer-between-host-and-device">
      9.1. Data Transfer Between Host and Device
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#pinned-memory">
        9.1.1. Pinned Memory
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-and-overlapping-transfers-with-computation">
        9.1.2. Asynchronous and Overlapping Transfers with Computation
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#zero-copy">
        9.1.3. Zero Copy
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#unified-virtual-addressing">
        9.1.4. Unified Virtual Addressing
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#device-memory-spaces">
      9.2. Device Memory Spaces
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#coalesced-access-to-global-memory">
        9.2.1. Coalesced Access to Global Memory
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#a-simple-access-pattern">
          9.2.1.1. A Simple Access Pattern
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#a-sequential-but-misaligned-access-pattern">
          9.2.1.2. A Sequential but Misaligned Access Pattern
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#effects-of-misaligned-accesses">
          9.2.1.3. Effects of Misaligned Accesses
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strided-accesses">
          9.2.1.4. Strided Accesses
         </a>
        </li>
       </ul>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#l2-cache">
        9.2.2. L2 Cache
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#l2-cache-access-window">
          9.2.2.1. L2 Cache Access Window
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#tuning-the-access-window-hit-ratio">
          9.2.2.2. Tuning the Access Window Hit-Ratio
         </a>
        </li>
       </ul>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory">
        9.2.3. Shared Memory
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-and-memory-banks">
          9.2.3.1. Shared Memory and Memory Banks
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab">
          9.2.3.2. Shared Memory in Matrix Multiplication (C=AB)
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-aat">
          9.2.3.3. Shared Memory in Matrix Multiplication (C=AAT)
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-copy-from-global-memory-to-shared-memory">
          9.2.3.4. Asynchronous Copy from Global Memory to Shared Memory
         </a>
        </li>
       </ul>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#local-memory">
        9.2.4. Local Memory
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#texture-memory">
        9.2.5. Texture Memory
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#additional-texture-capabilities">
          9.2.5.1. Additional Texture Capabilities
         </a>
        </li>
       </ul>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#constant-memory">
        9.2.6. Constant Memory
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#registers">
        9.2.7. Registers
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#register-pressure">
          9.2.7.1. Register Pressure
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#allocation">
      9.3. Allocation
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#numa-best-practices">
      9.4. NUMA Best Practices
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#execution-configuration-optimizations">
    10. Execution Configuration Optimizations
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy">
      10.1. Occupancy
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#calculating-occupancy">
        10.1.1. Calculating Occupancy
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#hiding-register-dependencies">
      10.2. Hiding Register Dependencies
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#thread-and-block-heuristics">
      10.3. Thread and Block Heuristics
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#effects-of-shared-memory">
      10.4. Effects of Shared Memory
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#concurrent-kernel-execution">
      10.5. Concurrent Kernel Execution
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#multiple-contexts">
      10.6. Multiple contexts
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#instruction-optimization">
    11. Instruction Optimization
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#arithmetic-instructions">
      11.1. Arithmetic Instructions
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#division-modulo-operations">
        11.1.1. Division Modulo Operations
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#loop-counters-signed-vs-unsigned">
        11.1.2. Loop Counters Signed vs. Unsigned
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#reciprocal-square-root">
        11.1.3. Reciprocal Square Root
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#other-arithmetic-instructions">
        11.1.4. Other Arithmetic Instructions
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#exponentiation-with-small-fractional-arguments">
        11.1.5. Exponentiation With Small Fractional Arguments
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#math-libraries">
        11.1.6. Math Libraries
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#precision-related-compiler-flags">
        11.1.7. Precision-related Compiler Flags
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-instructions">
      11.2. Memory Instructions
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#control-flow">
    12. Control Flow
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#branching-and-divergence">
      12.1. Branching and Divergence
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#branch-predication">
      12.2. Branch Predication
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#deploying-cuda-applications">
    13. Deploying CUDA Applications
   </a>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#understanding-the-programming-environment">
    14. Understanding the Programming Environment
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
      14.1. CUDA Compute Capability
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#additional-hardware-data">
      14.2. Additional Hardware Data
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#which-compute-capability-target">
      14.3. Which Compute Capability Target
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-runtime">
      14.4. CUDA Runtime
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-developer-s-guide">
    15. CUDA Compatibility Developerâs Guide
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-toolkit-versioning">
      15.1. CUDA Toolkit Versioning
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#source-compatibility">
      15.2. Source Compatibility
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility">
      15.3. Binary Compatibility
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-binary-cubin-compatibility">
        15.3.1. CUDA Binary (cubin) Compatibility
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-across-minor-releases">
      15.4. CUDA Compatibility Across Minor Releases
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#existing-cuda-applications-within-minor-versions-of-cuda">
        15.4.1. Existing CUDA Applications within Minor Versions of CUDA
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#handling-new-cuda-features-and-driver-apis">
          15.4.1.1. Handling New CUDA Features and Driver APIs
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#using-ptx">
          15.4.1.2. Using PTX
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#dynamic-code-generation">
          15.4.1.3. Dynamic Code Generation
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#recommendations-for-building-a-minor-version-compatible-library">
          15.4.1.4. Recommendations for building a minor-version compatible library
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#recommendations-for-taking-advantage-of-minor-version-compatibility-in-your-application">
          15.4.1.5. Recommendations for taking advantage of minor version compatibility in your application
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#preparing-for-deployment">
    16. Preparing for Deployment
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#testing-for-cuda-availability">
      16.1. Testing for CUDA Availability
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#error-handling">
      16.2. Error Handling
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#building-for-maximum-compatibility">
      16.3. Building for Maximum Compatibility
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#distributing-the-cuda-runtime-and-libraries">
      16.4. Distributing the CUDA Runtime and Libraries
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-toolkit-library-redistribution">
        16.4.1. CUDA Toolkit Library Redistribution
       </a>
       <ul>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#which-files-to-redistribute">
          16.4.1.1. Which Files to Redistribute
         </a>
        </li>
        <li class="toctree-l4">
         <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#where-to-install-redistributed-cuda-libraries">
          16.4.1.2. Where to Install Redistributed CUDA Libraries
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#deployment-infrastructure-tools">
    17. Deployment Infrastructure Tools
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#nvidia-smi">
      17.1. Nvidia-SMI
     </a>
     <ul>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#queryable-state">
        17.1.1. Queryable state
       </a>
      </li>
      <li class="toctree-l3">
       <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#modifiable-state">
        17.1.2. Modifiable state
       </a>
      </li>
     </ul>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#nvml">
      17.2. NVML
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cluster-management-tools">
      17.3. Cluster Management Tools
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#compiler-jit-cache-management-tools">
      17.4. Compiler JIT Cache Management Tools
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-visible-devices">
      17.5. CUDA_VISIBLE_DEVICES
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#id3">
    18. Recommendations and Best Practices
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#overall-performance-optimization-strategies">
      18.1. Overall Performance Optimization Strategies
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#nvcc-compiler-switches">
    19. nvcc Compiler Switches
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#nvcc">
      19.1. nvcc
     </a>
    </li>
   </ul>
  </li>
  <li class="toctree-l1">
   <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#notices">
    20. Notices
   </a>
   <ul>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#notice">
      20.1. Notice
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#opencl">
      20.2. OpenCL
     </a>
    </li>
    <li class="toctree-l2">
     <a class="reference internal" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#trademarks">
      20.3. Trademarks
     </a>
    </li>
   </ul>
  </li>
 </ul>
 <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/contents.html">
  CUDA C++ Best Practices Guide
 </a>
 <ul class="wy-breadcrumbs">
  <li>
   <a class="icon icon-home" href="https://docs.nvidia.com/cuda/index.html">
   </a>
   Â»
  </li>
  <li>
   <span class="section-number">
    1.
   </span>
   Preface
  </li>
  <li class="wy-breadcrumbs-aside">
   <span>
    v12.5 |
   </span>
   <a class="reference external" href="https://docs.nvidia.com/cuda/pdf/CUDA_C_Best_Practices_Guide.pdf">
    PDF
   </a>
   <span>
    |
   </span>
   <a class="reference external" href="https://developer.nvidia.com/cuda-toolkit-archive">
    Archive
   </a>
   <span>
    Â
   </span>
  </li>
 </ul>
 <p class="rubric-h1 rubric">
  CUDA C++ Best Practices Guide
 </p>
 <p>
  The programming guide to using the CUDA Toolkit to obtain the best performance from NVIDIA GPUs.
 </p>
 <h1>
  <span class="section-number">
   1.
  </span>
  Preface
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#preface" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   1.1.
  </span>
  What Is This Document?
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#what-is-this-document" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This Best Practices Guide is a manual to help developers obtain the best performance from NVIDIA
  Â®
  CUDA
  Â®
  GPUs. It presents established parallelization and optimization techniques and explains coding metaphors and idioms that can greatly simplify programming for CUDA-capable GPU architectures.
 </p>
 <p>
  While the contents can be used as a reference manual, you should be aware that some topics are revisited in different contexts as various programming and configuration topics are explored. As a result, it is recommended that first-time readers proceed through the guide sequentially. This approach will greatly improve your understanding of effective programming practices and enable you to better use the guide for reference later.
 </p>
 <h2>
  <span class="section-number">
   1.2.
  </span>
  Who Should Read This Guide?
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#who-should-read-this-guide" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The discussions in this guide all use the C++ programming language, so you should be comfortable reading C++ code.
 </p>
 <p>
  This guide refers to and relies on several other documents that you should have at your disposal for reference, all of which are available at no cost from the CUDA website
  <a class="reference external" href="https://docs.nvidia.com/cuda/">
   https://docs.nvidia.com/cuda/
  </a>
  . The following documents are especially important resources:
 </p>
 <ul class="simple">
  <li>
   <p>
    CUDA Installation Guide
   </p>
  </li>
  <li>
   <p>
    CUDA C++ Programming Guide
   </p>
  </li>
  <li>
   <p>
    CUDA Toolkit Reference Manual
   </p>
  </li>
 </ul>
 <p>
  In particular, the optimization section of this guide assumes that you have already successfully downloaded and installed the CUDA Toolkit (if not, please refer to the relevant CUDA Installation Guide for your platform) and that you have a basic familiarity with the CUDA C++ programming language and environment (if not, please refer to the CUDA C++ Programming Guide).
 </p>
 <h2>
  <span class="section-number">
   1.3.
  </span>
  Assess, Parallelize, Optimize, Deploy
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#assess-parallelize-optimize-deploy" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This guide introduces the
  Assess, Parallelize, Optimize, Deploy(APOD)
  design cycle for applications with the goal of helping application developers to rapidly identify the portions of their code that would most readily benefit from GPU acceleration, rapidly realize that benefit, and begin leveraging the resulting speedups in production as early as possible.
 </p>
 <p>
  APOD is a cyclical process: initial speedups can be achieved, tested, and deployed with only minimal initial investment of time, at which point the cycle can begin again by identifying further optimization opportunities, seeing additional speedups, and then deploying the even faster versions of the application into production.
 </p>
 <h3>
  <span class="section-number">
   1.3.1.
  </span>
  Assess
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#assess" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  For an existing project, the first step is to assess the application to locate the parts of the code that are responsible for the bulk of the execution time. Armed with this knowledge, the developer can evaluate these bottlenecks for parallelization and start to investigate GPU acceleration.
 </p>
 <p>
  By understanding the end-userâs requirements and constraints and by applying Amdahlâs and Gustafsonâs laws, the developer can determine the upper bound of performance improvement from acceleration of the identified portions of the application.
 </p>
 <h3>
  <span class="section-number">
   1.3.2.
  </span>
  Parallelize
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#parallelize" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. Depending on the original code, this can be as simple as calling into an existing GPU-optimized library such as
  <span class="pre">
   cuBLAS
  </span>
  ,
  <span class="pre">
   cuFFT
  </span>
  , or
  <span class="pre">
   Thrust
  </span>
  , or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler.
 </p>
 <p>
  On the other hand, some applicationsâ designs will require some amount of refactoring to expose their inherent parallelism. As even CPU architectures will require exposing parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc.) aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on CUDA-capable GPUs designed for maximum parallel throughput.
 </p>
 <h3>
  <span class="section-number">
   1.3.3.
  </span>
  Optimize
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#optimize" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. Since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. However, as with APOD as a whole, program optimization is an iterative process (identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat), meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. Instead, strategies can be applied incrementally as they are learned.
 </p>
 <p>
  Optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to fine-tuning floating-point operation sequences. The available profiling tools are invaluable for guiding this process, as they can help suggest a next-best course of action for the developerâs optimization efforts and provide references into the relevant portions of the optimization section of this guide.
 </p>
 <h3>
  <span class="section-number">
   1.3.4.
  </span>
  Deploy
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#deploy" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. Recall that the initial
  assess
  step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots.
 </p>
 <p>
  Before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. This is important for a number of reasons; for example, it allows the user to profit from their investment as early as possible (the speedup may be partial but is still valuable), and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application.
 </p>
 <h2>
  <span class="section-number">
   1.4.
  </span>
  Recommendations and Best Practices
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#recommendations-and-best-practices" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Throughout this guide, specific recommendations are made regarding the design and implementation of CUDA C++ code. These recommendations are categorized by priority, which is a blend of the effect of the recommendation and its scope. Actions that present substantial improvements for most CUDA applications have the highest priority, while small optimizations that affect only very specific situations are given a lower priority.
 </p>
 <p>
  Before implementing lower priority recommendations, it is good practice to make sure all higher priority recommendations that are relevant have already been applied. This approach will tend to provide the best results for the time invested and will avoid the trap of premature optimization.
 </p>
 <p>
  The criteria of benefit and scope for establishing priority will vary depending on the nature of the program. In this guide, they represent a typical case. Your code might reflect different priority factors. Regardless of this possibility, it is good practice to verify that no higher-priority recommendations have been overlooked before undertaking lower-priority items.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Code samples throughout the guide omit error checking for conciseness. Production code should, however, systematically check the error code returned by each API call and check for failures in kernel launches by calling
  <span class="pre">
   cudaGetLastError()
  </span>
  .
 </p>
 <h2>
  <span class="section-number">
   1.5.
  </span>
  Assessing Your Application
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#assessing-your-application" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  From supercomputers to mobile phones, modern processors increasingly rely on parallelism to provide performance. The core computational unit, which includes control, arithmetic, registers and typically some cache, is replicated some number of times and connected to memory via a network. As a result, all modern processors require parallel code in order to achieve good utilization of their computational power.
 </p>
 <p>
  While processors are evolving to expose more fine-grained parallelism to the programmer, many existing applications have evolved either as serial codes or as coarse-grained parallel codes (for example, where the data is decomposed into regions processed in parallel, with sub-regions shared using MPI). In order to profit from any modern processor architecture, GPUs included, the first steps are to assess the application to identify the hotspots, determine whether they can be parallelized, and understand the relevant workloads both now and in the future.
 </p>
 <h1>
  <span class="section-number">
   2.
  </span>
  Heterogeneous Computing
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#heterogeneous-computing" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  CUDA programming involves running code on two different platforms concurrently: a
  host
  system with one or more CPUs and one or more CUDA-enabled NVIDIA GPU
  devices
  .
 </p>
 <p>
  While NVIDIA GPUs are frequently associated with graphics, they are also powerful arithmetic engines capable of running thousands of lightweight threads in parallel. This capability makes them well suited to computations that can leverage parallel execution.
 </p>
 <p>
  However, the device is based on a distinctly different design from the host system, and itâs important to understand those differences and how they determine the performance of CUDA applications in order to use CUDA effectively.
 </p>
 <h2>
  <span class="section-number">
   2.1.
  </span>
  Differences between Host and Device
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#differences-between-host-and-device" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The primary differences are in threading model and in separate physical memories:
 </p>
 Threading resources
 <p>
  Execution pipelines on host systems can support a limited number of concurrent threads. For example, servers that have two 32 core processors can run only 64 threads concurrently (or small multiple of that if the CPUs support simultaneous multithreading). By comparison, the
  smallest
  executable unit of parallelism on a CUDA device comprises 32 threads (termed a
  warp
  of threads). Modern NVIDIA GPUs can support up to 2048 active threads concurrently per multiprocessor (see Features and Specifications of the CUDA C++ Programming Guide) On GPUs with 80 multiprocessors, this leads to more than 160,000 concurrently active threads.
 </p>
 Threads
 <p>
  Threads on a CPU are generally heavyweight entities. The operating system must swap threads on and off CPU execution channels to provide multithreading capability. Context switches (when two threads are swapped) are therefore slow and expensive. By comparison, threads on GPUs are extremely lightweight. In a typical system, thousands of threads are queued up for work (in warps of 32 threads each). If the GPU must wait on one warp of threads, it simply begins executing work on another. Because separate registers are allocated to all active threads, no swapping of registers or other state need occur when switching among GPU threads. Resources stay allocated to each thread until it completes its execution. In short, CPU cores are designed to
  minimize latency
  for a small number of threads at a time each, whereas GPUs are designed to handle a large number of concurrent, lightweight threads in order to
  maximize throughput
  .
 </p>
 RAM
 <p>
  The host system and the device each have their own distinct attached physical memories
  <a class="footnote-reference brackets" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#fn1" id="id1">
   1
  </a>
  . As the host and device memories are separated, items in the host memory must occasionally be communicated between device memory and host memory as described in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#what-runs-on-cuda-enabled-device">
   What Runs on a CUDA-Enabled Device?
  </a>
  .
 </p>
 <p>
  These are the primary hardware differences between CPU hosts and GPU devices with respect to parallel programming. Other differences are discussed as they arise elsewhere in this document. Applications composed with these differences in mind can treat the host and device together as a cohesive heterogeneous system wherein each processing unit is leveraged to do the kind of work it does best: sequential work on the host and parallel work on the device.
 </p>
 <h2>
  <span class="section-number">
   2.2.
  </span>
  What Runs on a CUDA-Enabled Device?
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#what-runs-on-a-cuda-enabled-device" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The following issues should be considered when determining what parts of an application to run on the device:
 </p>
 <ul>
  <li>
   <p>
    The device is ideally suited for computations that can be run on numerous data elements simultaneously in parallel. This typically involves arithmetic on large data sets (such as matrices) where the same operation can be performed across thousands, if not millions, of elements at the same time. This is a requirement for good performance on CUDA: the software must use a large number (generally thousands or tens of thousands) of concurrent threads. The support for running numerous threads in parallel derives from CUDAâs use of a lightweight threading model described above.
   </p>
  </li>
  <li>
   <p>
    To use CUDA, data values must be transferred from the host to the device. These transfers are costly in terms of performance and should be minimized. (See
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#data-transfer-between-host-and-device">
     Data Transfer Between Host and Device
    </a>
    .) This cost has several ramifications:
   </p>
   <ul>
    <li>
     <p>
      The complexity of operations should justify the cost of moving data to and from the device. Code that transfers data for brief use by a small number of threads will see little or no performance benefit. The ideal scenario is one in which many threads perform a substantial amount of work.
     </p>
     <p>
      For example, transferring two matrices to the device to perform a matrix addition and then transferring the results back to the host will not realize much performance benefit. The issue here is the number of operations performed per data element transferred. For the preceding procedure, assuming matrices of size NxN, there are N
      2
      operations (additions) and 3N
      2
      elements transferred, so the ratio of operations to elements transferred is 1:3 or O(1). Performance benefits can be more readily achieved when this ratio is higher. For example, a matrix multiplication of the same matrices requires N
      3
      operations (multiply-add), so the ratio of operations to elements transferred is O(N), in which case the larger the matrix the greater the performance benefit. The types of operations are an additional factor, as additions have different complexity profiles than, for example, trigonometric functions. It is important to include the overhead of transferring data to and from the device in determining whether operations should be performed on the host or on the device.
     </p>
    </li>
    <li>
     <p>
      Data should be kept on the device as long as possible. Because transfers should be minimized, programs that run multiple kernels on the same data should favor leaving the data on the device between kernel calls, rather than transferring intermediate results to the host and then sending them back to the device for subsequent calculations. So, in the previous example, had the two matrices to be added already been on the device as a result of some previous calculation, or if the results of the addition would be used in some subsequent calculation, the matrix addition should be performed locally on the device. This approach should be used even if one of the steps in a sequence of calculations could be performed faster on the host. Even a relatively slow kernel may be advantageous if it avoids one or more transfers between host and device memory.
      <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#data-transfer-between-host-and-device">
       Data Transfer Between Host and Device
      </a>
      provides further details, including the measurements of bandwidth between the host and the device versus within the device proper.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    For best performance, there should be some coherence in memory access by adjacent threads running on the device. Certain memory access patterns enable the hardware to coalesce groups of reads or writes of multiple data items into one operation. Data that cannot be laid out so as to enable
    coalescing
    , or that doesnât have enough locality to use the L1 or texture caches effectively, will tend to see lesser speedups when used in computations on GPUs. A noteworthy exception to this are completely random memory access patterns. In general, they should be avoided, because compared to peak capabilities any architecture processes these memory access patterns at a low efficiency. However, compared to cache based architectures, like CPUs, latency hiding architectures, like GPUs, tend to cope better with completely random memory access patterns.
   </p>
  </li>
 </ul>
 <span class="brackets">
  <a class="fn-backref" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#id1">
   1
  </a>
 </span>
 <p>
  On Systems on a Chip with integrated GPUs, such as NVIDIAÂ® TegraÂ®, host and device memory are physically the same, but there is still a logical distinction between host and device memory. See the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-for-tegra-appnote">
   Application Note on CUDA for Tegra
  </a>
  for details.
 </p>
 <h1>
  <span class="section-number">
   3.
  </span>
  Application Profiling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#application-profiling" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   3.1.
  </span>
  Profile
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#profile" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Many codes accomplish a significant portion of the work with a relatively small amount of code. Using a profiler, the developer can identify such hotspots and start to compile a list of candidates for parallelization.
 </p>
 <h3>
  <span class="section-number">
   3.1.1.
  </span>
  Creating the Profile
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#creating-the-profile" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  There are many possible approaches to profiling the code, but in all cases the objective is the same: to identify the function or functions in which the application is spending most of its execution time.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  High Priority:
  To maximize developer productivity, profile the application to determine hotspots and bottlenecks.
 </p>
 <p>
  The most important consideration with any profiling activity is to ensure that the workload is realistic - i.e., that information gained from the test and decisions based upon that information are relevant to real data. Using unrealistic workloads can lead to sub-optimal results and wasted effort both by causing developers to optimize for unrealistic problem sizes and by causing developers to concentrate on the wrong functions.
 </p>
 <p>
  There are a number of tools that can be used to generate the profile. The following example is based on
  <span class="pre">
   gprof
  </span>
  , which is an open-source profiler for Linux platforms from the GNU Binutils collection.
 </p>
 <pre>$ gcc -O2 -g -pg myprog.c
$ gprof ./a.out &gt; profile.txt
Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total
 time   seconds   seconds    calls  ms/call  ms/call  name
 33.34      0.02     0.02     7208     0.00     0.00  genTimeStep
 16.67      0.03     0.01      240     0.04     0.12  calcStats
 16.67      0.04     0.01        8     1.25     1.25  calcSummaryData
 16.67      0.05     0.01        7     1.43     1.43  write
 16.67      0.06     0.01                             mcount
  0.00      0.06     0.00      236     0.00     0.00  tzset
  0.00      0.06     0.00      192     0.00     0.00  tolower
  0.00      0.06     0.00       47     0.00     0.00  strlen
  0.00      0.06     0.00       45     0.00     0.00  strchr
  0.00      0.06     0.00        1     0.00    50.00  main
  0.00      0.06     0.00        1     0.00     0.00  memcpy
  0.00      0.06     0.00        1     0.00    10.11  print
  0.00      0.06     0.00        1     0.00     0.00  profil
  0.00      0.06     0.00        1     0.00    50.00  report
</pre>
 <h3>
  <span class="section-number">
   3.1.2.
  </span>
  Identifying Hotspots
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#identifying-hotspots" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  In the example above, we can clearly see that the function
  <span class="pre">
   genTimeStep()
  </span>
  takes one-third of the total running time of the application. This should be our first candidate function for parallelization.
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#understanding-scaling">
   Understanding Scaling
  </a>
  discusses the potential benefit we might expect from such parallelization.
 </p>
 <p>
  It is worth noting that several of the other functions in the above example also take up a significant portion of the overall running time, such as
  <span class="pre">
   calcStats()
  </span>
  and
  <span class="pre">
   calcSummaryData()
  </span>
  . Parallelizing these functions as well should increase our speedup potential. However, since APOD is a cyclical process, we might opt to parallelize these functions in a subsequent APOD pass, thereby limiting the scope of our work in any given pass to a smaller set of incremental changes.
 </p>
 <h3>
  <span class="section-number">
   3.1.3.
  </span>
  Understanding Scaling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#understanding-scaling" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The amount of performance benefit an application will realize by running on CUDA depends entirely on the extent to which it can be parallelized. Code that cannot be sufficiently parallelized should run on the host, unless doing so would result in excessive transfers between the host and the device.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  High Priority:
  To get the maximum benefit from CUDA, focus first on finding ways to parallelize sequential code.
 </p>
 <p>
  By understanding how applications can scale it is possible to set expectations and plan an incremental parallelization strategy.
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strong-scaling-and-amdahls-law">
   Strong Scaling and Amdahlâs Law
  </a>
  describes strong scaling, which allows us to set an upper bound for the speedup with a fixed problem size.
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#weak-scaling-and-gustafsons-law">
   Weak Scaling and Gustafsonâs Law
  </a>
  describes weak scaling, where the speedup is attained by growing the problem size. In many applications, a combination of strong and weak scaling is desirable.
 </p>
 <h4>
  <span class="section-number">
   3.1.3.1.
  </span>
  Strong Scaling and Amdahlâs Law
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strong-scaling-and-amdahl-s-law" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Strong scaling is a measure of how, for a fixed overall problem size, the time to solution decreases as more processors are added to a system. An application that exhibits linear strong scaling has a speedup equal to the number of processors used.
 </p>
 <p>
  Strong scaling is usually equated with Amdahlâs Law, which specifies the maximum speedup that can be expected by parallelizing portions of a serial program. Essentially, it states that the maximum speedup
  S
  of a program is:
 </p>
 <p>
  <span class="math notranslate nohighlight">
   \(S = \frac{1}{(1 - P) + \frac{P}{N}}\)
  </span>
 </p>
 <p>
  Here
  P
  is the fraction of the total serial execution time taken by the portion of code that can be parallelized and
  N
  is the number of processors over which the parallel portion of the code runs.
 </p>
 <p>
  The larger
  N
  is(that is, the greater the number of processors), the smaller the
  P/N
  fraction. It can be simpler to view
  N
  as a very large number, which essentially transforms the equation into
  <span class="math notranslate nohighlight">
   \(S = 1/(1 - P)\)
  </span>
  . Now, if 3/4 of the running time of a sequential program is parallelized, the maximum speedup over serial code is 1 / (1 - 3/4) = 4.
 </p>
 <p>
  In reality, most applications do not exhibit perfectly linear strong scaling, even if they do exhibit some degree of strong scaling. For most purposes, the key point is that the larger the parallelizable portion
  P
  is, the greater the potential speedup. Conversely, if
  P
  is a small number (meaning that the application is not substantially parallelizable), increasing the number of processors
  N
  does little to improve performance. Therefore, to get the largest speedup for a fixed problem size, it is worthwhile to spend effort on increasing
  P
  , maximizing the amount of code that can be parallelized.
 </p>
 <h4>
  <span class="section-number">
   3.1.3.2.
  </span>
  Weak Scaling and Gustafsonâs Law
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#weak-scaling-and-gustafson-s-law" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Weak scaling is a measure of how the time to solution changes as more processors are added to a system with a fixed problem size
  per processor
  ; i.e., where the overall problem size increases as the number of processors is increased.
 </p>
 <p>
  Weak scaling is often equated with Gustafsonâs Law, which states that in practice, the problem size scales with the number of processors. Because of this, the maximum speedup
  S
  of a program is:
 </p>
 <p>
  <span class="math notranslate nohighlight">
   \(S = N + (1 - P)(1 - N)\)
  </span>
 </p>
 <p>
  Here
  P
  is the fraction of the total serial execution time taken by the portion of code that can be parallelized and
  N
  is the number of processors over which the parallel portion of the code runs.
 </p>
 <p>
  Another way of looking at Gustafsonâs Law is that it is not the problem size that remains constant as we scale up the system but rather the execution time. Note that Gustafsonâs Law assumes that the ratio of serial to parallel execution remains constant, reflecting additional cost in setting up and handling the larger problem.
 </p>
 <h4>
  <span class="section-number">
   3.1.3.3.
  </span>
  Applying Strong and Weak Scaling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#applying-strong-and-weak-scaling" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Understanding which type of scaling is most applicable to an application is an important part of estimating speedup. For some applications the problem size will remain constant and hence only strong scaling is applicable. An example would be modeling how two molecules interact with each other, where the molecule sizes are fixed.
 </p>
 <p>
  For other applications, the problem size will grow to fill the available processors. Examples include modeling fluids or structures as meshes or grids and some Monte Carlo simulations, where increasing the problem size provides increased accuracy.
 </p>
 <p>
  Having understood the application profile, the developer should understand how the problem size would change if the computational performance changes and then apply either Amdahlâs or Gustafsonâs Law to determine an upper bound for the speedup.
 </p>
 <h1>
  <span class="section-number">
   4.
  </span>
  Parallelizing Your Application
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#parallelizing-your-application" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  Having identified the hotspots and having done the basic exercises to set goals and expectations, the developer needs to parallelize the code. Depending on the original code, this can be as simple as calling into an existing GPU-optimized library such as
  <span class="pre">
   cuBLAS
  </span>
  ,
  <span class="pre">
   cuFFT
  </span>
  , or
  <span class="pre">
   Thrust
  </span>
  , or it could be as simple as adding a few preprocessor directives as hints to a parallelizing compiler.
 </p>
 <p>
  On the other hand, some applicationsâ designs will require some amount of refactoring to expose their inherent parallelism. As even CPU architectures require exposing this parallelism in order to improve or simply maintain the performance of sequential applications, the CUDA family of parallel programming languages (CUDA C++, CUDA Fortran, etc.) aims to make the expression of this parallelism as simple as possible, while simultaneously enabling operation on CUDA-capable GPUs designed for maximum parallel throughput.
 </p>
 <h1>
  <span class="section-number">
   5.
  </span>
  Getting Started
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#getting-started" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  There are several key strategies for parallelizing sequential code. While the details of how to apply these strategies to a particular application is a complex and problem-specific topic, the general themes listed here apply regardless of whether we are parallelizing code to run on for multicore CPUs or for use on CUDA GPUs.
 </p>
 <h2>
  <span class="section-number">
   5.1.
  </span>
  Parallel Libraries
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#parallel-libraries" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The most straightforward approach to parallelizing an application is to leverage existing libraries that take advantage of parallel architectures on our behalf. The CUDA Toolkit includes a number of such libraries that have been fine-tuned for NVIDIA CUDA GPUs, such as
  <span class="pre">
   cuBLAS
  </span>
  ,
  <span class="pre">
   cuFFT
  </span>
  , and so on.
 </p>
 <p>
  The key here is that libraries are most useful when they match well with the needs of the application. Applications already using other BLAS libraries can often quite easily switch to
  <span class="pre">
   cuBLAS
  </span>
  , for example, whereas applications that do little to no linear algebra will have little use for
  <span class="pre">
   cuBLAS
  </span>
  . The same goes for other CUDA Toolkit libraries:
  <span class="pre">
   cuFFT
  </span>
  has an interface similar to that of
  <span class="pre">
   FFTW
  </span>
  , etc.
 </p>
 <p>
  Also of note is the Thrust library, which is a parallel C++ template library similar to the C++ Standard Template Library. Thrust provides a rich collection of data parallel primitives such as scan, sort, and reduce, which can be composed together to implement complex algorithms with concise, readable source code. By describing your computation in terms of these high-level abstractions you provide Thrust with the freedom to select the most efficient implementation automatically. As a result, Thrust can be utilized in rapid prototyping of CUDA applications, where programmer productivity matters most, as well as in production, where robustness and absolute performance are crucial.
 </p>
 <h2>
  <span class="section-number">
   5.2.
  </span>
  Parallelizing Compilers
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#parallelizing-compilers" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Another common approach to parallelization of sequential codes is to make use of parallelizing compilers. Often this means the use of directives-based approaches, where the programmer uses a pragma or other similar notation to provide hints to the compiler about where parallelism can be found without needing to modify or adapt the underlying code itself. By exposing parallelism to the compiler, directives allow the compiler to do the detailed work of mapping the computation onto the parallel architecture.
 </p>
 <p>
  The OpenACC standard provides a set of compiler directives to specify loops and regions of code in standard C, C++ and Fortran that should be offloaded from a host CPU to an attached accelerator such as a CUDA GPU. The details of managing the accelerator device are handled implicitly by an OpenACC-enabled compiler and runtime.
 </p>
 <p>
  See
  <a class="reference external" href="http://www.openacc.org/">
   http://www.openacc.org/
  </a>
  for details.
 </p>
 <h2>
  <span class="section-number">
   5.3.
  </span>
  Coding to Expose Parallelism
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#coding-to-expose-parallelism" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  For applications that need additional functionality or performance beyond what existing parallel libraries or parallelizing compilers can provide, parallel programming languages such as CUDA C++ that integrate seamlessly with existing sequential code are essential.
 </p>
 <p>
  Once we have located a hotspot in our applicationâs profile assessment and determined that custom code is the best approach, we can use CUDA C++ to expose the parallelism in that portion of our code as a CUDA kernel. We can then launch this kernel onto the GPU and retrieve the results without requiring major rewrites to the rest of our application.
 </p>
 <p>
  This approach is most straightforward when the majority of the total running time of our application is spent in a few relatively isolated portions of the code. More difficult to parallelize are applications with a very flat profile - i.e., applications where the time spent is spread out relatively evenly across a wide portion of the code base. For the latter variety of application, some degree of code refactoring to expose the inherent parallelism in the application might be necessary, but keep in mind that this refactoring work will tend to benefit all future architectures, CPU and GPU alike, so it is well worth the effort should it become necessary.
 </p>
 <h1>
  <span class="section-number">
   6.
  </span>
  Getting the Right Answer
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#getting-the-right-answer" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  Obtaining the right answer is clearly the principal goal of all computation. On parallel systems, it is possible to run into difficulties not typically found in traditional serial-oriented programming. These include threading issues, unexpected values due to the way floating-point values are computed, and challenges arising from differences in the way CPU and GPU processors operate. This chapter examines issues that can affect the correctness of returned data and points to appropriate solutions.
 </p>
 <h2>
  <span class="section-number">
   6.1.
  </span>
  Verification
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#verification" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <h3>
  <span class="section-number">
   6.1.1.
  </span>
  Reference Comparison
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#reference-comparison" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  A key aspect of correctness verification for modifications to any existing program is to establish some mechanism whereby previous known-good reference outputs from representative inputs can be compared to new results. After each change is made, ensure that the results match using whatever criteria apply to the particular algorithm. Some will expect bitwise identical results, which is not always possible, especially where floating-point arithmetic is concerned; see
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#numerical-accuracy-and-precision">
   Numerical Accuracy and Precision
  </a>
  regarding numerical accuracy. For other algorithms, implementations may be considered correct if they match the reference within some small epsilon.
 </p>
 <p>
  Note that the process used for validating numerical results can easily be extended to validate performance results as well. We want to ensure that each change we make is correct
  and
  that it improves performance (and by how much). Checking these things frequently as an integral part of our cyclical APOD process will help ensure that we achieve the desired results as rapidly as possible.
 </p>
 <h3>
  <span class="section-number">
   6.1.2.
  </span>
  Unit Testing
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#unit-testing" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  A useful counterpart to the reference comparisons described above is to structure the code itself in such a way that is readily verifiable at the unit level. For example, we can write our CUDA kernels as a collection of many short
  <span class="pre">
   __device__
  </span>
  functions rather than one large monolithic
  <span class="pre">
   __global__
  </span>
  function; each device function can be tested independently before hooking them all together.
 </p>
 <p>
  For example, many kernels have complex addressing logic for accessing memory in addition to their actual computation. If we validate our addressing logic separately prior to introducing the bulk of the computation, then this will simplify any later debugging efforts. (Note that the CUDA compiler considers any device code that does not contribute to a write to global memory as dead code subject to elimination, so we must at least write
  something
  out to global memory as a result of our addressing logic in order to successfully apply this strategy.)
 </p>
 <p>
  Going a step further, if most functions are defined as
  <span class="pre">
   __host__
  </span>
  <span class="pre">
   __device__
  </span>
  rather than just
  <span class="pre">
   __device__
  </span>
  functions, then these functions can be tested on both the CPU and the GPU, thereby increasing our confidence that the function is correct and that there will not be any unexpected differences in the results. If there
  are
  differences, then those differences will be seen early and can be understood in the context of a simple function.
 </p>
 <p>
  As a useful side effect, this strategy will allow us a means to reduce code duplication should we wish to include both CPU and GPU execution paths in our application: if the bulk of the work of our CUDA kernels is done in
  <span class="pre">
   __host__
  </span>
  <span class="pre">
   __device__
  </span>
  functions, we can easily call those functions from both the host code
  and
  the device code without duplication.
 </p>
 <h2>
  <span class="section-number">
   6.2.
  </span>
  Debugging
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#debugging" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA-GDB is a port of the GNU Debugger that runs on Linux and Mac; see:
  <a class="reference external" href="https://developer.nvidia.com/cuda-gdb">
   https://developer.nvidia.com/cuda-gdb
  </a>
  .
 </p>
 <p>
  The NVIDIA Nsight Visual Studio Edition for Microsoft Windows 7, Windows HPC Server 2008, Windows 8.1, and Windows 10 is available as a free plugin for Microsoft Visual Studio; see:
  <a class="reference external" href="https://developer.nvidia.com/nsight-visual-studio-edition">
   https://developer.nvidia.com/nsight-visual-studio-edition
  </a>
  .
 </p>
 <p>
  Several third-party debuggers support CUDA debugging as well; see:
  <a class="reference external" href="https://developer.nvidia.com/debugging-solutions">
   https://developer.nvidia.com/debugging-solutions
  </a>
  for more details.
 </p>
 <h2>
  <span class="section-number">
   6.3.
  </span>
  Numerical Accuracy and Precision
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#numerical-accuracy-and-precision" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Incorrect or unexpected results arise principally from issues of floating-point accuracy due to the way floating-point values are computed and stored. The following sections explain the principal items of interest. Other peculiarities of floating-point arithmetic are presented in Features and Technical Specifications of the CUDA C++ Programming Guide as well as in a whitepaper and accompanying webinar on floating-point precision and performance available from
  <a class="reference external" href="https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus">
   https://developer.nvidia.com/content/precision-performance-floating-point-and-ieee-754-compliance-nvidia-gpus
  </a>
  .
 </p>
 <h3>
  <span class="section-number">
   6.3.1.
  </span>
  Single vs. Double Precision
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#single-vs-double-precision" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Devices of
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
   compute capability
  </a>
  1.3 and higher provide native support for double-precision floating-point values (that is, values 64 bits wide). Results obtained using double-precision arithmetic will frequently differ from the same operation performed via single-precision arithmetic due to the greater precision of the former and due to rounding issues. Therefore, it is important to be sure to compare values of like precision and to express the results within a certain tolerance rather than expecting them to be exact.
 </p>
 <h3>
  <span class="section-number">
   6.3.2.
  </span>
  Floating Point Math Is not Associative
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#floating-point-math-is-not-associative" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Each floating-point arithmetic operation involves a certain amount of rounding. Consequently, the order in which arithmetic operations are performed is important. If A, B, and C are floating-point values, (A+B)+C is not guaranteed to equal A+(B+C) as it is in symbolic math. When you parallelize computations, you potentially change the order of operations and therefore the parallel results might not match sequential results. This limitation is not specific to CUDA, but an inherent part of parallel computation on floating-point values.
 </p>
 <h3>
  <span class="section-number">
   6.3.3.
  </span>
  IEEE 754 Compliance
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#ieee-754-compliance" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  All CUDA compute devices follow the IEEE 754 standard for binary floating-point representation, with some small exceptions. These exceptions, which are detailed in Features and Technical Specifications of the CUDA C++ Programming Guide, can lead to results that differ from IEEE 754 values computed on the host system.
 </p>
 <p>
  One of the key differences is the fused multiply-add (FMA) instruction, which combines multiply-add operations into a single instruction execution. Its result will often differ slightly from results obtained by doing the two operations separately.
 </p>
 <h3>
  <span class="section-number">
   6.3.4.
  </span>
  x86 80-bit Computations
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#x86-80-bit-computations" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  x86 processors can use an 80-bit
  double extended precision
  math when performing floating-point calculations. The results of these calculations can frequently differ from pure 64-bit operations performed on the CUDA device. To get a closer match between values, set the x86 host processor to use regular double or single precision (64 bits and 32 bits, respectively). This is done with the
  <span class="pre">
   FLDCW
  </span>
  x86 assembly instruction or the equivalent operating system API.
 </p>
 <h1>
  <span class="section-number">
   7.
  </span>
  Optimizing CUDA Applications
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#optimizing-cuda-applications" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  After each round of application parallelization is complete, the developer can move to optimizing the implementation to improve performance. Since there are many possible optimizations that can be considered, having a good understanding of the needs of the application can help to make the process as smooth as possible. However, as with APOD as a whole, program optimization is an iterative process (identify an opportunity for optimization, apply and test the optimization, verify the speedup achieved, and repeat), meaning that it is not necessary for a programmer to spend large amounts of time memorizing the bulk of all possible optimization strategies prior to seeing good speedups. Instead, strategies can be applied incrementally as they are learned.
 </p>
 <p>
  Optimizations can be applied at various levels, from overlapping data transfers with computation all the way down to fine-tuning floating-point operation sequences. The available profiling tools are invaluable for guiding this process, as they can help suggest a next-best course of action for the developerâs optimization efforts and provide references into the relevant portions of the optimization section of this guide.
 </p>
 <h1>
  <span class="section-number">
   8.
  </span>
  Performance Metrics
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#performance-metrics" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  When attempting to optimize CUDA code, it pays to know how to measure performance accurately and to understand the role that bandwidth plays in performance measurement. This chapter discusses how to correctly measure performance using CPU timers and CUDA events. It then explores how bandwidth affects performance metrics and how to mitigate some of the challenges it poses.
 </p>
 <h2>
  <span class="section-number">
   8.1.
  </span>
  Timing
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#timing" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA calls and kernel executions can be timed using either CPU or GPU timers. This section examines the functionality, advantages, and pitfalls of both approaches.
 </p>
 <h3>
  <span class="section-number">
   8.1.1.
  </span>
  Using CPU Timers
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#using-cpu-timers" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Any CPU timer can be used to measure the elapsed time of a CUDA call or kernel execution. The details of various CPU timing approaches are outside the scope of this document, but developers should always be aware of the resolution their timing calls provide.
 </p>
 <p>
  When using CPU timers, it is critical to remember that many CUDA API functions are asynchronous; that is, they return control back to the calling CPU thread prior to completing their work. All kernel launches are asynchronous, as are memory-copy functions with the
  <span class="pre">
   Async
  </span>
  suffix on their names. Therefore, to accurately measure the elapsed time for a particular call or sequence of CUDA calls, it is necessary to synchronize the CPU thread with the GPU by calling
  <span class="pre">
   cudaDeviceSynchronize()
  </span>
  immediately before starting and stopping the CPU timer.
  <span class="pre">
   cudaDeviceSynchronize()
  </span>
  blocks the calling CPU thread until all CUDA calls previously issued by the thread are completed.
 </p>
 <p>
  Although it is also possible to synchronize the CPU thread with a particular stream or event on the GPU, these synchronization functions are not suitable for timing code in streams other than the default stream.
  <span class="pre">
   cudaStreamSynchronize()
  </span>
  blocks the CPU thread until all CUDA calls previously issued into the given stream have completed.
  <span class="pre">
   cudaEventSynchronize()
  </span>
  blocks until a given event in a particular stream has been recorded by the GPU. Because the driver may interleave execution of CUDA calls from other non-default streams, calls in other streams may be included in the timing.
 </p>
 <p>
  Because the default stream, stream 0, exhibits serializing behavior for work on the device (an operation in the default stream can begin only after all preceding calls in any stream have completed; and no subsequent operation in any stream can begin until it finishes), these functions can be used reliably for timing in the default stream.
 </p>
 <p>
  Be aware that CPU-to-GPU synchronization points such as those mentioned in this section imply a stall in the GPUâs processing pipeline and should thus be used sparingly to minimize their performance impact.
 </p>
 <h3>
  <span class="section-number">
   8.1.2.
  </span>
  Using CUDA GPU Timers
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#using-cuda-gpu-timers" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The CUDA event API provides calls that create and destroy events, record events (including a timestamp), and convert timestamp differences into a floating-point value in milliseconds.
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#using-cuda-gpu-timers__how-to-time-code-using-cuda-events">
   How to time code using CUDA events
  </a>
  illustrates their use.
 </p>
 <p class="title sectiontitle rubric" id="how-to-time-code-using-cuda-events">
  How to time code using CUDA events
 </p>
 <pre><span class="n">cudaEvent_t</span><span class="n">start</span><span class="p">,</span><span class="n">stop</span><span class="p">;</span>
<span class="kt">float</span><span class="n">time</span><span class="p">;</span>

<span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">start</span><span class="p">);</span>
<span class="n">cudaEventCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stop</span><span class="p">);</span>

<span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">start</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="n">threads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">d_odata</span><span class="p">,</span><span class="n">d_idata</span><span class="p">,</span><span class="n">size_x</span><span class="p">,</span><span class="n">size_y</span><span class="p">,</span>
<span class="n">NUM_REPS</span><span class="p">);</span>
<span class="n">cudaEventRecord</span><span class="p">(</span><span class="n">stop</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="n">cudaEventSynchronize</span><span class="p">(</span><span class="n">stop</span><span class="p">);</span>

<span class="n">cudaEventElapsedTime</span><span class="p">(</span><span class="o">&amp;</span><span class="n">time</span><span class="p">,</span><span class="n">start</span><span class="p">,</span><span class="n">stop</span><span class="p">);</span>
<span class="n">cudaEventDestroy</span><span class="p">(</span><span class="n">start</span><span class="p">);</span>
<span class="n">cudaEventDestroy</span><span class="p">(</span><span class="n">stop</span><span class="p">);</span>
</pre>
 <p>
  Here
  <span class="pre">
   cudaEventRecord()
  </span>
  is used to place the
  <span class="pre">
   start
  </span>
  and
  <span class="pre">
   stop
  </span>
  events into the default stream, stream 0. The device will record a timestamp for the event when it reaches that event in the stream. The
  <span class="pre">
   cudaEventElapsedTime()
  </span>
  function returns the time elapsed between the recording of the
  <span class="pre">
   start
  </span>
  and
  <span class="pre">
   stop
  </span>
  events. This value is expressed in milliseconds and has a resolution of approximately half a microsecond. Like the other calls in this listing, their specific operation, parameters, and return values are described in the
  CUDA Toolkit Reference Manual
  . Note that the timings are measured on the GPU clock, so the timing resolution is operating-system-independent.
 </p>
 <h2>
  <span class="section-number">
   8.2.
  </span>
  Bandwidth
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#bandwidth" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Bandwidth - the rate at which data can be transferred - is one of the most important gating factors for performance. Almost all changes to code should be made in the context of how they affect bandwidth. As described in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations">
   Memory Optimizations
  </a>
  of this guide, bandwidth can be dramatically affected by the choice of memory in which data is stored, how the data is laid out and the order in which it is accessed, as well as other factors.
 </p>
 <p>
  To measure performance accurately, it is useful to calculate theoretical and effective bandwidth. When the latter is much lower than the former, design or implementation details are likely to reduce bandwidth, and it should be the primary goal of subsequent optimization efforts to increase it.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  High Priority:
  Use the effective bandwidth of your computation as a metric when measuring performance and optimization benefits.
 </p>
 <h3>
  <span class="section-number">
   8.2.1.
  </span>
  Theoretical Bandwidth Calculation
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#theoretical-bandwidth-calculation" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Theoretical bandwidth can be calculated using hardware specifications available in the product literature. For example, the NVIDIA Tesla V100 uses HBM2 (double data rate) RAM with a memory clock rate of 877 MHz and a 4096-bit-wide memory interface.
 </p>
 <p>
  Using these data items, the peak theoretical memory bandwidth of the NVIDIA Tesla V100 is 898 GB/s:
 </p>
 <p>
  <span class="math notranslate nohighlight">
   \(\left. \left( 0.877 \times 10^{9} \right. \times (4096/8) \times 2 \right) \div 10^{9} = 898\text{GB/s}\)
  </span>
 </p>
 <p>
  In this calculation, the memory clock rate is converted in to Hz, multiplied by the interface width (divided by 8, to convert bits to bytes) and multiplied by 2 due to the double data rate. Finally, this product is divided by 10
  9
  to convert the result to GB/s.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Some calculations use 1024
  3
  instead of 10
  9
  for the final calculation. In such a case, the bandwidth would be 836.4 GiB/s. It is important to use the same divisor when calculating theoretical and effective bandwidth so that the comparison is valid.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  On GPUs with GDDR memory with ECC enabled the available DRAM is reduced by 6.25% to allow for the storage of ECC bits. Fetching ECC bits for each memory transaction also reduced the effective bandwidth by approximately 20% compared to the same GPU with ECC disabled, though the exact impact of ECC on bandwidth can be higher and depends on the memory access pattern. HBM2 memories, on the other hand, provide dedicated ECC resources, allowing overhead-free ECC protection.
  <a class="footnote-reference brackets" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#fn2" id="id2">
   2
  </a>
 </p>
 <h3>
  <span class="section-number">
   8.2.2.
  </span>
  Effective Bandwidth Calculation
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#effective-bandwidth-calculation" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Effective bandwidth is calculated by timing specific program activities and by knowing how data is accessed by the program. To do so, use this equation:
 </p>
 <p>
  <span class="math notranslate nohighlight">
   \(\text{Effective\ bandwidth} = \left( {\left( B_{r} + B_{w} \right) \div 10^{9}} \right) \div \text{time}\)
  </span>
 </p>
 <p>
  Here, the effective bandwidth is in units of GB/s, B
  r
  is the number of bytes read per kernel, B
  w
  is the number of bytes written per kernel, and time is given in seconds.
 </p>
 <p>
  For example, to compute the effective bandwidth of a 2048 x 2048 matrix copy, the following formula could be used:
 </p>
 <p>
  <span class="math notranslate nohighlight">
   \(\text{Effective\ bandwidth} = \left( {\left( 2048^{2} \times 4 \times 2 \right) \div 10^{9}} \right) \div \text{time}\)
  </span>
 </p>
 <p>
  The number of elements is multiplied by the size of each element (4 bytes for a float), multiplied by 2 (because of the read
  and
  write), divided by 10
  9
  (or 1,024
  3
  ) to obtain GB of memory transferred. This number is divided by the time in seconds to obtain GB/s.
 </p>
 <h3>
  <span class="section-number">
   8.2.3.
  </span>
  Throughput Reported by Visual Profiler
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#throughput-reported-by-visual-profiler" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  For devices with
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
   compute capability
  </a>
  of 2.0 or greater, the Visual Profiler can be used to collect several different memory throughput measures. The following throughput metrics can be displayed in the Details or Detail Graphs view:
 </p>
 <ul class="simple">
  <li>
   <p>
    Requested Global Load Throughput
   </p>
  </li>
  <li>
   <p>
    Requested Global Store Throughput
   </p>
  </li>
  <li>
   <p>
    Global Load Throughput
   </p>
  </li>
  <li>
   <p>
    Global Store Throughput
   </p>
  </li>
  <li>
   <p>
    DRAM Read Throughput
   </p>
  </li>
  <li>
   <p>
    DRAM Write Throughput
   </p>
  </li>
 </ul>
 <p>
  The Requested Global Load Throughput and Requested Global Store Throughput values indicate the global memory throughput requested by the kernel and therefore correspond to the effective bandwidth obtained by the calculation shown under
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#effective-bandwidth-calculation">
   Effective Bandwidth Calculation
  </a>
  .
 </p>
 <p>
  Because the minimum memory transaction size is larger than most word sizes, the actual memory throughput required for a kernel can include the transfer of data not used by the kernel. For global memory accesses, this actual throughput is reported by the Global Load Throughput and Global Store Throughput values.
 </p>
 <p>
  Itâs important to note that both numbers are useful. The actual memory throughput shows how close the code is to the hardware limit, and a comparison of the effective or requested bandwidth to the actual bandwidth presents a good estimate of how much bandwidth is wasted by suboptimal coalescing of memory accesses (see
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#coalesced-access-to-global-memory">
   Coalesced Access to Global Memory
  </a>
  ). For global memory accesses, this comparison of requested memory bandwidth to actual memory bandwidth is reported by the Global Memory Load Efficiency and Global Memory Store Efficiency metrics.
 </p>
 <span class="brackets">
  <a class="fn-backref" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#id2">
   2
  </a>
 </span>
 <p>
  As an exception, scattered writes to HBM2 see some overhead from ECC but much less than the overhead with similar access patterns on ECC-protected GDDR5 memory.
 </p>
 <h1>
  <span class="section-number">
   9.
  </span>
  Memory Optimizations
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  Memory optimizations are the most important area for performance. The goal is to maximize the use of the hardware by maximizing bandwidth. Bandwidth is best served by using as much fast memory and as little slow-access memory as possible. This chapter discusses the various kinds of memory on the host and device and how best to set up data items to use the memory effectively.
 </p>
 <h2>
  <span class="section-number">
   9.1.
  </span>
  Data Transfer Between Host and Device
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#data-transfer-between-host-and-device" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The peak theoretical bandwidth between the device memory and the GPU is much higher (898 GB/s on the NVIDIA Tesla V100, for example) than the peak theoretical bandwidth between host memory and device memory (16 GB/s on the PCIe x16 Gen3). Hence, for best overall application performance, it is important to minimize data transfer between the host and the device, even if that means running kernels on the GPU that do not demonstrate any speedup compared with running them on the host CPU.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  High Priority:
  Minimize data transfer between the host and the device, even if it means running some kernels on the device that do not show performance gains when compared with running them on the host CPU.
 </p>
 <p>
  Intermediate data structures should be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory.
 </p>
 <p>
  Also, because of the overhead associated with each transfer, batching many small transfers into one larger transfer performs significantly better than making each transfer separately, even if doing so requires packing non-contiguous regions of memory into a contiguous buffer and then unpacking after the transfer.
 </p>
 <p>
  Finally, higher bandwidth between the host and the device is achieved when using
  page-locked
  (or
  pinned
  ) memory, as discussed in the CUDA C++ Programming Guide and the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#pinned-memory">
   Pinned Memory
  </a>
  section of this document.
 </p>
 <h3>
  <span class="section-number">
   9.1.1.
  </span>
  Pinned Memory
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#pinned-memory" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Page-locked or pinned memory transfers attain the highest bandwidth between the host and the device. On PCIe x16 Gen3 cards, for example, pinned memory can attain roughly 12 GB/s transfer rates.
 </p>
 <p>
  Pinned memory is allocated using the
  <span class="pre">
   cudaHostAlloc()
  </span>
  functions in the Runtime API. The
  <span class="pre">
   bandwidthTest
  </span>
  CUDA Sample shows how to use these functions as well as how to measure memory transfer performance.
 </p>
 <p>
  For regions of system memory that have already been pre-allocated,
  <span class="pre">
   cudaHostRegister()
  </span>
  can be used to pin the memory on-the-fly without the need to allocate a separate buffer and copy the data into it.
 </p>
 <p>
  Pinned memory should not be overused. Excessive use can reduce overall system performance because pinned memory is a scarce resource, but how much is too much is difficult to know in advance. Furthermore, the pinning of system memory is a heavyweight operation compared to most normal system memory allocations, so as with all optimizations, test the application and the systems it runs on for optimal performance parameters.
 </p>
 <h3>
  <span class="section-number">
   9.1.2.
  </span>
  Asynchronous and Overlapping Transfers with Computation
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-and-overlapping-transfers-with-computation" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Data transfers between the host and the device using
  <span class="pre">
   cudaMemcpy()
  </span>
  are blocking transfers; that is, control is returned to the host thread only after the data transfer is complete. The
  <span class="pre">
   cudaMemcpyAsync()
  </span>
  function is a non-blocking variant of
  <span class="pre">
   cudaMemcpy()
  </span>
  in which control is returned immediately to the host thread. In contrast with
  <span class="pre">
   cudaMemcpy()
  </span>
  , the asynchronous transfer version
  requires
  pinned host memory (see
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#pinned-memory">
   Pinned Memory
  </a>
  ), and it contains an additional argument, a stream ID. A
  stream
  is simply a sequence of operations that are performed in order on the device. Operations in different streams can be interleaved and in some cases overlapped - a property that can be used to hide data transfers between the host and the device.
 </p>
 <p>
  Asynchronous transfers enable overlap of data transfers with computation in two different ways. On all CUDA-enabled devices, it is possible to overlap host computation with asynchronous data transfers and with device computations. For example,
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation__overlapping-computation-and-data-transfers">
   Overlapping computation and data transfers
  </a>
  demonstrates how host computation in the routine
  <span class="pre">
   cpuFunction()
  </span>
  is performed while data is transferred to the device and a kernel using the device is executed.
 </p>
 <p class="title sectiontitle rubric" id="overlapping-computation-and-data-transfers">
  Overlapping computation and data transfers
 </p>
 <pre><span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">a_d</span><span class="p">,</span><span class="n">a_h</span><span class="p">,</span><span class="n">size</span><span class="p">,</span><span class="n">cudaMemcpyHostToDevice</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="n">block</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">a_d</span><span class="p">);</span>
<span class="n">cpuFunction</span><span class="p">();</span>
</pre>
 <p>
  The last argument to the
  <span class="pre">
   cudaMemcpyAsync()
  </span>
  function is the stream ID, which in this case uses the default stream, stream 0. The kernel also uses the default stream, and it will not begin execution until the memory copy completes; therefore, no explicit synchronization is needed. Because the memory copy and the kernel both return control to the host immediately, the host function
  <span class="pre">
   cpuFunction()
  </span>
  overlaps their execution.
 </p>
 <p>
  In
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation__overlapping-computation-and-data-transfers">
   Overlapping computation and data transfers
  </a>
  , the memory copy and kernel execution occur sequentially. On devices that are capable of concurrent copy and compute, it is possible to overlap kernel execution on the device with data transfers between the host and the device. Whether a device has this capability is indicated by the
  <span class="pre">
   asyncEngineCount
  </span>
  field of the
  <span class="pre">
   cudaDeviceProp
  </span>
  structure (or listed in the output of the
  <span class="pre">
   deviceQuery
  </span>
  CUDA Sample). On devices that have this capability, the overlap once again requires pinned host memory, and, in addition, the data transfer and kernel must use different, non-default streams (streams with non-zero stream IDs). Non-default streams are required for this overlap because memory copy, memory set functions, and kernel calls that use the default stream begin only after all preceding calls on the device (in any stream) have completed, and no operation on the device (in any stream) commences until they are finished.
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation__concurrent-copy-and-execute">
   Concurrent copy and execute
  </a>
  illustrates the basic technique.
 </p>
 <p class="title sectiontitle rubric" id="concurrent-copy-and-execute">
  Concurrent copy and execute
 </p>
 <pre><span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream1</span><span class="p">);</span>
<span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream2</span><span class="p">);</span>
<span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">a_d</span><span class="p">,</span><span class="n">a_h</span><span class="p">,</span><span class="n">size</span><span class="p">,</span><span class="n">cudaMemcpyHostToDevice</span><span class="p">,</span><span class="n">stream1</span><span class="p">);</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="n">block</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">stream2</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">otherData_d</span><span class="p">);</span>
</pre>
 <p>
  In this code, two streams are created and used in the data transfer and kernel executions as specified in the last arguments of the
  <span class="pre">
   cudaMemcpyAsync
  </span>
  call and the kernelâs execution configuration.
 </p>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation__concurrent-copy-and-execute">
   Concurrent copy and execute
  </a>
  demonstrates how to overlap kernel execution with asynchronous data transfer. This technique could be used when the data dependency is such that the data can be broken into chunks and transferred in multiple stages, launching multiple kernels to operate on each chunk as it arrives.
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation__sequential-copy-and-execute">
   Sequential copy and execute
  </a>
  and
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation__staged-concurrent-copy-and-execute">
   Staged concurrent copy and execute
  </a>
  demonstrate this. They produce equivalent results. The first segment shows the reference sequential implementation, which transfers and operates on an array of
  N
  floats (where
  N
  is assumed to be evenly divisible by nThreads).
 </p>
 <p class="title sectiontitle rubric" id="sequential-copy-and-execute">
  Sequential copy and execute
 </p>
 <pre><span class="n">cudaMemcpy</span><span class="p">(</span><span class="n">a_d</span><span class="p">,</span><span class="n">a_h</span><span class="p">,</span><span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="n">dir</span><span class="p">);</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">N</span><span class="o">/</span><span class="n">nThreads</span><span class="p">,</span><span class="n">nThreads</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">a_d</span><span class="p">);</span>
</pre>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation__staged-concurrent-copy-and-execute">
   Staged concurrent copy and execute
  </a>
  shows how the transfer and kernel execution can be broken up into nStreams stages. This approach permits some overlapping of the data transfer and execution.
 </p>
 <p class="title sectiontitle rubric" id="staged-concurrent-copy-and-execute">
  Staged concurrent copy and execute
 </p>
 <pre><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="o">/</span><span class="n">nStreams</span><span class="p">;</span>
<span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">nStreams</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="p">{</span>
<span class="n">offset</span><span class="o">=</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="o">/</span><span class="n">nStreams</span><span class="p">;</span>
<span class="n">cudaMemcpyAsync</span><span class="p">(</span><span class="n">a_d</span><span class="o">+</span><span class="n">offset</span><span class="p">,</span><span class="n">a_h</span><span class="o">+</span><span class="n">offset</span><span class="p">,</span><span class="n">size</span><span class="p">,</span><span class="n">dir</span><span class="p">,</span><span class="n">stream</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">N</span><span class="o">/</span><span class="p">(</span><span class="n">nThreads</span><span class="o">*</span><span class="n">nStreams</span><span class="p">),</span><span class="n">nThreads</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span>
<span class="n">stream</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">a_d</span><span class="o">+</span><span class="n">offset</span><span class="p">);</span>
<span class="p">}</span>
</pre>
 <p>
  (In
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation__staged-concurrent-copy-and-execute">
   Staged concurrent copy and execute
  </a>
  , it is assumed that
  N
  is evenly divisible by
  <span class="pre">
   nThreads*nStreams
  </span>
  .) Because execution within a stream occurs sequentially, none of the kernels will launch until the data transfers in their respective streams complete. Current GPUs can simultaneously process asynchronous data transfers and execute kernels. GPUs with a single copy engine can perform one asynchronous data transfer and execute kernels whereas GPUs with two copy engines can simultaneously perform one asynchronous data transfer from the host to the device, one asynchronous data transfer from the device to the host, and execute kernels. The number of copy engines on a GPU is given by the
  <span class="pre">
   asyncEngineCount
  </span>
  field of the
  <span class="pre">
   cudaDeviceProp
  </span>
  structure, which is also listed in the output of the
  <span class="pre">
   deviceQuery
  </span>
  CUDA Sample. (It should be mentioned that it is not possible to overlap a blocking transfer with an asynchronous transfer, because the blocking transfer occurs in the default stream, so it will not begin until all previous CUDA calls complete. It will not allow any other CUDA call to begin until it has completed.) A diagram depicting the timeline of execution for the two code segments is shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation__timeline-comparison-for-copy-and-kernel-execution">
   Figure 1
  </a>
  , and
  <span class="pre">
   nStreams
  </span>
  is equal to 4 for
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation__staged-concurrent-copy-and-execute">
   Staged concurrent copy and execute
  </a>
  in the bottom half of the figure.
 </p>
 <p>
  <span class="caption-text">
   Timeline comparison for copy and kernel execution
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation-timeline-comparison-for-copy-and-kernel-execution" title="Permalink to this image">
   ï
  </a>
 </p>
 Top
 <p>
  Sequential
 </p>
 Bottom
 <p>
  Concurrent
 </p>
 <p>
  For this example, it is assumed that the data transfer and kernel execution times are comparable. In such cases, and when the execution time (
  tE
  ) exceeds the transfer time (
  tT
  ), a rough estimate for the overall time is
  tE + tT/nStreams
  for the staged version versus
  tE + tT
  for the sequential version. If the transfer time exceeds the execution time, a rough estimate for the overall time is
  tT + tE/nStreams
  .
 </p>
 <h3>
  <span class="section-number">
   9.1.3.
  </span>
  Zero Copy
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#zero-copy" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Zero copy
  is a feature that was added in version 2.2 of the CUDA Toolkit. It enables GPU threads to directly access host memory. For this purpose, it requires mapped pinned (non-pageable) memory. On integrated GPUs (i.e., GPUs with the integrated field of the CUDA device properties structure set to 1), mapped pinned memory is always a performance gain because it avoids superfluous copies as integrated GPU and CPU memory are physically the same. On discrete GPUs, mapped pinned memory is advantageous only in certain cases. Because the data is not cached on the GPU, mapped pinned memory should be read or written only once, and the global loads and stores that read and write the memory should be coalesced. Zero copy can be used in place of streams because kernel-originated data transfers automatically overlap kernel execution without the overhead of setting up and determining the optimal number of streams.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Low Priority:
  Use zero-copy operations on integrated GPUs for CUDA Toolkit version 2.2 and later.
 </p>
 <p>
  The host code in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#zero-copy__zero-copy-host-code">
   Zero-copy host code
  </a>
  shows how zero copy is typically set up.
 </p>
 <p class="title sectiontitle rubric" id="zero-copy-host-code">
  Zero-copy host code
 </p>
 <pre><span class="kt">float</span><span class="o">*</span><span class="n">a_h</span><span class="p">,</span><span class="o">*</span><span class="n">a_map</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">cudaGetDeviceProperties</span><span class="p">(</span><span class="o">&amp;</span><span class="n">prop</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="o">!</span><span class="n">prop</span><span class="p">.</span><span class="n">canMapHostMemory</span><span class="p">)</span>
<span class="n">exit</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="n">cudaSetDeviceFlags</span><span class="p">(</span><span class="n">cudaDeviceMapHost</span><span class="p">);</span>
<span class="n">cudaHostAlloc</span><span class="p">(</span><span class="o">&amp;</span><span class="n">a_h</span><span class="p">,</span><span class="n">nBytes</span><span class="p">,</span><span class="n">cudaHostAllocMapped</span><span class="p">);</span>
<span class="n">cudaHostGetDevicePointer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">a_map</span><span class="p">,</span><span class="n">a_h</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="n">gridSize</span><span class="p">,</span><span class="n">blockSize</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">a_map</span><span class="p">);</span>
</pre>
 <p>
  In this code, the
  <span class="pre">
   canMapHostMemory
  </span>
  field of the structure returned by
  <span class="pre">
   cudaGetDeviceProperties()
  </span>
  is used to check that the device supports mapping host memory to the deviceâs address space. Page-locked memory mapping is enabled by calling
  <span class="pre">
   cudaSetDeviceFlags()
  </span>
  with
  <span class="pre">
   cudaDeviceMapHost
  </span>
  . Note that
  <span class="pre">
   cudaSetDeviceFlags()
  </span>
  must be called prior to setting a device or making a CUDA call that requires state (that is, essentially, before a context is created). Page-locked mapped host memory is allocated using
  <span class="pre">
   cudaHostAlloc()
  </span>
  , and the pointer to the mapped device address space is obtained via the function
  <span class="pre">
   cudaHostGetDevicePointer()
  </span>
  . In the code in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#zero-copy__zero-copy-host-code">
   Zero-copy host code
  </a>
  ,
  <span class="pre">
   kernel()
  </span>
  can reference the mapped pinned host memory using the pointer
  <span class="pre">
   a_map
  </span>
  in exactly the same was as it would if a_map referred to a location in device memory.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Mapped pinned host memory allows you to overlap CPU-GPU memory transfers with computation while avoiding the use of CUDA streams. But since any repeated access to such memory areas causes repeated CPU-GPU transfers, consider creating a second area in device memory to manually cache the previously read host memory data.
 </p>
 <h3>
  <span class="section-number">
   9.1.4.
  </span>
  Unified Virtual Addressing
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#unified-virtual-addressing" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Devices of
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
   compute capability
  </a>
  2.0 and later support a special addressing mode called
  Unified Virtual Addressing
  (UVA) on 64-bit Linux and Windows. With UVA, the host memory and the device memories of all installed supported devices share a single virtual address space.
 </p>
 <p>
  Prior to UVA, an application had to keep track of which pointers referred to device memory (and for which device) and which referred to host memory as a separate bit of metadata (or as hard-coded information in the program) for each pointer. Using UVA, on the other hand, the physical memory space to which a pointer points can be determined simply by inspecting the value of the pointer using
  <span class="pre">
   cudaPointerGetAttributes()
  </span>
  .
 </p>
 <p>
  Under UVA, pinned host memory allocated with
  <span class="pre">
   cudaHostAlloc()
  </span>
  will have identical host and device pointers, so it is not necessary to call
  <span class="pre">
   cudaHostGetDevicePointer()
  </span>
  for such allocations. Host memory allocations pinned after-the-fact via
  <span class="pre">
   cudaHostRegister()
  </span>
  , however, will continue to have different device pointers than their host pointers, so
  <span class="pre">
   cudaHostGetDevicePointer()
  </span>
  remains necessary in that case.
 </p>
 <p>
  UVA is also a necessary precondition for enabling peer-to-peer (P2P) transfer of data directly across the PCIe bus or NVLink for supported GPUs in supported configurations, bypassing host memory.
 </p>
 <p>
  See the CUDA C++ Programming Guide for further explanations and software requirements for UVA and P2P.
 </p>
 <h2>
  <span class="section-number">
   9.2.
  </span>
  Device Memory Spaces
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#device-memory-spaces" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA devices use several memory spaces, which have different characteristics that reflect their distinct usages in CUDA applications. These memory spaces include global, local, shared, texture, and registers, as shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#device-memory-spaces__memory-spaces-cuda-device">
   Figure 2
  </a>
  .
 </p>
 <p>
  <span class="caption-text">
   Memory spaces on a CUDA device
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#device-memory-spaces-memory-spaces-cuda-device" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  Of these different memory spaces, global memory is the most plentiful; see Features and Technical Specifications of the CUDA C++ Programming Guide for the amounts of memory available in each memory space at each
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
   compute capability
  </a>
  level. Global, local, and texture memory have the greatest access latency, followed by constant memory, shared memory, and the register file.
 </p>
 <p>
  The various principal traits of the memory types are shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#device-memory-spaces__salient-features-device-memory">
   Table 1
  </a>
  .
 </p>
 <table class="table-no-stripes docutils align-default" id="id4">
  <span class="caption-text">
   Table 1. Salient Features of Device Memory
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#id4" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Memory
    </p>
   </th>
   <th class="head">
    <p>
     Location on/off chip
    </p>
   </th>
   <th class="head">
    <p>
     Cached
    </p>
   </th>
   <th class="head">
    <p>
     Access
    </p>
   </th>
   <th class="head">
    <p>
     Scope
    </p>
   </th>
   <th class="head">
    <p>
     Lifetime
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Register
    </p>
   </td>
   <td>
    <p>
     On
    </p>
   </td>
   <td>
    <p>
     n/a
    </p>
   </td>
   <td>
    <p>
     R/W
    </p>
   </td>
   <td>
    <p>
     1 thread
    </p>
   </td>
   <td>
    <p>
     Thread
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Local
    </p>
   </td>
   <td>
    <p>
     Off
    </p>
   </td>
   <td>
    <p>
     Yesâ â
    </p>
   </td>
   <td>
    <p>
     R/W
    </p>
   </td>
   <td>
    <p>
     1 thread
    </p>
   </td>
   <td>
    <p>
     Thread
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Shared
    </p>
   </td>
   <td>
    <p>
     On
    </p>
   </td>
   <td>
    <p>
     n/a
    </p>
   </td>
   <td>
    <p>
     R/W
    </p>
   </td>
   <td>
    <p>
     All threads in block
    </p>
   </td>
   <td>
    <p>
     Block
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Global
    </p>
   </td>
   <td>
    <p>
     Off
    </p>
   </td>
   <td>
    <p>
     â
    </p>
   </td>
   <td>
    <p>
     R/W
    </p>
   </td>
   <td>
    <p>
     All threads + host
    </p>
   </td>
   <td>
    <p>
     Host allocation
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Constant
    </p>
   </td>
   <td>
    <p>
     Off
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
   <td>
    <p>
     R
    </p>
   </td>
   <td>
    <p>
     All threads + host
    </p>
   </td>
   <td>
    <p>
     Host allocation
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Texture
    </p>
   </td>
   <td>
    <p>
     Off
    </p>
   </td>
   <td>
    <p>
     Yes
    </p>
   </td>
   <td>
    <p>
     R
    </p>
   </td>
   <td>
    <p>
     All threads + host
    </p>
   </td>
   <td>
    <p>
     Host allocation
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     â
     Cached in L1 and L2 by default on devices of compute capability 6.0 and 7.x; cached only in L2 by default on devices of lower compute capabilities, though some allow opt-in to caching in L1 as well via compilation flags.
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     â â
     Cached in L1 and L2 by default except on devices of compute capability 5.x; devices of compute capability 5.x cache locals only in L2.
    </p>
   </td>
  </tr>
 </table>
 <p>
  In the case of texture access, if a texture reference is bound to a linear array in global memory, then the device code can write to the underlying array. Texture references that are bound to CUDA arrays can be written to via surface-write operations by binding a surface to the same underlying CUDA array storage). Reading from a texture while writing to its underlying global memory array in the same kernel launch should be avoided because the texture caches are read-only and are not invalidated when the associated global memory is modified.
 </p>
 <h3>
  <span class="section-number">
   9.2.1.
  </span>
  Coalesced Access to Global Memory
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#coalesced-access-to-global-memory" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  A very important performance consideration in programming for CUDA-capable GPU architectures is the coalescing of global memory accesses. Global memory loads and stores by threads of a warp are coalesced by the device into as few as possible transactions.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  High Priority:
  Ensure global memory accesses are coalesced whenever possible.
 </p>
 <p>
  The access requirements for coalescing depend on the compute capability of the device and are documented in the CUDA C++ Programming Guide.
 </p>
 <p>
  For devices of compute capability 6.0 or higher, the requirements can be summarized quite easily: the concurrent accesses of the threads of a warp will coalesce into a number of transactions equal to the number of 32-byte transactions necessary to service all of the threads of the warp.
 </p>
 <p>
  For certain devices of compute capability 5.2, L1-caching of accesses to global memory can be optionally enabled. If L1-caching is enabled on these devices, the number of required transactions is equal to the number of required 128-byte aligned segments.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  On devices of compute capability 6.0 or higher, L1-caching is the default, however the data access unit is 32-byte regardless of whether global loads are cached in L1 or not.
 </p>
 <p>
  On devices with GDDR memory, accessing memory in a coalesced way is even more important when ECC is turned on. Scattered accesses increase ECC memory transfer overhead, especially when writing data to global memory.
 </p>
 <p>
  Coalescing concepts are illustrated in the following simple examples. These examples assume compute capability 6.0 or higher and that accesses are for 4-byte words, unless otherwise noted.
 </p>
 <h4>
  <span class="section-number">
   9.2.1.1.
  </span>
  A Simple Access Pattern
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#a-simple-access-pattern" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  The first and simplest case of coalescing can be achieved by any CUDA-enabled device of compute capability 6.0 or higher: the
  k
  -th thread accesses the
  k
  -th word in a 32-byte aligned array. Not all threads need to participate.
 </p>
 <p>
  For example, if the threads of a warp access adjacent 4-byte words (e.g., adjacent
  <span class="pre">
   float
  </span>
  values), four coalesced 32-byte transactions will service that memory access. Such a pattern is shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#simple-access-pattern__coalesced-access">
   Figure 3
  </a>
  .
 </p>
 <p>
  <span class="caption-text">
   Coalesced access
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#simple-access-pattern-coalesced-access" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  This access pattern results in four 32-byte transactions, indicated by the red rectangles.
 </p>
 <p>
  If from any of the four 32-byte segments only a subset of the words are requested (e.g. if several threads had accessed the same word or if some threads did not participate in the access), the full segment is fetched anyway. Furthermore, if accesses by the threads of the warp had been permuted within or accross the four segments, still only four 32-byte transactions would have been performed by a device with
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
   compute capability
  </a>
  6.0 or higher.
 </p>
 <h4>
  <span class="section-number">
   9.2.1.2.
  </span>
  A Sequential but Misaligned Access Pattern
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#a-sequential-but-misaligned-access-pattern" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  If sequential threads in a warp access memory that is sequential but not aligned with a 32-byte segment, five 32-byte segments will be requested, as shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#sequential-but-misaligned-access-pattern__misaligned-sequential-addresses-fall-5-32-byte-L2-cache-seqments">
   Figure 4
  </a>
  .
 </p>
 <p>
  <span class="caption-text">
   Misaligned sequential addresses that fall within five 32-byte segments
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#sequential-but-misaligned-access-pattern-misaligned-sequential-addresses-fall-5-32-byte-l2-cache-seqments" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  Memory allocated through the CUDA Runtime API, such as via
  <span class="pre">
   cudaMalloc()
  </span>
  , is guaranteed to be aligned to at least 256 bytes. Therefore, choosing sensible thread block sizes, such as multiples of the warp size (i.e., 32 on current GPUs), facilitates memory accesses by warps that are properly aligned. (Consider what would happen to the memory addresses accessed by the second, third, and subsequent thread blocks if the thread block size was not a multiple of warp size, for example.)
 </p>
 <h4>
  <span class="section-number">
   9.2.1.3.
  </span>
  Effects of Misaligned Accesses
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#effects-of-misaligned-accesses" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  It is easy and informative to explore the ramifications of misaligned accesses using a simple copy kernel, such as the one in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#effects-of-misaligned-accesses__copy-kernel-illustrates-misaligned-accesses">
   A copy kernel that illustrates misaligned accesses
  </a>
  .
 </p>
 <p class="title sectiontitle rubric" id="a-copy-kernel-that-illustrates-misaligned-accesses">
  A copy kernel that illustrates misaligned accesses
 </p>
 <pre><span class="n">__global__</span><span class="kt">void</span><span class="n">offsetCopy</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="n">odata</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="n">idata</span><span class="p">,</span><span class="kt">int</span><span class="n">offset</span><span class="p">)</span>
<span class="p">{</span>
<span class="kt">int</span><span class="n">xid</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">offset</span><span class="p">;</span>
<span class="n">odata</span><span class="p">[</span><span class="n">xid</span><span class="p">]</span><span class="o">=</span><span class="n">idata</span><span class="p">[</span><span class="n">xid</span><span class="p">];</span>
<span class="p">}</span>
</pre>
 <p>
  In
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#effects-of-misaligned-accesses__copy-kernel-illustrates-misaligned-accesses">
   A copy kernel that illustrates misaligned accesses
  </a>
  , data is copied from the input array
  <span class="pre">
   idata
  </span>
  to the output array, both of which exist in global memory. The kernel is executed within a loop in host code that varies the parameter
  <span class="pre">
   offset
  </span>
  from 0 to 32. (e.g.
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#sequential-but-misaligned-access-pattern__misaligned-sequential-addresses-fall-5-32-byte-L2-cache-seqments">
   Figure 4
  </a>
  corresponds to this misalignments) The effective bandwidth for the copy with various offsets on an NVIDIA Tesla V100 (
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
   compute capability
  </a>
  7.0) is shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#effects-of-misaligned-accesses__performance-offsetcopy-kernel">
   Figure 5
  </a>
  .
 </p>
 <p>
  <span class="caption-text">
   Performance of offsetCopy kernel
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#effects-of-misaligned-accesses-performance-offsetcopy-kernel" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  For the NVIDIA Tesla V100, global memory accesses with no offset or with offsets that are multiples of 8 words result in four 32-byte transactions. The achieved bandwidth is approximately 790 GB/s. Otherwise, five 32-byte segments are loaded per warp, and we would expect approximately 4/5
  th
  of the memory throughput achieved with no offsets.
 </p>
 <p>
  In this particular example, the offset memory throughput achieved is, however, approximately 9/10
  th
  , because adjacent warps reuse the cache lines their neighbors fetched. So while the impact is still evident it is not as large as we might have expected. It would have been more so if adjacent warps had not exhibited such a high degree of reuse of the over-fetched cache lines.
 </p>
 <h4>
  <span class="section-number">
   9.2.1.4.
  </span>
  Strided Accesses
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strided-accesses" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  As seen above, in the case of misaligned sequential accesses, caches help to alleviate the performance impact. It may be different with non-unit-strided accesses, however, and this is a pattern that occurs frequently when dealing with multidimensional data or matrices. For this reason, ensuring that as much as possible of the data in each cache line fetched is actually used is an important part of performance optimization of memory accesses on these devices.
 </p>
 <p>
  To illustrate the effect of strided access on effective bandwidth, see the kernel
  <span class="pre">
   strideCopy()
  </span>
  in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strided-accesses__kernel-illustrate-non-unit-stride-data-copy">
   A kernel to illustrate non-unit stride data copy
  </a>
  , which copies data with a stride of stride elements between threads from
  <span class="pre">
   idata
  </span>
  to
  <span class="pre">
   odata
  </span>
  .
 </p>
 <p class="title sectiontitle rubric" id="a-kernel-to-illustrate-non-unit-stride-data-copy">
  A kernel to illustrate non-unit stride data copy
 </p>
 <pre><span class="n">__global__</span><span class="kt">void</span><span class="n">strideCopy</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="n">odata</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="n">idata</span><span class="p">,</span><span class="kt">int</span><span class="n">stride</span><span class="p">)</span>
<span class="p">{</span>
<span class="kt">int</span><span class="n">xid</span><span class="o">=</span><span class="p">(</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">stride</span><span class="p">;</span>
<span class="n">odata</span><span class="p">[</span><span class="n">xid</span><span class="p">]</span><span class="o">=</span><span class="n">idata</span><span class="p">[</span><span class="n">xid</span><span class="p">];</span>
<span class="p">}</span>
</pre>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strided-accesses__adjacent-threads-accessing-memory-with-stride-of-2">
   Figure 6
  </a>
  illustrates such a situation; in this case, threads within a warp access words in memory with a stride of 2. This action leads to a load of eight L2 cache segments per warp on the Tesla V100 (compute capability 7.0).
 </p>
 <p>
  <span class="caption-text">
   Adjacent threads accessing memory with a stride of 2
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strided-accesses-adjacent-threads-accessing-memory-with-stride-of-2" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  A stride of 2 results in a 50% of load/store efficiency since half the elements in the transaction are not used and represent wasted bandwidth. As the stride increases, the effective bandwidth decreases until the point where 32 32-byte segments are loaded for the 32 threads in a warp, as indicated in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strided-accesses__performance-of-stridecopy-kernel">
   Figure 7
  </a>
  .
 </p>
 <p>
  <span class="caption-text">
   Performance of strideCopy kernel
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strided-accesses-performance-of-stridecopy-kernel" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  As illustrated in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#strided-accesses__performance-of-stridecopy-kernel">
   Figure 7
  </a>
  , non-unit-stride global memory accesses should be avoided whenever possible. One method for doing so utilizes shared memory, which is discussed in the next section.
 </p>
 <h3>
  <span class="section-number">
   9.2.2.
  </span>
  L2 Cache
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#l2-cache" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Starting with CUDA 11.0, devices of compute capability 8.0 and above have the capability to influence persistence of data in the L2 cache. Because L2 cache is on-chip, it potentially provides higher bandwidth and lower latency accesses to global memory.
 </p>
 <p>
  For more details refer to the L2 Access Management section in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#L2_access_intro">
   CUDA C++ Programming Guide
  </a>
  .
 </p>
 <h4>
  <span class="section-number">
   9.2.2.1.
  </span>
  L2 Cache Access Window
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#l2-cache-access-window" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  When a CUDA kernel accesses a data region in the global memory repeatedly, such data accesses can be considered to be
  persisting
  . On the other hand, if the data is only accessed once, such data accesses can be considered to be
  streaming
  . A portion of the L2 cache can be set aside for persistent accesses to a data region in global memory. If this set-aside portion is not used by persistent accesses, then streaming or normal data accesses can use it.
 </p>
 <p>
  The L2 cache set-aside size for persisting accesses may be adjusted, within limits:
 </p>
 <pre><span class="n">cudaGetDeviceProperties</span><span class="p">(</span><span class="o">&amp;</span><span class="n">prop</span><span class="p">,</span><span class="n">device_id</span><span class="p">);</span>
<span class="n">cudaDeviceSetLimit</span><span class="p">(</span><span class="n">cudaLimitPersistingL2CacheSize</span><span class="p">,</span><span class="n">prop</span><span class="p">.</span><span class="n">persistingL2CacheMaxSize</span><span class="p">);</span><span class="cm">/* Set aside max possible size of L2 cache for persisting accesses */</span>
</pre>
 <p>
  Mapping of user data to L2 set-aside portion can be controlled using an access policy window on a CUDA stream or CUDA graph kernel node. The example below shows how to use the access policy window on a CUDA stream.
 </p>
 <pre><span class="n">cudaStreamAttrValue</span><span class="n">stream_attribute</span><span class="p">;</span><span class="c1">// Stream level attributes data structure</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">base_ptr</span><span class="o">=</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">ptr</span><span class="p">);</span><span class="c1">// Global Memory data pointer</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">num_bytes</span><span class="o">=</span><span class="n">num_bytes</span><span class="p">;</span><span class="c1">// Number of bytes for persisting accesses.</span>
<span class="c1">// (Must be less than cudaDeviceProp::accessPolicyMaxWindowSize)</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">hitRatio</span><span class="o">=</span><span class="mf">1.0</span><span class="p">;</span><span class="c1">// Hint for L2 cache hit ratio for persisting accesses in the num_bytes region</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">hitProp</span><span class="o">=</span><span class="n">cudaAccessPropertyPersisting</span><span class="p">;</span><span class="c1">// Type of access property on cache hit</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">missProp</span><span class="o">=</span><span class="n">cudaAccessPropertyStreaming</span><span class="p">;</span><span class="c1">// Type of access property on cache miss.</span>

<span class="c1">//Set the attributes to a CUDA stream of type cudaStream_t</span>
<span class="n">cudaStreamSetAttribute</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span><span class="n">cudaStreamAttributeAccessPolicyWindow</span><span class="p">,</span><span class="o">&amp;</span><span class="n">stream_attribute</span><span class="p">);</span>
</pre>
 <p>
  The access policy window requires a value for
  <span class="pre">
   hitRatio
  </span>
  and
  <span class="pre">
   num_bytes
  </span>
  . Depending on the value of the
  <span class="pre">
   num_bytes
  </span>
  parameter and the size of L2 cache, one may need to tune the value of
  <span class="pre">
   hitRatio
  </span>
  to avoid thrashing of L2 cache lines.
 </p>
 <h4>
  <span class="section-number">
   9.2.2.2.
  </span>
  Tuning the Access Window Hit-Ratio
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#tuning-the-access-window-hit-ratio" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  The
  <span class="pre">
   hitRatio
  </span>
  parameter can be used to specify the fraction of accesses that receive the
  <span class="pre">
   hitProp
  </span>
  property. For example, if the
  <span class="pre">
   hitRatio
  </span>
  value is 0.6, 60% of the memory accesses in the global memory region [ptr..ptr+num_bytes) have the persisting property and 40% of the memory accesses have the streaming property. To understand the effect of
  <span class="pre">
   hitRatio
  </span>
  and
  <span class="pre">
   num_bytes
  </span>
  , we use a sliding window micro benchmark.
 </p>
 <p>
  This microbenchmark uses a 1024 MB region in GPU global memory. First, we set aside 30 MB of the L2 cache for persisting accesses using
  <span class="pre">
   cudaDeviceSetLimit()
  </span>
  , as discussed above. Then, as shown in the figure below, we specify that the accesses to the first
  <span class="pre">
   freqSize
  </span>
  <span class="pre">
   *
  </span>
  <span class="pre">
   sizeof(int)
  </span>
  bytes of the memory region are persistent. This data will thus use the L2 set-aside portion. In our experiment, we vary the size of this persistent data region from 10 MB to 60 MB to model various scenarios where data fits in or exceeds the available L2 set-aside portion of 30 MB. Note that the NVIDIA Tesla A100 GPU has 40 MB of total L2 cache capacity. Accesses to the remaining data of the memory region (i.e., streaming data) are considered normal or streaming accesses and will thus use the remaining 10 MB of the non set-aside L2 portion (unless part of the L2 set-aside portion is unused).
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/sliding-window-l2.png">
 </a>
 <p>
  <span class="caption-text">
   Mapping Persistent data accesses to set-aside L2 in sliding window experiment
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#l2-cache-hit-ratio-sliding-window-l2" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  Consider the following kernel code and access window parameters, as the implementation of the sliding window experiment.
 </p>
 <pre><span class="n">__global__</span><span class="kt">void</span><span class="n">kernel</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="n">data_persistent</span><span class="p">,</span><span class="kt">int</span><span class="o">*</span><span class="n">data_streaming</span><span class="p">,</span><span class="kt">int</span><span class="n">dataSize</span><span class="p">,</span><span class="kt">int</span><span class="n">freqSize</span><span class="p">)</span><span class="p">{</span>
<span class="kt">int</span><span class="n">tid</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>

<span class="cm">/*Each CUDA thread accesses one element in the persistent data section</span>
<span class="cm">      and one element in the streaming data section.</span>
<span class="cm">      Because the size of the persistent memory region (freqSize * sizeof(int) bytes) is much</span>
<span class="cm">      smaller than the size of the streaming memory region (dataSize * sizeof(int) bytes), data</span>
<span class="cm">      in the persistent region is accessed more frequently*/</span>

<span class="n">data_persistent</span><span class="p">[</span><span class="n">tid</span><span class="o">%</span><span class="n">freqSize</span><span class="p">]</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">data_persistent</span><span class="p">[</span><span class="n">tid</span><span class="o">%</span><span class="n">freqSize</span><span class="p">];</span>
<span class="n">data_streaming</span><span class="p">[</span><span class="n">tid</span><span class="o">%</span><span class="n">dataSize</span><span class="p">]</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">data_streaming</span><span class="p">[</span><span class="n">tid</span><span class="o">%</span><span class="n">dataSize</span><span class="p">];</span>
<span class="p">}</span>

<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">base_ptr</span><span class="o">=</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data_persistent</span><span class="p">);</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">num_bytes</span><span class="o">=</span><span class="n">freqSize</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">);</span><span class="c1">//Number of bytes for persisting accesses in range 10-60 MB</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">hitRatio</span><span class="o">=</span><span class="mf">1.0</span><span class="p">;</span><span class="c1">//Hint for cache hit ratio. Fixed value 1.0</span>
</pre>
 <p>
  The performance of the above kernel is shown in the chart below. When the persistent data region fits well into the 30 MB set-aside portion of the L2 cache, a performance increase of as much as 50% is observed. However, once the size of this persistent data region exceeds the size of the L2 set-aside cache portion, approximately 10% performance drop is observed due to thrashing of L2 cache lines.
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/l2-hitratio-before.png">
 </a>
 <p>
  <span class="caption-text">
   The performance of the sliding-window benchmark with fixed hit-ratio of 1.0
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#l2-cache-hit-ratio-l2-hitratio-before" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  In order to optimize the performance, when the size of the persistent data is more than the size of the set-aside L2 cache portion, we tune the
  <span class="pre">
   num_bytes
  </span>
  and
  <span class="pre">
   hitRatio
  </span>
  parameters in the access window as below.
 </p>
 <pre><span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">base_ptr</span><span class="o">=</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data_persistent</span><span class="p">);</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">num_bytes</span><span class="o">=</span><span class="mi">20</span><span class="o">*</span><span class="mi">1024</span><span class="o">*</span><span class="mi">1024</span><span class="p">;</span><span class="c1">//20 MB</span>
<span class="n">stream_attribute</span><span class="p">.</span><span class="n">accessPolicyWindow</span><span class="p">.</span><span class="n">hitRatio</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="o">*</span><span class="mi">1024</span><span class="o">*</span><span class="mi">1024</span><span class="p">)</span><span class="o">/</span><span class="p">((</span><span class="kt">float</span><span class="p">)</span><span class="n">freqSize</span><span class="o">*</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">int</span><span class="p">));</span><span class="c1">//Such that up to 20MB of data is resident.</span>
</pre>
 <p>
  We fix the
  <span class="pre">
   num_bytes
  </span>
  in the access window to 20 MB and tune the
  <span class="pre">
   hitRatio
  </span>
  such that a random 20 MB of the total persistent data is resident in the L2 set-aside cache portion. The remaining portion of this persistent data will be accessed using the streaming property. This helps in reducing cache thrashing. The results are shown in the chart below, where we see good performance regardless of whether the persistent data fits in the L2 set-aside or not.
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/l2-hitratio-after.png">
 </a>
 <p>
  <span class="caption-text">
   The performance of the sliding-window benchmark with tuned hit-ratio
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#l2-cache-hit-ratio-l2-hitratio-after" title="Permalink to this image">
   ï
  </a>
 </p>
 <h3>
  <span class="section-number">
   9.2.3.
  </span>
  Shared Memory
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Because it is on-chip, shared memory has much higher bandwidth and lower latency than local and global memory - provided there are no bank conflicts between the threads, as detailed in the following section.
 </p>
 <h4>
  <span class="section-number">
   9.2.3.1.
  </span>
  Shared Memory and Memory Banks
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-and-memory-banks" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  To achieve high memory bandwidth for concurrent accesses, shared memory is divided into equally sized memory modules (
  banks
  ) that can be accessed simultaneously. Therefore, any memory load or store of
  n
  addresses that spans
  n
  distinct memory banks can be serviced simultaneously, yielding an effective bandwidth that is
  n
  times as high as the bandwidth of a single bank.
 </p>
 <p>
  However, if multiple addresses of a memory request map to the same memory bank, the accesses are serialized. The hardware splits a memory request that has bank conflicts into as many separate conflict-free requests as necessary, decreasing the effective bandwidth by a factor equal to the number of separate memory requests. The one exception here is when multiple threads in a warp address the same shared memory location, resulting in a broadcast. In this case, multiple broadcasts from different banks are coalesced into a single multicast from the requested shared memory locations to the threads.
 </p>
 <p>
  To minimize bank conflicts, it is important to understand how memory addresses map to memory banks and how to optimally schedule memory requests.
 </p>
 <p>
  On devices of compute capability 5.x or newer, each bank has a bandwidth of 32 bits every clock cycle, and successive 32-bit words are assigned to successive banks. The warp size is 32 threads and the number of banks is also 32, so bank conflicts can occur between any threads in the warp. See Compute Capability 5.x in the CUDA C++ Programming Guide for further details.
 </p>
 <h4>
  <span class="section-number">
   9.2.3.2.
  </span>
  Shared Memory in Matrix Multiplication (C=AB)
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Shared memory enables cooperation between threads in a block. When multiple threads in a block use the same data from global memory, shared memory can be used to access the data from global memory only once. Shared memory can also be used to avoid uncoalesced memory accesses by loading and storing data in a coalesced pattern from global memory and then reordering it in shared memory. Aside from memory bank conflicts, there is no penalty for non-sequential or unaligned accesses by a warp in shared memory.
 </p>
 <p>
  The use of shared memory is illustrated via the simple example of a matrix multiplication C = AB for the case with A of dimension Mxw, B of dimension wxN, and C of dimension MxN. To keep the kernels simple, M and N are multiples of 32, since the warp size (w) is 32 for current devices.
 </p>
 <p>
  A natural decomposition of the problem is to use a block and tile size of wxw threads. Therefore, in terms of wxw tiles, A is a column matrix, B is a row matrix, and C is their outer product; see
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab__block-column-matrix-A-multiplied-block-row-matrix-B-product-matrix-c">
   Figure 11
  </a>
  . A grid of N/w by M/w blocks is launched, where each thread block calculates the elements of a different tile in C from a single tile of A and a single tile of B.
 </p>
 <p>
  <span class="caption-text">
   Block-column matrix multiplied by block-row matrix. Block-column matrix (A) multiplied by block-row matrix (B) with resulting product matrix (C).
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab-block-column-matrix-a-multiplied-block-row-matrix-b-product-matrix-c" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  To do this, the
  <span class="pre">
   simpleMultiply
  </span>
  kernel (
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab__unoptimized-matrix-multiplication">
   Unoptimized matrix multiplication
  </a>
  ) calculates the output elements of a tile of matrix C.
 </p>
 <p class="title sectiontitle rubric" id="unoptimized-matrix-multiplication">
  Unoptimized matrix multiplication
 </p>
 <pre><span class="n">__global__</span><span class="kt">void</span><span class="n">simpleMultiply</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="n">b</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="n">c</span><span class="p">,</span>
<span class="kt">int</span><span class="n">N</span><span class="p">)</span>
<span class="p">{</span>
<span class="kt">int</span><span class="n">row</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="kt">int</span><span class="n">col</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="kt">float</span><span class="n">sum</span><span class="o">=</span><span class="mf">0.0f</span><span class="p">;</span>
<span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">TILE_DIM</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="p">{</span>
<span class="n">sum</span><span class="o">+=</span><span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">];</span>
<span class="p">}</span>
<span class="n">c</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">]</span><span class="o">=</span><span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>
</pre>
 <p>
  In
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab__unoptimized-matrix-multiplication">
   Unoptimized matrix multiplication
  </a>
  ,
  <span class="pre">
   a
  </span>
  ,
  <span class="pre">
   b
  </span>
  , and
  <span class="pre">
   c
  </span>
  are pointers to global memory for the matrices A, B, and C, respectively;
  <span class="pre">
   blockDim.x
  </span>
  ,
  <span class="pre">
   blockDim.y
  </span>
  , and
  <span class="pre">
   TILE_DIM
  </span>
  are all equal to w. Each thread in the wxw-thread block calculates one element in a tile of C.
  <span class="pre">
   row
  </span>
  and
  <span class="pre">
   col
  </span>
  are the row and column of the element in C being calculated by a particular thread. The
  <span class="pre">
   for
  </span>
  loop over
  <span class="pre">
   i
  </span>
  multiplies a row of A by a column of B, which is then written to C.
 </p>
 <p>
  The effective bandwidth of this kernel is 119.9 GB/s on an NVIDIA Tesla V100. To analyze performance, it is necessary to consider how warps access global memory in the
  <span class="pre">
   for
  </span>
  loop. Each warp of threads calculates one row of a tile of C, which depends on a single row of A and an entire tile of B as illustrated in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab__computing-row-c-tile-c-row-a-tile-b">
   Figure 12
  </a>
  .
 </p>
 <p>
  <span class="caption-text">
   Computing a row of a tile. Computing a row of a tile in C using one row of A and an entire tile of B.
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab-computing-row-c-tile-c-row-a-tile-b" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  For each iteration
  i
  of the
  <span class="pre">
   for
  </span>
  loop, the threads in a warp read a row of the B tile, which is a sequential and coalesced access for all compute capabilities.
 </p>
 <p>
  However, for each iteration
  i
  , all threads in a warp read the same value from global memory for matrix A, as the index
  <span class="pre">
   row*TILE_DIM+i
  </span>
  is constant within a warp. Even though such an access requires only 1 transaction on devices of compute capability 2.0 or higher, there is wasted bandwidth in the transaction, because only one 4-byte word out of 8 words in a 32-byte cache segment is used. We can reuse this cache line in subsequent iterations of the loop, and we would eventually utilize all 8 words; however, when many warps execute on the same multiprocessor simultaneously, as is generally the case, the cache line may easily be evicted from the cache between iterations
  i
  and
  i+1
  .
 </p>
 <p>
  The performance on a device of any compute capability can be improved by reading a tile of A into shared memory as shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab__using-shared-memory-improve-global-memory-load-efficiency-matrix-multiplication">
   Using shared memory to improve the global memory load efficiency in matrix multiplication
  </a>
  .
 </p>
 <p class="title sectiontitle rubric" id="using-shared-memory-to-improve-the-global-memory-load-efficiency-in-matrix-multiplication">
  Using shared memory to improve the global memory load efficiency in matrix multiplication
 </p>
 <pre><span class="n">__global__</span><span class="kt">void</span><span class="n">coalescedMultiply</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="n">b</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="n">c</span><span class="p">,</span>
<span class="kt">int</span><span class="n">N</span><span class="p">)</span>
<span class="p">{</span>
<span class="n">__shared__</span><span class="kt">float</span><span class="n">aTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="p">];</span>

<span class="kt">int</span><span class="n">row</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="kt">int</span><span class="n">col</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="kt">float</span><span class="n">sum</span><span class="o">=</span><span class="mf">0.0f</span><span class="p">;</span>
<span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="o">=</span><span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="n">__syncwarp</span><span class="p">();</span>
<span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">TILE_DIM</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="p">{</span>
<span class="n">sum</span><span class="o">+=</span><span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">];</span>
<span class="p">}</span>
<span class="n">c</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">]</span><span class="o">=</span><span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>
</pre>
 <p>
  In
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab__using-shared-memory-improve-global-memory-load-efficiency-matrix-multiplication">
   Using shared memory to improve the global memory load efficiency in matrix multiplication
  </a>
  , each element in a tile of A is read from global memory only once, in a fully coalesced fashion (with no wasted bandwidth), to shared memory. Within each iteration of the
  <span class="pre">
   for
  </span>
  loop, a value in shared memory is broadcast to all threads in a warp. Instead of a
  <span class="pre">
   __syncthreads()
  </span>
  synchronization barrier call, a
  <span class="pre">
   __syncwarp()
  </span>
  is sufficient after reading the tile of A into shared memory because only threads within the warp that write the data into shared memory read this data. This kernel has an effective bandwidth of 144.4 GB/s on an NVIDIA Tesla V100. This illustrates the use of the shared memory as a
  user-managed cache
  when the hardware L1 cache eviction policy does not match up well with the needs of the application or when L1 cache is not used for reads from global memory.
 </p>
 <p>
  A further improvement can be made to how
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab__using-shared-memory-improve-global-memory-load-efficiency-matrix-multiplication">
   Using shared memory to improve the global memory load efficiency in matrix multiplication
  </a>
  deals with matrix B. In calculating each of the rows of a tile of matrix C, the entire tile of B is read. The repeated reading of the B tile can be eliminated by reading it into shared memory once (
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab__improvement-reading-additional-data-shared-memory">
   Improvement by reading additional data into shared memory
  </a>
  ).
 </p>
 <p class="title sectiontitle rubric" id="improvement-by-reading-additional-data-into-shared-memory">
  Improvement by reading additional data into shared memory
 </p>
 <pre><span class="n">__global__</span><span class="kt">void</span><span class="n">sharedABMultiply</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="n">b</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="n">c</span><span class="p">,</span>
<span class="kt">int</span><span class="n">N</span><span class="p">)</span>
<span class="p">{</span>
<span class="n">__shared__</span><span class="kt">float</span><span class="n">aTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="p">],</span>
<span class="n">bTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="p">];</span>
<span class="kt">int</span><span class="n">row</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="kt">int</span><span class="n">col</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="kt">float</span><span class="n">sum</span><span class="o">=</span><span class="mf">0.0f</span><span class="p">;</span>
<span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="o">=</span><span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="n">bTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="o">=</span><span class="n">b</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">];</span>
<span class="n">__syncthreads</span><span class="p">();</span>
<span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">TILE_DIM</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="p">{</span>
<span class="n">sum</span><span class="o">+=</span><span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">bTile</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="p">}</span>
<span class="n">c</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">N</span><span class="o">+</span><span class="n">col</span><span class="p">]</span><span class="o">=</span><span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>
</pre>
 <p>
  Note that in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab__improvement-reading-additional-data-shared-memory">
   Improvement by reading additional data into shared memory
  </a>
  , a
  <span class="pre">
   __syncthreads()
  </span>
  call is required after reading the B tile because a warp reads data from shared memory that were written to shared memory by different warps. The effective bandwidth of this routine is 195.5 GB/s on an NVIDIA Tesla V100. Note that the performance improvement is not due to improved coalescing in either case, but to avoiding redundant transfers from global memory.
 </p>
 <p>
  The results of the various optimizations are summarized in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab__performance-improvements-optimizing-c-ab-matrix">
   Table 2
  </a>
  .
 </p>
 <table class="docutils align-default" id="id5">
  <span class="caption-text">
   Table 2. Performance Improvements Optimizing C = AB Matrix Multiply
:class table-no-stripes
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#id5" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Optimization
    </p>
   </th>
   <th class="head">
    <p>
     NVIDIA Tesla V100
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     No optimization
    </p>
   </td>
   <td>
    <p>
     119.9 GB/s
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Coalesced using shared memory to store a tile of A
    </p>
   </td>
   <td>
    <p>
     144.4 GB/s
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Using shared memory to eliminate redundant reads of a tile of B
    </p>
   </td>
   <td>
    <p>
     195.5 GB/s
    </p>
   </td>
  </tr>
 </table>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Medium Priority:
  Use shared memory to avoid redundant transfers from global memory.
 </p>
 <h4>
  <span class="section-number">
   9.2.3.3.
  </span>
  Shared Memory in Matrix Multiplication (C=AAT)
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-aat" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  A variant of the previous matrix multiplication can be used to illustrate how strided accesses to global memory, as well as shared memory bank conflicts, are handled. This variant simply uses the transpose of A in place of B, so C = AA
  T
  .
 </p>
 <p>
  A simple implementation for C = AA
  T
  is shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-aa__unoptimized-handling-strided-accesses-global-memory">
   Unoptimized handling of strided accesses to global memory
  </a>
 </p>
 <p class="title sectiontitle rubric" id="unoptimized-handling-of-strided-accesses-to-global-memory">
  Unoptimized handling of strided accesses to global memory
 </p>
 <pre><span class="n">__global__</span><span class="kt">void</span><span class="n">simpleMultiply</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="n">c</span><span class="p">,</span><span class="kt">int</span><span class="n">M</span><span class="p">)</span>
<span class="p">{</span>
<span class="kt">int</span><span class="n">row</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="kt">int</span><span class="n">col</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="kt">float</span><span class="n">sum</span><span class="o">=</span><span class="mf">0.0f</span><span class="p">;</span>
<span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">TILE_DIM</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="p">{</span>
<span class="n">sum</span><span class="o">+=</span><span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">a</span><span class="p">[</span><span class="n">col</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
<span class="n">c</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">M</span><span class="o">+</span><span class="n">col</span><span class="p">]</span><span class="o">=</span><span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>
</pre>
 <p>
  In
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-aa__unoptimized-handling-strided-accesses-global-memory">
   Unoptimized handling of strided accesses to global memory
  </a>
  , the
  row
  -th,
  col
  -th element of C is obtained by taking the dot product of the
  row
  -th and
  col
  -th rows of A. The effective bandwidth for this kernel is 12.8 GB/s on an NVIDIA Tesla V100. These results are substantially lower than the corresponding measurements for the C = AB kernel. The difference is in how threads in a half warp access elements of A in the second term,
  <span class="pre">
   a[col*TILE_DIM+i]
  </span>
  , for each iteration
  <span class="pre">
   i
  </span>
  . For a warp of threads,
  <span class="pre">
   col
  </span>
  represents sequential columns of the transpose of A, and therefore
  <span class="pre">
   col*TILE_DIM
  </span>
  represents a strided access of global memory with a stride of w, resulting in plenty of wasted bandwidth.
 </p>
 <p>
  The way to avoid strided access is to use shared memory as before, except in this case a warp reads a row of A into a column of a shared memory tile, as shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-aa__optimized-version-coalesced-reads-global-memory">
   An optimized handling of strided accesses using coalesced reads from global memory
  </a>
  .
 </p>
 <p class="title sectiontitle rubric" id="an-optimized-handling-of-strided-accesses-using-coalesced-reads-from-global-memory">
  An optimized handling of strided accesses using coalesced reads from global memory
 </p>
 <pre><span class="n">__global__</span><span class="kt">void</span><span class="n">coalescedMultiply</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="n">a</span><span class="p">,</span><span class="kt">float</span><span class="o">*</span><span class="n">c</span><span class="p">,</span><span class="kt">int</span><span class="n">M</span><span class="p">)</span>
<span class="p">{</span>
<span class="n">__shared__</span><span class="kt">float</span><span class="n">aTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="p">],</span>
<span class="n">transposedTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="p">];</span>
<span class="kt">int</span><span class="n">row</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">y</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">y</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">;</span>
<span class="kt">int</span><span class="n">col</span><span class="o">=</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
<span class="kt">float</span><span class="n">sum</span><span class="o">=</span><span class="mf">0.0f</span><span class="p">;</span>
<span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="o">=</span><span class="n">a</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="n">transposedTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">]</span><span class="o">=</span>
<span class="n">a</span><span class="p">[(</span><span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">)</span><span class="o">*</span><span class="n">TILE_DIM</span><span class="o">+</span>
<span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="n">__syncthreads</span><span class="p">();</span>
<span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">TILE_DIM</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="p">{</span>
<span class="n">sum</span><span class="o">+=</span><span class="n">aTile</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">y</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">transposedTile</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="p">}</span>
<span class="n">c</span><span class="p">[</span><span class="n">row</span><span class="o">*</span><span class="n">M</span><span class="o">+</span><span class="n">col</span><span class="p">]</span><span class="o">=</span><span class="n">sum</span><span class="p">;</span>
<span class="p">}</span>
</pre>
 <p>
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-aa__optimized-version-coalesced-reads-global-memory">
   An optimized handling of strided accesses using coalesced reads from global memory
  </a>
  uses the shared
  <span class="pre">
   transposedTile
  </span>
  to avoid uncoalesced accesses in the second term in the dot product and the shared
  <span class="pre">
   aTile
  </span>
  technique from the previous example to avoid uncoalesced accesses in the first term. The effective bandwidth of this kernel is 140.2 GB/s on an NVIDIA Tesla V100.These results are lower than those obtained by the final kernel for C = AB. The cause of the difference is shared memory bank conflicts.
 </p>
 <p>
  The reads of elements in
  <span class="pre">
   transposedTile
  </span>
  within the for loop are free of conflicts, because threads of each half warp read across rows of the tile, resulting in unit stride across the banks. However, bank conflicts occur when copying the tile from global memory into shared memory. To enable the loads from global memory to be coalesced, data are read from global memory sequentially. However, this requires writing to shared memory in columns, and because of the use of wxw tiles in shared memory, this results in a stride between threads of w banks - every thread of the warp hits the same bank (Recall that w is selected as 32). These many-way bank conflicts are very expensive. The simple remedy is to pad the shared memory array so that it has an extra column, as in the following line of code.
 </p>
 <pre><span class="n">__shared__</span><span class="kt">float</span><span class="n">transposedTile</span><span class="p">[</span><span class="n">TILE_DIM</span><span class="p">][</span><span class="n">TILE_DIM</span><span class="o">+</span><span class="mi">1</span><span class="p">];</span>
</pre>
 <p>
  This padding eliminates the conflicts entirely, because now the stride between threads is w+1 banks (i.e., 33 for current devices), which, due to modulo arithmetic used to compute bank indices, is equivalent to a unit stride. After this change, the effective bandwidth is 199.4 GB/s on an NVIDIA Tesla V100, which is comparable to the results from the last C = AB kernel.
 </p>
 <p>
  The results of these optimizations are summarized in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-aa__performance-improvements-optimizing-c">
   Table 3
  </a>
  .
 </p>
 <table class="table-no-stripes docutils align-default" id="id6">
  <span class="caption-text">
   Table 3. Performance Improvements Optimizing C = AA
   T
   Matrix Multiplication
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#id6" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Optimization
    </p>
   </th>
   <th class="head">
    <p>
     NVIDIA Tesla V100
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     No optimization
    </p>
   </td>
   <td>
    <p>
     12.8 GB/s
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Using shared memory to coalesce global reads
    </p>
   </td>
   <td>
    <p>
     140.2 GB/s
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Removing bank conflicts
    </p>
   </td>
   <td>
    <p>
     199.4 GB/s
    </p>
   </td>
  </tr>
 </table>
 <p>
  These results should be compared with those in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c-ab__performance-improvements-optimizing-c-ab-matrix">
   Table 2
  </a>
  . As can be seen from these tables, judicious use of shared memory can dramatically improve performance.
 </p>
 <p>
  The examples in this section have illustrated three reasons to use shared memory:
 </p>
 <ul class="simple">
  <li>
   <p>
    To enable coalesced accesses to global memory, especially to avoid large strides (for general matrices, strides are much larger than 32)
   </p>
  </li>
  <li>
   <p>
    To eliminate (or reduce) redundant loads from global memory
   </p>
  </li>
  <li>
   <p>
    To avoid wasted bandwidth
   </p>
  </li>
 </ul>
 <h4>
  <span class="section-number">
   9.2.3.4.
  </span>
  Asynchronous Copy from Global Memory to Shared Memory
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-copy-from-global-memory-to-shared-memory" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  CUDA 11.0 introduces an
  async-copy
  feature that can be used within device code to explicitly manage the asynchronous copying of data from global memory to shared memory. This feature enables CUDA kernels to overlap copying data from global to shared memory with computation. It also avoids an intermediary register file access traditionally present between the global memory read and the shared memory write.
 </p>
 <p>
  For more details refer to the
  <span class="pre">
   memcpy_async
  </span>
  section in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#async_data_operations">
   CUDA C++ Programming Guide
  </a>
  .
 </p>
 <p>
  To understand the performance difference between synchronous copy and asynchronous copy of data from global memory to shared memory, consider the following micro benchmark CUDA kernels for demonstrating the synchronous and asynchronous approaches. Asynchronous copies are hardware accelerated for NVIDIA A100 GPU.
 </p>
 <pre><span class="n">template</span><span class="o">&lt;</span><span class="n">typename</span><span class="n">T</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="kt">void</span><span class="n">pipeline_kernel_sync</span><span class="p">(</span><span class="n">T</span><span class="o">*</span><span class="n">global</span><span class="p">,</span><span class="kt">uint64_t</span><span class="o">*</span><span class="n">clock</span><span class="p">,</span><span class="kt">size_t</span><span class="n">copy_count</span><span class="p">)</span><span class="p">{</span>
<span class="k">extern</span><span class="n">__shared__</span><span class="kt">char</span><span class="n">s</span><span class="p">[];</span>
<span class="n">T</span><span class="o">*</span><span class="n">shared</span><span class="o">=</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">s</span><span class="p">);</span>

<span class="kt">uint64_t</span><span class="n">clock_start</span><span class="o">=</span><span class="n">clock64</span><span class="p">();</span>

<span class="k">for</span><span class="p">(</span><span class="kt">size_t</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">copy_count</span><span class="p">;</span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="p">{</span>
<span class="n">shared</span><span class="p">[</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="o">=</span><span class="n">global</span><span class="p">[</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
<span class="p">}</span>

<span class="kt">uint64_t</span><span class="n">clock_end</span><span class="o">=</span><span class="n">clock64</span><span class="p">();</span>

<span class="n">atomicAdd</span><span class="p">(</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">unsigned</span><span class="kt">long</span><span class="kt">long</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">clock</span><span class="p">),</span>
<span class="n">clock_end</span><span class="o">-</span><span class="n">clock_start</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">template</span><span class="o">&lt;</span><span class="n">typename</span><span class="n">T</span><span class="o">&gt;</span>
<span class="n">__global__</span><span class="kt">void</span><span class="n">pipeline_kernel_async</span><span class="p">(</span><span class="n">T</span><span class="o">*</span><span class="n">global</span><span class="p">,</span><span class="kt">uint64_t</span><span class="o">*</span><span class="n">clock</span><span class="p">,</span><span class="kt">size_t</span><span class="n">copy_count</span><span class="p">)</span><span class="p">{</span>
<span class="k">extern</span><span class="n">__shared__</span><span class="kt">char</span><span class="n">s</span><span class="p">[];</span>
<span class="n">T</span><span class="o">*</span><span class="n">shared</span><span class="o">=</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="n">T</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">s</span><span class="p">);</span>

<span class="kt">uint64_t</span><span class="n">clock_start</span><span class="o">=</span><span class="n">clock64</span><span class="p">();</span>

<span class="c1">//pipeline pipe;</span>
<span class="k">for</span><span class="p">(</span><span class="kt">size_t</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">copy_count</span><span class="p">;</span><span class="o">++</span><span class="n">i</span><span class="p">)</span><span class="p">{</span>
<span class="n">__pipeline_memcpy_async</span><span class="p">(</span><span class="o">&amp;</span><span class="n">shared</span><span class="p">[</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">],</span>
<span class="o">&amp;</span><span class="n">global</span><span class="p">[</span><span class="n">blockDim</span><span class="p">.</span><span class="n">x</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">],</span><span class="k">sizeof</span><span class="p">(</span><span class="n">T</span><span class="p">));</span>
<span class="p">}</span>
<span class="n">__pipeline_commit</span><span class="p">();</span>
<span class="n">__pipeline_wait_prior</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

<span class="kt">uint64_t</span><span class="n">clock_end</span><span class="o">=</span><span class="n">clock64</span><span class="p">();</span>

<span class="n">atomicAdd</span><span class="p">(</span><span class="n">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">unsigned</span><span class="kt">long</span><span class="kt">long</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">clock</span><span class="p">),</span>
<span class="n">clock_end</span><span class="o">-</span><span class="n">clock_start</span><span class="p">);</span>
<span class="p">}</span>
</pre>
 <p>
  The synchronous version for the kernel loads an element from global memory to an intermediate register and then stores the intermediate register value to shared memory. In the asynchronous version of the kernel, instructions to load from global memory and store directly into shared memory are issued as soon as
  <span class="pre">
   __pipeline_memcpy_async()
  </span>
  function is called. The
  <span class="pre">
   __pipeline_wait_prior(0)
  </span>
  will wait until all the instructions in the pipe object have been executed. Using asynchronous copies does not use any intermediate register. Not using intermediate registers can help reduce register pressure and can increase kernel occupancy. Data copied from global memory to shared memory using asynchronous copy instructions can be cached in the L1 cache or the L1 cache can be optionally bypassed. If individual CUDA threads are copying elements of 16 bytes, the L1 cache can be bypassed. This difference is illustrated in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#async-copy__sync-vs-async">
   Figure 13
  </a>
  .
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/sync-vs-async.png">
 </a>
 <p>
  <span class="caption-text">
   Comparing Synchronous vs Asynchronous Copy from Global Memory to Shared Memory
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#async-copy-sync-vs-async" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  We evaluate the performance of both kernels using elements of size 4B, 8B and 16B per thread i.e., using
  <span class="pre">
   int
  </span>
  ,
  <span class="pre">
   int2
  </span>
  and
  <span class="pre">
   int4
  </span>
  for the template parameter. We adjust the
  <span class="pre">
   copy_count
  </span>
  in the kernels such that each thread block copies from 512 bytes up to 48 MB. The performance of the kernels is shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#async-copy__async-perf">
   Figure 14
  </a>
  .
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/async-perf.png">
 </a>
 <p>
  <span class="caption-text">
   Comparing Performance of Synchronous vs Asynchronous Copy from Global Memory to Shared Memory
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#async-copy-async-perf" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  From the performance chart, the following observations can be made for this experiment.
 </p>
 <ul class="simple">
  <li>
   <p>
    Best performance with synchronous copy is achieved when the
    <span class="pre">
     copy_count
    </span>
    parameter is a multiple of 4 for all three element sizes. The compiler can optimize groups of 4 load and store instructions. This is evident from the saw tooth curves.
   </p>
  </li>
  <li>
   <p>
    Asynchronous copy achieves better performance in nearly all cases.
   </p>
  </li>
  <li>
   <p>
    The async-copy does not require the
    <span class="pre">
     copy_count
    </span>
    parameter to be a multiple of 4, to maximize performance through compiler optimizations.
   </p>
  </li>
  <li>
   <p>
    Overall, best performance is achieved when using asynchronous copies with an element of size 8 or 16 bytes.
   </p>
  </li>
 </ul>
 <h3>
  <span class="section-number">
   9.2.4.
  </span>
  Local Memory
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#local-memory" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Local memory is so named because its scope is local to the thread, not because of its physical location. In fact, local memory is off-chip. Hence, access to local memory is as expensive as access to global memory. In other words, the term
  local
  in the name does not imply faster access.
 </p>
 <p>
  Local memory is used only to hold automatic variables. This is done by the
  <span class="pre">
   nvcc
  </span>
  compiler when it determines that there is insufficient register space to hold the variable. Automatic variables that are likely to be placed in local memory are large structures or arrays that would consume too much register space and arrays that the compiler determines may be indexed dynamically.
 </p>
 <p>
  Inspection of the PTX assembly code (obtained by compiling with
  <span class="pre">
   -ptx
  </span>
  or
  <span class="pre">
   -keep
  </span>
  command-line options to
  <span class="pre">
   nvcc
  </span>
  ) reveals whether a variable has been placed in local memory during the first compilation phases. If it has, it will be declared using the
  <span class="pre">
   .local
  </span>
  mnemonic and accessed using the
  <span class="pre">
   ld.local
  </span>
  and
  <span class="pre">
   st.local
  </span>
  mnemonics. If it has not, subsequent compilation phases might still decide otherwise, if they find the variable consumes too much register space for the targeted architecture. There is no way to check this for a specific variable, but the compiler reports total local memory usage per kernel (lmem) when run with the
  <span class="pre">
   --ptxas-options=-v
  </span>
  option.
 </p>
 <h3>
  <span class="section-number">
   9.2.5.
  </span>
  Texture Memory
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#texture-memory" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The read-only texture memory space is cached. Therefore, a texture fetch costs one device memory read only on a cache miss; otherwise, it just costs one read from the texture cache. The texture cache is optimized for 2D spatial locality, so threads of the same warp that read texture addresses that are close together will achieve best performance. Texture memory is also designed for streaming fetches with a constant latency; that is, a cache hit reduces DRAM bandwidth demand, but not fetch latency.
 </p>
 <p>
  In certain addressing situations, reading device memory through texture fetching can be an advantageous alternative to reading device memory from global or constant memory.
 </p>
 <h4>
  <span class="section-number">
   9.2.5.1.
  </span>
  Additional Texture Capabilities
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#additional-texture-capabilities" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  If textures are fetched using
  <span class="pre">
   tex1D()
  </span>
  ,
  <span class="pre">
   tex2D()
  </span>
  , or
  <span class="pre">
   tex3D()
  </span>
  rather than
  <span class="pre">
   tex1Dfetch()
  </span>
  , the hardware provides other capabilities that might be useful for some applications such as image processing, as shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#additional-texture-capabilities__useful-features-tex1D-tex2D-tex3D-fetches">
   Table 4
  </a>
  .
 </p>
 <table class="table-no-stripes docutils align-default" id="id7">
  <span class="caption-text">
   Table 4. Useful Features for tex1D(), tex2D(), and tex3D() Fetches
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#id7" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Feature
    </p>
   </th>
   <th class="head">
    <p>
     Use
    </p>
   </th>
   <th class="head">
    <p>
     Caveat
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Filtering
    </p>
   </td>
   <td>
    <p>
     Fast, low-precision interpolation between texels
    </p>
   </td>
   <td>
    <p>
     Valid only if the texture reference returns floating-point data
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     Normalized texture coordinates
    </p>
   </td>
   <td>
    <p>
     Resolution-independent coding
    </p>
   </td>
   <td>
    <p>
     None
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     Addressing modes
    </p>
   </td>
   <td>
    <p>
     Automatic handling of boundary cases
     1
    </p>
   </td>
   <td>
    <p>
     Can be used only with normalized texture coordinates
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     1
     The automatic handling of boundary cases in the bottom row of
     <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#additional-texture-capabilities__useful-features-tex1D-tex2D-tex3D-fetches">
      Table 4
     </a>
     refers to how a texture coordinate is resolved when it falls outside the valid addressing range. There are two options:
     clamp
     and
     wrap
     . If
     x
     is the coordinate and
     N
     is the number of texels for a one-dimensional texture, then with clamp,
     x
     is replaced by
     0
     if
     x
     &lt; 0 and by 1-1/
     N
     if 1
     &lt;
     x
     . With wrap,
     x
     is replaced by
     frac(x)
     where
     frac(x) = x - floor(x)
     . Floor returns the largest integer less than or equal to
     x
     . So, in clamp mode where
     N
     = 1, an
     x
     of 1.3 is clamped to 1.0; whereas in wrap mode, it is converted to 0.3
    </p>
   </td>
  </tr>
 </table>
 <p>
  Within a kernel call, the texture cache is not kept coherent with respect to global memory writes, so texture fetches from addresses that have been written via global stores in the same kernel call return undefined data. That is, a thread can safely read a memory location via texture if the location has been updated by a previous kernel call or memory copy, but not if it has been previously updated by the same thread or another thread within the same kernel call.
 </p>
 <h3>
  <span class="section-number">
   9.2.6.
  </span>
  Constant Memory
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#constant-memory" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  There is a total of 64 KB constant memory on a device. The constant memory space is cached. As a result, a read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp accesses only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access.
 </p>
 <h3>
  <span class="section-number">
   9.2.7.
  </span>
  Registers
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#registers" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  Generally, accessing a register consumes zero extra clock cycles per instruction, but delays may occur due to register read-after-write dependencies and register memory bank conflicts.
 </p>
 <p>
  The compiler and hardware thread scheduler will schedule instructions as optimally as possible to avoid register memory bank conflicts. An application has no direct control over these bank conflicts. In particular, there is no register-related reason to pack data into vector data types such as
  <span class="pre">
   float4
  </span>
  or
  <span class="pre">
   int4
  </span>
  types.
 </p>
 <h4>
  <span class="section-number">
   9.2.7.1.
  </span>
  Register Pressure
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#register-pressure" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Register pressure occurs when there are not enough registers available for a given task. Even though each multiprocessor contains thousands of 32-bit registers (see Features and Technical Specifications of the CUDA C++ Programming Guide), these are partitioned among concurrent threads. To prevent the compiler from allocating too many registers, use the
  <span class="pre">
   -maxrregcount=N
  </span>
  compiler command-line option (see
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#nvcc">
   nvcc
  </a>
  ) or the launch bounds kernel definition qualifier (see Execution Configuration of the CUDA C++ Programming Guide) to control the maximum number of registers to allocated per thread.
 </p>
 <h2>
  <span class="section-number">
   9.3.
  </span>
  Allocation
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#allocation" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Device memory allocation and de-allocation via
  <span class="pre">
   cudaMalloc()
  </span>
  and
  <span class="pre">
   cudaFree()
  </span>
  are expensive operations. It is recommended to use
  <span class="pre">
   cudaMallocAsync()
  </span>
  and
  <span class="pre">
   cudaFreeAsync()
  </span>
  which are stream ordered pool allocators to manage device memory.
 </p>
 <h2>
  <span class="section-number">
   9.4.
  </span>
  NUMA Best Practices
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#numa-best-practices" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Some recent Linux distributions enable automatic NUMA balancing (or â
  <a class="reference external" href="https://lwn.net/Articles/488709/">
   AutoNUMA
  </a>
  â) by default. In some instances, operations performed by automatic NUMA balancing may degrade the performance of applications running on NVIDIA GPUs. For optimal performance, users should manually tune the NUMA characteristics of their application.
 </p>
 <p>
  The optimal NUMA tuning will depend on the characteristics and desired hardware affinities of each application and node, but in general applications computing on NVIDIA GPUs are advised to choose a policy that disables automatic NUMA balancing. For example, on IBM Newell POWER9 nodes (where the CPUs correspond to NUMA nodes 0 and 8), use:
 </p>
 <pre>numactl --membind=0,8
</pre>
 <p>
  to bind memory allocations to the CPUs.
 </p>
 <h1>
  <span class="section-number">
   10.
  </span>
  Execution Configuration Optimizations
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#execution-configuration-optimizations" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  One of the keys to good performance is to keep the multiprocessors on the device as busy as possible. A device in which work is poorly balanced across the multiprocessors will deliver suboptimal performance. Hence, itâs important to design your application to use threads and blocks in a way that maximizes hardware utilization and to limit practices that impede the free distribution of work. A key concept in this effort is occupancy, which is explained in the following sections.
 </p>
 <p>
  Hardware utilization can also be improved in some cases by designing your application so that multiple, independent kernels can execute at the same time. Multiple kernels executing at the same time is known as concurrent kernel execution. Concurrent kernel execution is described below.
 </p>
 <p>
  Another important concept is the management of system resources allocated for a particular task. How to manage this resource utilization is discussed in the final sections of this chapter.
 </p>
 <h2>
  <span class="section-number">
   10.1.
  </span>
  Occupancy
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Thread instructions are executed sequentially in CUDA, and, as a result, executing other warps when one warp is paused or stalled is the only way to hide latencies and keep the hardware busy. Some metric related to the number of active warps on a multiprocessor is therefore important in determining how effectively the hardware is kept busy. This metric is
  occupancy
  .
 </p>
 <p>
  Occupancy is the ratio of the number of active warps per multiprocessor to the maximum number of possible active warps. (To determine the latter number, see the
  <span class="pre">
   deviceQuery
  </span>
  CUDA Sample or refer to Compute Capabilities in the CUDA C++ Programming Guide.) Another way to view occupancy is the percentage of the hardwareâs ability to process warps that is actively in use.
 </p>
 <p>
  Higher occupancy does not always equate to higher performance-there is a point above which additional occupancy does not improve performance. However, low occupancy always interferes with the ability to hide memory latency, resulting in performance degradation.
 </p>
 <p>
  Per thread resources required by a CUDA kernel might limit the maximum block size in an unwanted way. In order to maintain forward compatibility to future hardware and toolkits and to ensure that at least one thread block can run on an SM, developers should include the single argument
  <span class="pre">
   __launch_bounds__(maxThreadsPerBlock)
  </span>
  which specifies the largest block size that the kernel will be launched with. Failure to do so could lead to âtoo many resources requested for launchâ errors. Providing the two argument version of
  <span class="pre">
   __launch_bounds__(maxThreadsPerBlock,minBlocksPerMultiprocessor)
  </span>
  can improve performance in some cases. The right value for
  <span class="pre">
   minBlocksPerMultiprocessor
  </span>
  should be determined using a detailed per kernel analysis.
 </p>
 <h3>
  <span class="section-number">
   10.1.1.
  </span>
  Calculating Occupancy
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#calculating-occupancy" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  One of several factors that determine occupancy is register availability. Register storage enables threads to keep local variables nearby for low-latency access. However, the set of registers (known as the
  register file
  ) is a limited commodity that all threads resident on a multiprocessor must share. Registers are allocated to an entire block all at once. So, if each thread block uses many registers, the number of thread blocks that can be resident on a multiprocessor is reduced, thereby lowering the occupancy of the multiprocessor. The maximum number of registers per thread can be set manually at compilation time per-file using the
  <span class="pre">
   -maxrregcount
  </span>
  option or per-kernel using the
  <span class="pre">
   __launch_bounds__
  </span>
  qualifier (see
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#register-pressure">
   Register Pressure
  </a>
  ).
 </p>
 <p>
  For purposes of calculating occupancy, the number of registers used by each thread is one of the key factors. For example, on devices of
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
   compute capability
  </a>
  7.0 each multiprocessor has 65,536 32-bit registers and can have a maximum of 2048 simultaneous threads resident (64 warps x 32 threads per warp). This means that in one of these devices, for a multiprocessor to have 100% occupancy, each thread can use at most 32 registers. However, this approach of determining how register count affects occupancy does not take into account the register allocation granularity. For example, on a device of compute capability 7.0, a kernel with 128-thread blocks using 37 registers per thread results in an occupancy of 75% with 12 active 128-thread blocks per multi-processor, whereas a kernel with 320-thread blocks using the same 37 registers per thread results in an occupancy of 63% because only four 320-thread blocks can reside on a multiprocessor. Furthermore, register allocations are rounded up to the nearest 256 registers per warp.
 </p>
 <p>
  The number of registers available, the maximum number of simultaneous threads resident on each multiprocessor, and the register allocation granularity vary over different compute capabilities. Because of these nuances in register allocation and the fact that a multiprocessorâs shared memory is also partitioned between resident thread blocks, the exact relationship between register usage and occupancy can be difficult to determine. The
  <span class="pre">
   --ptxas
  </span>
  Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â
  <span class="pre">
   options=v
  </span>
  option of
  <span class="pre">
   nvcc
  </span>
  details the number of registers used per thread for each kernel. See Hardware Multithreading of the CUDA C++ Programming Guide for the register allocation formulas for devices of various compute capabilities and Features and Technical Specifications of the CUDA C++ Programming Guide for the total number of registers available on those devices. Alternatively, NVIDIA provides an occupancy calculator in the form of an Excel spreadsheet that enables developers to hone in on the optimal balance and to test different possible scenarios more easily. This spreadsheet, shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#calculating-occupancy__cuda-occupancy-calculator-usage-project-gpu-multi-occupancy">
   Figure 15
  </a>
  , is called
  <span class="pre">
   CUDA_Occupancy_Calculator.xls
  </span>
  and is located in the tools subdirectory of the CUDA Toolkit installation.
 </p>
 <p>
  <span class="caption-text">
   Using the CUDA Occupancy Calculator to project GPU multiprocessor occupancy
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#calculating-occupancy-cuda-occupancy-calculator-usage-project-gpu-multi-occupancy" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  In addition to the calculator spreadsheet, occupancy can be determined using the NVIDIA Nsight Compute Profiler. Details about occupancy are displayed in the Occupancy section.
 </p>
 <p>
  An application can also use the Occupancy API from the CUDA Runtime, e.g.
  <span class="pre">
   cudaOccupancyMaxActiveBlocksPerMultiprocessor
  </span>
  , to dynamically select launch configurations based on runtime parameters.
 </p>
 <h2>
  <span class="section-number">
   10.2.
  </span>
  Hiding Register Dependencies
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#hiding-register-dependencies" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Medium Priority:
  To hide latency arising from register dependencies, maintain sufficient numbers of active threads per multiprocessor (i.e., sufficient occupancy).
 </p>
 <p>
  Register dependencies arise when an instruction uses a result stored in a register written by an instruction before it. The latency of most arithmetic instructions is typically 4 cycles on devices of compute capability 7.0. So threads must wait approximatly 4 cycles before using an arithmetic result. However, this latency can be completely hidden by the execution of threads in other warps. See
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#registers">
   Registers
  </a>
  for details.
 </p>
 <h2>
  <span class="section-number">
   10.3.
  </span>
  Thread and Block Heuristics
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#thread-and-block-heuristics" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Medium Priority:
  The number of threads per block should be a multiple of 32 threads, because this provides optimal computing efficiency and facilitates coalescing.
 </p>
 <p>
  The dimension and size of blocks per grid and the dimension and size of threads per block are both important factors. The multidimensional aspect of these parameters allows easier mapping of multidimensional problems to CUDA and does not play a role in performance. As a result, this section discusses size but not dimension.
 </p>
 <p>
  Latency hiding and occupancy depend on the number of active warps per multiprocessor, which is implicitly determined by the execution parameters along with resource (register and shared memory) constraints. Choosing execution parameters is a matter of striking a balance between latency hiding (occupancy) and resource utilization.
 </p>
 <p>
  Choosing the execution configuration parameters should be done in tandem; however, there are certain heuristics that apply to each parameter individually. When choosing the first execution configuration parameter-the number of blocks per grid, or
  grid size
  - the primary concern is keeping the entire GPU busy. The number of blocks in a grid should be larger than the number of multiprocessors so that all multiprocessors have at least one block to execute. Furthermore, there should be multiple active blocks per multiprocessor so that blocks that arenât waiting for a
  <span class="pre">
   __syncthreads()
  </span>
  can keep the hardware busy. This recommendation is subject to resource availability; therefore, it should be determined in the context of the second execution parameter - the number of threads per block, or
  block size
  - as well as shared memory usage. To scale to future devices, the number of blocks per kernel launch should be in the thousands.
 </p>
 <p>
  When choosing the block size, it is important to remember that multiple concurrent blocks can reside on a multiprocessor, so occupancy is not determined by block size alone. In particular, a larger block size does not imply a higher occupancy.
 </p>
 <p>
  As mentioned in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy">
   Occupancy
  </a>
  , higher occupancy does not always equate to better performance. For example, improving occupancy from 66 percent to 100 percent generally does not translate to a similar increase in performance. A lower occupancy kernel will have more registers available per thread than a higher occupancy kernel, which may result in less register spilling to local memory; in particular, with a high degree of exposed instruction-level parallelism (ILP) it is, in some cases, possible to fully cover latency with a low occupancy.
 </p>
 <p>
  There are many such factors involved in selecting block size, and inevitably some experimentation is required. However, a few rules of thumb should be followed:
 </p>
 <ul class="simple">
  <li>
   <p>
    Threads per block should be a multiple of warp size to avoid wasting computation on under-populated warps and to facilitate coalescing.
   </p>
  </li>
  <li>
   <p>
    A minimum of 64 threads per block should be used, and only if there are multiple concurrent blocks per multiprocessor.
   </p>
  </li>
  <li>
   <p>
    Between 128 and 256 threads per block is a good initial range for experimentation with different block sizes.
   </p>
  </li>
  <li>
   <p>
    Use several smaller thread blocks rather than one large thread block per multiprocessor if latency affects performance. This is particularly beneficial to kernels that frequently call
    <span class="pre">
     __syncthreads()
    </span>
    .
   </p>
  </li>
 </ul>
 <p>
  Note that when a thread block allocates more registers than are available on a multiprocessor, the kernel launch fails, as it will when too much shared memory or too many threads are requested.
 </p>
 <h2>
  <span class="section-number">
   10.4.
  </span>
  Effects of Shared Memory
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#effects-of-shared-memory" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Shared memory can be helpful in several situations, such as helping to coalesce or eliminate redundant access to global memory. However, it also can act as a constraint on occupancy. In many cases, the amount of shared memory required by a kernel is related to the block size that was chosen, but the mapping of threads to shared memory elements does not need to be one-to-one. For example, it may be desirable to use a 64x64 element shared memory array in a kernel, but because the maximum number of threads per block is 1024, it is not possible to launch a kernel with 64x64 threads per block. In such cases, kernels with 32x32 or 64x16 threads can be launched with each thread processing four elements of the shared memory array. The approach of using a single thread to process multiple elements of a shared memory array can be beneficial even if limits such as threads per block are not an issue. This is because some operations common to each element can be performed by the thread once, amortizing the cost over the number of shared memory elements processed by the thread.
 </p>
 <p>
  A useful technique to determine the sensitivity of performance to occupancy is through experimentation with the amount of dynamically allocated shared memory, as specified in the third parameter of the execution configuration. By simply increasing this parameter (without modifying the kernel), it is possible to effectively reduce the occupancy of the kernel and measure its effect on performance.
 </p>
 <h2>
  <span class="section-number">
   10.5.
  </span>
  Concurrent Kernel Execution
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#concurrent-kernel-execution" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  As described in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#asynchronous-transfers-and-overlapping-transfers-with-computation">
   Asynchronous and Overlapping Transfers with Computation
  </a>
  , CUDA streams can be used to overlap kernel execution with data transfers. On devices that are capable of concurrent kernel execution, streams can also be used to execute multiple kernels simultaneously to more fully take advantage of the deviceâs multiprocessors. Whether a device has this capability is indicated by the
  <span class="pre">
   concurrentKernels
  </span>
  field of the
  <span class="pre">
   cudaDeviceProp
  </span>
  structure (or listed in the output of the
  <span class="pre">
   deviceQuery
  </span>
  CUDA Sample). Non-default streams (streams other than stream 0) are required for concurrent execution because kernel calls that use the default stream begin only after all preceding calls on the device (in any stream) have completed, and no operation on the device (in any stream) commences until they are finished.
 </p>
 <p>
  The following example illustrates the basic technique. Because
  <span class="pre">
   kernel1
  </span>
  and
  <span class="pre">
   kernel2
  </span>
  are executed in different, non-default streams, a capable device can execute the kernels at the same time.
 </p>
 <pre><span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream1</span><span class="p">);</span>
<span class="n">cudaStreamCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">stream2</span><span class="p">);</span>
<span class="n">kernel1</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="n">block</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">stream1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">data_1</span><span class="p">);</span>
<span class="n">kernel2</span><span class="o">&lt;&lt;&lt;</span><span class="n">grid</span><span class="p">,</span><span class="n">block</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">stream2</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">data_2</span><span class="p">);</span>
</pre>
 <h2>
  <span class="section-number">
   10.6.
  </span>
  Multiple contexts
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#multiple-contexts" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA work occurs within a process space for a particular GPU known as a
  context
  . The context encapsulates kernel launches and memory allocations for that GPU as well as supporting constructs such as the page tables. The context is explicit in the CUDA Driver API but is entirely implicit in the CUDA Runtime API, which creates and manages contexts automatically.
 </p>
 <p>
  With the CUDA Driver API, a CUDA application process can potentially create more than one context for a given GPU. If multiple CUDA application processes access the same GPU concurrently, this almost always implies multiple contexts, since a context is tied to a particular host process unless
  <a class="reference external" href="https://docs.nvidia.com/deploy/mps/index.html">
   Multi-Process Service
  </a>
  is in use.
 </p>
 <p>
  While multiple contexts (and their associated resources such as global memory allocations) can be allocated concurrently on a given GPU, only one of these contexts can execute work at any given moment on that GPU; contexts sharing the same GPU are time-sliced. Creating additional contexts incurs memory overhead for per-context data and time overhead for context switching. Furthermore, the need for context switching can reduce utilization when work from several contexts could otherwise execute concurrently (see also
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#concurrent-kernel-execution">
   Concurrent Kernel Execution
  </a>
  ).
 </p>
 <p>
  Therefore, it is best to avoid multiple contexts per GPU within the same CUDA application. To assist with this, the CUDA Driver API provides methods to access and manage a special context on each GPU called the
  primary context
  . These are the same contexts used implicitly by the CUDA Runtime when there is not already a current context for a thread.
 </p>
 <pre><span class="c1">// When initializing the program/library</span>
<span class="n">CUcontext</span><span class="n">ctx</span><span class="p">;</span>
<span class="n">cuDevicePrimaryCtxRetain</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">,</span><span class="n">dev</span><span class="p">);</span>

<span class="c1">// When the program/library launches work</span>
<span class="n">cuCtxPushCurrent</span><span class="p">(</span><span class="n">ctx</span><span class="p">);</span>
<span class="n">kernel</span><span class="o">&lt;&lt;&lt;</span><span class="p">...</span><span class="o">&gt;&gt;&gt;</span><span class="p">(...);</span>
<span class="n">cuCtxPopCurrent</span><span class="p">(</span><span class="o">&amp;</span><span class="n">ctx</span><span class="p">);</span>

<span class="c1">// When the program/library is finished with the context</span>
<span class="n">cuDevicePrimaryCtxRelease</span><span class="p">(</span><span class="n">dev</span><span class="p">);</span>
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  NVIDIA-SMI can be used to configure a GPU for
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-modes">
   exclusive process mode
  </a>
  , which limits the number of contexts per GPU to one. This context can be current to as many threads as desired within the creating process, and
  <span class="pre">
   cuDevicePrimaryCtxRetain
  </span>
  will fail if a non-primary context that was created with the CUDA driver API already exists on the device.
 </p>
 <h1>
  <span class="section-number">
   11.
  </span>
  Instruction Optimization
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#instruction-optimization" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  Awareness of how instructions are executed often permits low-level optimizations that can be useful, especially in code that is run frequently (the so-called hot spot in a program). Best practices suggest that this optimization be performed after all higher-level optimizations have been completed.
 </p>
 <h2>
  <span class="section-number">
   11.1.
  </span>
  Arithmetic Instructions
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#arithmetic-instructions" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Single-precision floats provide the best performance, and their use is highly encouraged. The throughput of individual arithmetic operations is detailed in the CUDA C++ Programming Guide.
 </p>
 <h3>
  <span class="section-number">
   11.1.1.
  </span>
  Division Modulo Operations
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#division-modulo-operations" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Low Priority:
  Use shift operations to avoid expensive division and modulo calculations.
 </p>
 <p>
  Integer division and modulo operations are particularly costly and should be avoided or replaced with bitwise operations whenever possible: If
  <span class="math notranslate nohighlight">
   \(n\)
  </span>
  is a power of 2, (
  <span class="math notranslate nohighlight">
   \(i/n\)
  </span>
  ) is equivalent to (
  <span class="math notranslate nohighlight">
   \(i \gg {log2}(n)\)
  </span>
  ) and (
  <span class="math notranslate nohighlight">
   \(i\% n\)
  </span>
  ) is equivalent to (
  <span class="math notranslate nohighlight">
   \(i\&amp;\left( {n - 1} \right)\)
  </span>
  ).
 </p>
 <p>
  The compiler will perform these conversions if n is literal. (For further information, refer to Performance Guidelines in the CUDA C++ Programming Guide).
 </p>
 <h3>
  <span class="section-number">
   11.1.2.
  </span>
  Loop Counters Signed vs. Unsigned
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#loop-counters-signed-vs-unsigned" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Low Medium Priority:
  Use signed integers rather than unsigned integers as loop counters.
 </p>
 <p>
  In the C language standard, unsigned integer overflow semantics are well defined, whereas signed integer overflow causes undefined results. Therefore, the compiler can optimize more aggressively with signed arithmetic than it can with unsigned arithmetic. This is of particular note with loop counters: since it is common for loop counters to have values that are always positive, it may be tempting to declare the counters as unsigned. For slightly better performance, however, they should instead be declared as signed.
 </p>
 <p>
  For example, consider the following code:
 </p>
 <pre><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">i</span><span class="o">&lt;</span><span class="n">n</span><span class="p">;</span><span class="n">i</span><span class="o">++</span><span class="p">)</span><span class="p">{</span>
<span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">in</span><span class="p">[</span><span class="n">offset</span><span class="o">+</span><span class="n">stride</span><span class="o">*</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
</pre>
 <p>
  Here, the sub-expression
  <span class="pre">
   stride*i
  </span>
  could overflow a 32-bit integer, so if
  <span class="pre">
   i
  </span>
  is declared as unsigned, the overflow semantics prevent the compiler from using some optimizations that might otherwise have applied, such as strength reduction. If instead
  <span class="pre">
   i
  </span>
  is declared as signed, where the overflow semantics are undefined, the compiler has more leeway to use these optimizations.
 </p>
 <h3>
  <span class="section-number">
   11.1.3.
  </span>
  Reciprocal Square Root
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#reciprocal-square-root" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The reciprocal square root should always be invoked explicitly as
  <span class="pre">
   rsqrtf()
  </span>
  for single precision and
  <span class="pre">
   rsqrt()
  </span>
  for double precision. The compiler optimizes
  <span class="pre">
   1.0f/sqrtf(x)
  </span>
  into
  <span class="pre">
   rsqrtf()
  </span>
  only when this does not violate IEEE-754 semantics.
 </p>
 <h3>
  <span class="section-number">
   11.1.4.
  </span>
  Other Arithmetic Instructions
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#other-arithmetic-instructions" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Low Priority:
  Avoid automatic conversion of doubles to floats.
 </p>
 <p>
  The compiler must on occasion insert conversion instructions, introducing additional execution cycles. This is the case for:
 </p>
 <ul class="simple">
  <li>
   <p>
    Functions operating on
    <span class="pre">
     char
    </span>
    or
    <span class="pre">
     short
    </span>
    whose operands generally need to be converted to an
    <span class="pre">
     int
    </span>
   </p>
  </li>
  <li>
   <p>
    Double-precision floating-point constants (defined without any type suffix) used as input to single-precision floating-point computations
   </p>
  </li>
 </ul>
 <p>
  The latter case can be avoided by using single-precision floating-point constants, defined with an
  <span class="pre">
   f
  </span>
  suffix such as
  <span class="pre">
   3.141592653589793f
  </span>
  ,
  <span class="pre">
   1.0f
  </span>
  ,
  <span class="pre">
   0.5f
  </span>
  .
 </p>
 <p>
  For single-precision code, use of the float type and the single-precision math functions are highly recommended.
 </p>
 <p>
  It should also be noted that the CUDA math libraryâs complementary error function,
  <span class="pre">
   erfcf()
  </span>
  , is particularly fast with full single-precision accuracy.
 </p>
 <h3>
  <span class="section-number">
   11.1.5.
  </span>
  Exponentiation With Small Fractional Arguments
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#exponentiation-with-small-fractional-arguments" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  For some fractional exponents, exponentiation can be accelerated significantly compared to the use of
  <span class="pre">
   pow()
  </span>
  by using square roots, cube roots, and their inverses. For those exponentiations where the exponent is not exactly representable as a floating-point number, such as 1/3, this can also provide much more accurate results, as use of
  <span class="pre">
   pow()
  </span>
  magnifies the initial representational error.
 </p>
 <p>
  The formulas in the table below are valid for
  <span class="pre">
   x
  </span>
  <span class="pre">
   &gt;=
  </span>
  <span class="pre">
   0,
  </span>
  <span class="pre">
   x
  </span>
  <span class="pre">
   !=
  </span>
  <span class="pre">
   -0
  </span>
  , that is,
  <span class="pre">
   signbit(x)
  </span>
  <span class="pre">
   ==
  </span>
  <span class="pre">
   0
  </span>
  .
 </p>
 <table class="table-no-stripes docutils align-default" id="id8">
  <span class="caption-text">
   Table 5. Formulae for exponentiation by small fractions
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#id8" title="Permalink to this table">
   ï
  </a>
  <tr class="row-odd">
   <th class="head">
    <p>
     Computation
    </p>
   </th>
   <th class="head">
    <p>
     Formula
    </p>
   </th>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     x
     1/9
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      rcbrt(rcbrt(x))
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     x
     -1/9
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      cbrt(rcbrt(x))
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     x
     1/6
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      rcbrt(rsqrt(x))
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     x
     -1/6
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      rcbrt(sqrt(x))
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     x
     1/4
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      rsqrt(rsqrt(x))
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     x
     -1/4
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      sqrt(rsqrt(x))
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     x
     1/3
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      cbrt(x)
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     x
     -1/3
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      rcbrt(x)
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     x
     1/2
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      sqrt(x)
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     x
     -1/2
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      rsqrt(x)
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     x
     2/3
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      cbrt(x);
     </span>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      r*r
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     x
     -2/3
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      rcbrt(x);
     </span>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      r*r
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     x
     3/4
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      sqrt(x);
     </span>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      r*sqrt(r)
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     x
     -3/4
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      rsqrt(x);
     </span>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      r*sqrt(r)
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     x
     7/6
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      x*rcbrt(rsqrt(x))
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     x
     -7/6
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      (1/x)
     </span>
     <span class="pre">
      *
     </span>
     <span class="pre">
      rcbrt(sqrt(x))
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     x
     5/4
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      x*rsqrt(rsqrt(x))
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     x
     -5/4
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      (1/x)*sqrt(rsqrt(x))
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     x
     4/3
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      x*cbrt(x)
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     x
     -4/3
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      (1/x)*rcbrt(x)
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-even">
   <td>
    <p>
     x
     3/2
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      x*sqrt(x)
     </span>
    </p>
   </td>
  </tr>
  <tr class="row-odd">
   <td>
    <p>
     x
     -3/2
    </p>
   </td>
   <td>
    <p>
     <span class="pre">
      r
     </span>
     <span class="pre">
      =
     </span>
     <span class="pre">
      (1/x)*rsqrt(x)
     </span>
    </p>
   </td>
  </tr>
 </table>
 <h3>
  <span class="section-number">
   11.1.6.
  </span>
  Math Libraries
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#math-libraries" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Medium Priority:
  Use the fast math library whenever speed trumps precision.
 </p>
 <p>
  Two types of runtime math operations are supported. They can be distinguished by their names: some have names with prepended underscores, whereas others do not (e.g.,
  <span class="pre">
   __functionName()
  </span>
  versus
  <span class="pre">
   functionName()
  </span>
  ). Functions following the
  <span class="pre">
   __functionName()
  </span>
  naming convention map directly to the hardware level. They are faster but provide somewhat lower accuracy (e.g.,
  <span class="pre">
   __sinf(x)
  </span>
  and
  <span class="pre">
   __expf(x)
  </span>
  ). Functions following
  <span class="pre">
   functionName()
  </span>
  naming convention are slower but have higher accuracy (e.g.,
  <span class="pre">
   sinf(x)
  </span>
  and
  <span class="pre">
   expf(x)
  </span>
  ). The throughput of
  <span class="pre">
   __sinf(x)
  </span>
  ,
  <span class="pre">
   __cosf(x)
  </span>
  , and
  <span class="pre">
   __expf(x)
  </span>
  is much greater than that of
  <span class="pre">
   sinf(x)
  </span>
  ,
  <span class="pre">
   cosf(x)
  </span>
  , and
  <span class="pre">
   expf(x)
  </span>
  . The latter become even more expensive (about an order of magnitude slower) if the magnitude of the argument
  <span class="pre">
   x
  </span>
  needs to be reduced. Moreover, in such cases, the argument-reduction code uses local memory, which can affect performance even more because of the high latency of local memory. More details are available in the
  CUDA C++ Programming Guide
  .
 </p>
 <p>
  Note also that whenever sine and cosine of the same argument are computed, the
  <span class="pre">
   sincos
  </span>
  family of instructions should be used to optimize performance:
 </p>
 <ul class="simple">
  <li>
   <p>
    <span class="pre">
     __sincosf()
    </span>
    for single-precision fast math (see next paragraph)
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     sincosf()
    </span>
    for regular single-precision
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     sincos()
    </span>
    for double precision
   </p>
  </li>
 </ul>
 <p>
  The
  <span class="pre">
   -use_fast_math
  </span>
  compiler option of
  <span class="pre">
   nvcc
  </span>
  coerces every
  <span class="pre">
   functionName()
  </span>
  call to the equivalent
  <span class="pre">
   __functionName()
  </span>
  call. It also disables single-precision denormal support and lowers the precision of single-precision division in general. This is an aggressive optimization that can both reduce numerical accuracy and alter special case handling. A more robust approach is to selectively introduce calls to fast intrinsic functions only if merited by performance gains and where altered behavior can be tolerated. Note this switch is effective only on single-precision floating point.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Medium Priority:
  Prefer faster, more specialized math functions over slower, more general ones when possible.
 </p>
 <p>
  For small integer powers (e.g.,
  x2
  or
  x3
  ), explicit multiplication is almost certainly faster than the use of general exponentiation routines such as
  <span class="pre">
   pow()
  </span>
  . While compiler optimization improvements continually seek to narrow this gap, explicit multiplication (or the use of an equivalent purpose-built inline function or macro) can have a significant advantage. This advantage is increased when several powers of the same base are needed (e.g., where both
  x2
  and
  x5
  are calculated in close proximity), as this aids the compiler in its common sub-expression elimination (CSE) optimization.
 </p>
 <p>
  For exponentiation using base 2 or 10, use the functions
  <span class="pre">
   exp2()
  </span>
  or
  <span class="pre">
   expf2()
  </span>
  and
  <span class="pre">
   exp10()
  </span>
  or
  <span class="pre">
   expf10()
  </span>
  rather than the functions
  <span class="pre">
   pow()
  </span>
  or
  <span class="pre">
   powf()
  </span>
  . Both
  <span class="pre">
   pow()
  </span>
  and
  <span class="pre">
   powf()
  </span>
  are heavy-weight functions in terms of register pressure and instruction count due to the numerous special cases arising in general exponentiation and the difficulty of achieving good accuracy across the entire ranges of the base and the exponent. The functions
  <span class="pre">
   exp2()
  </span>
  ,
  <span class="pre">
   exp2f()
  </span>
  ,
  <span class="pre">
   exp10()
  </span>
  , and
  <span class="pre">
   exp10f()
  </span>
  , on the other hand, are similar to
  <span class="pre">
   exp()
  </span>
  and
  <span class="pre">
   expf()
  </span>
  in terms of performance, and can be as much as ten times faster than their
  <span class="pre">
   pow()
  </span>
  /
  <span class="pre">
   powf()
  </span>
  equivalents.
 </p>
 <p>
  For exponentiation with an exponent of 1/3, use the
  <span class="pre">
   cbrt()
  </span>
  or
  <span class="pre">
   cbrtf()
  </span>
  function rather than the generic exponentiation functions
  <span class="pre">
   pow()
  </span>
  or
  <span class="pre">
   powf()
  </span>
  , as the former are significantly faster than the latter. Likewise, for exponentation with an exponent of -1/3, use
  <span class="pre">
   rcbrt()
  </span>
  or
  <span class="pre">
   rcbrtf()
  </span>
  .
 </p>
 <p>
  Replace
  <span class="pre">
   sin(Ï*&lt;expr&gt;)
  </span>
  with
  <span class="pre">
   sinpi(&lt;expr&gt;)
  </span>
  ,
  <span class="pre">
   cos(Ï*&lt;expr&gt;)
  </span>
  with
  <span class="pre">
   cospi(&lt;expr&gt;)
  </span>
  , and
  <span class="pre">
   sincos(Ï*&lt;expr&gt;)
  </span>
  with
  <span class="pre">
   sincospi(&lt;expr&gt;)
  </span>
  . This is advantageous with regard to both accuracy and performance. As a particular example, to evaluate the sine function in degrees instead of radians, use
  <span class="pre">
   sinpi(x/180.0)
  </span>
  . Similarly, the single-precision functions
  <span class="pre">
   sinpif()
  </span>
  ,
  <span class="pre">
   cospif()
  </span>
  , and
  <span class="pre">
   sincospif()
  </span>
  should replace calls to
  <span class="pre">
   sinf()
  </span>
  ,
  <span class="pre">
   cosf()
  </span>
  , and
  <span class="pre">
   sincosf()
  </span>
  when the function argument is of the form
  <span class="pre">
   Ï*&lt;expr&gt;
  </span>
  . (The performance advantage
  <span class="pre">
   sinpi()
  </span>
  has over
  <span class="pre">
   sin()
  </span>
  is due to simplified argument reduction; the accuracy advantage is because
  <span class="pre">
   sinpi()
  </span>
  multiplies by
  <span class="pre">
   Ï
  </span>
  only implicitly, effectively using an infinitely precise mathematical
  <span class="pre">
   Ï
  </span>
  rather than a single- or double-precision approximation thereof.)
 </p>
 <h3>
  <span class="section-number">
   11.1.7.
  </span>
  Precision-related Compiler Flags
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#precision-related-compiler-flags" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  By default, the
  <span class="pre">
   nvcc
  </span>
  compiler generates IEEE-compliant code, but it also provides options to generate code that somewhat less accurate but faster:
 </p>
 <ul class="simple">
  <li>
   <p>
    <span class="pre">
     -ftz=true
    </span>
    (denormalized numbers are flushed to zero)
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     -prec-div=false
    </span>
    (less precise division)
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     -prec-sqrt=false
    </span>
    (less precise square root)
   </p>
  </li>
 </ul>
 <p>
  Another, more aggressive, option is
  <span class="pre">
   -use_fast_math
  </span>
  , which coerces every
  <span class="pre">
   functionName()
  </span>
  call to the equivalent
  <span class="pre">
   __functionName()
  </span>
  call. This makes the code run faster at the cost of diminished precision and accuracy. See
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#math-libraries">
   Math Libraries
  </a>
  .
 </p>
 <h2>
  <span class="section-number">
   11.2.
  </span>
  Memory Instructions
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-instructions" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p class="admonition-title">
  Note
 </p>
 <p>
  High Priority:
  Minimize the use of global memory. Prefer shared memory access where possible.
 </p>
 <p>
  Memory instructions include any instruction that reads from or writes to shared, local, or global memory. When accessing uncached local or global memory, there are hundreds of clock cycles of memory latency.
 </p>
 <p>
  As an example, the assignment operator in the following sample code has a high throughput, but, crucially, there is a latency of hundreds of clock cycles to read data from global memory:
 </p>
 <pre><span class="n">__shared__</span><span class="kt">float</span><span class="n">shared</span><span class="p">[</span><span class="mi">32</span><span class="p">];</span>
<span class="n">__device__</span><span class="kt">float</span><span class="n">device</span><span class="p">[</span><span class="mi">32</span><span class="p">];</span>
<span class="n">shared</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">]</span><span class="o">=</span><span class="n">device</span><span class="p">[</span><span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">];</span>
</pre>
 <p>
  Much of this global memory latency can be hidden by the thread scheduler if there are sufficient independent arithmetic instructions that can be issued while waiting for the global memory access to complete. However, it is best to avoid accessing global memory whenever possible.
 </p>
 <h1>
  <span class="section-number">
   12.
  </span>
  Control Flow
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#control-flow" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   12.1.
  </span>
  Branching and Divergence
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#branching-and-divergence" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p class="admonition-title">
  Note
 </p>
 <p>
  High Priority:
  Avoid different execution paths within the same warp.
 </p>
 <p>
  Flow control instructions (
  <span class="pre">
   if
  </span>
  ,
  <span class="pre">
   switch
  </span>
  ,
  <span class="pre">
   do
  </span>
  ,
  <span class="pre">
   for
  </span>
  ,
  <span class="pre">
   while
  </span>
  ) can significantly affect the instruction throughput by causing threads of the same warp to diverge; that is, to follow different execution paths. If this happens, the different execution paths must be executed separately; this increases the total number of instructions executed for this warp.
 </p>
 <p>
  To obtain best performance in cases where the control flow depends on the thread ID, the controlling condition should be written so as to minimize the number of divergent warps.
 </p>
 <p>
  This is possible because the distribution of the warps across the block is deterministic as mentioned in SIMT Architecture of the CUDA C++ Programming Guide. A trivial example is when the controlling condition depends only on (
  <span class="pre">
   threadIdx
  </span>
  /
  <span class="pre">
   WSIZE
  </span>
  ) where
  <span class="pre">
   WSIZE
  </span>
  is the warp size.
 </p>
 <p>
  In this case, no warp diverges because the controlling condition is perfectly aligned with the warps.
 </p>
 <p>
  For branches including just a few instructions, warp divergence generally results in marginal performance losses. For example, the compiler may use predication to avoid an actual branch. Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads execute the instructions. Threads with a false predicate do not write results, and also do not evaluate addresses or read operands.
 </p>
 <p>
  Starting with the Volta architecture, Independent Thread Scheduling allows a warp to remain diverged outside of the data-dependent conditional block. An explicit
  <span class="pre">
   __syncwarp()
  </span>
  can be used to guarantee that the warp has reconverged for subsequent instructions.
 </p>
 <h2>
  <span class="section-number">
   12.2.
  </span>
  Branch Predication
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#branch-predication" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p class="admonition-title">
  Note
 </p>
 <p>
  Low Priority:
  Make it easy for the compiler to use branch predication in lieu of loops or control statements.
 </p>
 <p>
  Sometimes, the compiler may unroll loops or optimize out
  <span class="pre">
   if
  </span>
  or
  <span class="pre">
   switch
  </span>
  statements by using branch predication instead. In these cases, no warp can ever diverge. The programmer can also control loop unrolling using
 </p>
 <pre><span class="cp">#pragma unroll</span>
</pre>
 <p>
  For more information on this pragma, refer to the CUDA C++ Programming Guide.
 </p>
 <p>
  When using branch predication, none of the instructions whose execution depends on the controlling condition is skipped. Instead, each such instruction is associated with a per-thread condition code or predicate that is set to true or false according to the controlling condition. Although each of these instructions is scheduled for execution, only the instructions with a true predicate are actually executed. Instructions with a false predicate do not write results, and they also do not evaluate addresses or read operands.
 </p>
 <p>
  The compiler replaces a branch instruction with predicated instructions only if the number of instructions controlled by the branch condition is less than or equal to a certain threshold.
 </p>
 <h1>
  <span class="section-number">
   13.
  </span>
  Deploying CUDA Applications
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#deploying-cuda-applications" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  Having completed the GPU acceleration of one or more components of the application it is possible to compare the outcome with the original expectation. Recall that the initial
  assess
  step allowed the developer to determine an upper bound for the potential speedup attainable by accelerating given hotspots.
 </p>
 <p>
  Before tackling other hotspots to improve the total speedup, the developer should consider taking the partially parallelized implementation and carry it through to production. This is important for a number of reasons; for example, it allows the user to profit from their investment as early as possible (the speedup may be partial but is still valuable), and it minimizes risk for the developer and the user by providing an evolutionary rather than revolutionary set of changes to the application.
 </p>
 <h1>
  <span class="section-number">
   14.
  </span>
  Understanding the Programming Environment
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#understanding-the-programming-environment" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  With each generation of NVIDIA processors, new features are added to the GPU that CUDA can leverage. Consequently, itâs important to understand the characteristics of the architecture.
 </p>
 <p>
  Programmers should be aware of two version numbers. The first is the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
   compute capability
  </a>
  , and the second is the version number of the CUDA Runtime and CUDA Driver APIs.
 </p>
 <h2>
  <span class="section-number">
   14.1.
  </span>
  CUDA Compute Capability
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The
  compute capability
  describes the features of the hardware and reflects the set of instructions supported by the device as well as other specifications, such as the maximum number of threads per block and the number of registers per multiprocessor. Higher compute capability versions are supersets of lower (that is, earlier) versions, so they are backward compatible.
 </p>
 <p>
  The compute capability of the GPU in the device can be queried programmatically as illustrated in the
  <span class="pre">
   deviceQuery
  </span>
  CUDA Sample. The output for that program is shown in
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability__sample-cuda-configuration-data-reported-devicequery">
   Figure 16
  </a>
  . This information is obtained by calling
  <span class="pre">
   cudaGetDeviceProperties()
  </span>
  and accessing the information in the structure it returns.
 </p>
 <a class="reference internal image-reference" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/_images/sample-cuda-configuration-data.png">
 </a>
 <p>
  <span class="caption-text">
   Sample CUDA configuration data reported by deviceQuery
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability-sample-cuda-configuration-data-reported-devicequery" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  The major and minor revision numbers of the compute capability are shown on the seventh line of
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability__sample-cuda-configuration-data-reported-devicequery">
   Figure 16
  </a>
  . Device 0 of this system has compute capability 7.0.
 </p>
 <p>
  More details about the compute capabilities of various GPUs are in CUDA-Enabled GPUs and Compute Capabilities of the CUDA C++ Programming Guide. In particular, developers should note the number of multiprocessors on the device, the number of registers and the amount of memory available, and any special capabilities of the device.
 </p>
 <h2>
  <span class="section-number">
   14.2.
  </span>
  Additional Hardware Data
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#additional-hardware-data" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Certain hardware features are not described by the compute capability. For example, the ability to overlap kernel execution with asynchronous data transfers between the host and the device is available on most but not all GPUs irrespective of the compute capability. In such cases, call
  <span class="pre">
   cudaGetDeviceProperties()
  </span>
  to determine whether the device is capable of a certain feature. For example, the
  <span class="pre">
   asyncEngineCount
  </span>
  field of the device property structure indicates whether overlapping kernel execution and data transfers is possible (and, if so, how many concurrent transfers are possible); likewise, the
  <span class="pre">
   canMapHostMemory
  </span>
  field indicates whether zero-copy data transfers can be performed.
 </p>
 <h2>
  <span class="section-number">
   14.3.
  </span>
  Which Compute Capability Target
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#which-compute-capability-target" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  To target specific versions of NVIDIA hardware and CUDA software, use the
  <span class="pre">
   -arch
  </span>
  ,
  <span class="pre">
   -code
  </span>
  , and
  <span class="pre">
   -gencode
  </span>
  options of
  <span class="pre">
   nvcc
  </span>
  . Code that uses the warp shuffle operation, for example, must be compiled with
  <span class="pre">
   -arch=sm_30
  </span>
  (or higher compute capability).
 </p>
 <p>
  See
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#building-for-maximum-compatibility">
   Building for Maximum Compatibility
  </a>
  for further discussion of the flags used for building code for multiple generations of CUDA-capable device simultaneously.
 </p>
 <h2>
  <span class="section-number">
   14.4.
  </span>
  CUDA Runtime
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-runtime" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The host runtime component of the CUDA software environment can be used only by host functions. It provides functions to handle the following:
 </p>
 <ul class="simple">
  <li>
   <p>
    Device management
   </p>
  </li>
  <li>
   <p>
    Context management
   </p>
  </li>
  <li>
   <p>
    Memory management
   </p>
  </li>
  <li>
   <p>
    Code module management
   </p>
  </li>
  <li>
   <p>
    Execution control
   </p>
  </li>
  <li>
   <p>
    Texture reference management
   </p>
  </li>
  <li>
   <p>
    Interoperability with OpenGL and Direct3D
   </p>
  </li>
 </ul>
 <p>
  As compared to the lower-level CUDA Driver API, the CUDA Runtime greatly eases device management by providing implicit initialization, context management, and device code module management. The C++ host code generated by
  <span class="pre">
   nvcc
  </span>
  utilizes the CUDA Runtime, so applications that link to this code will depend on the CUDA Runtime; similarly, any code that uses the
  <span class="pre">
   cuBLAS
  </span>
  ,
  <span class="pre">
   cuFFT
  </span>
  , and other CUDA Toolkit libraries will also depend on the CUDA Runtime, which is used internally by these libraries.
 </p>
 <p>
  The functions that make up the CUDA Runtime API are explained in the CUDA Toolkit Reference Manual.
 </p>
 <p>
  The CUDA Runtime handles kernel loading and setting up kernel parameters and launch configuration before the kernel is launched. The implicit driver version checking, code initialization, CUDA context management, CUDA module management (cubin to function mapping), kernel configuration, and parameter passing are all performed by the CUDA Runtime.
 </p>
 <p>
  It comprises two principal parts:
 </p>
 <ul class="simple">
  <li>
   <p>
    A C-style function interface (
    <span class="pre">
     cuda_runtime_api.h
    </span>
    ).
   </p>
  </li>
  <li>
   <p>
    C++-style convenience wrappers (
    <span class="pre">
     cuda_runtime.h
    </span>
    ) built on top of the C-style functions.
   </p>
  </li>
 </ul>
 <p>
  For more information on the Runtime API, refer to CUDA Runtime of the CUDA C++ Programming Guide.
 </p>
 <h1>
  <span class="section-number">
   15.
  </span>
  CUDA Compatibility Developerâs Guide
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-developer-s-guide" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  CUDA Toolkit is released on a monthly release cadence to deliver new features, performance improvements, and critical bug fixes. CUDA compatibility allows users to update the latest CUDA Toolkit software (including the compiler, libraries, and tools) without requiring update to the entire driver stack.
 </p>
 <p>
  The CUDA software environment consists of three parts:
 </p>
 <ul class="simple">
  <li>
   <p>
    CUDA Toolkit (libraries, CUDA runtime and developer tools) - SDK for developers to build CUDA applications.
   </p>
  </li>
  <li>
   <p>
    CUDA driver - User-mode driver component used to run CUDA applications (e.g. libcuda.so on Linux systems).
   </p>
  </li>
  <li>
   <p>
    NVIDIA GPU device driver - Kernel-mode driver component for NVIDIA GPUs.
   </p>
  </li>
 </ul>
 <p>
  On Linux systems, the CUDA driver and kernel mode components are delivered together in the NVIDIA display driver package. This is shown in Figure 1.
 </p>
 <p>
  <span class="caption-text">
   Components of CUDA
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#id9" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  The CUDA compiler (nvcc), provides a way to handle CUDA and non-CUDA code (by splitting and steering compilation), along with the CUDA runtime, is part of the CUDA compiler toolchain. The CUDA Runtime API provides developers with high-level C++ interface for simplified management of devices, kernel executions etc., While the CUDA driver API provides (
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/index.html">
   CUDA Driver API
  </a>
  ) a low-level programming interface for applications to target NVIDIA hardware.
 </p>
 <p>
  Built on top of these technologies are CUDA libraries, some of which are included in the CUDA Toolkit, while others such as cuDNN may be released independently of the CUDA Toolkit.
 </p>
 <h2>
  <span class="section-number">
   15.1.
  </span>
  CUDA Toolkit Versioning
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-toolkit-versioning" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Starting with CUDA 11, the toolkit versions are based on an industry-standard semantic versioning scheme: .X.Y.Z, where:
 </p>
 <ul class="simple">
  <li>
   <p>
    .X stands for the major version - APIs have changed and binary compatibility is broken.
   </p>
  </li>
  <li>
   <p>
    .Y stands for the minor version - Introduction of new APIs, deprecation of old APIs, and source compatibility might be broken but binary compatibility is maintained.
   </p>
  </li>
  <li>
   <p>
    .Z stands for the release/patch version - new updates and patches will increment this.
   </p>
  </li>
 </ul>
 <p>
  Each component in the toolkit is recommended to be semantically versioned. From CUDA 11.3 NVRTC is also semantically versioned. We will note some of them later on in the document. The versions of the components in the toolkit are available in this
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-major-component-versions">
   table
  </a>
  .
 </p>
 <p>
  Compatibility of the CUDA platform is thus intended to address a few scenarios:
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    NVIDIA driver upgrades to systems with GPUs running in production for enterprises or datacenters can be complex and may need advance planning. Delays in rolling out new NVIDIA drivers could mean that users of such systems may not have access to new features available in CUDA releases. Not requiring driver updates for new CUDA releases can mean that new versions of the software can be made available faster to users.
   </p>
  </li>
  <li>
   <p>
    Many software libraries and applications built on top of CUDA (e.g. math libraries or deep learning frameworks) do not have a direct dependency on the CUDA runtime, compiler or driver. In such cases, users or developers can still benefit from not having to upgrade the entire CUDA Toolkit or driver to use these libraries or frameworks.
   </p>
  </li>
  <li>
   <p>
    Upgrading dependencies is error-prone and time consuming, and in some corner cases, can even change the semantics of a program. Constantly recompiling with the latest CUDA Toolkit means forcing upgrades on the end-customers of an application product. Package managers facilitate this process but unexpected issues can still arise and if a bug is found, it necessitates a repeat of the above upgrade process.
   </p>
  </li>
 </ol>
 <p>
  CUDA supports several compatibility choices:
 </p>
 <ol class="arabic simple">
  <li>
   <p>
    First introduced in CUDA 10, the
    CUDA Forward Compatible Upgrade
    is designed to allow users to get access to new CUDA features and run applications built with new CUDA releases on systems with older installations of the NVIDIA datacenter driver.
   </p>
  </li>
  <li>
   <p>
    First introduced in CUDA 11.1,
    CUDA Enhanced Compatibility
    provides two benefits:
   </p>
   <ul class="simple">
    <li>
     <p>
      By leveraging semantic versioning across components in the CUDA Toolkit, an application can be built for one CUDA minor release (for example 11.1) and work across all future minor releases within the major family (i.e. 11.x).
     </p>
    </li>
    <li>
     <p>
      The CUDA runtime has relaxed the minimum driver version check and thus no longer requires a driver upgrade when moving to a new minor release.
     </p>
    </li>
   </ul>
  </li>
  <li>
   <p>
    The CUDA driver ensures backward Binary Compatibility is maintained for compiled CUDA applications. Applications compiled with CUDA toolkit versions as old as 3.2 will run on newer drivers.
   </p>
  </li>
 </ol>
 <h2>
  <span class="section-number">
   15.2.
  </span>
  Source Compatibility
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#source-compatibility" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  We define source compatibility as a set of guarantees provided by the library, where a well-formed application built against a specific version of the library (using the SDK) will continue to build and run without errors when a newer version of the SDK is installed.
 </p>
 <p>
  Both the CUDA driver and the CUDA runtime are not source compatible across the different SDK releases. APIs can be deprecated and removed. Therefore, an application that compiled successfully on an older version of the toolkit may require changes in order to compile against a newer version of the toolkit.
 </p>
 <p>
  Developers are notified through deprecation and documentation mechanisms of any current or upcoming changes. This does not mean that application binaries compiled using an older toolkit will not be supported anymore. Application binaries rely on CUDA Driver API interface and even though the CUDA Driver API itself may also have changed across toolkit versions, CUDA guarantees Binary Compatibility of the CUDA Driver API interface.
 </p>
 <h2>
  <span class="section-number">
   15.3.
  </span>
  Binary Compatibility
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#binary-compatibility" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  We define binary compatibility as a set of guarantees provided by the library, where an application targeting the said library will continue to work when dynamically linked against a different version of the library.
 </p>
 <p>
  The CUDA Driver API has a versioned C-style ABI, which guarantees that applications that were running against an older driver (for example CUDA 3.2) will still run and function correctly against a modern driver (for example one shipped with CUDA 11.0). This means that even though an application source might need to be changed if it has to be recompiled against a newer CUDA Toolkit in order to use the newer features, replacing the driver components installed in a system with a newer version will always support existing applications and its functions.
 </p>
 <p>
  The CUDA Driver API thus is binary-compatible (the OS loader can pick up a newer version and the application continues to work) but not source-compatible (rebuilding your application against a newer SDK might require source changes).
 </p>
 <p>
  <span class="caption-text">
   CUDA Toolkit and Minimum Driver Versions
  </span>
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#id10" title="Permalink to this image">
   ï
  </a>
 </p>
 <p>
  Before we proceed further on this topic, itâs important for developers to understand the concept of Minimum Driver Version and how that may affect them.
 </p>
 <p>
  Each version of the CUDA Toolkit (and runtime) requires a minimum version of the NVIDIA driver. Applications compiled against a CUDA Toolkit version will only run on systems with the specified minimum driver version for that toolkit version. Prior to CUDA 11.0, the minimum driver version for a toolkit was the same as the driver shipped with that version of the CUDA Toolkit.
 </p>
 <p>
  So, when an application is built with CUDA 11.0, it can only run on a system with an R450 or later driver. If such an application is run on a system with the R418 driver installed, CUDA initialization will return an error as can be seen in the example below.
 </p>
 <p>
  In this example, the deviceQuery sample is compiled with CUDA 11.1 and is run on a system with R418. In this scenario, CUDA initialization returns an error due to the minimum driver requirement.
 </p>
 <pre>ubuntu@:~/samples/1_Utilities/deviceQuery
$ make
/usr/local/cuda-11.1/bin/nvcc -ccbin g++ -I../../common/inc  -m64    -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery.o -c deviceQuery.cpp

/usr/local/cuda-11.1/bin/nvcc -ccbin g++   -m64      -gencode arch=compute_35,code=sm_35 -gencode arch=compute_37,code=sm_37 -gencode arch=compute_50,code=sm_50 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61 -gencode arch=compute_70,code=sm_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -o deviceQuery deviceQuery.o

$ nvidia-smi

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.165.02   Driver Version: 418.165.02   CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |
| N/A   42C    P0    28W /  70W |      0MiB / 15079MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+


$ samples/bin/x86_64/linux/release/deviceQuery
samples/bin/x86_64/linux/release/deviceQuery Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

cudaGetDeviceCount returned 3
-&gt; initialization error
Result = FAIL
</pre>
 <p>
  Refer to the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">
   CUDA Toolkit Release Notes
  </a>
  for details for the minimum driver version and the version of the driver shipped with the toolkit.
 </p>
 <h3>
  <span class="section-number">
   15.3.1.
  </span>
  CUDA Binary (cubin) Compatibility
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-binary-cubin-compatibility" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  A slightly related but important topic is one of application binary compatibility across GPU architectures in CUDA.
 </p>
 <p>
  CUDA C++ provides a simple path for users familiar with the C++ programming language to easily write programs for execution by the device. Kernels can be written using the CUDA instruction set architecture, called PTX, which is described in the PTX reference manual. It is however usually more effective to use a high-level programming language such as C++. In both cases, kernels must be compiled into binary code by nvcc (called cubins) to execute on the device.
 </p>
 <p>
  The cubins are architecture-specific. Binary compatibility for cubins is guaranteed from one compute capability minor revision to the next one, but not from one compute capability minor revision to the previous one or across major compute capability revisions. In other words, a cubin object generated for compute capability
  X.y
  will only execute on devices of compute capability
  X.z
  where
  zâ¥y
  .
 </p>
 <p>
  To execute code on devices of specific compute capability, an application must load binary or PTX code that is compatible with this compute capability. For portability, that is, to be able to execute code on future GPU architectures with higher compute capability (for which no binary code can be generated yet), an application must load PTX code that will be just-in-time compiled by the NVIDIA driver for these future devices.
 </p>
 <p>
  More information on cubins, PTX and application compatibility can be found in the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#binary-compatibility">
   CUDA C++ Programming Guide
  </a>
  .
 </p>
 <h2>
  <span class="section-number">
   15.4.
  </span>
  CUDA Compatibility Across Minor Releases
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compatibility-across-minor-releases" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  By leveraging the semantic versioning, starting with CUDA 11, components in the CUDA Toolkit will remain binary compatible across the minor versions of the toolkit. In order to maintain binary compatibility across minor versions, the CUDA runtime no longer bumps up the minimum driver version required for every minor release - this only happens when a major release is shipped.
 </p>
 <p>
  One of the main reasons a new toolchain requires a new minimum driver is to handle the JIT compilation of PTX code and the JIT linking of binary code.
 </p>
 <p>
  In this section, we will review the usage patterns that may require new user workflows when taking advantage of the compatibility features of the CUDA platform.
 </p>
 <h3>
  <span class="section-number">
   15.4.1.
  </span>
  Existing CUDA Applications within Minor Versions of CUDA
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#existing-cuda-applications-within-minor-versions-of-cuda" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <pre><span class="n">$</span><span class="n">nvidia</span><span class="o">-</span><span class="n">smi</span>

<span class="o">+-----------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">NVIDIA</span><span class="o">-</span><span class="n">SMI</span><span class="mf">450.80.02</span><span class="n">Driver</span><span class="n">Version</span><span class="o">:</span><span class="mf">450.80.02</span><span class="n">CUDA</span><span class="n">Version</span><span class="o">:</span><span class="mf">11.0</span><span class="o">|</span>
<span class="o">|-------------------------------+----------------------+----------------------+</span>
<span class="o">|</span><span class="n">GPU</span><span class="n">Name</span><span class="n">Persistence</span><span class="o">-</span><span class="n">M</span><span class="o">|</span><span class="n">Bus</span><span class="o">-</span><span class="n">Id</span><span class="n">Disp</span><span class="p">.</span><span class="n">A</span><span class="o">|</span><span class="n">Volatile</span><span class="n">Uncorr</span><span class="p">.</span><span class="n">ECC</span><span class="o">|</span>
<span class="o">|</span><span class="n">Fan</span><span class="n">Temp</span><span class="n">Perf</span><span class="n">Pwr</span><span class="o">:</span><span class="n">Usage</span><span class="o">/</span><span class="n">Cap</span><span class="o">|</span><span class="n">Memory</span><span class="o">-</span><span class="n">Usage</span><span class="o">|</span><span class="n">GPU</span><span class="o">-</span><span class="n">Util</span><span class="n">Compute</span><span class="n">M</span><span class="p">.</span><span class="o">|</span>
<span class="o">|</span><span class="o">|</span><span class="o">|</span><span class="n">MIG</span><span class="n">M</span><span class="p">.</span><span class="o">|</span>
<span class="o">|===============================+======================+======================|</span>
<span class="o">|</span><span class="mi">0</span><span class="n">Tesla</span><span class="n">T4</span><span class="n">On</span><span class="o">|</span><span class="mo">00000000</span><span class="o">:</span><span class="mo">00</span><span class="o">:</span><span class="mi">1</span><span class="n">E</span><span class="mf">.0</span><span class="n">Off</span><span class="o">|</span><span class="mi">0</span><span class="o">|</span>
<span class="o">|</span><span class="n">N</span><span class="o">/</span><span class="n">A</span><span class="mi">39</span><span class="n">C</span><span class="n">P8</span><span class="mi">9</span><span class="n">W</span><span class="o">/</span><span class="mi">70</span><span class="n">W</span><span class="o">|</span><span class="mi">0</span><span class="n">MiB</span><span class="o">/</span><span class="mi">15109</span><span class="n">MiB</span><span class="o">|</span><span class="mi">0</span><span class="o">%</span><span class="n">Default</span><span class="o">|</span>
<span class="o">|</span><span class="o">|</span><span class="o">|</span><span class="n">N</span><span class="o">/</span><span class="n">A</span><span class="o">|</span>
<span class="o">+-------------------------------+----------------------+----------------------+</span>

<span class="o">+-----------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">Processes</span><span class="o">:</span><span class="o">|</span>
<span class="o">|</span><span class="n">GPU</span><span class="n">GI</span><span class="n">CI</span><span class="n">PID</span><span class="n">Type</span><span class="n">Process</span><span class="n">name</span><span class="n">GPU</span><span class="n">Memory</span><span class="o">|</span>
<span class="o">|</span><span class="n">ID</span><span class="n">ID</span><span class="n">Usage</span><span class="o">|</span>
<span class="o">|=============================================================================|</span>
<span class="o">|</span><span class="n">No</span><span class="n">running</span><span class="n">processes</span><span class="n">found</span><span class="o">|</span>
<span class="o">+-----------------------------------------------------------------------------+</span>
</pre>
 <p>
  When our CUDA 11.1 application (i.e. cudart 11.1 is statically linked) is run on the system, we see that it runs successfully even when the driver reports a 11.0 version - that is, without requiring the driver or other toolkit components to be updated on the system.
 </p>
 <pre><span class="n">$</span><span class="n">samples</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">x86_64</span><span class="o">/</span><span class="n">linux</span><span class="o">/</span><span class="n">release</span><span class="o">/</span><span class="n">deviceQuery</span>
<span class="n">samples</span><span class="o">/</span><span class="n">bin</span><span class="o">/</span><span class="n">x86_64</span><span class="o">/</span><span class="n">linux</span><span class="o">/</span><span class="n">release</span><span class="o">/</span><span class="n">deviceQuery</span><span class="n">Starting</span><span class="p">...</span>

<span class="n">CUDA</span><span class="n">Device</span><span class="n">Query</span><span class="p">(</span><span class="n">Runtime</span><span class="n">API</span><span class="p">)</span><span class="n">version</span><span class="p">(</span><span class="n">CUDART</span><span class="k">static</span><span class="n">linking</span><span class="p">)</span>

<span class="n">Detected</span><span class="mi">1</span><span class="n">CUDA</span><span class="n">Capable</span><span class="n">device</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

<span class="n">Device</span><span class="mi">0</span><span class="o">:</span><span class="s">"Tesla T4"</span>
<span class="n">CUDA</span><span class="n">Driver</span><span class="n">Version</span><span class="o">/</span><span class="n">Runtime</span><span class="n">Version</span><span class="mf">11.0</span><span class="o">/</span><span class="mf">11.1</span>
<span class="n">CUDA</span><span class="n">Capability</span><span class="n">Major</span><span class="o">/</span><span class="n">Minor</span><span class="n">version</span><span class="n">number</span><span class="o">:</span><span class="mf">7.5</span>

<span class="p">...</span><span class="o">&lt;</span><span class="n">snip</span><span class="o">&gt;</span><span class="p">...</span>

<span class="n">deviceQuery</span><span class="p">,</span><span class="n">CUDA</span><span class="n">Driver</span><span class="o">=</span><span class="n">CUDART</span><span class="p">,</span><span class="n">CUDA</span><span class="n">Driver</span><span class="n">Version</span><span class="o">=</span><span class="mf">11.0</span><span class="p">,</span><span class="n">CUDA</span><span class="n">Runtime</span><span class="n">Version</span><span class="o">=</span><span class="mf">11.1</span><span class="p">,</span><span class="n">NumDevs</span><span class="o">=</span><span class="mi">1</span>
<span class="n">Result</span><span class="o">=</span><span class="n">PASS</span>
</pre>
 <p>
  By using new CUDA versions, users can benefit from new CUDA programming model APIs, compiler optimizations and math library features.
 </p>
 <p>
  The following sections discuss some caveats and considerations.
 </p>
 <h4>
  <span class="section-number">
   15.4.1.1.
  </span>
  Handling New CUDA Features and Driver APIs
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#handling-new-cuda-features-and-driver-apis" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  A subset of CUDA APIs donât need a new driver and they can all be used without any driver dependencies. For example,
  <span class="pre">
   cuMemMap
  </span>
  APIs or any of APIs introduced prior to CUDA 11.0, such as
  <span class="pre">
   cudaDeviceSynchronize
  </span>
  , do not require a driver upgrade. To use other CUDA APIs introduced in a minor release (that require a new driver), one would have to implement fallbacks or fail gracefully. This situation is not different from what is available today where developers use macros to compile out features based on CUDA versions. Users should refer to the CUDA headers and documentation for new CUDA APIs introduced in a release.
 </p>
 <p>
  When working with a feature exposed in a minor version of the toolkit, the feature might not be available at runtime if the application is running against an older CUDA driver. Users wishing to take advantage of such a feature should query its availability with a dynamic check in the code:
 </p>
 <pre><span class="k">static</span><span class="kt">bool</span><span class="n">hostRegisterFeatureSupported</span><span class="o">=</span><span class="nb">false</span><span class="p">;</span>
<span class="k">static</span><span class="kt">bool</span><span class="n">hostRegisterIsDeviceAddress</span><span class="o">=</span><span class="nb">false</span><span class="p">;</span>

<span class="k">static</span><span class="n">error_t</span><span class="nf">cuFooFunction</span><span class="p">(</span><span class="kt">int</span><span class="o">*</span><span class="n">ptr</span><span class="p">)</span>
<span class="p">{</span>
<span class="kt">int</span><span class="o">*</span><span class="n">dptr</span><span class="o">=</span><span class="n">null</span><span class="p">;</span>
<span class="k">if</span><span class="p">(</span><span class="n">hostRegisterFeatureSupported</span><span class="p">)</span><span class="p">{</span>
<span class="n">cudaHostRegister</span><span class="p">(</span><span class="n">ptr</span><span class="p">,</span><span class="n">size</span><span class="p">,</span><span class="n">flags</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">hostRegisterIsDeviceAddress</span><span class="p">)</span><span class="p">{</span>
<span class="n">qptr</span><span class="o">=</span><span class="n">ptr</span><span class="p">;</span>
<span class="p">}</span>
<span class="k">else</span><span class="p">{</span>
<span class="n">cudaHostGetDevicePointer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">qptr</span><span class="p">,</span><span class="n">ptr</span><span class="p">,</span><span class="mi">0</span><span class="p">);</span>
<span class="p">}</span>
<span class="p">}</span>
<span class="k">else</span><span class="p">{</span>
<span class="c1">// cudaMalloc();</span>
<span class="c1">// cudaMemcpy();</span>
<span class="p">}</span>
<span class="n">gemm</span><span class="o">&lt;&lt;&lt;</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="o">&gt;&gt;&gt;</span><span class="p">(</span><span class="n">dptr</span><span class="p">);</span>
<span class="n">cudaDeviceSynchronize</span><span class="p">();</span>
<span class="p">}</span>

<span class="kt">int</span><span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
<span class="c1">// rest of code here</span>
<span class="n">cudaDeviceGetAttribute</span><span class="p">(</span>
<span class="o">&amp;</span><span class="n">hostRegisterFeatureSupported</span><span class="p">,</span>
<span class="n">cudaDevAttrHostRegisterSupported</span><span class="p">,</span>
<span class="mi">0</span><span class="p">);</span>
<span class="n">cudaDeviceGetAttribute</span><span class="p">(</span>
<span class="o">&amp;</span><span class="n">hostRegisterIsDeviceAddress</span><span class="p">,</span>
<span class="n">cudaDevAttrCanUseHostPointerForRegisteredMem</span><span class="p">,</span>
<span class="mi">0</span><span class="p">);</span>
<span class="n">cuFooFunction</span><span class="p">(</span><span class="cm">/* malloced pointer */</span><span class="p">);</span>
<span class="p">}</span>
</pre>
 <p>
  Alternatively the applicationâs interface might not work at all without a new CUDA driver and then its best to return an error right away:
 </p>
 <pre><span class="cp">#define MIN_VERSION 11010</span>
<span class="n">cudaError_t</span><span class="nf">foo</span><span class="p">()</span>
<span class="p">{</span>
<span class="kt">int</span><span class="n">version</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>
<span class="n">cudaGetDriverVersion</span><span class="p">(</span><span class="o">&amp;</span><span class="n">version</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">version</span><span class="o">&lt;</span><span class="n">MIN_VERSION</span><span class="p">)</span><span class="p">{</span>
<span class="k">return</span><span class="n">CUDA_ERROR_INSUFFICIENT_DRIVER</span><span class="p">;</span>
<span class="p">}</span>
<span class="c1">// proceed as normal</span>
<span class="p">}</span>
</pre>
 <p>
  A new error code is added to indicate that the functionality is missing from the driver you are running against:
  <span class="pre">
   cudaErrorCallRequiresNewerDriver
  </span>
  .
 </p>
 <h4>
  <span class="section-number">
   15.4.1.2.
  </span>
  Using PTX
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#using-ptx" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  PTX defines a virtual machine and ISA for general purpose parallel thread execution. PTX programs are translated at load time to the target hardware instruction set via the JIT Compiler which is part of the CUDA driver. As PTX is compiled by the CUDA driver, new toolchains will generate PTX that is not compatible with the older CUDA driver. This is not a problem when PTX is used for future device compatibility (the most common case), but can lead to issues when used for runtime compilation.
 </p>
 <p>
  For codes continuing to make use of PTX, in order to support compiling on an older driver, your code must be first transformed into device code via the static ptxjitcompiler library or NVRTC with the option of generating code for a specific architecture (e.g. sm_80) rather than a virtual architecture (e.g. compute_80). For this workflow, a new nvptxcompiler_static library is shipped with the CUDA Toolkit.
 </p>
 <p>
  We can see this usage in the following example:
 </p>
 <pre><span class="kt">char</span><span class="o">*</span><span class="nf">compilePTXToNVElf</span><span class="p">()</span>
<span class="p">{</span>
<span class="n">nvPTXCompilerHandle</span><span class="n">compiler</span><span class="o">=</span><span class="nb">NULL</span><span class="p">;</span>
<span class="n">nvPTXCompileResult</span><span class="n">status</span><span class="p">;</span>

<span class="kt">size_t</span><span class="n">elfSize</span><span class="p">,</span><span class="n">infoSize</span><span class="p">,</span><span class="n">errorSize</span><span class="p">;</span>
<span class="kt">char</span><span class="o">*</span><span class="n">elf</span><span class="p">,</span><span class="o">*</span><span class="n">infoLog</span><span class="p">,</span><span class="o">*</span><span class="n">errorLog</span><span class="p">;</span>
<span class="kt">int</span><span class="n">minorVer</span><span class="p">,</span><span class="n">majorVer</span><span class="p">;</span>

<span class="k">const</span><span class="kt">char</span><span class="o">*</span><span class="n">compile_options</span><span class="p">[]</span><span class="o">=</span><span class="p">{</span><span class="s">"--gpu-name=sm_80"</span><span class="p">,</span>
<span class="s">"--device-debug"</span>
<span class="p">};</span>

<span class="n">nvPTXCompilerGetVersion</span><span class="p">(</span><span class="o">&amp;</span><span class="n">majorVer</span><span class="p">,</span><span class="o">&amp;</span><span class="n">minorVer</span><span class="p">);</span>
<span class="n">nvPTXCompilerCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">compiler</span><span class="p">,</span><span class="p">(</span><span class="kt">size_t</span><span class="p">)</span><span class="n">strlen</span><span class="p">(</span><span class="n">ptxCode</span><span class="p">),</span><span class="n">ptxCode</span><span class="p">);</span>
<span class="n">status</span><span class="o">=</span><span class="n">nvPTXCompilerCompile</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">compile_options</span><span class="p">);</span>
<span class="k">if</span><span class="p">(</span><span class="n">status</span><span class="o">!=</span><span class="n">NVPTXCOMPILE_SUCCESS</span><span class="p">)</span><span class="p">{</span>
<span class="n">nvPTXCompilerGetErrorLogSize</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">errorSize</span><span class="p">);</span>

<span class="k">if</span><span class="p">(</span><span class="n">errorSize</span><span class="o">!=</span><span class="mi">0</span><span class="p">)</span><span class="p">{</span>
<span class="n">errorLog</span><span class="o">=</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">errorSize</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span>
<span class="n">nvPTXCompilerGetErrorLog</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">errorLog</span><span class="p">);</span>
<span class="n">printf</span><span class="p">(</span><span class="s">"Error log: %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">errorLog</span><span class="p">);</span>
<span class="n">free</span><span class="p">(</span><span class="n">errorLog</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">nvPTXCompilerGetCompiledProgramSize</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="o">&amp;</span><span class="n">elfSize</span><span class="p">));</span>
<span class="n">elf</span><span class="o">=</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">elfSize</span><span class="p">);</span>
<span class="n">nvPTXCompilerGetCompiledProgram</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">elf</span><span class="p">);</span>
<span class="n">nvPTXCompilerGetInfoLogSize</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="o">&amp;</span><span class="n">infoSize</span><span class="p">);</span>

<span class="k">if</span><span class="p">(</span><span class="n">infoSize</span><span class="o">!=</span><span class="mi">0</span><span class="p">)</span><span class="p">{</span>
<span class="n">infoLog</span><span class="o">=</span><span class="p">(</span><span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">malloc</span><span class="p">(</span><span class="n">infoSize</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span>
<span class="n">nvPTXCompilerGetInfoLog</span><span class="p">(</span><span class="n">compiler</span><span class="p">,</span><span class="p">(</span><span class="kt">void</span><span class="o">*</span><span class="p">)</span><span class="n">infoLog</span><span class="p">);</span>
<span class="n">printf</span><span class="p">(</span><span class="s">"Info log: %s</span><span class="se">\n</span><span class="s">"</span><span class="p">,</span><span class="n">infoLog</span><span class="p">);</span>
<span class="n">free</span><span class="p">(</span><span class="n">infoLog</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">nvPTXCompilerDestroy</span><span class="p">(</span><span class="o">&amp;</span><span class="n">compiler</span><span class="p">);</span>
<span class="k">return</span><span class="n">elf</span><span class="p">;</span>
<span class="p">}</span>
</pre>
 <h4>
  <span class="section-number">
   15.4.1.3.
  </span>
  Dynamic Code Generation
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#dynamic-code-generation" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  NVRTC is a runtime compilation library for CUDA C++. It accepts CUDA C++ source code in character string form and creates handles that can be used to obtain the PTX. The PTX string generated by NVRTC can be loaded by cuModuleLoadData and cuModuleLoadDataEx.
 </p>
 <p>
  Dealing with relocatable objects is not yet supported, therefore the
  <span class="pre">
   cuLink
  </span>
  * set of APIs in the CUDA driver will not work with enhanced compatibility. An upgraded driver matching the CUDA runtime version is currently required for those APIs.
 </p>
 <p>
  As mentioned in the PTX section, the compilation of PTX to device code lives along with the CUDA driver, hence the generated PTX might be newer than what is supported by the driver on the deployment system. When using NVRTC, it is recommended that the resulting PTX code is first transformed to the final device code via the steps outlined by the PTX user workflow. This ensures your code is compatible. Alternatively, NVRTC can generate cubins directly starting with CUDA 11.1. Applications using the new API can load the final device code directly using driver APIs
  <span class="pre">
   cuModuleLoadData
  </span>
  and
  <span class="pre">
   cuModuleLoadDataEx
  </span>
  .
 </p>
 <p>
  NVRTC used to support only virtual architectures through the option -arch, since it was only emitting PTX. It will now support actual architectures as well to emit SASS. The interface is augmented to retrieve either the PTX or cubin if an actual architecture is specified.
 </p>
 <p>
  The example below shows how an existing example can be adapted to use the new features, guarded by the
  <span class="pre">
   USE_CUBIN
  </span>
  macro in this case:
 </p>
 <pre><span class="cp">#include</span><span class="cpf">&lt;nvrtc.h&gt;</span>
<span class="cp">#include</span><span class="cpf">&lt;cuda.h&gt;</span>
<span class="cp">#include</span><span class="cpf">&lt;iostream&gt;</span>

<span class="kt">void</span><span class="nf">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcResult</span><span class="n">result</span><span class="p">)</span><span class="p">{</span>
<span class="k">if</span><span class="p">(</span><span class="n">result</span><span class="o">!=</span><span class="n">NVRTC_SUCCESS</span><span class="p">)</span><span class="p">{</span>
<span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="o">&lt;&lt;</span><span class="s">"</span><span class="se">\n</span><span class="s">nvrtc error: "</span><span class="o">&lt;&lt;</span><span class="n">nvrtcGetErrorString</span><span class="p">(</span><span class="n">result</span><span class="p">)</span><span class="o">&lt;&lt;</span><span class="sc">'\n'</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
<span class="p">}</span>

<span class="kt">void</span><span class="nf">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">CUresult</span><span class="n">result</span><span class="p">)</span><span class="p">{</span>
<span class="k">if</span><span class="p">(</span><span class="n">result</span><span class="o">!=</span><span class="n">CUDA_SUCCESS</span><span class="p">)</span><span class="p">{</span>
<span class="k">const</span><span class="kt">char</span><span class="o">*</span><span class="n">msg</span><span class="p">;</span>
<span class="n">cuGetErrorName</span><span class="p">(</span><span class="n">result</span><span class="p">,</span><span class="o">&amp;</span><span class="n">msg</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="n">cerr</span><span class="o">&lt;&lt;</span><span class="s">"</span><span class="se">\n</span><span class="s">cuda error: "</span><span class="o">&lt;&lt;</span><span class="n">msg</span><span class="o">&lt;&lt;</span><span class="sc">'\n'</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="p">}</span>
<span class="p">}</span>

<span class="k">const</span><span class="kt">char</span><span class="o">*</span><span class="n">hello</span><span class="o">=</span><span class="s">"                                           </span><span class="se">\n</span><span class="s">\</span>
<span class="s">extern </span><span class="se">\"</span><span class="s">C</span><span class="se">\"</span><span class="s"> __global__ void hello() {                          </span><span class="se">\n</span><span class="s">\</span>
<span class="s">  printf(</span><span class="se">\"</span><span class="s">hello world</span><span class="se">\\</span><span class="s">n</span><span class="se">\"</span><span class="s">);                                   </span><span class="se">\n</span><span class="s">\</span>
<span class="s">}                                                               </span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>

<span class="kt">int</span><span class="nf">main</span><span class="p">()</span>
<span class="p">{</span>
<span class="n">nvrtcProgram</span><span class="n">prog</span><span class="p">;</span>
<span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcCreateProgram</span><span class="p">(</span><span class="o">&amp;</span><span class="n">prog</span><span class="p">,</span><span class="n">hello</span><span class="p">,</span><span class="s">"hello.cu"</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="nb">NULL</span><span class="p">,</span><span class="nb">NULL</span><span class="p">));</span>
<span class="cp">#ifdef USE_CUBIN</span>
<span class="k">const</span><span class="kt">char</span><span class="o">*</span><span class="n">opts</span><span class="p">[]</span><span class="o">=</span><span class="p">{</span><span class="s">"-arch=sm_70"</span><span class="p">};</span>
<span class="cp">#else</span>
<span class="k">const</span><span class="kt">char</span><span class="o">*</span><span class="n">opts</span><span class="p">[]</span><span class="o">=</span><span class="p">{</span><span class="s">"-arch=compute_70"</span><span class="p">};</span>
<span class="cp">#endif</span>
<span class="n">nvrtcResult</span><span class="n">compileResult</span><span class="o">=</span><span class="n">nvrtcCompileProgram</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">opts</span><span class="p">);</span>
<span class="kt">size_t</span><span class="n">logSize</span><span class="p">;</span>
<span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetProgramLogSize</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="o">&amp;</span><span class="n">logSize</span><span class="p">));</span>
<span class="kt">char</span><span class="o">*</span><span class="n">log</span><span class="o">=</span><span class="n">new</span><span class="kt">char</span><span class="p">[</span><span class="n">logSize</span><span class="p">];</span>
<span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetProgramLog</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="n">log</span><span class="p">));</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="n">log</span><span class="o">&lt;&lt;</span><span class="sc">'\n'</span><span class="p">;</span>
<span class="n">delete</span><span class="p">[]</span><span class="n">log</span><span class="p">;</span>
<span class="k">if</span><span class="p">(</span><span class="n">compileResult</span><span class="o">!=</span><span class="n">NVRTC_SUCCESS</span><span class="p">)</span>
<span class="n">exit</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="kt">size_t</span><span class="n">codeSize</span><span class="p">;</span>
<span class="cp">#ifdef USE_CUBIN</span>
<span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetCUBINSize</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="o">&amp;</span><span class="n">codeSize</span><span class="p">));</span>
<span class="kt">char</span><span class="o">*</span><span class="n">code</span><span class="o">=</span><span class="n">new</span><span class="kt">char</span><span class="p">[</span><span class="n">codeSize</span><span class="p">];</span>
<span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetCUBIN</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="n">code</span><span class="p">));</span>
<span class="cp">#else</span>
<span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetPTXSize</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="o">&amp;</span><span class="n">codeSize</span><span class="p">));</span>
<span class="kt">char</span><span class="o">*</span><span class="n">code</span><span class="o">=</span><span class="n">new</span><span class="kt">char</span><span class="p">[</span><span class="n">codeSize</span><span class="p">];</span>
<span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcGetPTX</span><span class="p">(</span><span class="n">prog</span><span class="p">,</span><span class="n">code</span><span class="p">));</span>
<span class="cp">#endif</span>
<span class="n">NVRTC_SAFE_CALL</span><span class="p">(</span><span class="n">nvrtcDestroyProgram</span><span class="p">(</span><span class="o">&amp;</span><span class="n">prog</span><span class="p">));</span>
<span class="n">CUdevice</span><span class="n">cuDevice</span><span class="p">;</span>
<span class="n">CUcontext</span><span class="n">context</span><span class="p">;</span>
<span class="n">CUmodule</span><span class="n">module</span><span class="p">;</span>
<span class="n">CUfunction</span><span class="n">kernel</span><span class="p">;</span>
<span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuInit</span><span class="p">(</span><span class="mi">0</span><span class="p">));</span>
<span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuDeviceGet</span><span class="p">(</span><span class="o">&amp;</span><span class="n">cuDevice</span><span class="p">,</span><span class="mi">0</span><span class="p">));</span>
<span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuCtxCreate</span><span class="p">(</span><span class="o">&amp;</span><span class="n">context</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">cuDevice</span><span class="p">));</span>
<span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuModuleLoadDataEx</span><span class="p">(</span><span class="o">&amp;</span><span class="n">module</span><span class="p">,</span><span class="n">code</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">));</span>
<span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuModuleGetFunction</span><span class="p">(</span><span class="o">&amp;</span><span class="n">kernel</span><span class="p">,</span><span class="n">module</span><span class="p">,</span><span class="s">"hello"</span><span class="p">));</span>
<span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuLaunchKernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="nb">NULL</span><span class="p">,</span><span class="nb">NULL</span><span class="p">,</span><span class="mi">0</span><span class="p">));</span>
<span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuCtxSynchronize</span><span class="p">());</span>
<span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuModuleUnload</span><span class="p">(</span><span class="n">module</span><span class="p">));</span>
<span class="n">CUDA_SAFE_CALL</span><span class="p">(</span><span class="n">cuCtxDestroy</span><span class="p">(</span><span class="n">context</span><span class="p">));</span>
<span class="n">delete</span><span class="p">[]</span><span class="n">code</span><span class="p">;</span>
<span class="p">}</span>
</pre>
 <h4>
  <span class="section-number">
   15.4.1.4.
  </span>
  Recommendations for building a minor-version compatible library
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#recommendations-for-building-a-minor-version-compatible-library" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  We recommend that the CUDA runtime be statically linked to minimize dependencies. Verify that your library doesnât leak dependencies, breakages, namespaces, etc. outside your established ABI contract.
 </p>
 <p>
  Follow semantic versioning for your libraryâs soname. Having a semantically versioned ABI means the interfaces need to be maintained and versioned. The library should follow semantic rules and increment the version number when a change is made that affects this ABI contract. Missing dependencies is also a binary compatibility break, hence you should provide fallbacks or guards for functionality that depends on those interfaces. Increment major versions when there are ABI breaking changes such as API deprecation and modifications. New APIs can be added in minor versions.
 </p>
 <p>
  Conditionally use features to remain compatible against older drivers. If no new features are used (or if they are used conditionally with fallbacks provided) youâll be able to remain compatible.
 </p>
 <p>
  Donât expose ABI structures that can change. A pointer to a structure with a size embedded is a better solution.
 </p>
 <p>
  When linking with dynamic libraries from the toolkit, the library must be equal to or newer than what is needed by any one of the components involved in the linking of your application. For example, if you link against the CUDA 11.1 dynamic runtime, and use functionality from 11.1, as well as a separate shared library that was linked against the CUDA 11.2 dynamic runtime that requires 11.2 functionality, the final link step must include a CUDA 11.2 or newer dynamic runtime.
 </p>
 <h4>
  <span class="section-number">
   15.4.1.5.
  </span>
  Recommendations for taking advantage of minor version compatibility in your application
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#recommendations-for-taking-advantage-of-minor-version-compatibility-in-your-application" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Certain functionality might not be available so you should query where applicable. This is common for building applications that are GPU architecture, platform and compiler agnostic. However we now add âthe underlying driverâ to that mix.
 </p>
 <p>
  As with the previous section on library building recommendations, if using the CUDA runtime, we recommend linking to the CUDA runtime statically when building your application. When using the driver APIs directly, we recommend using the new driver entry point access API (
  <span class="pre">
   cuGetProcAddress
  </span>
  ) documented here:
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-driver-api/group__CUDA__DRIVER__ENTRY__POINT.html#group__CUDA__DRIVER__ENTRY__POINT">
   CUDA Driver API :: CUDA Toolkit Documentation
  </a>
  .
 </p>
 <p>
  When using a shared or static library, follow the release notes of said library to determine if the library supports minor version compatibility.
 </p>
 <h1>
  <span class="section-number">
   16.
  </span>
  Preparing for Deployment
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#preparing-for-deployment" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   16.1.
  </span>
  Testing for CUDA Availability
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#testing-for-cuda-availability" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  When deploying a CUDA application, it is often desirable to ensure that the application will continue to function properly even if the target machine does not have a CUDA-capable GPU and/or a sufficient version of the NVIDIA Driver installed. (Developers targeting a single machine with known configuration may choose to skip this section.)
 </p>
 <p>
  Detecting a CUDA-Capable GPU
 </p>
 <p>
  When an application will be deployed to target machines of arbitrary/unknown configuration, the application should explicitly test for the existence of a CUDA-capable GPU in order to take appropriate action when no such device is available. The
  <span class="pre">
   cudaGetDeviceCount()
  </span>
  function can be used to query for the number of available devices. Like all CUDA Runtime API functions, this function will fail gracefully and return
  <span class="pre">
   cudaErrorNoDevice
  </span>
  to the application if there is no CUDA-capable GPU or
  <span class="pre">
   cudaErrorInsufficientDriver
  </span>
  if there is not an appropriate version of the NVIDIA Driver installed. If
  <span class="pre">
   cudaGetDeviceCount()
  </span>
  reports an error, the application should fall back to an alternative code path.
 </p>
 <p>
  A system with multiple GPUs may contain GPUs of different hardware versions and capabilities. When using multiple GPUs from the same application, it is recommended to use GPUs of the same type, rather than mixing hardware generations. The
  <span class="pre">
   cudaChooseDevice()
  </span>
  function can be used to select the device that most closely matches a desired set of features.
 </p>
 <p>
  Detecting Hardware and Software Configuration
 </p>
 <p>
  When an application depends on the availability of certain hardware or software capabilities to enable certain functionality, the CUDA API can be queried for details about the configuration of the available device and for the installed software versions.
 </p>
 <p>
  The
  <span class="pre">
   cudaGetDeviceProperties()
  </span>
  function reports various features of the available devices, including the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
   CUDA Compute Capability
  </a>
  of the device (see also the Compute Capabilities section of the CUDA C++ Programming Guide). See
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART____VERSION.html#group__CUDART____VERSION">
   Version Management
  </a>
  for details on how to query the available CUDA software API versions.
 </p>
 <h2>
  <span class="section-number">
   16.2.
  </span>
  Error Handling
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#error-handling" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  All CUDA Runtime API calls return an error code of type
  <span class="pre">
   cudaError_t
  </span>
  ; the return value will be equal to
  <span class="pre">
   cudaSuccess
  </span>
  if no errors have occurred. (The exceptions to this are kernel launches, which return void, and
  <span class="pre">
   cudaGetErrorString()
  </span>
  , which returns a character string describing the
  <span class="pre">
   cudaError_t
  </span>
  code that was passed into it.) The CUDA Toolkit libraries (
  <span class="pre">
   cuBLAS
  </span>
  ,
  <span class="pre">
   cuFFT
  </span>
  , etc.) likewise return their own sets of error codes.
 </p>
 <p>
  Since some CUDA API calls and all kernel launches are asynchronous with respect to the host code, errors may be reported to the host asynchronously as well; often this occurs the next time the host and device synchronize with each other, such as during a call to
  <span class="pre">
   cudaMemcpy()
  </span>
  or to
  <span class="pre">
   cudaDeviceSynchronize()
  </span>
  .
 </p>
 <p>
  Always check the error return values on all CUDA API functions, even for functions that are not expected to fail, as this will allow the application to detect and recover from errors as soon as possible should they occur. To check for errors occurring during kernel launches using the
  <span class="pre">
   &lt;&lt;&lt;...&gt;&gt;&gt;
  </span>
  syntax, which does not return any error code, the return code of
  <span class="pre">
   cudaGetLastError()
  </span>
  should be checked immediately after the kernel launch. Applications that do not check for CUDA API errors could at times run to completion without having noticed that the data calculated by the GPU is incomplete, invalid, or uninitialized.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  The CUDA Toolkit Samples provide several helper functions for error checking with the various CUDA APIs; these helper functions are located in the
  <span class="pre">
   samples/common/inc/helper_cuda.h
  </span>
  file in the CUDA Toolkit.
 </p>
 <h2>
  <span class="section-number">
   16.3.
  </span>
  Building for Maximum Compatibility
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#building-for-maximum-compatibility" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Each generation of CUDA-capable device has an associated
  compute capability
  version that indicates the feature set supported by the device (see
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
   CUDA Compute Capability
  </a>
  ). One or more
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-compute-capability">
   compute capability
  </a>
  versions can be specified to the nvcc compiler while building a file; compiling for the native compute capability for the target GPU(s) of the application is important to ensure that application kernels achieve the best possible performance and are able to use the features that are available on a given generation of GPU.
 </p>
 <p>
  When an application is built for multiple compute capabilities simultaneously (using several instances of the
  <span class="pre">
   -gencode
  </span>
  flag to nvcc), the binaries for the specified compute capabilities are combined into the executable, and the CUDA Driver selects the most appropriate binary at runtime according to the compute capability of the present device. If an appropriate native binary (
  cubin
  ) is not available, but the intermediate
  PTX
  code (which targets an abstract virtual instruction set and is used for forward-compatibility) is available, then the kernel will be compiled
  Just In Time
  (JIT) (see
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#compiler-jit-cache-management">
   Compiler JIT Cache Management Tools
  </a>
  ) from the PTX to the native cubin for the device. If the PTX is also not available, then the kernel launch will fail.
 </p>
 <p>
  Windows
 </p>
 <pre>nvcc.exe -ccbin "C:\vs2008\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT"
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_60,code=sm_60
  -gencode=arch=compute_70,code=sm_70
  -gencode=arch=compute_75,code=sm_75
  -gencode=arch=compute_75,code=compute_75
  --compile -o "Release\mykernel.cu.obj" "mykernel.cu"
</pre>
 <p>
  Mac/Linux
 </p>
 <pre>/usr/local/cuda/bin/nvcc
  -gencode=arch=compute_30,code=sm_30
  -gencode=arch=compute_35,code=sm_35
  -gencode=arch=compute_50,code=sm_50
  -gencode=arch=compute_60,code=sm_60
  -gencode=arch=compute_70,code=sm_70
  -gencode=arch=compute_75,code=sm_75
  -gencode=arch=compute_75,code=compute_75
  -O2 -o mykernel.o -c mykernel.cu
</pre>
 <p>
  Alternatively, the
  <span class="pre">
   nvcc
  </span>
  command-line option
  <span class="pre">
   -arch=sm_XX
  </span>
  can be used as a shorthand equivalent to the following more explicit
  <span class="pre">
   -gencode=
  </span>
  command-line options described above:
 </p>
 <pre>-gencode=arch=compute_XX,code=sm_XX
-gencode=arch=compute_XX,code=compute_XX
</pre>
 <p>
  However, while the
  <span class="pre">
   -arch=sm_XX
  </span>
  command-line option does result in inclusion of a PTX back-end target by default (due to the
  <span class="pre">
   code=compute_XX
  </span>
  target it implies), it can only specify a single target
  <span class="pre">
   cubin
  </span>
  architecture at a time, and it is not possible to use multiple
  <span class="pre">
   -arch=
  </span>
  options on the same
  <span class="pre">
   nvcc
  </span>
  command line, which is why the examples above use
  <span class="pre">
   -gencode=
  </span>
  explicitly.
 </p>
 <h2>
  <span class="section-number">
   16.4.
  </span>
  Distributing the CUDA Runtime and Libraries
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#distributing-the-cuda-runtime-and-libraries" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  CUDA applications are built against the CUDA Runtime library, which handles device, memory, and kernel management. Unlike the CUDA Driver, the CUDA Runtime guarantees neither forward nor backward binary compatibility across versions. It is therefore best to
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#redistribution">
   redistribute
  </a>
  the CUDA Runtime library with the application when using dynamic linking or else to statically link against the CUDA Runtime. This will ensure that the executable will be able to run even if the user does not have the same CUDA Toolkit installed that the application was built against.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  When statically linking to the CUDA Runtime, multiple versions of the runtime can peacably coexist in the same application process simultaneously; for example, if an application uses one version of the CUDA Runtime, and a plugin to that application is statically linked to a different version, that is perfectly acceptable, as long as the installed NVIDIA Driver is sufficient for both.
 </p>
 <p class="title sectiontitle rubric" id="statically-linked-cuda-runtime">
  Statically-linked CUDA Runtime
 </p>
 <p>
  The easiest option is to statically link against the CUDA Runtime. This is the default if using
  <span class="pre">
   nvcc
  </span>
  to link in CUDA 5.5 and later. Static linking makes the executable slightly larger, but it ensures that the correct version of runtime library functions are included in the application binary without requiring separate redistribution of the CUDA Runtime library.
 </p>
 <p class="title sectiontitle rubric" id="dynamically-linked-cuda-runtime">
  Dynamically-linked CUDA Runtime
 </p>
 <p>
  If static linking against the CUDA Runtime is impractical for some reason, then a dynamically-linked version of the CUDA Runtime library is also available. (This was the default and only option provided in CUDA versions 5.0 and earlier.)
 </p>
 <p>
  To use dynamic linking with the CUDA Runtime when using the
  <span class="pre">
   nvcc
  </span>
  from CUDA 5.5 or later to link the application, add the
  <span class="pre">
   --cudart=shared
  </span>
  flag to the link command line; otherwise the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#distributing-cuda-runtime-and-libraries__static-cudart">
   statically-linked CUDA Runtime library
  </a>
  is used by default.
 </p>
 <p>
  After the application is dynamically linked against the CUDA Runtime, this version of the runtime library should be
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#redistribution">
   bundled with
  </a>
  the application. It can be copied into the same directory as the application executable or into a subdirectory of that installation path.
 </p>
 <p class="title sectiontitle rubric" id="other-cuda-libraries">
  Other CUDA Libraries
 </p>
 <p>
  Although the CUDA Runtime provides the option of static linking, some libraries included in the CUDA Toolkit are available only in dynamically-linked form. As with the
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#distributing-cuda-runtime-and-libraries__dynamic-cudart">
   dynamically-linked version of the CUDA Runtime library
  </a>
  , these libraries should be
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#redistribution">
   bundled with
  </a>
  the application executable when distributing that application.
 </p>
 <h3>
  <span class="section-number">
   16.4.1.
  </span>
  CUDA Toolkit Library Redistribution
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-toolkit-library-redistribution" title="Permalink to this headline">
   ï
  </a>
 </h3>
 <p>
  The CUDA Toolkitâs End-User License Agreement (EULA) allows for redistribution of many of the CUDA libraries under certain terms and conditions. This allows applications that depend on these libraries
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#redistribution--which-files">
   to redistribute the exact versions
  </a>
  of the libraries against which they were built and tested, thereby avoiding any trouble for end users who might have a different version of the CUDA Toolkit (or perhaps none at all) installed on their machines. Please refer to the EULA for details.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  This does
  not
  apply to the NVIDIA Driver; the end user must still download and install an NVIDIA Driver appropriate to their GPU(s) and operating system.
 </p>
 <h4>
  <span class="section-number">
   16.4.1.1.
  </span>
  Which Files to Redistribute
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#which-files-to-redistribute" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  When redistributing the dynamically-linked versions of one or more CUDA libraries, it is important to identify the exact files that need to be redistributed. The following examples use the cuBLAS library from CUDA Toolkit 5.5 as an illustration:
 </p>
 <p>
  Linux
 </p>
 <p>
  In a shared library on Linux, there is a string field called the
  <span class="pre">
   SONAME
  </span>
  that indicates the binary compatibility level of the library. The
  <span class="pre">
   SONAME
  </span>
  of the library against which the application was built must match the filename of the library that is redistributed with the application.
 </p>
 <p>
  For example, in the standard CUDA Toolkit installation, the files
  <span class="pre">
   libcublas.so
  </span>
  and
  <span class="pre">
   libcublas.so.5.5
  </span>
  are both symlinks pointing to a specific build of cuBLAS, which is named like
  <span class="pre">
   libcublas.so.5.5.x
  </span>
  , where
  x
  is the build number (e.g.,
  <span class="pre">
   libcublas.so.5.5.17
  </span>
  ). However, the
  <span class="pre">
   SONAME
  </span>
  of this library is given as â
  <span class="pre">
   libcublas.so.5.5
  </span>
  â:
 </p>
 <pre>$ objdump -p /usr/local/cuda/lib64/libcublas.so | grep SONAME
   SONAME               libcublas.so.5.5
</pre>
 <p>
  Because of this, even if
  <span class="pre">
   -lcublas
  </span>
  (with no version number specified) is used when linking the application, the
  <span class="pre">
   SONAME
  </span>
  found at link time implies that â
  <span class="pre">
   libcublas.so.5.5
  </span>
  â is the name of the file that the dynamic loader will look for when loading the application and therefore must be the name of the file (or a symlink to the same) that is redistributed with the application.
 </p>
 <p>
  The
  <span class="pre">
   ldd
  </span>
  tool is useful for identifying the exact filenames of the libraries that the application expects to find at runtime as well as the path, if any, of the copy of that library that the dynamic loader would select when loading the application given the current library search path:
 </p>
 <pre>$ ldd a.out | grep libcublas
   libcublas.so.5.5 =&gt; /usr/local/cuda/lib64/libcublas.so.5.5
</pre>
 <p>
  Mac
 </p>
 <p>
  In a shared library on Mac OS X, there is a field called the
  <span class="pre">
   install
  </span>
  <span class="pre">
   name
  </span>
  that indicates the expected installation path and filename the library; the CUDA libraries also use this filename to indicate binary compatibility. The value of this field is propagated into an application built against the library and is used to locate the library of the correct version at runtime.
 </p>
 <p>
  For example, if the install name of the cuBLAS library is given as
  <span class="pre">
   @rpath/libcublas.5.5.dylib
  </span>
  , then the library is version 5.5 and the copy of this library redistributed with the application must be named
  <span class="pre">
   libcublas.5.5.dylib
  </span>
  , even though only
  <span class="pre">
   -lcublas
  </span>
  (with no version number specified) is used at link time. Furthermore, this file should be installed into the
  <span class="pre">
   @rpath
  </span>
  of the application; see
  <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#redistribution--where-to-install">
   Where to Install Redistributed CUDA Libraries
  </a>
  .
 </p>
 <p>
  To view a libraryâs install name, use the
  <span class="pre">
   otool
  </span>
  <span class="pre">
   -L
  </span>
  command:
 </p>
 <pre>$ otool -L a.out
a.out:
        @rpath/libcublas.5.5.dylib (...)
</pre>
 <p>
  Windows
 </p>
 <p>
  The binary compatibility version of the CUDA libraries on Windows is indicated as part of the filename.
 </p>
 <p>
  For example, a 64-bit application linked to cuBLAS 5.5 will look for
  <span class="pre">
   cublas64_55.dll
  </span>
  at runtime, so this is the file that should be redistributed with that application, even though
  <span class="pre">
   cublas.lib
  </span>
  is the file that the application is linked against. For 32-bit applications, the file would be
  <span class="pre">
   cublas32_55.dll
  </span>
  .
 </p>
 <p>
  To verify the exact DLL filename that the application expects to find at runtime, use the
  <span class="pre">
   dumpbin
  </span>
  tool from the Visual Studio command prompt:
 </p>
 <pre>$ dumpbin /IMPORTS a.exe
Microsoft (R) COFF/PE Dumper Version 10.00.40219.01
Copyright (C) Microsoft Corporation.  All rights reserved.


Dump of file a.exe

File Type: EXECUTABLE IMAGE

  Section contains the following imports:

    ...
    cublas64_55.dll
    ...
</pre>
 <h4>
  <span class="section-number">
   16.4.1.2.
  </span>
  Where to Install Redistributed CUDA Libraries
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#where-to-install-redistributed-cuda-libraries" title="Permalink to this headline">
   ï
  </a>
 </h4>
 <p>
  Once the correct library files are identified for redistribution, they must be configured for installation into a location where the application will be able to find them.
 </p>
 <p>
  On Windows, if the CUDA Runtime or other dynamically-linked CUDA Toolkit library is placed in the same directory as the executable, Windows will locate it automatically. On Linux and Mac, the
  <span class="pre">
   -rpath
  </span>
  linker option should be used to instruct the executable to search its local path for these libraries before searching the system paths:
 </p>
 <p>
  Linux/Mac
 </p>
 <pre>nvcc -I $(CUDA_HOME)/include
  -Xlinker "-rpath '$ORIGIN'" --cudart=shared
  -o myprogram myprogram.cu
</pre>
 <p>
  Windows
 </p>
 <pre>nvcc.exe -ccbin "C:\vs2008\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT" --cudart=shared
  -o "Release\myprogram.exe" "myprogram.cu"
</pre>
 <p class="admonition-title">
  Note
 </p>
 <p>
  It may be necessary to adjust the value of
  <span class="pre">
   -ccbin
  </span>
  to reflect the location of your Visual Studio installation.
 </p>
 <p>
  To specify an alternate path where the libraries will be distributed, use linker options similar to those below:
 </p>
 <p>
  Linux/Mac
 </p>
 <pre>nvcc -I $(CUDA_HOME)/include
  -Xlinker "-rpath '$ORIGIN/lib'" --cudart=shared
  -o myprogram myprogram.cu
</pre>
 <p>
  Windows
 </p>
 <pre>nvcc.exe -ccbin "C:\vs2008\VC\bin"
  -Xcompiler "/EHsc /W3 /nologo /O2 /Zi /MT /DELAY" --cudart=shared
  -o "Release\myprogram.exe" "myprogram.cu"
</pre>
 <p>
  For Linux and Mac, the
  <span class="pre">
   -rpath
  </span>
  option is used as before. For Windows, the
  <span class="pre">
   /DELAY
  </span>
  option is used; this requires that the application call
  <span class="pre">
   SetDllDirectory()
  </span>
  before the first call to any CUDA API function in order to specify the directory containing the CUDA DLLs.
 </p>
 <p class="admonition-title">
  Note
 </p>
 <p>
  For Windows 8,
  <span class="pre">
   SetDefaultDLLDirectories()
  </span>
  and
  <span class="pre">
   AddDllDirectory()
  </span>
  should be used instead of
  <span class="pre">
   SetDllDirectory()
  </span>
  . Please see the MSDN documentation for these routines for more information.
 </p>
 <h1>
  <span class="section-number">
   17.
  </span>
  Deployment Infrastructure Tools
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#deployment-infrastructure-tools" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   17.1.
  </span>
  Nvidia-SMI
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#nvidia-smi" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The NVIDIA System Management Interface (
  <span class="pre">
   nvidia-smi
  </span>
  ) is a command line utility that aids in the management and monitoring of NVIDIA GPU devices. This utility allows administrators to query GPU device state and, with the appropriate privileges, permits administrators to modify GPU device state.
  <span class="pre">
   nvidia-smi
  </span>
  is targeted at Tesla and certain Quadro GPUs, though limited support is also available on other NVIDIA GPUs.
  <span class="pre">
   nvidia-smi
  </span>
  ships with NVIDIA GPU display drivers on Linux, and with 64-bit Windows Server 2008 R2 and Windows 7.
  <span class="pre">
   nvidia-smi
  </span>
  can output queried information as XML or as human-readable plain text either to standard output or to a file. See the nvidia-smi documenation for details. Please note that new versions of nvidia-smi are not guaranteed to be backward-compatible with previous versions.
 </p>
 <h3>
  <span class="section-number">
   17.1.1.
  </span>
  Queryable state
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#queryable-state" title="Permalink to this headline">
   ï
  </a>
 </h3>
 ECC error counts
 <p>
  Both correctable single-bit and detectable double-bit errors are reported. Error counts are provided for both the current boot cycle and the lifetime of the GPU.
 </p>
 GPU utilization
 <p>
  Current utilization rates are reported for both the compute resources of the GPU and the memory interface.
 </p>
 Active compute process
 <p>
  The list of active processes running on the GPU is reported, along with the corresponding process name/ID and allocated GPU memory.
 </p>
 Clocks and performance state
 <p>
  Max and current clock rates are reported for several important clock domains, as well as the current GPU performance state (
  pstate
  ).
 </p>
 Temperature and fan speed
 <p>
  The current GPU core temperature is reported, along with fan speeds for products with active cooling.
 </p>
 Power management
 <p>
  The current board power draw and power limits are reported for products that report these measurements.
 </p>
 Identification
 <p>
  Various dynamic and static information is reported, including board serial numbers, PCI device IDs, VBIOS/Inforom version numbers and product names.
 </p>
 <h3>
  <span class="section-number">
   17.1.2.
  </span>
  Modifiable state
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#modifiable-state" title="Permalink to this headline">
   ï
  </a>
 </h3>
 ECC mode
 <p>
  Enable and disable ECC reporting.
 </p>
 ECC reset
 <p>
  Clear single-bit and double-bit ECC error counts.
 </p>
 Compute mode
 <p>
  Indicate whether compute processes can run on the GPU and whether they run exclusively or concurrently with other compute processes.
 </p>
 Persistence mode
 <p>
  Indicate whether the NVIDIA driver stays loaded when no applications are connected to the GPU. It is best to enable this option in most circumstances.
 </p>
 GPU reset
 <p>
  Reinitialize the GPU hardware and software state via a secondary bus reset.
 </p>
 <h2>
  <span class="section-number">
   17.2.
  </span>
  NVML
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#nvml" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The NVIDIA Management Library (NVML) is a C-based interface that provides direct access to the queries and commands exposed via
  <span class="pre">
   nvidia-smi
  </span>
  intended as a platform for building 3rd-party system management applications. The NVML API is shipped with the CUDA Toolkit (since version 8.0) and is also available standalone on the NVIDIA developer website as part of the GPU Deployment Kit through a single header file accompanied by PDF documentation, stub libraries, and sample applications; see
  <a class="reference external" href="https://developer.nvidia.com/gpu-deployment-kit">
   https://developer.nvidia.com/gpu-deployment-kit
  </a>
  . Each new version of NVML is backward-compatible.
 </p>
 <p>
  An additional set of Perl and Python bindings are provided for the NVML API. These bindings expose the same features as the C-based interface and also provide backwards compatibility. The Perl bindings are provided via CPAN and the Python bindings via PyPI.
 </p>
 <p>
  All of these products (
  <span class="pre">
   nvidia-smi
  </span>
  , NVML, and the NVML language bindings) are updated with each new CUDA release and provide roughly the same functionality.
 </p>
 <p>
  See
  <a class="reference external" href="https://developer.nvidia.com/nvidia-management-library-nvml">
   https://developer.nvidia.com/nvidia-management-library-nvml
  </a>
  for additional information.
 </p>
 <h2>
  <span class="section-number">
   17.3.
  </span>
  Cluster Management Tools
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cluster-management-tools" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Managing your GPU cluster will help achieve maximum GPU utilization and help you and your users extract the best possible performance. Many of the industryâs most popular cluster management tools support CUDA GPUs via NVML. For a listing of some of these tools, see
  <a class="reference external" href="https://developer.nvidia.com/cluster-management">
   https://developer.nvidia.com/cluster-management
  </a>
  .
 </p>
 <h2>
  <span class="section-number">
   17.4.
  </span>
  Compiler JIT Cache Management Tools
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#compiler-jit-cache-management-tools" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Any PTX device code loaded by an application at runtime is compiled further to binary code by the device driver. This is called
  just-in-time compilation
  (
  JIT
  ). Just-in-time compilation increases application load time but allows applications to benefit from latest compiler improvements. It is also the only way for applications to run on devices that did not exist at the time the application was compiled.
 </p>
 <p>
  When JIT compilation of PTX device code is used, the NVIDIA driver caches the resulting binary code on disk. Some aspects of this behavior such as cache location and maximum cache size can be controlled via the use of environment variables; see Just in Time Compilation of the CUDA C++ Programming Guide.
 </p>
 <h2>
  <span class="section-number">
   17.5.
  </span>
  CUDA_VISIBLE_DEVICES
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#cuda-visible-devices" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  It is possible to rearrange the collection of installed CUDA devices that will be visible to and enumerated by a CUDA application prior to the start of that application by way of the
  <span class="pre">
   CUDA_VISIBLE_DEVICES
  </span>
  environment variable.
 </p>
 <p>
  Devices to be made visible to the application should be included as a comma-separated list in terms of the system-wide list of enumerable devices. For example, to use only devices 0 and 2 from the system-wide list of devices, set
  <span class="pre">
   CUDA_VISIBLE_DEVICES=0,2
  </span>
  before launching the application. The application will then enumerate these devices as device 0 and device 1, respectively.
 </p>
 <h1>
  <span class="section-number">
   18.
  </span>
  Recommendations and Best Practices
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#id3" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <p>
  This chapter contains a summary of the recommendations for optimization that are explained in this document.
 </p>
 <h2>
  <span class="section-number">
   18.1.
  </span>
  Overall Performance Optimization Strategies
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#overall-performance-optimization-strategies" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  Performance optimization revolves around three basic strategies:
 </p>
 <ul class="simple">
  <li>
   <p>
    Maximizing parallel execution
   </p>
  </li>
  <li>
   <p>
    Optimizing memory usage to achieve maximum memory bandwidth
   </p>
  </li>
  <li>
   <p>
    Optimizing instruction usage to achieve maximum instruction throughput
   </p>
  </li>
 </ul>
 <p>
  Maximizing parallel execution starts with structuring the algorithm in a way that exposes as much parallelism as possible. Once the parallelism of the algorithm has been exposed, it needs to be mapped to the hardware as efficiently as possible. This is done by carefully choosing the execution configuration of each kernel launch. The application should also maximize parallel execution at a higher level by explicitly exposing concurrent execution on the device through streams, as well as maximizing concurrent execution between the host and the device.
 </p>
 <p>
  Optimizing memory usage starts with minimizing data transfers between the host and the device because those transfers have much lower bandwidth than internal device data transfers. Kernel access to global memory also should be minimized by maximizing the use of shared memory on the device. Sometimes, the best optimization might even be to avoid any data transfer in the first place by simply recomputing the data whenever it is needed.
 </p>
 <p>
  The effective bandwidth can vary by an order of magnitude depending on the access pattern for each type of memory. The next step in optimizing memory usage is therefore to organize memory accesses according to the optimal memory access patterns. This optimization is especially important for global memory accesses, because latency of access costs hundreds of clock cycles. Shared memory accesses, in counterpoint, are usually worth optimizing only when there exists a high degree of bank conflicts.
 </p>
 <p>
  As for optimizing instruction usage, the use of arithmetic instructions that have low throughput should be avoided. This suggests trading precision for speed when it does not affect the end result, such as using intrinsics instead of regular functions or single precision instead of double precision. Finally, particular attention must be paid to control flow instructions due to the SIMT (single instruction multiple thread) nature of the device.
 </p>
 <h1>
  <span class="section-number">
   19.
  </span>
  nvcc Compiler Switches
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#nvcc-compiler-switches" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   19.1.
  </span>
  nvcc
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#nvcc" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  The NVIDIA
  <span class="pre">
   nvcc
  </span>
  compiler driver converts
  <span class="pre">
   .cu
  </span>
  files into C++ for the host system and CUDA assembly or binary instructions for the device. It supports a number of command-line parameters, of which the following are especially useful for optimization and related best practices:
 </p>
 <ul class="simple">
  <li>
   <p>
    <span class="pre">
     -maxrregcount=N
    </span>
    specifies the maximum number of registers kernels can use at a per-file level. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#register-pressure">
     Register Pressure
    </a>
    . (See also the
    <span class="pre">
     __launch_bounds__
    </span>
    qualifier discussed in Execution Configuration of the CUDA C++ Programming Guide to control the number of registers used on a per-kernel basis.)
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     --ptxas-options=-v
    </span>
    or
    <span class="pre">
     -Xptxas=-v
    </span>
    lists per-kernel register, shared, and constant memory usage.
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     -ftz=true
    </span>
    (denormalized numbers are flushed to zero)
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     -prec-div=false
    </span>
    (less precise division)
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     -prec-sqrt=false
    </span>
    (less precise square root)
   </p>
  </li>
  <li>
   <p>
    <span class="pre">
     -use_fast_math
    </span>
    compiler option of
    <span class="pre">
     nvcc
    </span>
    coerces every
    <span class="pre">
     functionName()
    </span>
    call to the equivalent
    <span class="pre">
     __functionName()
    </span>
    call. This makes the code run faster at the cost of diminished precision and accuracy. See
    <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#math-libraries">
     Math Libraries
    </a>
    .
   </p>
  </li>
 </ul>
 <h1>
  <span class="section-number">
   20.
  </span>
  Notices
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#notices" title="Permalink to this headline">
   ï
  </a>
 </h1>
 <h2>
  <span class="section-number">
   20.1.
  </span>
  Notice
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#notice" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (âNVIDIAâ) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
 </p>
 <p>
  NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
 </p>
 <p>
  Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
 </p>
 <p>
  NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (âTerms of Saleâ). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
 </p>
 <p>
  NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customerâs own risk.
 </p>
 <p>
  NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customerâs sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customerâs product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
 </p>
 <p>
  No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
 </p>
 <p>
  Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
 </p>
 <p>
  THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, âMATERIALSâ) ARE BEING PROVIDED âAS IS.â NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIAâs aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.
 </p>
 <h2>
  <span class="section-number">
   20.2.
  </span>
  OpenCL
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#opencl" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.
 </p>
 <h2>
  <span class="section-number">
   20.3.
  </span>
  Trademarks
  <a class="headerlink" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#trademarks" title="Permalink to this headline">
   ï
  </a>
 </h2>
 <p>
  NVIDIA and the NVIDIA logo are trademarks or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.
 </p>
 <p class="notices">
  <a href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" target="_blank">
   Privacy Policy
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" target="_blank">
   Manage My Privacy
  </a>
  |
  <a href="https://www.nvidia.com/en-us/preferences/start/" target="_blank">
   Do Not Sell or Share My Data
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" target="_blank">
   Terms of Service
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" target="_blank">
   Accessibility
  </a>
  |
  <a href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" target="_blank">
   Corporate Policies
  </a>
  |
  <a href="https://www.nvidia.com/en-us/product-security/" target="_blank">
   Product Security
  </a>
  |
  <a href="https://www.nvidia.com/en-us/contact/" target="_blank">
   Contact
  </a>
 </p>
 <p>
  Copyright Â© 2007-2024, NVIDIA Corporation &amp; affiliates. All rights reserved.
 </p>
 <p>
  <span class="lastupdated">
   Last updated on Jul 1, 2024.
  </span>
 </p>
</body>
</body></html>