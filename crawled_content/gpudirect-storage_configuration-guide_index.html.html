<html><head><title>NVIDIA GPUDirect Storage Benchmarking and Configuration Guide - NVIDIA Docs</title></head><body><body class="Page-body">
 <!-- Putting icons here, so we don't have to include in a bunch of -body hbs's -->
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <ul class="Navigation-items">
  <li class="Navigation-items-item">
   <a href="https://developer.nvidia.com/" target="_blank">
    NVIDIA Developer
   </a>
  </li>
  <li class="Navigation-items-item">
   <a href="https://developer.nvidia.com/blog/" target="_blank">
    Blog
   </a>
  </li>
  <li class="Navigation-items-item">
   <a href="https://forums.developer.nvidia.com/" target="_blank">
    Forums
   </a>
  </li>
  <li class="Navigation-items-item">
   <a class="Button" data-size="small" data-theme="secondary" href="https://docs.nvidia.com/login" rel="nofollow noopener" style="--button-border-radius: 0px">
    Join
   </a>
  </li>
 </ul>
 <a aria-label="home page" href="https://docs.nvidia.com/">
 </a>
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <ul class="Navigation-items">
  <li class="Navigation-items-item">
   <a data-cms-ai="0" href="https://developer.nvidia.com/" target="_blank">
    NVIDIA Developer
   </a>
  </li>
  <li class="Navigation-items-item">
   <a data-cms-ai="0" href="https://developer.nvidia.com/blog/" target="_blank">
    Blog
   </a>
  </li>
  <li class="Navigation-items-item">
   <a data-cms-ai="0" href="https://forums.developer.nvidia.com/" target="_blank">
    Forums
   </a>
  </li>
  <li class="Navigation-items-item">
   <a class="Button" data-cms-ai="0" data-size="small" data-theme="secondary" href="https://docs.nvidia.com/login" rel="nofollow noopener" style="--button-border-radius: 0px">
    Join
   </a>
  </li>
 </ul>
 <span class="label">
  Menu
 </span>
 <a class="Page-BackToTop" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html">
 </a>
 <h1 class="PageHeading-title">
  NVIDIA GPUDirect Storage Benchmarking and Configuration Guide
 </h1>
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <span class="sr-only">
  Submit Search
 </span>
 <!--***********************************************************************************************************************************************************************************
        This script code is added only when moreAttributes is "mobile-only" just because on Books and Topics "mobile-only" search is the last added, so I've added with the last one
        because the external script "nvidia-in-book-search-page-widget.js" searches for the hidden fields for both desktop and mobile, so if I load the script before all fields are
        rendered, the script fails
    **************************************************************************************************************************************************************************************-->
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/">
  NVIDIA Docs Hub
 </a>
 <span class="Link">
  NVIDIA GPUDirect Storage (GDS)
 </span>
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/index.html">
  NVIDIA GPUDirect Storage
 </a>
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html">
  NVIDIA GPUDirect Storage Benchmarking and Configuration Guide
 </a>
 <span class="TopicPage-version">
  NVIDIA GPUDirect Storage (GDS) (Latest Release)
 </span>
 <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/pdf/configuration-guide.pdf">
  Download PDF
 </a>
 <h2>
  <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#abstract" id="abstract" name="abstract" shape="rect">
   NVIDIA GPUDirect Storage Benchmarking and Configuration Guide
  </a>
 </h2>
 <p>
  The Benchmarking and Configuration Guide helps you evaluate and test GDS functionality and performance by using sample applications.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#introduction">
   1. Introduction
  </a>
 </h2>
 <p>
  NVIDIA® GPUDirect® Storage (GDS) is the newest addition to the GPUDirect family. GDS enables a direct data path for direct memory access (DMA) transfers between GPU memory and storage, which avoids a bounce buffer through the CPU. This direct path increases system bandwidth and decreases the latency and utilization load on the CPU.
 </p>
 <p>
  The purpose of this guide is to help the user evaluate and test GDS functionality and performance by using sample applications. These applications can be run after you set up and install GDS and before you run the custom applications that have been modified to take advantage of GDS. Refer to the following guides for more information about GDS:
 </p>
 <ul>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html" shape="rect">
    GPUDirect Storage Design Guide
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html" shape="rect">
    GPUDirect Storage Overview Guide
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html" shape="rect">
    cuFile API Reference Guide
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html" shape="rect">
    GPUDirect Storage Release Notes
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html" shape="rect">
    GPUDirect Storage Best Practices Guide
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html" shape="rect">
    GPUDirect Storage Troubleshooting Guide
   </a>
  </li>
  <li>
   <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html" shape="rect">
    GPUDirect Storage O_DIRECT Requirements Guide
   </a>
  </li>
 </ul>
 <p>
  To learn more about GDS, refer to the following posts:
  <a class="Link" data-cms-ai="0" id="introduction__ul_ccg_1ty_2nb" name="introduction__ul_ccg_1ty_2nb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   <a class="xref" data-cms-ai="0" href="https://devblogs.nvidia.com/gpudirect-storage/" shape="rect">
    GPUDirect Storage: A Direct Path Between Storage and GPU Memory
   </a>
   .
  </li>
  <li>
   The
   <a class="xref" data-cms-ai="0" href="https://developer.nvidia.com/blog/tag/magnum-io/" shape="rect">
    Magnum IO
   </a>
   series.
  </li>
 </ul>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#about-this-guide">
   2. About this Guide
  </a>
 </h2>
 <p>
  Configuration and benchmarking are very tightly coupled activities. Benchmarking provides the ability to determine the potential performance based on the current system configuration, and the impact of configuration changes. Configuration changes are sometimes required to achieve optimal benchmark results, which will potentially translate into increased performance of production workloads.
 </p>
 <p>
  This guide provides information and examples of the various system configuration attributes, both hardware and software, and how they factor into the delivered performance of GPUDirect Storage. Local drive configurations (Direct Attached Storage - DAS) and Network storage (Network Attached Storage - NAS) are covered. The benchmarking tool included when GDS is installed, gdsio, is covered and its use demonstrated.
 </p>
 <p>
  Appendix A covers benchmarking and performance in general, along with considerations when benchmarking storage systems.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-gds">
   3. Benchmarking GPUDirect Storage
  </a>
 </h2>
 <p>
  GDS enables high throughput and low latency data transfer between storage and GPU memory, which allows you to program the DMA engine of a PCIe device with the correct mappings to move data in and out of a target GPU’s memory. As such, it becomes clear the path between the GPU and the network card or storage device/controller factors significantly into delivered performance, both throughput and latency. The PCIe topology, PCIe root complex, switches and the physical location of the GPU and network and storage devices need to be examined and factored into the configuration details when benchmarking GDS. Achieving optimal performance with GDS benchmarking requires working through the PCIe topology and determining:
 </p>
 <ul>
  <li>
   which IO devices and GPUs are on the same PCIe switch or root complex
  </li>
  <li>
   which device communication paths require traversing multiple PCIe ports and possibly crossing CPU socket boundaries
  </li>
 </ul>
 <p>
  The diagram in the following section illustrates an example of PCIe topology, showing different devices across multiple PCIe switches. Determining PCIe device proximity is not necessarily an easy task, as it requires using multiple Linux utilities to correlate device names and numbers to the hierarchical numbering scheme used to identify PCIe devices, referred to as BDF notation (
  <span>
   bus:device.func
  </span>
  ) or extended BDF notation, which adds a PCIe domain identifier to the notation, as in
  <span>
   domain:bus:device.func
  </span>
  .
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>$ lspci | grep -i nvidia
36:00.0 3D controller: NVIDIA Corporation Device 20b0 (rev a1)

$ lspci -D | grep -i nvidia
0000:36:00.0 3D controller: NVIDIA Corporation Device 20b0 (rev a1)</p>
        </pre>
 <p>
  <a class="Link" data-cms-ai="0" id="benchmarking-gds__docs-internal-guid-8285073b-7fff-92b6-6c39-8963bae5d482" name="benchmarking-gds__docs-internal-guid-8285073b-7fff-92b6-6c39-8963bae5d482" shape="rect">
  </a>
  In the first example, note the standard PCIe BDF notation for the first NVIDIA GPU,
  <span>
   36:00.0
  </span>
  . In the second example, the
  <span>
   -D
  </span>
  flag was added to show the PCIe domain (extended BDF),
  <span>
   0000:36:00.0
  </span>
  .
  <a class="Link" data-cms-ai="0" id="determining-pcie-device-affinity" name="determining-pcie-device-affinity" shape="rect">
  </a>
 </p>
 <h3 id="determining-pcie-device-affinity">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#determining-pcie-device-affinity">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#determining-pcie-device-affinity" id="determining-pcie-device-affinity" name="determining-pcie-device-affinity" shape="rect">
    3.1. Determining PCIe Device Affinity
   </a>
  </a>
 </h3>
 <p>
  The examples in this section were performed on an NVIDIA® DGX-2™ system. The figure below shows a subset of the DGX-2 system architecture, illustrating the PCIe topology:
 </p>
 <p>
  Figure 1. PCIe Topology
 </p>
 <p>
  A DGX-2 system has two CPU sockets, and each socket has two PCIe trees. Each of the four PCIe trees (only one is shown above) has two levels of switches. Up to four NVMe drives hang off of the first level of switches. Each second-level switch has a connection to the first level switch, a PCIe slot that can be populated with a NIC or RAID card, and two GPUs.
 </p>
 <p>
  The commands and methodology in the following sample output apply to any system that runs Linux. The goal is to associate GPUs and NVMe drives in the PCIe hierarchy and determine which device names to use for GPUs and NVMe drives that share the same upstream PCIe switch. To resolve this issue, you must correlate Linux device names with PCIe BDF values. For the locally attached NVMe disks, here is an example that uses Linux
  <span>
   /dev/disk/by-path
  </span>
  directory entries:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; ls -l /dev/disk/by-path
total 0
lrwxrwxrwx 1 root root  9 Nov 19 12:08 pci-0000:00:14.0-usb-0:8.1:1.0-scsi-0:0:0:0 -&gt; ../../sr0
lrwxrwxrwx 1 root root  9 Nov 19 12:08 pci-0000:00:14.0-usb-0:8.2:1.0-scsi-0:0:0:0 -&gt; ../../sda
lrwxrwxrwx 1 root root 13 Nov 19 12:08 pci-0000:01:00.0-nvme-1 -&gt; ../../nvme0n1
lrwxrwxrwx 1 root root 15 Nov 19 12:08 pci-0000:01:00.0-nvme-1-part1 -&gt; ../../nvme0n1p1
lrwxrwxrwx 1 root root 15 Nov 19 12:08 pci-0000:01:00.0-nvme-1-part2 -&gt; ../../nvme0n1p2
lrwxrwxrwx 1 root root 13 Nov 19 12:08 pci-0000:05:00.0-nvme-1 -&gt; ../../nvme1n1
lrwxrwxrwx 1 root root 15 Nov 19 12:08 pci-0000:05:00.0-nvme-1-part1 -&gt; ../../nvme1n1p1
lrwxrwxrwx 1 root root 15 Nov 19 12:08 pci-0000:05:00.0-nvme-1-part2 -&gt; ../../nvme1n1p2
lrwxrwxrwx 1 root root 13 Nov 19 12:08 pci-0000:2e:00.0-nvme-1 -&gt; ../../nvme2n1
lrwxrwxrwx 1 root root 13 Nov 19 12:08 pci-0000:2f:00.0-nvme-1 -&gt; ../../nvme3n1
lrwxrwxrwx 1 root root 13 Nov 19 12:08 pci-0000:51:00.0-nvme-1 -&gt; ../../nvme4n1
lrwxrwxrwx 1 root root 13 Nov 19 12:08 pci-0000:52:00.0-nvme-1 -&gt; ../../nvme5n1
lrwxrwxrwx 1 root root 13 Nov 19 12:08 pci-0000:b1:00.0-nvme-1 -&gt; ../../nvme6n1
lrwxrwxrwx 1 root root 13 Nov 19 12:08 pci-0000:b2:00.0-nvme-1 -&gt; ../../nvme7n1
lrwxrwxrwx 1 root root 13 Nov 19 12:08 pci-0000:da:00.0-nvme-1 -&gt; ../../nvme8n1
lrwxrwxrwx 1 root root 13 Nov 19 12:08 pci-0000:db:00.0-nvme-1 -&gt; ../../nvme9n1</p>
        </pre>
 <p>
  Since the current system configuration has nvme0 and nvme1 devices configured into a RAID0 device (
  <span>
   /dev/md0
  </span>
  not shown here), the focus is on the remaining available nvme devices,
  <span>
   nvme2
  </span>
  through
  <span>
   nvme9
  </span>
  . You can get the same PCIe-to-device information for the GPUs that use the
  <span>
   nvidia-smi
  </span>
  utility and specify the GPU attributes to query:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; nvidia-smi --query-gpu=index,name,pci.domain,pci.bus,pci.device,pci.device_id,pci.sub_device_id --format=csv index, name, pci.domain, pci.bus, pci.device, pci.device_id, pci.sub_device_id
0, Tesla V100-SXM3-32GB, 0x0000, 0x34, 0x00, 0x1DB810DE, 0x12AB10DE
1, Tesla V100-SXM3-32GB, 0x0000, 0x36, 0x00, 0x1DB810DE, 0x12AB10DE
2, Tesla V100-SXM3-32GB, 0x0000, 0x39, 0x00, 0x1DB810DE, 0x12AB10DE
3, Tesla V100-SXM3-32GB, 0x0000, 0x3B, 0x00, 0x1DB810DE, 0x12AB10DE
4, Tesla V100-SXM3-32GB, 0x0000, 0x57, 0x00, 0x1DB810DE, 0x12AB10DE
5, Tesla V100-SXM3-32GB, 0x0000, 0x59, 0x00, 0x1DB810DE, 0x12AB10DE
6, Tesla V100-SXM3-32GB, 0x0000, 0x5C, 0x00, 0x1DB810DE, 0x12AB10DE
7, Tesla V100-SXM3-32GB, 0x0000, 0x5E, 0x00, 0x1DB810DE, 0x12AB10DE
8, Tesla V100-SXM3-32GB, 0x0000, 0xB7, 0x00, 0x1DB810DE, 0x12AB10DE
9, Tesla V100-SXM3-32GB, 0x0000, 0xB9, 0x00, 0x1DB810DE, 0x12AB10DE
10, Tesla V100-SXM3-32GB, 0x0000, 0xBC, 0x00, 0x1DB810DE, 0x12AB10DE
11, Tesla V100-SXM3-32GB, 0x0000, 0xBE, 0x00, 0x1DB810DE, 0x12AB10DE
12, Tesla V100-SXM3-32GB, 0x0000, 0xE0, 0x00, 0x1DB810DE, 0x12AB10DE
13, Tesla V100-SXM3-32GB, 0x0000, 0xE2, 0x00, 0x1DB810DE, 0x12AB10DE
14, Tesla V100-SXM3-32GB, 0x0000, 0xE5, 0x00, 0x1DB810DE, 0x12AB10DE
15, Tesla V100-SXM3-32GB, 0x0000, 0xE7, 0x00, 0x1DB810DE, 0x12AB10DE</p>
        </pre>
 <p>
  Use the Linux
  <span>
   lspci
  </span>
  command to tie it all together:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; lspci -tv | egrep -i "nvidia | micron"
-+-[0000:d7]-+-00.0-[d8-e7]----00.0-[d9-e7]--+-00.0-[da]----00.0  Micron Technology Inc 9200 PRO NVMe SSD
 |           |                               +-01.0-[db]----00.0  Micron Technology Inc 9200 PRO NVMe SSD
 |           |                               +-04.0-[de-e2]----00.0-[df-e2]--+-00.0-[e0]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               |                               \-10.0-[e2]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               \-0c.0-[e3-e7]----00.0-[e4-e7]--+-00.0-[e5]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                                                               \-10.0-[e7]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 +-[0000:ae]-+-00.0-[af-c7]----00.0-[b0-c7]--+-00.0-[b1]----00.0  Micron Technology Inc 9200 PRO NVMe SSD
 |           |                               +-01.0-[b2]----00.0  Micron Technology Inc 9200 PRO NVMe SSD
 |           |                               +-04.0-[b5-b9]----00.0-[b6-b9]--+-00.0-[b7]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               |                               \-10.0-[b9]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               +-0c.0-[ba-be]----00.0-[bb-be]--+-00.0-[bc]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               |                               \-10.0-[be]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               \-10.0-[bf-c7]----00.0-[c0-c7]--+-02.0-[c1]----00.0  NVIDIA Corporation Device 1ac2
 |           |                                                               +-03.0-[c2]----00.0  NVIDIA Corporation Device 1ac2
 |           |                                                               +-04.0-[c3]----00.0  NVIDIA Corporation Device 1ac2
 |           |                                                               +-0a.0-[c5]----00.0  NVIDIA Corporation Device 1ac2
 |           |                                                               +-0b.0-[c6]----00.0  NVIDIA Corporation Device 1ac2
 |           |                                                               \-0c.0-[c7]----00.0  NVIDIA Corporation Device 1ac2
 +-[0000:4e]-+-00.0-[4f-67]----00.0-[50-67]--+-00.0-[51]----00.0  Micron Technology Inc 9200 PRO NVMe SSD
 |           |                               +-01.0-[52]----00.0  Micron Technology Inc 9200 PRO NVMe SSD
 |           |                               +-04.0-[55-59]----00.0-[56-59]--+-00.0-[57]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               |                               \-10.0-[59]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               +-0c.0-[5a-5e]----00.0-[5b-5e]--+-00.0-[5c]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               |                               \-10.0-[5e]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               \-10.0-[5f-67]----00.0-[60-67]--+-02.0-[61]----00.0  NVIDIA Corporation Device 1ac2
 |           |                                                               +-03.0-[62]----00.0  NVIDIA Corporation Device 1ac2
 |           |                                                               +-04.0-[63]----00.0  NVIDIA Corporation Device 1ac2
 |           |                                                               +-0a.0-[65]----00.0  NVIDIA Corporation Device 1ac2
 |           |                                                               +-0b.0-[66]----00.0  NVIDIA Corporation Device 1ac2
 |           |                                                               \-0c.0-[67]----00.0  NVIDIA Corporation Device 1ac2
 +-[0000:2b]-+-00.0-[2c-3b]----00.0-[2d-3b]--+-00.0-[2e]----00.0  Micron Technology Inc 9200 PRO NVMe SSD
 |           |                               +-01.0-[2f]----00.0  Micron Technology Inc 9200 PRO NVMe SSD
 |           |                               +-04.0-[32-36]----00.0-[33-36]--+-00.0-[34]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               |                               \-10.0-[36]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                               \-0c.0-[37-3b]----00.0-[38-3b]--+-00.0-[39]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]
 |           |                                                               \-10.0-[3b]----00.0  NVIDIA Corporation GV100GL [Tesla V100 SXM3 32GB]</p>
        </pre>
 <p>
  In the above example, we explicitly searched for SSDs from the given vendor. To determine the manufacturer of the NVMe SSD devices on your system, simply run
  <span>
   lsblk -o NAME,MODEL
  </span>
  . Alternatively, use
  <span>
   nvme
  </span>
  as the string to match with
  <span>
   nvidia
  </span>
  .
 </p>
 <p>
  A few things to note here. First, the NVME SSD devices are grouped in pairs on each of the PCIe upstream switches, as shown in the displayed extended BDF format (left most column), showing domain zero, and Bus IDs 0xd7 (
  <span>
   0000:d7
  </span>
  ), 0xae, 0x4e and 0x2b. Also, two distinct NVIDIA device IDs are revealed (right-most column) - 0x1db8 and 0x1ac2. The 0x1db8 devices are the Tesla V100 SXM3 32GB GPUs, and the 0x1ac2 devices are NVSwitches. Our interest here is in the GPU devices, and the topology shows that there will be an optimal performance path between a pair of NVMe SSDs and four possible V100 GPUs. Given this information, we can create a RAID0 device comprised of two NVMe SSDs on the same PCIe switch, and determine which GPUs are on the same PCIe upstream switch.
 </p>
 <p>
  Starting at the top of the
  <span>
   lspci
  </span>
  output, note two NVMe drives at PCIe bus 0xda and 0xdb. The disk-by-path data indicates these are nvme8 and nvme9 devices. The four GPUs on the same segment, 0xe0, 0xe2, 0xe5 and 0xe7, are GPUs 12, 13, 14 and 15 respectively, as determined from the
  <span>
   nvidia-smi
  </span>
  output. The following table shows the PCIe GPU-to-NVMe affinity for all installed GPUs and corresponding NVMe SSD pairs.
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 1. DGX-2 GPU / NVMe Affinity (example)
  <tr class="row">
   <th class="entry" colspan="2" id="d561e313" rowspan="1" valign="top">
    See
    <span>
     nvidia-smi
    </span>
    command output
   </th>
   <th class="entry" colspan="2" id="d561e319" rowspan="1" valign="top">
    See
    <span>
     /dev/disk/by-path
    </span>
    entries
   </th>
  </tr>
  <tr class="row">
   <th class="entry" colspan="1" id="d561e328" rowspan="1" valign="top" width="25%">
    GPU #
   </th>
   <th class="entry" colspan="1" id="d561e331" rowspan="1" valign="top" width="25%">
    GPU PCIe
   </th>
   <th class="entry" colspan="1" id="d561e334" rowspan="1" valign="top" width="25%">
    NVMe #
   </th>
   <th class="entry" colspan="1" id="d561e337" rowspan="1" valign="top" width="25%">
    NVMe PCIe
   </th>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e313 d561e328" rowspan="1" valign="top" width="25%">
    0, 1, 2, 3
   </td>
   <td class="entry" colspan="1" headers="d561e313 d561e331" rowspan="1" valign="top" width="25%">
    0x34, 0x36, 0x39, 0x3b
   </td>
   <td class="entry" colspan="1" headers="d561e319 d561e334" rowspan="1" valign="top" width="25%">
    nvme2, nvme3
   </td>
   <td class="entry" colspan="1" headers="d561e319 d561e337" rowspan="1" valign="top" width="25%">
    0x2e, 0x2f
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e313 d561e328" rowspan="1" valign="top" width="25%">
    4, 5, 6, 7
   </td>
   <td class="entry" colspan="1" headers="d561e313 d561e331" rowspan="1" valign="top" width="25%">
    0x57, 0x59, 0x5c, 0x5e
   </td>
   <td class="entry" colspan="1" headers="d561e319 d561e334" rowspan="1" valign="top" width="25%">
    nvme4, nvme5
   </td>
   <td class="entry" colspan="1" headers="d561e319 d561e337" rowspan="1" valign="top" width="25%">
    0x51, 0x52
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e313 d561e328" rowspan="1" valign="top" width="25%">
    8, 9, 10, 11
   </td>
   <td class="entry" colspan="1" headers="d561e313 d561e331" rowspan="1" valign="top" width="25%">
    0xb7, 0xb9, 0xbc, 0xbe
   </td>
   <td class="entry" colspan="1" headers="d561e319 d561e334" rowspan="1" valign="top" width="25%">
    nvme6, nvme7
   </td>
   <td class="entry" colspan="1" headers="d561e319 d561e337" rowspan="1" valign="top" width="25%">
    0xb1, 0xb2
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e313 d561e328" rowspan="1" valign="top" width="25%">
    12, 13, 14, 15
   </td>
   <td class="entry" colspan="1" headers="d561e313 d561e331" rowspan="1" valign="top" width="25%">
    0xe0, 0xe2, 0xe5, 0xe7
   </td>
   <td class="entry" colspan="1" headers="d561e319 d561e334" rowspan="1" valign="top" width="25%">
    nvme8, nvme9
   </td>
   <td class="entry" colspan="1" headers="d561e319 d561e337" rowspan="1" valign="top" width="25%">
    0xda, 0xdb
   </td>
  </tr>
 </table>
 <p>
  With this information, we can configure a target workload for optimal throughput and latency, leveraging PCIe topology and device proximity of the GPUs and NVMe SSDs. This will be demonstrated in the next couple sections. Note that it is not guaranteed the actual PCIe BDF values will be the same for every NVIDIA DGX-2. This is because enumeration of the PCIe topology is based on specific configuration details and determined at boot time.
 </p>
 <p>
  The same logic applies to storage that is network attached (NAS). The network interface (NIC) becomes the “storage controller”, in terms of the data flow between the GPUs and storage. Fortunately, determining PCIe topology is a much easier task for GPUs and NICs, as the
  <span>
   nvidia-smi
  </span>
  utility includes options for generating this information. Specifically,
  <span>
   nvidia-smi topo -mp
  </span>
  generates a simple topology map in the form of a matrix showing the connection(s) at the intersection of the installed GPUs and network interfaces. For readability, the sample output below from a DGX-2 system shows the first eight columns, and the first four Mellanox device rows, not the entire table generated when executing
  <span>
   nvidia-smi topo -mp
  </span>
  .
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; nvidia-smi topo -mp
        GPU0   GPU1   GPU2   GPU3   GPU4   GPU5   GPU6   GPU7
GPU0     X     PIX    PXB    PXB    NODE   NODE   NODE   NODE
GPU1    PIX     X     PXB    PXB    NODE   NODE   NODE   NODE
GPU2    PXB    PXB     X     PIX    NODE   NODE   NODE   NODE
GPU3    PXB    PXB    PIX     X     NODE   NODE   NODE   NODE
GPU4    NODE   NODE   NODE   NODE    X     PIX    PXB    PXB
GPU5    NODE   NODE   NODE   NODE   PIX     X     PXB    PXB
GPU6    NODE   NODE   NODE   NODE   PXB    PXB     X     PIX
GPU7    NODE   NODE   NODE   NODE   PXB    PXB    PIX     X
GPU8    SYS    SYS    SYS    SYS    SYS    SYS    SYS    SYS
GPU9    SYS    SYS    SYS    SYS    SYS    SYS    SYS    SYS
GPU10   SYS    SYS    SYS    SYS    SYS    SYS    SYS    SYS
GPU11   SYS    SYS    SYS    SYS    SYS    SYS    SYS    SYS
GPU12   SYS    SYS    SYS    SYS    SYS    SYS    SYS    SYS
GPU13   SYS    SYS    SYS    SYS    SYS    SYS    SYS    SYS
GPU14   SYS    SYS    SYS    SYS    SYS    SYS    SYS    SYS
GPU15   SYS    SYS    SYS    SYS    SYS    SYS    SYS    SYS
mlx5_0  PIX    PIX    PXB    PXB    NODE   NODE   NODE   NODE
mlx5_1  PXB    PXB    PIX    PIX    NODE   NODE   NODE   NODE
mlx5_2  NODE   NODE   NODE   NODE   PIX    PIX    PXB    PXB
mlx5_3  NODE   NODE   NODE   NODE   PXB    PXB    PIX    PIX

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge</p>
        </pre>
 <p>
  The optimal path between a GPU and NIC will be one PCIe switch path designated as PIX. The least optimal path is designated as
  <span>
   SYS
  </span>
  , which indicates that the data path requires traversing the CPU-to-CPU interconnect (NUMA nodes).
 </p>
 <p>
  If you use this data when you configure and test GDS performance, the ideal setup would be, for example, a data flow from mlx5_0 to/from GPUs 0 and 1, mlx5_1 to/from GPUs 1 and 2, and so on.
 </p>
 <h3 id="gds-configuration-parameters">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-configuration-parameters">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-configuration-parameters" id="gds-configuration-parameters" name="gds-configuration-parameters" shape="rect">
    3.2. GPUDirect Storage Configuration Parameters
   </a>
  </a>
 </h3>
 <p>
  There are various parameters and settings that will factor into delivered performance. In addition to storage/filesystem-specific parameters, there are system settings and GDS-specific parameters defined in
  <span>
   /etc/cufile.json
  </span>
  .
  <a class="Link" data-cms-ai="0" id="system-parameters" name="system-parameters" shape="rect">
  </a>
 </p>
 <h3 id="system-parameters">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#system-parameters">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#system-parameters" id="system-parameters" name="system-parameters" shape="rect">
    3.2.1. System Parameters
   </a>
  </a>
 </h3>
 <p>
  On the system side, the following should be checked:
 </p>
 <ul>
  <li>
   PCIe Access Control Service (ACS)
   <p>
    PCIe ACS is a security feature for peer-to-peer transactions Each transaction is checked to determine whether peer-to-peer communication is allowed between the source and destination devices. Each such transaction must be routed through the root complex, which induces latency and impacts sustainable throughput. The best GDS performance is obtained when PCIe ACS is disabled.
   </p>
  </li>
  <li>
   IOMMU
   <p>
    The PCIe Input/Output Memory Management Unit (IOMMU) is a facility for handling address translations for IO devices, and requires routing though the PCIe root complex. Optimal GDS performance is achieved when the IOMMU is disabled.
   </p>
  </li>
 </ul>
 <h3 id="gds-parameters">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-parameters">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-parameters" id="gds-parameters" name="gds-parameters" shape="rect">
    3.2.2. GPUDirect Storage Parameters
   </a>
  </a>
 </h3>
 <p>
  This section describes the JSON configuration parameters used by GDS.
 </p>
 When GDS is installed, the
 <span>
  /etc/cufile.json
 </span>
 parameter file is installed with default values. The implementation allows for generic GDS settings and parameters specific to a file system or storage partner.
 Note:
 <p>
  Consider
  <span>
   compat_mode
  </span>
  for systems or mounts that are not yet set up with GDS support.
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 2. GPUDirect Storage cufile.json Variables
  <tr class="row">
   <th class="entry" colspan="1" id="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    Parameter
   </th>
   <th class="entry" colspan="1" id="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    Default Value
   </th>
   <th class="entry" colspan="1" id="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Description
   </th>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     logging:dir
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    CWD
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Location of the GDS log file.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     logging:level
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    ERROR
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Verbosity of logging.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     profile:nvtx
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    false
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Boolean which if set to true, generates NVTX traces for profiling.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     profile:cufile_stats
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    0
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Enable cuFile IO stats. Level 0 means no cuFile statistics.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     profile:io_batchsize
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    128
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Maximum size of the batch allowed.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:max_direct_io_size_kb
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    16384
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Maximum IO chunk size (4K aligned) used by cuFile for each IO request (in KB).
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:max_device_cache_size_kb
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    131072
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Maximum device memory size (4K aligned) for reserving bounce buffers for the entire GPU (in KB).
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:max_device_pinned_mem_size_kb
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    33554432
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Maximum per-GPU memory size in KB, including the memory for the internal bounce buffers, that can be pinned.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:use_poll_mode
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    false
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Boolean that indicates whether the cuFile library uses polling or synchronous wait for the storage to complete IO. Polling might be useful for small IO transactions. Refer to
    Poll Mode
    below.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:poll_mode_max_size_kb
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    4
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Maximum IO request size (4K aligned) in or equal to which library will be polled (in KB).
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties.force_compat_mode
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    false
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    If true, this option can be used to force all IO to use compatibility mode. Alternatively the admin can unload the
    <span>
     nvidia_fs.ko
    </span>
    or not expose the character devices in the docker container environment.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:allow_compat_mode
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    false
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    If true, enables the compatibility mode, which allows cuFile to issue POSIX read/write. To switch to GDS-enabled I/O, set this to
    <span>
     false
    </span>
    . Refer to
    Compatibility Mode
    below.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:rdma_dev_addr_list
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    []
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Provides the list of relevant client IPv4 addresses for all the interfaces that can be used for RDMA.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:rdma_load_balancing_policy
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    RoundRobin
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Specifies the load balancing policy for RDMA memory registration. By default, this value is set to RoundRobin. Here are the valid values that can be used for this property:
    <p>
     <a class="Link" data-cms-ai="0" id="gds-parameters__docs-internal-guid-97c2d14c-7fff-b923-5f1a-2b969aae1e02" name="gds-parameters__docs-internal-guid-97c2d14c-7fff-b923-5f1a-2b969aae1e02" shape="rect">
     </a>
     <span>
      FirstFit
     </span>
     - Suitable for cases where
     <span>
      numGpus
     </span>
     matches
     <span>
      numPeers
     </span>
     and GPU PCIe lane width is greater or equal to the peer PCIe lane width.
    </p>
    <p>
     <span>
      MaxMinFit
     </span>
     - This will try to assign peers in a manner that there is least sharing. Suitable for cases, where all GPUs are loaded uniformly.
    </p>
    <p>
     <a class="Link" data-cms-ai="0" id="gds-parameters__docs-internal-guid-01bd279f-7fff-7b2d-e6cc-3742981dd08c" name="gds-parameters__docs-internal-guid-01bd279f-7fff-7b2d-e6cc-3742981dd08c" shape="rect">
     </a>
     <span>
      RoundRobin
     </span>
     - This parameter uses only the NICs that are the closest to the GPU for memory registration in a round robin fashion.
    </p>
    <p>
     <span>
      RoundRobinMaxMin
     </span>
     - Similar to RoundRobin but uses peers with least sharing.
    </p>
    <p>
     <span>
      Randomized
     </span>
     - This parameter uses only the NICs that are the closest to the GPU for memory registration in a randomized fashion.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:rdma_dynamic_routing
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    false
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Boolean parameter applicable only to Network Based File Systems. This could be enabled for platforms where GPUs and NICs do not share a common PCIe-root port.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:rdma_dynamic_routing_order
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    <span>
     [ “GPU_MEM_NVLINKS”, “GPU_MEM”, “SYS_MEM”, “P2P” ]
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    The routing order applies only if
    <span>
     rdma_dynamic_routing
    </span>
    is enabled. Users can specify an ordered list of routing policies selected when routing an IO on a first-fit basis.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:io_batchsize
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    128
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    <p>
     <a class="Link" data-cms-ai="0" id="gds-parameters__docs-internal-guid-f6cbf525-7fff-ad89-5f0e-d84b436b129f" name="gds-parameters__docs-internal-guid-f6cbf525-7fff-ad89-5f0e-d84b436b129f" shape="rect">
     </a>
     The max number of IO operations per batch.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:gds_rdma_write_support
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    true
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Enable GDS write support for RDMA based storage.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     properties:io_priority
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    default
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    <p>
     <a class="Link" data-cms-ai="0" id="gds-parameters__docs-internal-guid-6bd4ebe1-7fff-85d9-7f98-22db5ad05dc4" name="gds-parameters__docs-internal-guid-6bd4ebe1-7fff-85d9-7f98-22db5ad05dc4" shape="rect">
     </a>
     Enable io priority w.r.t. compute streams
    </p>
    <p>
     Valid options are “default”, “low”, “med”, “high”
    </p>
    <p>
     Tuning this might be helpful in cases where
     <span>
      cudaMemcpy
     </span>
     is not performing as expected because of the GPU being consumed by the compute.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     fs:generic:posix_unaligned_writes
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    false
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Setting to
    <span>
     true
    </span>
    forces the use of a POSIX write instead of cuFileWrite for unaligned writes.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     fs:lustre:posix_gds_min_kb
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    4KB
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Applicable only for the EXAScaler filesystem. This is applicable for reads and writes. IO threshold for read/write (4K aligned) that is equal to or below the threshold that cufile will use for a POSIX read/write.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     fs:lustre:rdma_dev_addr_list
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    []
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Provides the list of relevant client IPv4 addresses for all the interfaces that can be used by a single lustre mount. This property is used by the cuFile dynamic routing feature to infer preferred RDMA devices.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     fs:lustre:mount_table
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    []
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Specifies a dictionary of IPv4 mount addresses against a Lustre mount point.This property is used by the cuFile dynamic routing feature. Refer to the default
    <span>
     cufile.json
    </span>
    for sample usage.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     fs:nfs:rdma_dev_addr_list
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    []
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Provides the list of IPv4 addresses for all the interfaces a single NFS mount can use. This property is used by the cuFile dynamic routing feature to infer preferred RDMA devices.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     fs:nfs:mount_table
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    []
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Specifies a dictionary of IPv4 mount addresses against a Lustre mount point. This property is used by the cuFile dynamic routing feature. Refer to the default
    <span>
     cufile.json
    </span>
    for sample usage.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     fs:weka:rdma_write_support
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    false
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    If set to true, cuFileWrite will use RDMA writes instead of falling back to posix writes for a WekaFs mount.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     fs:weka:&lt;rdma_dev_addr_list&gt;
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    []
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Provides the list of relevant client IPv4 addresses for all the interfaces a single WekaFS mount can use. This property is also used by the cuFile dynamic routing feature to infer preferred rdma devices.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     fs:weka:mount_table
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    []
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Specifies a dictionary of IPv4 mount addresses against a WekaFS mount point. This property is used by the cuFile dynamic routing feature. Refer to the default
    <span>
     cufile.json
    </span>
    for sample usage.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     denylist:drivers
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    []
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Administrative setting that disables supported storage drivers on the node.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     denylist:devices
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    []
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    <p>
     Administrative setting that disables specific supported block devices on the node.
    </p>
    <p>
     Not applicable for DFS.
    </p>
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     denylist:mounts
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    []
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Administrative setting that disables specific mounts in the supported GDS-enabled filesystems on the node.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     denylist:filesystems
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    []
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Administrative setting that disables specific supported GDS-ready filesystems on the node.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     miscellaneous:skip_topology_detection
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    false
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Setting this to true will skip topology detection in compat mode. This will reduce the high startup latency seen in compat mode on systems with multiple PCI devices.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     execution::max_io_queue_depth
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    128
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    This specifies the maximum number of pending work items that can be held by the cuFile library’s internal threadpool sub-system.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     execution::max_io_threads
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    4
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    This specifies the number of threadpool threads that can process work items produced into a work queue corresponding to a single GPU on the system.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     execution::parallel_io
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    true
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    Setting this to true will allow parallel processing of work items by enqueuing into the threadpool subsystem provided by the cuFile library
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     execution::min_io_threshold_size_kb
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    8192
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    This option specifies the size in KB that the I/O work item submitted by the application would be split into, when enqueuing into threadpool sub-system, provided there are enough parallel buffers available.
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e527" rowspan="1" valign="top" width="53.443113772455085%">
    <span>
     execution::max_request_parallelism
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e530" rowspan="1" valign="top" width="14.970059880239523%">
    4
   </td>
   <td class="entry" colspan="1" headers="d561e533" rowspan="1" valign="top" width="31.58682634730539%">
    This number specifies the maximum number of parallel buffers available, that the original I/O work item buffer can be split into, when enqueuing into the threadpool sub-system.
   </td>
  </tr>
 </table>
 Note:
 <p>
  Workload/application-specific parameters can be set by using the
  <span>
   CUFILE_ENV_PATH_JSON
  </span>
  environment variable that is set to point to an alternate
  <span>
   cufile.json
  </span>
  file, for example,
  <span>
   CUFILE_ENV_PATH_JSON=/home/gds_user/my_cufile.json
  </span>
  .
 </p>
 There are two mode types that you can set in the
 <span>
  cufile.json
 </span>
 configuration file:
 <ul>
  <li>
   Poll Mode
   <p>
    The cuFile API set includes an interface to put the driver in polling mode. Refer to
    <span>
     cuFileDriverSetPollMode()
    </span>
    in the
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/cuda/cufile-api/index.html" shape="rect">
     cuFile API Reference Guide
    </a>
    for more information. When the poll mode is set, a read or write issued that is less than or equal to
    <span>
     properties:poll_mode_max_size_kb
    </span>
    (4KB by default) will result in the library polling for IO completion, rather than blocking (sleep). For small IO size workloads, enabling poll mode may reduce latency.
   </p>
  </li>
  <li>
   Compatibility Mode
   <p>
    There are several possible scenarios where GDS might not be available or supported, for example, when the GDS software is not installed, the target file system is not GDS supported,
    <span>
     O_DIRECT
    </span>
    cannot be enabled on the target file, and so on. When you enable compatibility mode, and GDS is not functional for the IO target, the code that uses the cuFile APIs fall backs to the standard POSIX read/write path. To learn more about compatibility mode, refer to
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/cuda/cufile-api/index.html#cufile-compatibility-mode" shape="rect">
     cuFile Compatibility Mode
    </a>
    .
   </p>
  </li>
 </ul>
 <p>
  From a benchmarking and performance perspective, the default settings work very well across a variety of IO loads and use cases. We recommended that you use the default values for
  <span>
   max_direct_io_size_kb
  </span>
  ,
  <span>
   max_device_cache_size_kb
  </span>
  , and
  <span>
   max_device_pinned_mem_size_kb
  </span>
  unless a storage provider has a specific recommendation, or analysis and testing show better performance after you change one or more of the defaults.
 </p>
 <p>
  The
  <span>
   cufile.json
  </span>
  file has been designed to be extensible such that parameters can be set that are either generic and apply to all supported file systems (
  <span>
   fs:generic
  </span>
  ), or file system specific (
  <span>
   fs:lustre
  </span>
  ). The
  <span>
   fs:generic:posix_unaligned_writes
  </span>
  parameter enables the use of the POSIX write path when unaligned writes are encountered. Unaligned writes are generally sub-optimal, as they can require read-modify-write operations.
 </p>
 <p>
  If the target workload generates unaligned writes, you might want to set
  <span>
   posix_unaligned_writes
  </span>
  to true, as the POSIX path for handling unaligned writes might be more performant, depending on the target filesystem and underlying storage. Also, in this case, the POSIX path will write to the page cache (system memory).
 </p>
 <p>
  When the IO size is less than or equal to
  <span>
   posix_gds_min_kb
  </span>
  , the
  <span>
   fs:lustre:posix_gds_min_kb
  </span>
  setting invokes the POSIX read/write path rather than cuFile path. When using Lustre, for small IO sizes, the POSIX path can have better (lower) latency.
 </p>
 <p>
  The GDS parameters are among several elements that factor into delivered storage IO performance. It is advisable to start with the defaults and only make changes based on recommendations from a storage vendor or based on empirical data obtained during testing and measurements of the target workload. The following is the JSON schema:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p># /etc/cufile.json
{
  "logging": {
    // log directory, if not enabled will create log file
    // under current working directory
    //"dir": "/home/&lt;xxxx&gt;",
    // ERROR|WARN|INFO|DEBUG|TRACE (in decreasing order of priority)
         
     "level": "ERROR"
  },
         
  "profile": {
    // nvtx profiling on/off
    "nvtx": false,
    // cufile stats level(0-3)
    "cufile_stats": 0
   },
   
"execution" : {
                    // max number of workitems in the queue;
                    "max_io_queue_depth": 128,
                    // max number of host threads per gpu to spawn for parallel IO
                    "max_io_threads" : 4,
                    // enable support for parallel IO
                    "parallel_io" : true,
                    // minimum IO threshold before splitting the IO
                    "min_io_threshold_size_kb" :8192,
                    // maximum parallelism for a single request
                    "max_request_parallelism" : 4
            },
     
   "properties": {
     // max IO size (4K aligned) issued by cuFile to nvidia-fs driver(in KB)
     "max_direct_io_size_kb" : 16384,
     // device memory size (4K aligned) for reserving bounce buffers
     // for the entire GPU (in KB)
     "max_device_cache_size_kb" : 131072,
     // limit on maximum memory (4K aligned) that can be pinned
     // for a given process (in KB)
     "max_device_pinned_mem_size_kb" : 33554432,
     // true or false (true will enable asynchronous io submission to nvidia-fs driver)
     "use_poll_mode" : false,
     // maximum IO request size (4K aligned) within or equal
     // to which library will poll (in KB)
     "poll_mode_max_size_kb": 4,
     // allow compat mode, this will enable use of cufile posix read/writes
     "allow_compat_mode": false,
     // client-side rdma addr list for user-space file-systems
        
     // (e.g ["10.0.1.0", "10.0.2.0"])
     "rdma_dev_addr_list": [ ]
   },
        
   "fs": {
     "generic": {
        // for unaligned writes, setting it to true 
        // will use posix write instead of cuFileWrite
        
        "posix_unaligned_writes" : false
      },
         
      "lustre": {
        // IO threshold for read/write (4K aligned)) equal to or below 
        // which cufile will use posix reads (KB)
        "posix_gds_min_kb" : 0
      }
    },
        
    "blacklist": {
      // specify list of vendor driver modules to blacklist for nvidia-fs
      "drivers": [ ],
      // specify list of block devices to prevent IO using libcufile
      "devices": [ ],
      // specify list of mount points to prevent IO using libcufile
      // (e.g. ["/mnt/test"])
      "mounts": [ ],
      // specify list of file-systems to prevent IO using libcufile
      // (e.g ["lustre", "wekafs", "vast"])
      "filesystems": [ ]
    }
    // Application can override custom configuration via 
    // export CUFILE_ENV_PATH_JSON=&lt;filepath&gt;
    // e.g : export CUFILE_ENV_PATH_JSON="/home/&lt;xxx&gt;/cufile.json"
  }</p>
        </pre>
 <h3 id="gds-benchmarking-tools">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-tools">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-tools" id="gds-benchmarking-tools" name="gds-benchmarking-tools" shape="rect">
    3.3. GPUDirect Storage Benchmarking Tools
   </a>
  </a>
 </h3>
 <p>
  There are several storage benchmarking tools and utilities for Linux systems, with varying degrees of features and functionality. The
  <a class="xref" data-cms-ai="0" href="https://linux.die.net/man/1/fio" shape="rect">
   fio
  </a>
  utility is one of the more popular and powerful tools that is used to generate storage IO loads and offers significant flexibility for tuning IO generation based on the desired IO load characteristics. For those familiar with fio on Linux systems, the use of
  <span>
   gdsio
  </span>
  will be very intuitive.
 </p>
 <p>
  Since GDS is relatively new technology, with support dependencies and a specific set of libraries and APIs that fall outside standard POSIX IO APIs, none of the existing storage IO load generation utilities include GDS support. As a result, the installation of GDS includes the
  <span>
   gdsio
  </span>
  load generator which provides several command line options that enable generating various storage IO load characteristics via both the traditional CPU and the GDS data path.
  <a class="Link" data-cms-ai="0" id="gdsio" name="gdsio" shape="rect">
  </a>
 </p>
 <h3 id="gdsio">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gdsio">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gdsio" id="gdsio" name="gdsio" shape="rect">
    3.3.1. gdsio Utility
   </a>
  </a>
 </h3>
 <p>
  The
  <span>
   gdsio
  </span>
  utility is similar to a number of disk/storage IO load generating tools. It supports a series of command line arguments to specify the target files, file sizes, IO sizes, number of IO threads, etc. Additionally,
  <span>
   gdsio
  </span>
  includes built-in support for using the traditional IO path (CPU), as well as the GDS path - storage to/from GPU memory.
 </p>
 <p>
  Starting 12.2, the tool also supports three new memory (-m &lt;2, 3, 4&gt;) types to exercise the host memory support option using cuFile APIs. The new memory type option is only supported with certain transfer modes i.e. -x (0, 5, 6, 7). Additionally support for non O_DIRECT file descriptors is also introduced. It can be specified by the option -O 1. By default the
  <span>
   gdsio
  </span>
  utility works with O_DIRECT file descriptors which is represented by -O 0, although that need not be specified explicitly.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; ./gdsio --help
gdsio version :1.1
Usage [using config file]: gdsio rw-sample.gdsio
Usage [using cmd line options]:./gdsio
         -f &lt;file name&gt;
         -D &lt;directory name&gt;
         -d &lt;gpu_index (refer nvidia-smi)&gt;
         -n &lt;numa node&gt;
         -m &lt;memory type(0 - (cudaMalloc), 1 - (cuMem), 2 - (cudaMallocHost), 3 - (malloc) 4 - (mmap))&gt;
         -w &lt;number of threads for a job&gt;
         -s &lt;file size(K|M|G)&gt;
         -o &lt;start offset(K|M|G)&gt;
         -i &lt;io_size(K|M|G)&gt; &lt;min_size:max_size:step_size&gt;
         -p &lt;enable nvlinks&gt;
         -b &lt;skip bufregister&gt;
         -o &lt;start file offset&gt;
         -V &lt;verify IO&gt;
         -x &lt;xfer_type&gt;
         -I &lt;(read) 0|(write)1| (randread) 2| (randwrite) 3&gt;
         -T &lt;duration in seconds&gt;
         -k &lt;random_seed&gt; (number e.g. 3456) to be used with random read/write&gt;
         -U &lt;use unaligned(4K) random offsets&gt;
         -R &lt;fill io buffer with random data&gt;
         -F &lt;refill io buffer with random data during each write&gt;
         -B

xfer_type:
0 - Storage-&gt;GPU (GDS)
1 - Storage-&gt;CPU
2 - Storage-&gt;CPU-&gt;GPU
3 - Storage-&gt;CPU-&gt;GPU_ASYNC
4 - Storage-&gt;PAGE_CACHE-&gt;CPU-&gt;GPU
5 - Storage-&gt;GPU_ASYNC_STREAM
6 - Storage-&gt;GPU_BATCH
7 - Storage-&gt;GPU_BATCH_STREAM


Note:
read test (-I 0) with verify option (-V) should be used with files written (-I 1) with -V option
read test (-I 2) with verify option (-V) should be used with files written (-I 3) with -V option, using same random seed (-k),
same number of threads(-w), offset(-o), and data size(-s)
write test (-I 1/3) with verify option (-V) will perform writes followed by read</p>
        </pre>
 <p>
  These
  <span>
   gdsio
  </span>
  options provide the necessary flexibility to construct IO tests based on a specific set of requirements, and/or simply to assess performance for several different load types. Important to note that when using the
  <span>
   -D
  </span>
  flag to specify a target directory,
  <span>
   gdsio
  </span>
  must first execute write loads (
  <span>
   -I 1
  </span>
  or
  <span>
   -I 3
  </span>
  ) to create the files. The number of files created is based on the thread count (
  <span>
   -w
  </span>
  flag); 1 file is created for each thread. This is an alternative to using the
  <span>
   -f
  </span>
  flag where file pathnames are specified. The
  <span>
   -D
  </span>
  and
  <span>
   -f
  </span>
  flags cannot be used together.
 </p>
 The transfer types (
 <span>
  -x
 </span>
 flag) are further defined in the following table:
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 3. gdsio Data Path Transfer Options
  <tr class="row">
   <th class="entry" colspan="1" id="d561e1336" rowspan="1" valign="top" width="4.130524576621231%">
    x
   </th>
   <th class="entry" colspan="1" id="d561e1339" rowspan="1" valign="top" width="28.211482858323013%">
    Transfer Type
   </th>
   <th class="entry" colspan="1" id="d561e1342" rowspan="1" valign="top" width="13.630731102850064%">
    File Open O_DIRECT?
   </th>
   <th class="entry" colspan="1" id="d561e1345" rowspan="1" valign="top" width="21.767864518793886%">
    Host Memory Allocation Type
   </th>
   <th class="entry" colspan="1" id="d561e1348" rowspan="1" valign="top" width="22.883106154481624%">
    Device Memory Allocation Type
   </th>
   <th class="entry" colspan="1" id="d561e1352" rowspan="1" valign="top" width="9.376290788930195%">
    Copies
   </th>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e1336" rowspan="1" valign="top" width="4.130524576621231%">
    0
   </td>
   <td class="entry" colspan="1" headers="d561e1339" rowspan="1" valign="top" width="28.211482858323013%">
    XFER_GPU_DIRECT
   </td>
   <td class="entry" colspan="1" headers="d561e1342" rowspan="1" valign="top" width="13.630731102850064%">
    Yes
   </td>
   <td class="entry" colspan="1" headers="d561e1345" rowspan="1" valign="top" width="21.767864518793886%">
    <span>
     cudaMallocHost
    </span>
    ,
    <span>
     malloc
    </span>
    or
    <span>
     mmap
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e1348" rowspan="1" valign="top" width="22.883106154481624%">
    <span>
     cudaMalloc()/cuMemMap()
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e1352" rowspan="1" valign="top" width="9.376290788930195%">
    Zero copy
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e1336" rowspan="1" valign="top" width="4.130524576621231%">
    1
   </td>
   <td class="entry" colspan="1" headers="d561e1339" rowspan="1" valign="top" width="28.211482858323013%">
    XFER_CPU_ONLY
   </td>
   <td class="entry" colspan="1" headers="d561e1342" rowspan="1" valign="top" width="13.630731102850064%">
    Yes
   </td>
   <td class="entry" colspan="1" headers="d561e1345" rowspan="1" valign="top" width="21.767864518793886%">
    Posix_mem_align (4k)
   </td>
   <td class="entry" colspan="1" headers="d561e1348" rowspan="1" valign="top" width="22.883106154481624%">
    N/A
   </td>
   <td class="entry" colspan="1" headers="d561e1352" rowspan="1" valign="top" width="9.376290788930195%">
    Zero copy
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e1336" rowspan="1" valign="top" width="4.130524576621231%">
    2
   </td>
   <td class="entry" colspan="1" headers="d561e1339" rowspan="1" valign="top" width="28.211482858323013%">
    XFER_CPU_GPU
   </td>
   <td class="entry" colspan="1" headers="d561e1342" rowspan="1" valign="top" width="13.630731102850064%">
    Yes
   </td>
   <td class="entry" colspan="1" headers="d561e1345" rowspan="1" valign="top" width="21.767864518793886%">
    <span>
     cudaMallocHost()
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e1348" rowspan="1" valign="top" width="22.883106154481624%">
    <p>
     <span>
      cudaMalloc()
     </span>
    </p>
    <p>
     (use of multiple CUDA streams for
     <span>
      cuMemcpyAsync()
     </span>
    </p>
   </td>
   <td class="entry" colspan="1" headers="d561e1352" rowspan="1" valign="top" width="9.376290788930195%">
    One copy
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e1336" rowspan="1" valign="top" width="4.130524576621231%">
    3
   </td>
   <td class="entry" colspan="1" headers="d561e1339" rowspan="1" valign="top" width="28.211482858323013%">
    XFER_CPU_ASYNC_GPU
   </td>
   <td class="entry" colspan="1" headers="d561e1342" rowspan="1" valign="top" width="13.630731102850064%">
    Yes
   </td>
   <td class="entry" colspan="1" headers="d561e1345" rowspan="1" valign="top" width="21.767864518793886%">
    <span>
     cudaMallocManaged()
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e1348" rowspan="1" valign="top" width="22.883106154481624%">
    <p>
     <span>
      cudaMallocManaged()
     </span>
    </p>
    <p>
     (use
     <span>
      cuMemAdvise()
     </span>
     to set hints for managed memory +
     <span>
      cuMemcpyPrefetchAsync()
     </span>
     )
    </p>
   </td>
   <td class="entry" colspan="1" headers="d561e1352" rowspan="1" valign="top" width="9.376290788930195%">
    One copy (streaming buffer)
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e1336" rowspan="1" valign="top" width="4.130524576621231%">
    4
   </td>
   <td class="entry" colspan="1" headers="d561e1339" rowspan="1" valign="top" width="28.211482858323013%">
    XFER_CPU_CACHED_GPU
   </td>
   <td class="entry" colspan="1" headers="d561e1342" rowspan="1" valign="top" width="13.630731102850064%">
    No (use page cache)
   </td>
   <td class="entry" colspan="1" headers="d561e1345" rowspan="1" valign="top" width="21.767864518793886%">
    <span>
     cudaMallocHost()
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e1348" rowspan="1" valign="top" width="22.883106154481624%">
    <p>
     <span>
      cudaMalloc()
     </span>
    </p>
    <p>
     (use of multiple CUDA streams for
     <span>
      cuMemcpyAsync()
     </span>
     )
    </p>
   </td>
   <td class="entry" colspan="1" headers="d561e1352" rowspan="1" valign="top" width="9.376290788930195%">
    Two copies
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e1336" rowspan="1" valign="top" width="4.130524576621231%">
    5
   </td>
   <td class="entry" colspan="1" headers="d561e1339" rowspan="1" valign="top" width="28.211482858323013%">
    XFER_GPU_DIRECT_ASYNC
   </td>
   <td class="entry" colspan="1" headers="d561e1342" rowspan="1" valign="top" width="13.630731102850064%">
    Yes
   </td>
   <td class="entry" colspan="1" headers="d561e1345" rowspan="1" valign="top" width="21.767864518793886%">
    <span>
     cudaMallocHost
    </span>
    ,
    <span>
     malloc
    </span>
    or
    <span>
     mmap
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e1348" rowspan="1" valign="top" width="22.883106154481624%">
    <span>
     cudaMalloc()/cuMemMap()
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e1352" rowspan="1" valign="top" width="9.376290788930195%">
    Zero copy
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e1336" rowspan="1" valign="top" width="4.130524576621231%">
    6
   </td>
   <td class="entry" colspan="1" headers="d561e1339" rowspan="1" valign="top" width="28.211482858323013%">
    XFER_GPU_BATCH
   </td>
   <td class="entry" colspan="1" headers="d561e1342" rowspan="1" valign="top" width="13.630731102850064%">
    Yes
   </td>
   <td class="entry" colspan="1" headers="d561e1345" rowspan="1" valign="top" width="21.767864518793886%">
    <span>
     cudaMallocHost
    </span>
    ,
    <span>
     malloc
    </span>
    or
    <span>
     mmap
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e1348" rowspan="1" valign="top" width="22.883106154481624%">
    <span>
     cudaMalloc()/cuMemMap()
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e1352" rowspan="1" valign="top" width="9.376290788930195%">
    Zero copy
   </td>
  </tr>
  <tr class="row">
   <td class="entry" colspan="1" headers="d561e1336" rowspan="1" valign="top" width="4.130524576621231%">
    7
   </td>
   <td class="entry" colspan="1" headers="d561e1339" rowspan="1" valign="top" width="28.211482858323013%">
    XFER_GPU_BATCH_STREAM
   </td>
   <td class="entry" colspan="1" headers="d561e1342" rowspan="1" valign="top" width="13.630731102850064%">
    Yes
   </td>
   <td class="entry" colspan="1" headers="d561e1345" rowspan="1" valign="top" width="21.767864518793886%">
    <span>
     cudaMallocHost
    </span>
    ,
    <span>
     malloc
    </span>
    or
    <span>
     mmap
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e1348" rowspan="1" valign="top" width="22.883106154481624%">
    <span>
     cudaMalloc()/cuMemMap()
    </span>
   </td>
   <td class="entry" colspan="1" headers="d561e1352" rowspan="1" valign="top" width="9.376290788930195%">
    Zero copy
   </td>
  </tr>
 </table>
 <p>
  * Starting from GDS 1.7 release, if the poll mode is enabled through
  <span>
   cufile.json
  </span>
  then it will use poll mode. Otherwise, the
  <span>
   XFER_GPU_DIRECT_ASYNC
  </span>
  option would exercise stream-based async I/O mechanism
 </p>
 <p>
  Similar to the Linux
  <span>
   fio
  </span>
  storage load generator,
  <span>
   gdsio
  </span>
  supports the use of config files that contain the parameter values to use for a
  <span>
   gdsio
  </span>
  execution. This offers an alternative to lengthy command line strings, and the ability to build a collection of config files that can easily be reused for testing different configurations and workloads. The
  <span>
   gdsio
  </span>
  config file syntax supports global parameters, as well as individual job parameters. There are sample
  <span>
   gdsio
  </span>
  config files installed with GDS in
  <span>
   /usr/local/cuda/gds/tools
  </span>
  . The files with the
  <span>
   .gdsio
  </span>
  extension are sample
  <span>
   gdsio
  </span>
  config files, and the README included in the same directory provides additional information on the command line and config file syntax for
  <span>
   gdsio
  </span>
  .
 </p>
 <p>
  With these options and the support of parameter config files, it is a relatively simple process to run
  <span>
   gdsio
  </span>
  and assess performance using different data paths to/from GPU/CPU memory.
 </p>
 <h3 id="gds-stats">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-stats">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-stats" id="gds-stats" name="gds-stats" shape="rect">
    3.3.2. gds-stats Tool
   </a>
  </a>
 </h3>
 <p>
  The
  <span>
   gds_stats
  </span>
  tool is used to extract per-process statistics on the GDS IO. It can be used in conjunction with other generic tools (Linux
  <span>
   iostat
  </span>
  ), and GPU-specific tools (
  <span>
   nvidia-smi
  </span>
  , the Data Center GPU Manager (DCGM) command line tool,
  <span>
   dcgmi
  </span>
  ) to get a complete picture of data flow on the target system.
 </p>
 <p>
  To use
  <span>
   gds_stats
  </span>
  , the
  <span>
   profile:cufile_stats
  </span>
  attribute in
  <span>
   /etc/cufile.json
  </span>
  must
  be set to 1, 2 or 3.
 </p>
 Note:
 <p>
  The default value of 0 disables statistics collection.
 </p>
 <p>
  The different levels provide an increasing amount of statistical data. When
  <span>
   profile:cufile_stats
  </span>
  is set to 3 (max level), the
  <span>
   gds_stats
  </span>
  utility provides a
  <span>
   -l
  </span>
  (level) CLI flag. Even when GDS is collecting level 3 stats, only level 1 or level 2 stats can be displayed.
 </p>
 <p>
  In the example below, a
  <span>
   gdsio
  </span>
  job is started in the background, and level 3
  <span>
   gds_stats
  </span>
  are extracted:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; gdsio -D /nvme23/gds_dir -d 2 -w 8 -s 1G -i 1M -x 0 -I 0 -T 300 &amp;
[1] 850272
dgx2&gt; gds_stats -p 850272 -l 3
cuFile STATS VERSION : 3
GLOBAL STATS:
Total Files: 8
Total Read Errors : 0
Total Read Size (MiB): 78193
Read BandWidth (GiB/s): 6.32129
Avg Read Latency (us): 1044
Total Write Errors : 0
Total Write Size (MiB): 0
Write BandWidth (GiB/s): 0
Avg Write Latency (us): 0
READ-WRITE SIZE HISTOGRAM :
0-4(KiB): 0  0
4-8(KiB): 0  0
8-16(KiB): 0  0
16-32(KiB): 0  0
32-64(KiB): 0  0
64-128(KiB): 0  0
128-256(KiB): 0  0
256-512(KiB): 0  0
512-1024(KiB): 0  0
1024-2048(KiB): 78193  0
2048-4096(KiB): 0  0
4096-8192(KiB): 0  0
8192-16384(KiB): 0  0
16384-32768(KiB): 0  0
32768-65536(KiB): 0  0
65536-...(KiB): 0  0
PER_GPU STATS:
GPU 0 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 r_sparse=0 r_inline=0 err=0 MiB=0 Write: bw=0 util(%)=0 n=0 posix=0 unalign=0 err=0 MiB=0 BufRegister: n=0 err=0 free=0 MiB=0
GPU 1 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 r_sparse=0 r_inline=0 err=0 MiB=0 Write: bw=0 util(%)=0 n=0 posix=0 unalign=0 err=0 MiB=0 BufRegister: n=0 err=0 free=0 MiB=0
GPU 2 Read: bw=6.32129 util(%)=797 n=78193 posix=0 unalign=0 r_sparse=0 r_inline=0 err=0 MiB=78193 Write: bw=0 util(%)=0 n=0 posix=0 unalign=0 err=0 MiB=0 BufRegister: n=8 err=0 free=0 MiB=8
GPU 3 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 r_sparse=0 r_inline=0 err=0 MiB=0 Write: bw=0 util(%)=0 n=0 posix=0 unalign=0 err=0 MiB=0 BufRegister: n=0 err=0 free=0 MiB=0
. . .
GPU 15 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 r_sparse=0 r_inline=0 err=0 MiB=0 Write: bw=0 util(%)=0 n=0 posix=0 unalign=0 err=0 MiB=0 BufRegister: n=0 err=0 free=0 MiB=0
PER_GPU POOL BUFFER STATS:
PER_GPU POSIX POOL BUFFER STATS:
GPU 0 4(KiB) :0/0 1024(KiB) :0/0 16384(KiB) :0/0
GPU 1 4(KiB) :0/0 1024(KiB) :0/0 16384(KiB) :0/0
GPU 2 4(KiB) :0/0 1024(KiB) :0/0 16384(KiB) :0/0
. . .
GPU 14 4(KiB) :0/0 1024(KiB) :0/0 16384(KiB) :0/0
GPU 15 4(KiB) :0/0 1024(KiB) :0/0 16384(KiB) :0/0

PER_GPU RDMA STATS:
GPU 0000:34:00.0 :
GPU 0000:36:00.0 :
. . .
GPU 0000:39:00.0 :
GPU 0000:e5:00.0 :
GPU 0000:e7:00.0 :

RDMA MRSTATS:
peer name   nr_mrs      mr_size(MiB)</p>
        </pre>
 <p>
  Here are the levels of
  <span>
   gds_stats
  </span>
  that are captured and displayed:
 </p>
 <ul>
  <li>
   Level 3
   .
   <p>
    Shown above, includes (tarting at the top), a summary section, GLOBAL STATS, followed by a READ-WRITE SIZE HISTOGRAM section, PER_GPU STATS, PER_GPU POOL BUFFER STATS, PER_GPU POSIX POOL BUFFER STATS, PER_GPU RDMA STATS and RDMA MRSTATS.
   </p>
  </li>
  <li>
   Level 2
   <p>
    The GLOBAL STATS and READ-WRITE SIZE HISTOGRAM sections.
   </p>
  </li>
  <li>
   Level 1
   <p>
    GLOBAL STATS.
   </p>
  </li>
 </ul>
 <p>
  These are described as:
 </p>
 <ul>
  <li>
   <p>
    GLOBAL STATS - Summary data including read/write throughput and latency.
   </p>
  </li>
  <li>
   <p>
    READ-WRITE SIZE HISTOGRAM - Distribution of the size of read and write IOs.
   </p>
  </li>
  <li>
   <p>
    PER_GPU STATS - Various statistics for each GPU, including read and write throughput, counters for sparse IOs, POSIX IOs, errors, unaligned IOs and data on registered buffers.
   </p>
  </li>
 </ul>
 <p>
  The next two stats provide information on the buffer pool used for bounce buffers for both GDS IO and POSIX IO. These pools use fixed size 1MB buffers in a 128MB pool (See “max_device_cache_size_kb” : 131072 in the
  <span>
   /etc/cufile.json
  </span>
  parameters). This pool is used when buffers are not registered, unaligned buffer or file offsets, and when the storage and GPU cross NUMA nodes (typically CPU sockets).
 </p>
 <ul>
  <li>
   <p>
    PER_GPU POOL BUFFER STATS - Bounce buffer stats when GDS is in use.
   </p>
  </li>
  <li>
   <p>
    PER_GPU POSIX POOL BUFFER STATS - System memory bounce buffer stats when compat mode (POSIX IO) is used.
   </p>
  </li>
 </ul>
 <p>
  These last two stats provide data related to RDMA traffic when GDS is configured with Network Attached Storage (NAS).
 </p>
 <ul>
  <li>
   <p>
    PER_GPU RDMA STATS - RDMA traffic.
   </p>
  </li>
  <li>
   <p>
    PER_GPU RDMA MRSTATS - RDMA memory registration data.
   </p>
  </li>
 </ul>
 <p>
  The
  <span>
   gds_stats
  </span>
  are very useful for understanding important aspects of the IO load. Not just performance (BandWidth and Latency), but also the IO size distribution for understanding an important attribute of the workload, and PER_GPU STATS enable a view into which GPUs are reading/writing data to/from the storage.
 </p>
 <p>
  There are various methods that you can use to monitor
  <span>
   gds_stats
  </span>
  data at regular intervals, such as shell wrappers that define intervals and extract the data of interest. Additionally, the Linux
  <span>
   watch
  </span>
  command can be used to monitor
  <span>
   gds_stats
  </span>
  data at regular intervals:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>Every 1.0s: gds_stats -p 951816 | grep 'BandWidth\|Latency'                    
psg-dgx2-g02: Fri Nov 20 13:16:36 2020

Read BandWidth (GiB/s): 6.38327
Avg Read Latency (us): 1261
Write BandWidth (GiB/s): 0
Avg Write Latency (us): 0</p>
        </pre>
 <p>
  In the above example,
  <span>
   gds_stats
  </span>
  was started using the Linux
  <span>
   watch
  </span>
  command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>watch -n 1 "gds_stats -p 31470 | grep ‘BandWidth\|Latency’"</p>
        </pre>
 <p>
  This command results in the bandwidth and latency stats being updated in your Command Prompt window every second.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-direct-attached-storage-das">
   4. GPUDirect Storage Benchmarking on Direct Attached Storage
  </a>
 </h2>
 <p>
  This section covers benchmarking GDS on storage directly attached to the server, typically in the form of NVMe SSD devices on the PCIe bus. The specific examples on DGX-2 and DGX A100 can be used as guidelines for any server configuration. Note that in the following examples, the output of various command line tools and utilities is included. In some cases, rows or columns are deleted to improve readability and clarity.
  <a class="Link" data-cms-ai="0" id="gds-perf-dgx2" name="gds-perf-dgx2" shape="rect">
  </a>
 </p>
 <h3 id="gds-perf-dgx2">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-perf-dgx2">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-perf-dgx2" id="gds-perf-dgx2" name="gds-perf-dgx2" shape="rect">
    4.1. GPUDirect Storage Performance on DGX-2 System
   </a>
  </a>
 </h3>
 <p>
  Currently, GDS supports NVMe devices as direct attached storage, where NVMe SSDs are plugged directly into the PCIe bus. The DGX-2 system comes configured with up to 16 of these devices that are typically configured as a large RAID metadevice. As per the previous section, the DGX-2 system used to execute these examples was very specifically configured, such that pairs of NVMe SSDs on the same PCIe switch are in a RAID0 group, and the gdsio command line intentionally selects GPUs that share the same upstream PCIe switch.
 </p>
 <p>
  A Simple Example: Writing to large files with a large IO size using the GDS path.
 </p>
 <p>
  This example uses a RAID0 device configured with nvme2 and nvme3 with an ext4 file system (mounted as
  <span>
   /nvme23
  </span>
  , with a
  <span>
   gds_dir
  </span>
  subdirectory to hold the generated files).
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; gdsio -D /nvme23/gds_dir -d 2 -w 8 -s 500M -i 1M -x 0 -I 0 -T 120
IoType: READ XferType: GPUD Threads: 8 DataSetSize: 818796544/4096000(KiB) IOSize: 1024(KiB) Throughput: 6.524658 GiB/sec, Avg_Latency: 1197.370995 usecs ops: 799606 total_time 119.679102 secs</p>
        </pre>
 <p>
  Here is some additional information about the options in the example:
  <a class="Link" data-cms-ai="0" id="gds-perf-dgx2__ul_ddt_sr4_hnb" name="gds-perf-dgx2__ul_ddt_sr4_hnb" shape="rect">
  </a>
 </p>
 <ul>
  <li>
   <span>
    -D /nvme23/gds_dir
   </span>
   , the target directory.
  </li>
  <li>
   <span>
    -d 2
   </span>
   , selects GPU # 2 for data target/destination.
  </li>
  <li>
   <span>
    -w 8
   </span>
   , 8 workers (8 IO threads)
  </li>
  <li>
   <span>
    -s 500M
   </span>
   , the target file size.
  </li>
  <li>
   <span>
    -i 1M
   </span>
   , IO size (important for assessing throughput).
  </li>
  <li>
   <span>
    -x 0
   </span>
   , the IO data path, in this case GDS.
   <p>
    See
    Table 2
    in
    <a class="xref" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gdsio" shape="rect" title="The gdsio utility is similar to a number of disk/storage IO load generating tools. It supports a series of command line arguments to specify the target files, file sizes, IO sizes, number of IO threads, etc. Additionally, gdsio includes built-in support for using the traditional IO path (CPU), as well as the GDS path - storage to/from GPU memory.">
     gdsio Utility
    </a>
    for more information.
   </p>
  </li>
  <li>
   <span>
    -I 0
   </span>
   , writes the IO load (0 is for reads, 1 is for writes)
  </li>
  <li>
   <span>
    -T 120
   </span>
   , runs for 120 seconds.
  </li>
 </ul>
 <p>
  The results generated by gdsio show expected performance, given that the storage IO target is a RAID 0 configuration of the two NVMe SSDs, where each SSD is configured with around 3.4GB/sec large read performance. The average sustained throughput was 6.5GB/sec, with a 1.2ms average latency. We can look at system data during the gdsio execution for additional data points on data rates and movement. This is often useful for validating results reported by load generators, as well as ensuring the data path is as expected. Using the Linux
  <span>
   iostat
  </span>
  utility (
  <span>
   iostat -cxzk 1
  </span>
  ):
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.03    0.00    0.42    7.87    0.00   91.68

Device            r/s     rkB/s   r_await rareq-sz  w/s   wkB/s . . .  %util
md127         54360.00 6958080.00  0.00   128.00    0.00  0.00  . . .   0.00
nvme2n1       27173.00 3478144.00  1.03   128.00    0.00  0.00  . . .   100.00
nvme3n1       27179.00 3478912.00  0.95   128.00    0.00  0.00  . . .   100.00</p>
        </pre>
 <p>
  Also, data from the
  <span>
   nvidia-smi dmon
  </span>
  command:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; nvidia-smi dmon -i 2 -s putcm
# gpu   pwr gtemp mtemp    sm   mem   enc   dec rxpci txpci  mclk  pclk    fb  bar1
# Idx     W     C     C     %     %     %     %  MB/s  MB/s   MHz   MHz    MB    MB
    2    63    37    37     0     4     0     0  8923     0   958   345   326    15
    2    63    37    37     0     4     0     0  8922     0   958   345   326    15
    2    63    37    37     0     4     0     0  8930     0   958   345   326    15
    2    63    37    37     0     4     0     0  8764     0   958   345   326    15</p>
        </pre>
 <p>
  This data is consistent with the results reported by gdsio. The iostat data shows just over 3.4GB/sec from each of the two NVMe drives, and close to 1ms latency per device. Note each drive sustained about 27k writes-per-second (IOPS). The second data set from the
  <span>
   dmon
  </span>
  subcommand of
  <span>
   nvidia-smi
  </span>
  , note the
  <span>
   rxpci
  </span>
  column. Recall our
  <span>
   gdsio
  </span>
  command line initiated GPUDirect Storage reads, so reads from the storage to the GPU. We see the selected GPU, 2, receiving over 8GB/sec over PCIe. This is GPUDirect Storage in action - the GPU reading (PCIe receive) directly from the NVMe drives over PCIe.
 </p>
 <p>
  While the preceding information is important to enable an optimal configuration, the GDS software will always attempt to maintain an optimal data path, in some cases via another GPU that has better affinity to the storage targets. By monitoring PCIe traffic with either
  <span>
   nvidia-smi
  </span>
  or
  <span>
   dcgmi
  </span>
  (the command line component of DCGM), we can observe data rates in and out of the GPUs. Using a previous example, running on the same RAID0 metadevice comprised of two NVMe drives on the same PCIe switch, but specifying GPU 12 this time, and capturing GPU PCIe traffic with dcgmi, we’ll observe sub-optimal performance as GPU 12 is not on the same downstream PCIe switch as our two NVMe drives.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; gdsio -D /nvme23/gds_dir -d 12 -w 8 -s 500M -i 1M -x 0 -I 0 -T 120
IoType: READ XferType: GPUD Threads: 8 DataSetSize: 491438080/4096000(KiB) IOSize: 1024(KiB) Throughput: 3.893747 GiB/sec, Avg_Latency: 2003.091575 usecs ops: 479920 total_time 120.365276 secs</p>
        </pre>
 <p>
  Note throughput dropped from 6.5GB/sec to 3.9GB/sec, and latency almost doubled to 2ms. The PCIe traffic data tells an interesting story:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; dcgmi dmon -e 1009,1010 -d 1000
# Entity                 PCITX                 PCIRX
      Id
    GPU 0            4712070210            5237742827
    GPU 1                435418                637272
. . .
    GPU 11                476420                739272
    GPU 12             528378278            4721644934
    GPU 13                481604                741403
    GPU 14                474700                736417
    GPU 15                382261                611617</p>
        </pre>
 <p>
  Note we observe PCIe traffic on GPU 12, but also traffic on GPU 0. This is GDS in action once again. The cuFile library will select a GPU for the data buffers (GPU memory) that is on the same PCIe switch as the storage. In this case, GPU 0 was selected by the library, as it is the first of four GPUs on the same PCIe switch as the NVMe devices. The data is then moved to the target GPU (12).
 </p>
 <p>
  The net effect of a sub-optimal device selection is an overall decrease in throughput, and increase in latency. With GPU 2, the average throughput was 6.5GB/sec, average latency 1ms. With GPU 12, the average throughput was 3.9GB/sec, average latency 2ms. Thus we observe a 40% decrease in throughput and a 2X increase in latency when a non-optimal configuration is used.
 </p>
 <p>
  Not all workloads are about throughput. Smaller IO sizes and random IO patterns are an attribute of many production workloads, and assessing IOPS (IO Operations Per Second) performance is a necessary component to the storage benchmarking process.
 </p>
 <p>
  A critical component to determining what peak performance levels can be achieved is ensuring there is sufficient load. Specifically, for storage benchmarking, the number of processes/threads generating IO is critical to determining maximum performance. The
  <span>
   gdsio
  </span>
  tool provides for specifying random reads or random writes (
  <span>
   -I
  </span>
  flag). In the examples below, once again we’re showing an optimal combination of GPU (0) and NVMe devices, generating a small (4k) random read low with an increasing number of threads (
  <span>
   -w
  </span>
  ).
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; gdsio -D /nvme23/gds_dir -d 0 -w 4 -s 1G -i 4K -x 0 -I 2 -k 0308 -T 120
IoType: RANDREAD XferType: GPUD Threads: 4 DataSetSize: 11736528/4194304(KiB) IOSize: 4(KiB) Throughput: 0.093338 GiB/sec, Avg_Latency: 163.478958 usecs ops: 2934132 total_time 119.917332 secs
dgx2&gt; gdsio -D /nvme23/gds_dir -d 0 -w 8 -s 1G -i 4K -x 0 -I 2 -k 0308 -T 120 IoType: RANDREAD XferType: GPUD Threads: 8 DataSetSize: 23454880/8388608(KiB) IOSize: 4(KiB) Throughput: 0.187890 GiB/sec, Avg_Latency: 162.422553 usecs ops: 5863720 total_time 119.049917 secs
dgx2&gt; gdsio -D /nvme23/gds_dir -d 0 -w 16 -s 1G -i 4K -x 0 -I 2 -k 0308 -T 120
IoType: RANDREAD XferType: GPUD Threads: 16 DataSetSize: 48209436/16777216(KiB) IOSize: 4(KiB) Throughput: 0.385008 GiB/sec, Avg_Latency: 158.918796 usecs ops: 12052359 total_time 119.415992 secs
dgx2&gt; gdsio -D /nvme23/gds_dir -d 0 -w 32 -s 1G -i 4K -x 0 -I 2 -k 0308 -T 120
IoType: RANDREAD XferType: GPUD Threads: 32 DataSetSize: 114100280/33554432(KiB) IOSize: 4(KiB) Throughput: 0.908862 GiB/sec, Avg_Latency: 139.107219 usecs ops: 28525070 total_time 119.726070 secs
dgx2&gt; gdsio -D /nvme23/gds_dir -d 0 -w 64 -s 1G -i 4K -x 0 -I 2 -k 0308 -T 120
IoType: RANDREAD XferType: GPUD Threads: 64 DataSetSize: 231576720/67108864(KiB) IOSize: 4(KiB) Throughput: 1.848647 GiB/sec, Avg_Latency: 134.554997 usecs ops: 57894180 total_time 119.465109 secs
dgx2&gt; gdsio -D /nvme23/gds_dir -d 0 -w 128 -s 1G -i 4K -x 0 -I 2 -k 0308 -T 120
IoType: RANDREAD XferType: GPUD Threads: 128 DataSetSize: 406924776/134217728(KiB) IOSize: 4(KiB) Throughput: 3.243165 GiB/sec, Avg_Latency: 151.508258 usecs ops: 101731194 total_time 119.658960 secs</p>
        </pre>
 <p>
  We can compute the IOPS by dividing the ops value by the total time. Note in all cases the total time was just over 119 seconds (120 seconds was specified as the run duration on the command line).
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d561e2096" rowspan="1" valign="top" width="50%">
    Thread Count (-w)
   </th>
   <th align="left" class="entry" colspan="1" id="d561e2099" rowspan="1" valign="top" width="50%">
    IOPS (ops / total_time)
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2096" rowspan="1" valign="top" width="50%">
    4
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2099" rowspan="1" valign="top" width="50%">
    24,468
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2096" rowspan="1" valign="top" width="50%">
    8
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2099" rowspan="1" valign="top" width="50%">
    49,255
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2096" rowspan="1" valign="top" width="50%">
    16
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2099" rowspan="1" valign="top" width="50%">
    100,928
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2096" rowspan="1" valign="top" width="50%">
    32
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2099" rowspan="1" valign="top" width="50%">
    238,245
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2096" rowspan="1" valign="top" width="50%">
    64
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2099" rowspan="1" valign="top" width="50%">
    484,612
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2096" rowspan="1" valign="top" width="50%">
    128
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2099" rowspan="1" valign="top" width="50%">
    850,240
   </td>
  </tr>
 </table>
 <p>
  It is interesting to observe the average latency on each run (Avg_Latency) actually gets better as the number of threads and IOPS increases, with 134.5us average latency at 484,612 IOPS running 64 threads. Increasing the thread count to 128, we observe a slight uptick in latency to 151.51us while sustaining 850,240 random reads per second. Tracking latency with throughput (or, in this case, IOPS) is important in characterizing delivered performance. In this example, the specification for the NVMe drives that make up the RAID0 device indicates a random read capability of about 800k IOPS per drive. Thus, even with 128 threads generating load, the latency is excellent as the load is well within drive specifications, as each of the two drives in the RAID0 device sustained about 425,000 IOPS. This was observed with the
  <span>
   iostat
  </span>
  utility:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          16.03    0.00    6.52   76.97    0.00    0.48

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     . . .  %util
md127         856792.00 3427172.00     0.00   0.00    0.00     4.00  . . .   0.00
nvme2n1       425054.00 1700216.00     0.00   0.00    0.13     4.00  . . .   100.80
nvme3n1       431769.00 1727080.00     0.00   0.00    0.13     4.00  . . .   100.00</p>
        </pre>
 <p>
  We observe the row showing the RAID0 metadevice, md127, displaying total reads per second (r/s) reflects the sum of the two underlying NVMe drives. Extending this example to demonstrate delivered performance when a GPU target is specified that is not part of the same PCIe segment:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; gdsio -D /nvme23/gds_dir -d 10 -w 64 -s 1G -i 4K -x 0 -I 2 -k 0308 -T 120
IoType: RANDREAD XferType: GPUD Threads: 64 DataSetSize: 13268776/67108864(KiB) IOSize: 4(KiB) Throughput: 0.105713 GiB/sec, Avg_Latency: 2301.201214 usecs ops: 3317194 total_time 119.702494 secs</p>
        </pre>
 <p>
  In this example we specified GPU 10 as the data read target. Note the dramatic difference in performance. With 64 threads generating random reads, latency went from 151.51us to 2.3ms, and IOPS dropped from 850k IOPS to about 28k IOPS. This is due to the overhead of GDS using a GPU on the same PCIe segment for the primary read buffer, then moving that data to the specified GPU. Again, this can be observed when monitoring GPU PCIe traffic:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; dcgmi dmon -e 1009,1010 -d 1000 # Entity                 PCITX                 PCIRX
      Id
    GPU 0             108216883             122481373
    GPU 1                185690                 61385
    . . .
    GPU 9                183268                 60918
    GPU 10              22110153             124205217
    . . .</p>
        </pre>
 <p>
  We observe PCIe traffic on both GPU 10, which was specified in the gdsio command line, and GPU 0, which was selected by GDS as the primary read buffer due to its proximity to the NVMe devices. Using gds_stats, we can see the buffer allocation on GPU 0:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgx2&gt; gds_stats -p 1545037 -l 3
cuFile STATS VERSION : 3
GLOBAL STATS:
Total Files: 64
Total Read Errors : 0
Total Read Size (MiB): 4996
Read BandWidth (GiB/s): 0.126041
Avg Read Latency (us): 2036
Total Write Errors : 0
Total Write Size (MiB): 0
Write BandWidth (GiB/s): 0
Avg Write Latency (us): 0
READ-WRITE SIZE HISTOGRAM :
0-4(KiB): 0  0
4-8(KiB): 1279109  0
8-16(KiB): 0  0
. . .
65536-...(KiB): 0  0
PER_GPU STATS:
GPU 0 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 r_sparse=0 r_inline=0 err=0 MiB=0 Write: bw=0 util(%)=0 n=0 posix=0 unalign=0 err=0 MiB=0 BufRegister: n=0 err=0 free=0 MiB=0
. . .
GPU 9 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 r_sparse=0 r_inline=0 err=0 MiB=0 Write: bw=0 util(%)=0 n=0 posix=0 unalign=0 err=0 MiB=0 BufRegister: n=0 err=0 free=0 MiB=0
GPU 10 Read: bw=0.124332 util(%)=6387 n=1279109 posix=0 unalign=0 r_sparse=0 r_inline=0 err=0 MiB=4996 Write: bw=0 util(%)=0 n=0 posix=0 unalign=0 err=0 MiB=0 BufRegister: n=64 err=0 free=0 MiB=0
GPU 11 Read: bw=0 util(%)=0 n=0 posix=0 unalign=0 r_sparse=0 r_inline=0 err=0 MiB=0 Write: bw=0 util(%)=0 n=0 posix=0 unalign=0 err=0 MiB=0 BufRegister: n=0 err=0 free=0 MiB=0
. . .
PER_GPU POOL BUFFER STATS:
GPU : 0 pool_size_MiB : 64 usage : 63/64 used_MiB : 63</p>
        </pre>
 <p>
  The output from
  <span>
   gds_stats
  </span>
  shows Read activity on GPU 10 (specified on the gdsio command line), and POOL BUFFER activity on GPU 0, with 63 of 64 1MB buffers in use. Recall GDS selected GPU 0 because it’s the first GPU on the same PCIe segment as the NVMe drives. This illustrates one of the uses of the GPU POOL BUFFER (see section on gds_stats).
 </p>
 <p>
  There are two key points to consider based on these results. First, for small, random IO loads, a large number of threads generating load are necessary to assess peak performance capability. Second, for small, random IO loads, the performance penalty of a sub-optimal configuration is much more severe than was observed with large throughput-oriented IO loads.
 </p>
 <h3 id="gds-perf-dgx-a100">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-perf-dgx-a100">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-perf-dgx-a100" id="gds-perf-dgx-a100" name="gds-perf-dgx-a100" shape="rect">
    4.2. GPUDirect Storage Performance on a DGX A100 System
   </a>
  </a>
 </h3>
 <p>
  GDS is also supported on DGX A100 system, the world’s first 5 peta FLOPS AI system built with a new generation of GPUs, NVMe drives and network interfaces. Please refer to the
  <a class="xref" data-cms-ai="0" href="https://www.nvidia.com/en-us/data-center/dgx-a100/" rel="noopener" shape="rect" target="_blank">
   DGX A100 product page
  </a>
  for details. In this section, we will use the same test methodology we used on the DGX-2 example to benchmark GDS performance on a DGX A100 system.
 </p>
 <p>
  First, we map out the GPU and NMVe drive affinity:
 </p>
 <ul>
  <li>
   Checking the NVMe drive name and PICe BFD values.
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>dgxuser@dgxa100:~$ ls -l /dev/disk/by-path/
total 0
lrwxrwxrwx 1 root root 13 Oct 26 10:52 pci-0000:08:00.0-nvme-1 -&gt; ../../nvme0n1
lrwxrwxrwx 1 root root 13 Oct 26 10:52 pci-0000:09:00.0-nvme-1 -&gt; ../../nvme1n1
lrwxrwxrwx 1 root root 13 Oct 26 10:51 pci-0000:22:00.0-nvme-1 -&gt; ../../nvme2n1
lrwxrwxrwx 1 root root 15 Oct 26 10:51 pci-0000:22:00.0-nvme-1-part1 -&gt; ../../nvme2n1p1
lrwxrwxrwx 1 root root 15 Oct 26 10:51 pci-0000:22:00.0-nvme-1-part2 -&gt; ../../nvme2n1p2
lrwxrwxrwx 1 root root 13 Oct 26 10:51 pci-0000:23:00.0-nvme-1 -&gt; ../../nvme3n1
lrwxrwxrwx 1 root root 15 Oct 26 10:51 pci-0000:23:00.0-nvme-1-part1 -&gt; ../../nvme3n1p1
lrwxrwxrwx 1 root root 15 Oct 26 10:51 pci-0000:23:00.0-nvme-1-part2 -&gt; ../../nvme3n1p2
lrwxrwxrwx 1 root root  9 Oct 26 10:51 pci-0000:25:00.3-usb-0:1.1:1.0-scsi-0:0:0:0 -&gt; ../../sr0
lrwxrwxrwx 1 root root 13 Oct 26 10:52 pci-0000:52:00.0-nvme-1 -&gt; ../../nvme4n1
lrwxrwxrwx 1 root root 13 Oct 26 10:52 pci-0000:53:00.0-nvme-1 -&gt; ../../nvme5n1
lrwxrwxrwx 1 root root 13 Oct 26 10:52 pci-0000:89:00.0-nvme-1 -&gt; ../../nvme6n1
lrwxrwxrwx 1 root root 13 Oct 26 10:52 pci-0000:8a:00.0-nvme-1 -&gt; ../../nvme7n1
lrwxrwxrwx 1 root root 13 Oct 26 10:52 pci-0000:c8:00.0-nvme-1 -&gt; ../../nvme8n1
lrwxrwxrwx 1 root root 13 Oct 26 10:52 pci-0000:c9:00.0-nvme-1 -&gt; ../../nvme9n1</p>
        </pre>
  </li>
  <li>
   Checking the GPU index numbering correlated to the PCIe BFD:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>dgxuser@gpu01:~$ nvidia-smi --query-gpu=index,name,pci.domain,pci.bus, --format=csv
index, name, pci.domain, pci.bus
0, A100-SXM4-40GB, 0x0000, 0x07
1, A100-SXM4-40GB, 0x0000, 0x0F
2, A100-SXM4-40GB, 0x0000, 0x47
3, A100-SXM4-40GB, 0x0000, 0x4E
4, A100-SXM4-40GB, 0x0000, 0x87
5, A100-SXM4-40GB, 0x0000, 0x90
6, A100-SXM4-40GB, 0x0000, 0xB7
7, A100-SXM4-40GB, 0x0000, 0xBD</p>
        </pre>
  </li>
  <li>
   Checking the NVMe drive and GPU PCIe slot relationship:
   Copy
   Copied!
   <pre class="language-" data-line="" id="play">
            
            <p>dgxuser@dgxa100:~$ lspci -tv | egrep -i "nvidia|NVMe"
 |           +-01.1-[b1-cb]----00.0-[b2-cb]--+-00.0-[b3-b7]----00.0-[b4-b7]----00.0-[b5-b7]----00.0-[b6-b7]----00.0-[b7]----00.0  NVIDIA Corporation Device 20b0
 |           |                               |                               \-10.0-[bb-bd]----00.0-[bc-bd]----00.0-[bd]----00.0  NVIDIA Corporation Device 20b0
 |           |                               +-08.0-[be-ca]----00.0-[bf-ca]--+-00.0-[c0-c7]----00.0-[c1-c7]--+-00.0-[c2]----00.0  NVIDIA Corporation Device 1af1
 |           |                               |                               |                               +-01.0-[c3]----00.0  NVIDIA Corporation Device 1af1
 |           |                               |                               |                               +-02.0-[c4]----00.0  NVIDIA Corporation Device 1af1
 |           |                               |                               |                               +-03.0-[c5]----00.0  NVIDIA Corporation Device 1af1
 |           |                               |                               |                               +-04.0-[c6]----00.0  NVIDIA Corporation Device 1af1
 |           |                               |                               |                               \-05.0-[c7]----00.0  NVIDIA Corporation Device 1af1
 |           |                               |                               +-04.0-[c8]----00.0  Samsung Electronics Co Ltd NVMe SSD Controller PM173X
 |           |                               |                               +-08.0-[c9]----00.0  Samsung Electronics Co Ltd NVMe SSD Controller PM173X
 |           +-01.1-[81-95]----00.0-[82-95]--+-00.0-[83-8a]----00.0-[84-8a]--+-00.0-[85-88]----00.0-[86-88]--+-00.0-[87]----00.0  NVIDIA Corporation Device 20b0
 |           |                               |                               +-10.0-[89]----00.0  Samsung Electronics Co Ltd NVMe SSD Controller PM173X
 |           |                               |                               \-14.0-[8a]----00.0  Samsung Electronics Co Ltd NVMe SSD Controller PM173X
 |           |                               |                               \-10.0-[8e-91]----00.0-[8f-91]--+-00.0-[90]----00.0  NVIDIA Corporation Device 20b0
 |           +-01.1-[41-55]----00.0-[42-55]--+-00.0-[43-48]----00.0-[44-48]----00.0-[45-48]----00.0-[46-48]--+-00.0-[47]----00.0  NVIDIA Corporation Device 20b0
 |           |                               |                               \-10.0-[4c-4f]----00.0-[4d-4f]--+-00.0-[4e]----00.0  NVIDIA Corporation Device 20b0
 |           |                               +-08.0-[50-54]----00.0-[51-54]--+-00.0-[52]----00.0  Samsung Electronics Co Ltd NVMe SSD Controller PM173X
 |           |                               |                               +-04.0-[53]----00.0  Samsung Electronics Co Ltd NVMe SSD Controller PM173X
 |           +-03.2-[22]----00.0  Samsung Electronics Co Ltd NVMe SSD Controller SM981/PM981/PM983
 |           +-03.3-[23]----00.0  Samsung Electronics Co Ltd NVMe SSD Controller SM981/PM981/PM983
             +-01.1-[01-13]----00.0-[02-13]--+-00.0-[03-09]----00.0-[04-09]--+-00.0-[05-07]----00.0-[06-07]----00.0-[07]----00.0  NVIDIA Corporation Device 20b0
             |                               |                               +-10.0-[08]----00.0  Samsung Electronics Co Ltd NVMe SSD Controller PM173X
             |                               |                               \-14.0-[09]----00.0  Samsung Electronics Co Ltd NVMe SSD Controller PM173X
             |                               |                               \-10.0-[0d-0f]----00.0-[0e-0f]----00.0-[0f]----00.0  NVIDIA Corporation Device 20b0
dgxuser@dgxa100:~$</p>
        </pre>
  </li>
 </ul>
 <p>
  Then mapping the NVMe and GPU affinities:
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 4. Mapping the NVMe and GPU Affinity
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d561e2287" rowspan="1" valign="top">
    GPU #
   </th>
   <th align="left" class="entry" colspan="1" id="d561e2290" rowspan="1" valign="top">
    GPU PCIe Bus #
   </th>
   <th align="left" class="entry" colspan="1" id="d561e2293" rowspan="1" valign="top">
    NVMe #
   </th>
   <th align="left" class="entry" colspan="1" id="d561e2296" rowspan="1" valign="top">
    NVMe PCIe Bus #
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2287" rowspan="1" valign="top">
    0, 1
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2290" rowspan="1" valign="top">
    0x07, 0x0F
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2293" rowspan="1" valign="top">
    nvme0, nvme1
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2296" rowspan="1" valign="top">
    0x08, 0x09
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2287" rowspan="1" valign="top">
    2, 3
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2290" rowspan="1" valign="top">
    0x47, 0x4e
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2293" rowspan="1" valign="top">
    nvme4, nvme5
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2296" rowspan="1" valign="top">
    0x52, 0x53
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2287" rowspan="1" valign="top">
    4, 5
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2290" rowspan="1" valign="top">
    0x87, 0x90
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2293" rowspan="1" valign="top">
    nvme6, nvme7
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2296" rowspan="1" valign="top">
    0x89, 0x8a
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2287" rowspan="1" valign="top">
    6, 7
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2290" rowspan="1" valign="top">
    0xb7, 0xbd
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2293" rowspan="1" valign="top">
    nvme8, nvme9
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2296" rowspan="1" valign="top">
    0xc8, 0xc9
   </td>
  </tr>
 </table>
 <p>
  By default, all NVMe drives in DGX systems are configured as a RAID0 storage array
  <span>
   /dev/md1
  </span>
  , but they are connected with different PCIe switches, as shown below.
 </p>
 <p>
  Figure 2. “DGX-A100 Block Diagram”
 </p>
 <p>
  This configuration might lead to suboptimal performance when data is moved across PCIe switches to reach the devices configured in
  <span>
   /dev/md1
  </span>
  . We can reconfigure the raid array and re-group the two NVMe drives attached to the same PCIe switches in a single RAID0 device, and create a testing workload between this RAID0 device and affiliated GPUs attached to the same PCIe switch. The following table provides information about the new mapping of raid array and GPU affinity:
 </p>
 <table border="1" cellpadding="4" cellspacing="0" class="table" frame="border" rules="all" summary="">
  Table 5. New Mapping of the RAID Array and GPU Affinity
  <tr class="row">
   <th align="left" class="entry" colspan="1" id="d561e2413" rowspan="1" valign="top">
    Raid Array
   </th>
   <th align="left" class="entry" colspan="1" id="d561e2416" rowspan="1" valign="top">
    GPU #
   </th>
   <th align="left" class="entry" colspan="1" id="d561e2419" rowspan="1" valign="top">
    GPU PCIe
   </th>
   <th align="left" class="entry" colspan="1" id="d561e2422" rowspan="1" valign="top">
    NVMe #
   </th>
   <th align="left" class="entry" colspan="1" id="d561e2425" rowspan="1" valign="top">
    NVMe PCIe
   </th>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2413" rowspan="1" valign="top">
    /raid1 (/dev/md1)
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2416" rowspan="1" valign="top">
    0, 1
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2419" rowspan="1" valign="top">
    0x07, 0x0F
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2422" rowspan="1" valign="top">
    nvme0, nvme1
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2425" rowspan="1" valign="top">
    0x08, 0x09
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2413" rowspan="1" valign="top">
    /raid2 (/dev/md2)
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2416" rowspan="1" valign="top">
    2, 3
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2419" rowspan="1" valign="top">
    0x47, 0x4e
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2422" rowspan="1" valign="top">
    nvme4, nvme5
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2425" rowspan="1" valign="top">
    0x52, 0x53
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2413" rowspan="1" valign="top">
    /raid3 (/dev/md3)
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2416" rowspan="1" valign="top">
    4, 5
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2419" rowspan="1" valign="top">
    0x87, 0x90
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2422" rowspan="1" valign="top">
    nvme6, nvme7
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2425" rowspan="1" valign="top">
    0x89, 0x8a
   </td>
  </tr>
  <tr class="row">
   <td align="left" class="entry" colspan="1" headers="d561e2413" rowspan="1" valign="top">
    /raid4 (/dev/md4)
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2416" rowspan="1" valign="top">
    6, 7
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2419" rowspan="1" valign="top">
    0xb7, 0xbd
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2422" rowspan="1" valign="top">
    nvme8, nvme9
   </td>
   <td align="left" class="entry" colspan="1" headers="d561e2425" rowspan="1" valign="top">
    0xc8, 0xc9
   </td>
  </tr>
 </table>
 <p>
  Now we can design our tests based on the above mapping. The GPU and raid array with NVMe drives in the same row has the affinity and will produce the optimized results.
 </p>
 <p>
  Example write operation, NVMe raid 0 device and GPUs are affiliated with the same PCIe switch:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgxuser@dgxa100:~$ /usr/local/gds/tools/gdsio -D /raid4/gds-io -d 7 -w 8 -s 500M -i 1M -x 0 -I 1 -T 120
IoType: WRITE XferType: GPUD Threads: 8 DataSetSize: 901008384/4096000(KiB) IOSize: 1024(KiB) Throughput: 7.181430 GiB/sec, Avg_Latency: 1087.870030 usecs ops: 879891 total_time 119.651462 secs</p>
        </pre>
 <p>
  Observe the throughput of GDS write operation is 7.18GB/s on
  <span>
   /raid4
  </span>
  , which is close to the performance limits of 2 NVMe SSDs configured in that
  <span>
   /raid4
  </span>
  device. This can be verified with other commands which check GPUs’ and NVMes’ I/O statistics:
 </p>
 <p>
  Checking GPU PCIe TX/RX IO from DCGM: this shows data is coming out of GPU7 at 7.6GB/s
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgxuser@dgxa100:~$ dcgmi dmon -e 1009,1010 -d 1000
# Entity                 PCITX         PCIRX
  Id
    GPU 0                282398                672747
    GPU 1                286415                701223
    GPU 2                362656                778953
    GPU 3                388259                817762
    GPU 4                 305924                787709
    GPU 5                318632                826865
    GPU 6                332509                831274
    GPU 7                  8168730609         472336198
    GPU 0                148856              65476
    GPU 1                149444              66021
    GPU 2                149408              65493
    GPU 3                148862              65479
    GPU 4                147445              65019
    GPU 5                145363             62444
    GPU 6                148393              65497
    GPU 7                  8368730609         492336198</p>
        </pre>
 <p>
  Checking the GPU PCIe TX/RX IO from
  <span>
   nvidia-smi
  </span>
  utility, this shows average 7.67GB/s from all the columns captured here. These are almost identical results from the
  <span>
   nvidia-smi
  </span>
  and
  <span>
   dcgmi
  </span>
  dmon commands.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgxuser@dgxa100:~$ nvidia-smi dmon -i 7 -s putcm
# gpu   pwr gtemp mtemp    sm   mem   enc   dec rxpci txpci  mclk  pclk    fb  bar1
# Idx     W     C     C     %     %     %     %  MB/s  MB/s   MHz   MHz    MB    MB
    7    66    37    36     0     0     0     0   255  8235  1215  1095   430    14
    7    66    37    36     0     0     0     0   298  7079  1215  1095   430    14
    7    66    37    35     0     0     0     0   323  8444  1215  1095   430    14
    7    66    37    36     0     0     0     0   325  8760  1215  1095   430    14
    7    66    37    36     0     0     0     0   308  7482  1215  1095   430    14
    7    66    37    36     0     0     0     0   274  7774  1215  1095   430    14
    7    66    37    36     0     0     0     0   326  8721  1215  1095   430    14
    7    66    37    36     0     0     0     0   332  8802  1215  1095   430    14
    7    66    37    35     0     0     0     0   290  4681  1215  1095   430    14
    7    66    37    35     0     0     0     0   336  8610  1215  1095   430    14</p>
        </pre>
 <p>
  Also check the NVMe IO statistics to verify
  <span>
   /md4
  </span>
  raid array (includes nvme8n1 and nvme9n1) with the
  <span>
   iostat -cxzm 1
  </span>
  command. This command computes the NVMe IO statistics every second so the number may fluctuate somewhat, but the 7.46GB/s reading here matches GDS and GPU readings approximately.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgxuser@dgxa100:~$ iostat -cxzm 1
Linux 5.4.0-52-generic (dgxa100)        10/25/20        _x86_64_        (256 CPU)
 
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.12    0.00    0.32    0.04    0.00   99.51
 
Device            r/s     w/s     rMB/s     wMB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
loop0            0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     1.00     0.00    1.00   0.00
nvme0n1          0.18    0.45      0.02      0.02     0.00     0.00   0.32   0.14    1.04    0.17   0.00   104.33    56.09   0.07   0.00
nvme2n1          3.86    3.03      0.03      0.13     0.01    29.58   0.25  90.70    0.14    0.30   0.00     7.53    42.80   0.16   0.11
nvme1n1          0.18    0.45      0.02      0.02     0.00     0.00   0.18   0.01    1.04    0.17   0.00   105.06    56.28   0.07   0.00
nvme3n1          0.02    3.00      0.00      0.13     0.01    29.60  27.90  90.78    0.15    0.30   0.00    55.85    43.21   0.33   0.10
nvme5n1          0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.09    0.00   0.00     6.34      0.00   0.16   0.00
nvme7n1          0.00    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.09    0.00   0.00     6.34      0.00   0.14   0.00
nvme6n1          0.00    0.00      0.00      0.00    0.00     0.00   0.00   0.00    0.10    0.00   0.00     6.26      4.00   0.14   0.00
nvme4n1          0.00    0.00      0.00      0.00     0.00     0.00   0.00  16.67    0.15    0.00   0.00     6.26      4.80   0.16   0.00
nvme8n1        442.13     668.76     47.16           82.00         0.00     0.00   0.00   0.00    0.35    0.64   0.01   109.22     125.56   0.03   3.53
nvme9n1        442.11     668.76     47.16           82.00         0.00     0.00   0.00   0.00    0.34    0.66   0.01   109.23     125.56   0.03   3.53
md1              0.36    0.90      0.04      0.05     0.00     0.00   0.00   0.00    0.00    0.00   0.00   112.84      57.43   0.00   0.00
md2              0.01    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     6.51       4.00   0.00   0.00
md4            884.23    1337.52     94.32           164.01     0.00     0.00   0.00   0.00    0.00    0.00   0.00   109.22     125.56   0.00   0.00
md3              0.01    0.00      0.00      0.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     6.51      4.00   0.00   0.00
md0              0.09   32.11      0.00      0.13     0.00     0.00   0.00   0.00    0.00    0.00   0.00    51.70      4.00   0.00   0.00
 
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.11    0.00    0.41    1.90    0.00   97.59
 
Device        r/s     w/s     rMB/s     wMB/s   rrqm/s   wrqm/s  %rrqm  %wrqm  r_await  w_await aqu-sz  rareq-sz  wareq-sz  svctm  %util
nvme8n1          0.00    30171.00      0.00   3771.14      0.00      1.00   0.00   0.00    0.00     0.72    0.08      0.00    127.99   0.03 100.00
nvme9n1          0.00    30160.00      0.00   3770.00      0.00      0.00   0.00   0.00    0.00     0.77    0.05      0.00    128.00   0.03 100.00
md4              0.00    60325.00      0.00   7540.01      0.00      0.00   0.00   0.00    0.00     0.00    0.00      0.00    127.99   0.00   0.00</p>
        </pre>
 <p>
  Test result shows 7.116GB/s throughput, which is lower than the previous 7.181GB/s with NVMe and GPU affinity. The average latency 1096.57 us is also higher than the previous case. The reason is GDS in this case will use the closest GPU (GPU 6 in this example), to the target as an intermediate device to move the data, thus introducing some performance penalty. This behavior can be observed.
 </p>
 <p>
  Checking GPU PCIe TX/RX IO from “nvidia-smi”: this shows data is moving from GPU 0 -&gt; GPU 6 -&gt; target NVMe drives:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgxuser@dgxa100:~/gds$ nvidia-smi dmon -i 0 -s putcm
# gpu   pwr gtemp  mtemp    sm   mem   enc   dec rxpci txpci  mclk  pclk    fb  bar1
# Idx     W     C        C      %     %        %     %  MB/s  MB/s   MHz   MHz    MB    MB
    0    65   32     32     39     0    0      0     3  7950  1215  1095   430    14
    0    65   32     31    34     0    0      0     3  9213  1215  1095   430    14
    0    65   32     32    32     0    0      0     2  9174  1215  1095   430    14
    0    65   32     31    39     0    0      0     2  4481  1215  1095   430    14
    0    65   32     31    37     0    0      0     3  8295  1215  1095   430    14
    0    65   32     31    35     0    0      0     2  8967  1215  1095   430    14
    0    65   32     32    39     0    0      0     4  8987  1215  1095   430    14
    0    66   32      32    36     0    0      0     3  8756  1215  1095   430    14
    0    65   32     31    38     0    0      0     2  9159  1215  1095   430    14
    0    66   32     32    22     0    0      0     2  9418  1215  1095   430      14
    0    66   32     32    34     0    0      0     3  9133  1215  1095   430    14
    0    65   32     32    35     0    0      0     3  8228  1215  1095   430    14
dgxuser@dgxa100:~/gds$ nvidia-smi dmon -i 6 -s putcm
# gpu   pwr gtemp mtemp    sm   mem   enc   dec rxpci txpci  mclk  pclk    fb  bar1
# Idx     W     C       C     %     %        %     %  MB/s  MB/s   MHz   MHz    MB    MB
    6    82   37    37    37     0    0     0  9247  9623  1215  1410   426      14
    6    81   37    37    40     0    0     0 10152  9594  1215  1410   426      14
    6    82   37    37    41     0    0     0  9343  9625  1215  1410   426      14
    6    82   37    37    40     0    0     0  7448  8348  1215  1410   426      14
    6    82   37    37    39     0    0     0  9012  8595  1215  1410   426      14
    6    81   37    37    38     0    0     0  8017  9110  1215  1410   426      14
    6    82   37    37    42     0    0     0  8021  8922  1215  1410   426      14
    6    82   37    37    36     0    0     0  8408  7697  1215  1410   426      14
    6    80   37    37    24     0    0     0  9455  9541  1215  1410   426      14
    6    82   37    37    41     0    0     0  8089  5032  1215  1410   426      14
    6    82   37    37    41     0    0     0  8515  8208  1215  1410   426      14
    6    82   37      37    38     0    0     0  9056  9449  1215  1410   426      14
 
dgxuser@dgxa100:~/gds$ iostat -cxzm 1</p>
        </pre>
 <p>
  The
  <span>
   iostat
  </span>
  command also shows IO performance on
  <span>
   /md4
  </span>
  (includes nvme8n1 and nvme9n1) is 7302.01MB/s, or 7.13GB/s, very close to 7.11GB/s reported by
  <span>
   gdsio
  </span>
  .
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>dgxuser@dgxa100:~/gds$ iostat -cxzm 1
Linux 5.4.0-52-generic (dgxa100)        10/25/20        _x86_64_        (256 CPU)
Avg-cpu:  %user   %nice %system %iowait  %steal   %idle
          0.69     0.00    0.42    1.49    0.00   97.41

Device        r/s      w/s      rMB/s    wMB/s     rrqm/s   wrqm/s  %rrqm  %wrqm  r_await w_await aqu-sz rareq-sz  wareq-sz  svctm   %util
nvme2n1          0.00     6.00      0.00         0.07     0.00    14.00   0.00  70.00    0.00    0.00   0.00     0.00     12.75    1.33    0.80
nvme3n1          0.00     6.00      0.00         0.07     0.00    14.00   0.00  70.00    0.00    0.00   0.00     0.00     12.75    1.33    0.80
nvme8n1          0.00 29210.00      0.00   3651.01     0.00     1.00   0.00   0.00    0.00    0.82   0.04     0.00    127.99    0.03  100.00
nvme9n1          0.00 29232.00      0.00   3654.00     0.00     0.00   0.00   0.00    0.00    0.28   0.00     0.00    128.00    0.03  100.00
md4              0.00 58419.00      0.00   7302.01     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00    127.99    0.00    0.00
md0              0.00    16.00      0.00         0.06     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00      4.00    0.00    0.00</p>
        </pre>
 <p>
  It is expected that the examples, from NVIDIA DGX-2 and DGX A100 systems, provide sufficient detail such that the commands and methodology used to achieve maximum performance can be applied to any server system with NVMe devices and NVIDIA GPUs that support GDS.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-nas">
   5. GPUDirect Storage Benchmarking on Network Attached Storage
  </a>
 </h2>
 <p>
  NAS configurations bring the network element into the storage equation, which of course must be factored in when assessing performance. Throughput, IOPS, and latency are all dependent on the network configuration, such as the number of configured interfaces, interface speed, as well as the backend storage configuration. For example, if the storage backend is NVMe SSDs, 4 such devices doing streaming reads will saturate a single 100GB network. The entire configuration end-to-end needs to be examined and sized appropriately to ensure any potential hardware bottlenecks are identified and resolved.
  <a class="Link" data-cms-ai="0" id="gds-benchmarking-on-nfs" name="gds-benchmarking-on-nfs" shape="rect">
  </a>
 </p>
 <h3 id="gds-benchmarking-on-nfs">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-nfs">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-nfs" id="gds-benchmarking-on-nfs" name="gds-benchmarking-on-nfs" shape="rect">
    5.1. GPUDirect Storage Benchmarking on NFS
   </a>
  </a>
 </h3>
 <p>
  The Network File System, NFS, was invented by Sun Microsystems in the early 1980s, and became one of the earliest network storage solutions broadly deployed in production environments. An NFS server with direct attached storage can export that storage over the network, making exported file systems available to any number of NFS clients. NFS offers ease of use and administration, a mature code base (it’s been around for decades) which makes it relatively robust. Early implementations relied on UDP or TCP as the protocol for transferring data over the network with NFS. With Remote Direct Memory Access (RDMA) capability, network overhead is significantly reduced, enabling higher throughput and lower latency read/write operations between NFS clients and the server.
 </p>
 <p>
  With GPUDirect Storage configured for NFS, the transport protocol is RDMA, leveraging the high-speed, low-latency data flow between the client(s) and server. The RDMA operations move data over the network via the IB interfaces, then data is moved to/from the IB cards to the GPUs via DMA operations, bypassing the CPU and system memory.
 </p>
 <p>
  As discussed previously, the PCIe topology is a factor in determining the optimal configuration on the NFS client. Ideally, the NICs issuing reads/writes to the NFS server should be on the same PCIe switch as the issuing GPU. On NVIDIA DGX systems, the nvidia-smi utility helps to determine the optimal NIC/GPU pairings:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>nfs_client&gt; nvidia-smi topo -mp
        GPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    . . . mlx5_0    mlx5_1  mlx5_2  mlx5_3
GPU0     X      PIX     PXB     PXB     NODE    NODE    NODE    NODE    . . . PIX       PXB     NODE    NODE
GPU1    PIX      X      PXB     PXB     NODE    NODE    NODE    NODE    . . . PIX       PXB     NODE    NODE
GPU2    PXB     PXB      X      PIX     NODE    NODE    NODE    NODE    . . . PXB       PIX     NODE    NODE
GPU3    PXB     PXB     PIX      X      NODE    NODE    NODE    NODE    . . . PXB       PIX     NODE    NODE
GPU4    NODE    NODE    NODE    NODE     X      PIX     PXB     PXB     . . . NODE      NODE    PIX     PXB
GPU5    NODE    NODE    NODE    NODE    PIX      X      PXB     PXB     . . . NODE      NODE    PIX     PXB
GPU6    NODE    NODE    NODE    NODE    PXB     PXB      X      PIX     . . . NODE      NODE    PXB     PIX
GPU7    NODE    NODE    NODE    NODE    PXB     PXB     PIX      X      . . . NODE      NODE    PXB     PIX
mlx5_0  PIX     PIX     PXB     PXB     NODE    NODE    NODE    NODE    . . .  X        PXB     NODE    NODE
mlx5_1  PXB     PXB     PIX     PIX     NODE    NODE    NODE    NODE    . . . PXB        X      NODE    NODE
mlx5_2  NODE    NODE    NODE    NODE    PIX     PIX     PXB     PXB     . . . NODE      NODE     X      PXB
mlx5_3  NODE    NODE    NODE    NODE    PXB     PXB     PIX     PIX     . . . NODE      NODE    PXB      X

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge</p>
        </pre>
 <p>
  The above example is generated on a DGX-2 system. For brevity, 8 of the 16 GPUs and 4 of the 8 NICs are shown. From the Legend provided with the topology map, we see the optimal path is represented as PIX (
  <span>
   Connection traversing at most a single PCIe bridge
  </span>
  ). Thus, for GPU 0 and 1, the
  <span>
   mlx5_0
  </span>
  is the closest NIC, for GPU 2 and 3, it’s
  <span>
   mlx5_1
  </span>
  , etc. The network device names in Linux use the prefix
  <span>
   ib
  </span>
  , so we need to see which interface names map to which device names. This is done with the
  <span>
   ibdev2netdev
  </span>
  utility:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>nfs_client&gt; ibdev2netdev
mlx5_0 port 1 ==&gt; ib0 (Up)
mlx5_1 port 1 ==&gt; ib1 (Up)
mlx5_2 port 1 ==&gt; ib2 (Up)
mlx5_3 port 1 ==&gt; ib3 (Up)
mlx5_4 port 1 ==&gt; ib4 (Down)
mlx5_5 port 1 ==&gt; ib5 (Down)
mlx5_6 port 1 ==&gt; ib6 (Up)
mlx5_7 port 1 ==&gt; ib7 (Up)
mlx5_8 port 1 ==&gt; ib8 (Up)
mlx5_9 port 1 ==&gt; ib9 (Up)</p>
        </pre>
 <p>
  The network configuration between the NFS server and client will obviously factor significantly into the delivered performance. It is beyond the scope of this document to detail the steps involved in configuring networks, subnets, routing tables. An overview covers the basics prior to getting into examples of running
  <span>
   gdsio
  </span>
  on NFS.
 </p>
 <p>
  On the server side, there is a single RAID0 device consisting of 8 locally attached NVMe SSDs. The device is configured with an ext4 file system, and made accessible via two mount points. There are two 100GB network interfaces configured on the server for NFS traffic, and each NIC is assigned multiple IP addresses to different subnets to balance network traffic from the client.
 </p>
 <p>
  On the client side, there are 8 100GB networks configured. The network configuration (subnets, routing tables) determines which interface on the client side will route to which of the two interfaces on the server side when the client issues an NFS mount. The client-side mount paths have been intentionally named to reflect the network path between the client and the server. Specifically, the mount points on the client are decomposed as:
 </p>
 <p>
  <span>
   /mnt/nfs/
   [client-side network interface]
   /data/
   [server-side network interface]
  </span>
 </p>
 <p>
  where the client network interface name is the actual device (ib9, ib8, etc.) and the server is either 0 or 1, indicating which one of the two server networks will handle the connection.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>nfs_client&gt; mount | grep nfs
192.168.0.10:/mnt/nfs_10/10 on /mnt/nfs/ib9/data/0 type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=rdma,port=20049,​timeo=600,retrans=2,sec=sys,mountaddr=192.168.0.10,mountvers=3,mountproto=tcp,​local_lock=none,addr=192.168.0.10)
192.168.0.11:/mnt/nfs_11/11 on /mnt/nfs/ib9/data/1 type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=rdma,port=20049,​timeo=600,retrans=2,sec=sys,mountaddr=192.168.0.11,mountvers=3,mountproto=tcp,​local_lock=none,addr=192.168.0.11)
192.168.1.10:/mnt/nfs_10/10 on /mnt/nfs/ib8/data/0 type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=rdma,port=20049,​timeo=600,retrans=2,sec=sys,mountaddr=192.168.1.10,mountvers=3,mountproto=tcp,​local_lock=none,addr=192.168.1.10)
192.168.1.11:/mnt/nfs_11/11 on /mnt/nfs/ib8/data/1 type nfs (rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=rdma,port=20049,​timeo=600,retrans=2,sec=sys,mountaddr=192.168.1.11,mountvers=3,mountproto=tcp,​local_lock=none,addr=192.168.1.11)
. . .</p>
        </pre>
 <p>
  In the
  <span>
   mount
  </span>
  output on the client (partial, shown above),
  <span>
   /mnt/nfs/ib9/data/0
  </span>
  will route through the ib9 interface on the client to the interface assigned 192.168.0.10 on the server (reflected as ‘0’ in the path string), and
  <span>
   /mnt/nfs/ib9/data/1
  </span>
  will also route through ib9 on the client, to the second interface on the server, 192.168.0.11 (reflected as ‘1’ in the path string). This convention is used for all the client-side mounts, so IO pathnames can be selected to effectively balance the load across all interfaces.
 </p>
 <p>
  From a performance/benchmarking perspective, the key aspects of the configuration under test, in addition to the balanced network setup just discussed, are the underlying storage config on the server (8 x NVMe SSD) and number and speed of network interfaces on the server (2 x 100Gb). In terms of throughput, two 100Gb networks can sustain about 12GB/sec each under ideal conditions. The storage servers can support (2 x 12GB/sec) 24GB/sec or so, but keep in mind there are many things that factor into delivered performance - protocol overhead, network MTU size, NFS attributes, software stack, load characteristics, etc.
 </p>
 <p>
  In this first example, gdsio is used to generate a random write load of small IOs (4k) to one of the NFS mount points, that will traverse ib0 on the client side. The ib0 interface is on the same PCIe segment as GPU 0 and 1.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>nfs_client&gt; gdsio -D /mnt/nfs/ib0/data/0/gds_dir -d 0 -w 32 -s 500M -i 4K -x 0 -I 3 -T 120
IoType: RANDWRITE XferType: GPUD Threads: 32 DataSetSize: 81017740/16384000(KiB) IOSize: 4(KiB) Throughput: 0.645307 GiB/sec, Avg_Latency: 189.166333 usecs ops: 20254435 total_time 119.732906 secs

nfs_client&gt; gdsio -D /mnt/nfs/ib0/data/0/gds_dir -d 12 -w 32 -s 500M -i 4K -x 0 -I 3 -T 120
IoType: RANDWRITE XferType: GPUD Threads: 32 DataSetSize: 71871140/16384000(KiB) IOSize: 4(KiB) Throughput: 0.572407 GiB/sec, Avg_Latency: 213.322597 usecs ops: 17967785 total_time 119.742801 secs</p>
        </pre>
 <p>
  The first invocation specifies GPU 0, the second GPU 12. Note the difference in ops and latency. In the optimal GPU/IB case, we observed just over 169k IOPS, 189.2us average latency. In the non-optimal case (GPU 12 to the same mount point), we see an increase in latency to 213.3 usec at about 150k IOPS. The performance difference is not huge (12% in latency, 19% in IOPS), but worth noting nonetheless.
 </p>
 <p>
  For random reads, the difference between the optimal and sub-optimal case is larger:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>nfs_client&gt; gdsio -D /mnt/nfs/ib0/data/0/gds_dir -d 0 -w 32 -s 500M -i 4K -x 0 -I 2 -T 120
IoType: RANDREAD XferType: GPUD Threads: 32 DataSetSize: 111181604/16384000(KiB) IOSize: 4(KiB) Throughput: 0.890333 GiB/sec, Avg_Latency: 137.105980 usecs ops: 27795401 total_time 119.091425 secs
nfs_client&gt; gdsio -D /mnt/nfs/ib0/data/0/gds_dir -d 10 -w 32 -s 500M -i 4K -x 0 -I 2 -T 120
IoType: RANDREAD XferType: GPUD Threads: 32 DataSetSize: 78621148/16384000(KiB) IOSize: 4(KiB) Throughput: 0.629393 GiB/sec, Avg_Latency: 193.975032 usecs ops: 19655287 total_time 119.129013 secs</p>
        </pre>
 <p>
  With GPU 0 and ib0, we see about 234k IOPS, 194us average latency. With GPU 10 and ib0, we see about 165k IOPS, 194us latency.
 </p>
 <p>
  Small, random IOs are all about IOPS and latency. For determining throughput, we use larger files sizes and much larger IO sizes.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>nfs_client&gt; gdsio -D /mnt/nfs/ib0/data/0/gds_dir -d 0 -w 32 -s 1G -i 1M -x 0 -I 1 -T 120
IoType: WRITE XferType: GPUD Threads: 64 DataSetSize: 876086272/67108864(KiB) IOSize: 1024(KiB) Throughput: 6.962237 GiB/sec, Avg_Latency: 8976.802942 usecs ops: 855553 total_time 120.004668 secs
nfs_client&gt; gdsio -D /mnt/nfs/ib0/data/0/gds_dir -d 0 -w 32 -s 1G -i 1M -x 0 -I 0 -T 120 IoType: READ XferType: GPUD Threads: 32 DataSetSize: 1196929024/33554432(KiB) IOSize: 1024(KiB) Throughput: 9.482088 GiB/sec, Avg_Latency: 3295.183817 usecs ops: 1168876 total_time 120.382817 secs</p>
        </pre>
 <p>
  Above a large sequential write, then read, was generated. In both cases, 32 threads were spawned (
  <span>
   -w 32
  </span>
  ) doing 1M IOs. In the write case, we sustained 6.9GB/sec, and in the read case 9.5GB/sec throughput. Both invocations used ib0 and GPU 0. Changing the GPU from 0 to 8:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>nfs_client&gt; gdsio -D /mnt/nfs/ib0/data/0/gds_dir -d 8 -w 32 -s 1G -i 1M -x 0 -I 0 -T 120
IoType: READ XferType: GPUD Threads: 32 DataSetSize: 1053419520/33554432(KiB) IOSize: 2048(KiB) Throughput: 8.352013 GiB/sec, Avg_Latency: 7480.408305 usecs ops: 514365 total_time 120.284676 secs</p>
        </pre>
 <p>
  We note again a decrease in throughput (8.3GB/sec from 9.5GB/sec) and increase in latency (3.4ms to 7.5ms).
 </p>
 <p>
  Using the supported
  <span>
   gdsio
  </span>
  config files facilitates an easily reusable “tool box” of IO loads and configurations. Also, with multiple jobs running,
  <span>
   gdsio
  </span>
  will aggregate the results, making it easier to see the complete performance picture.
 </p>
 <p>
  Here’s a sample configuration file to generate 4k random reads to multiple NFS mount points for 4 different GPUs. Note various parameters defined in the global section, then job-specific parameters (GPU, target mount point, number of threads) in each job section.
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>[global]
name=nfs_random_read
#0,1,2,3,4,5
xfer_type=0
#IO type, rw=read, rw=write, rw=randread, rw=randwrite
rw=randread
#block size, for variable block size can specify range e.g. bs=1M:4M:1M, (1M : start block size, 4M : end block size, 1M :steps in which size is varied)
bs=4k
#file-size
size=500M
#secs
runtime=120

[job1]
#numa node
numa_node=0
#gpu device index (check nvidia-smi)
gpu_dev_id=0
num_threads=16
directory=/mnt/nfs/ib0/data/0/gds_dir

[job2]
numa_node=0
gpu_dev_id=2
num_threads=16
directory=/mnt/nfs/ib1/data/0/gds_dir

[job3]
numa_node=0
gpu_dev_id=4
num_threads=16
directory=/mnt/nfs/ib2/data/0/gds_dir

[job4]
numa_node=0
gpu_dev_id=6
num_threads=16
directory=/mnt/nfs/ib3/data/0/gds_dir</p>
        </pre>
 <p>
  Executing
  <span>
   gdsio
  </span>
  using the above config file, simply pass the file name as the only argument:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>nfs_client&gt; gdsio nfs_rr.gdsio
IoType: RANDREAD XferType: GPUD Threads: 64 DataSetSize: 277467756/32768000(KiB) IOSize: 4(KiB) Throughput: 2.213928 GiB/sec, Avg_Latency: 110.279539 usecs ops: 69366939 total_time 119.522363 secs</p>
        </pre>
 <p>
  The sustained random read rate was about 580k IOPS (69366939 / 119.52).
 </p>
 <p>
  For throughput testing, the load attributes need to be changed so gdsio issues reads and writes, not random reads and random writes (see
  <span>
   rw=
  </span>
  in the global section). Also, the IO size (
  <span>
   bs=
  </span>
  in the global section) must be increased to maximize throughput.
 </p>
 <p>
  Using an edited gdsio config file reflecting those changes, we can generate a throughput-oriented workload with multiple GPUs. The configuration file:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>[global]
name=nfs_large_read
xfer_type=0
rw=read
bs=1M
size=1G
runtime=120
do_verify=0

[job1]
numa_node=0
gpu_dev_id=0
num_threads=8
directory=/mnt/nfs/ib0/data/0/gds_dir

[job3]
numa_node=0
gpu_dev_id=2
num_threads=8
directory=/mnt/nfs/ib1/data/1/gds_dir

[job5]
numa_node=0
gpu_dev_id=4
num_threads=8
directory=/mnt/nfs/ib2/data/0/gds_dir

[job7]
numa_node=0
gpu_dev_id=6
num_threads=8
directory=/mnt/nfs/ib3/data/1/gds_dir

[job9]
numa_node=1
gpu_dev_id=8
num_threads=8
directory=/mnt/nfs/ib6/data/0/gds_dir

[job11]
numa_node=1
gpu_dev_id=10
num_threads=8
directory=/mnt/nfs/ib7/data/1/gds_dir

[job13]
numa_node=1
gpu_dev_id=12
num_threads=8
directory=/mnt/nfs/ib8/data/0/gds_dir

[job15]
numa_node=1
gpu_dev_id=14
num_threads=8
directory=/mnt/nfs/ib9/data/1/gds_dir</p>
        </pre>
 <p>
  Running the large read load:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>nfs_client&gt; gdsio nfs_sr.gdsio
IoType: READ XferType: GPUD Threads: 64 DataSetSize: 1608664064/67108864(KiB) IOSize: 1024(KiB) Throughput: 12.763141 GiB/sec, Avg_Latency: 4896.861494 usecs ops: 1570961 total_time 120.200944 secs</p>
        </pre>
 <p>
  We see 12.76GB/sec sustained throughput with the configuration used.
 </p>
 <p>
  The examples shown are intended to serve as a starting point. NAS storage environments, NFS or partner solutions, can be complex to configure given the number of variables that come into play once a network is introduced in the storage IO path. Various configuration options can be changed, both in terms of the load generated (the gdsio command line or config file), as well as the system setup (network, NFS, etc) in order to determine the optimal configuration for the target workload.
 </p>
 <p>
  In order to observe the data rates while under load, there are a few options. Certainly, on the server side, Linux utilities like
  <span>
   iostat
  </span>
  should be used to monitor traffic and capture statistics on the backend storage, as well as
  <span>
   nfsstat
  </span>
  on both the client and server side for NFS specific statistics. For byte and packet rates over the networks, there is a dependency on the actual network devices and software stack, and whether or not any per-interface statistics are maintained. In our configuration, NVIDIA/Mellanox cards are used:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>nfs_client&gt; lspci -v | grep -i mellanox
35:00.0 Infiniband controller: Mellanox Technologies MT27800 Family [ConnectX-5]
    Subsystem: Mellanox Technologies MT27800 Family [ConnectX-5]
. . .</p>
        </pre>
 <p>
  In this environment, various counters are maintained that can be examined and, with a relatively simple script, per-second data and packet rates can be monitored. These counters can be found in
  <span>
   /sys/class/infiniband/[INTERFACE]/ports/[PORT NUMBER]/counters
  </span>
  , e.g.
  <span>
   /sys/class/infiniband/mlx5_19/ports/1/counters
  </span>
  . The counters of interest are:
 </p>
 <ul>
  <li>
   <span>
    port_rcv_data
   </span>
   - receive bytes
  </li>
  <li>
   <span>
    port_xmit_data
   </span>
   - transmit bytes
  </li>
  <li>
   <span>
    port_rcv_packets
   </span>
   - receive packets
  </li>
  <li>
   <span>
    port_xmit_packets
   </span>
   - transmit packets
  </li>
 </ul>
 <p>
  Note the
  <span>
   ibdev2netdev
  </span>
  utility should be used to determine the correct interface name that corresponds to the configured device name. And of course these same counters will be available on the client, assuming of course the same network hardware and software.
 </p>
 <p>
  On the client side, GDS maintains a stats file with useful counters specific to GDS:
 </p>
 Copy
 Copied!
 <pre class="language-" data-line="" id="play">
            
            <p>nfs_client&gt; cat /proc/driver/nvidia-fs/stats
GDS Version: 0.9.0.743
NVFS statistics(ver: 2.0)
NVFS Driver(version: 2:3:1)

Active Shadow-Buffer (MiB): 128
Active Process: 1
Reads                : n=424436862 ok=424436734 err=0 readMiB=21518771 io_state_err=0
Reads                : Bandwidth(MiB/s)=13081 Avg-Latency(usec)=9773
Sparse Reads                : n=19783000 io=0 holes=0 pages=0
Writes                : n=309770912 ok=309770912 err=0 writeMiB=9748727 io_state_err=0 pg-cache=0 pg-cache-fail=0 pg-cache-eio=0
Writes                : Bandwidth(MiB/s)=6958 Avg-Latency(usec)=18386
Mmap                : n=3584 ok=3584 err=0 munmap=3456
Bar1-map            : n=3584 ok=3584 err=0 free=3456 callbacks=0 active=128
Error                : cpu-gpu-pages=0 sg-ext=0 dma-map=0
Ops                : Read=128 Write=0
GPU 0000:be:00.0  uuid:87e5c586-88ed-583b-df45-fcee0f1e7917 : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=8 cross_root_port(%)=0
GPU 0000:e7:00.0  uuid:029faa3b-cb0d-2718-259c-6dc650c636eb : Registered_MiB=0 Cache_MiB=0 max_pinned_MiB=8 cross_root_port(%)=0
. .</p>
        </pre>
 <p>
  The GDS stats provide read and write operations counts (for example,
  <span>
   Reads : n=[read_count]
  </span>
  and
  <span>
   Writes : n=[write_count]
  </span>
  ), as well Bandwidth in MB/sec. Also error counters are maintained that should be monitored periodically.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#summary">
   6. Summary
  </a>
 </h2>
 <p>
  Configuring and benchmarking storage can be a complex task. Fortunately the technology and tools have reached a state of maturity in the industry, reducing complexity and enabling reduced time to set up, measure and deploy a configuration that meets production performance requirements.
 </p>
 <p>
  With the addition of GPUDirect Storage to the GPUDirect family, we now have the technology and the tools to feed data-hungry GPUs at extremely high throughput and very low latency. This translates into faster execution of computationally intensive workloads leveraging the immense compute power of NVIDIA GPUs.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-and-performance">
   A. Benchmarking and Performance
  </a>
 </h2>
 <p>
  Benchmarking is the process of running software specifically designed to generate data for the purpose of assessing performance. The scope of what is being utilized/measured will vary. Many benchmarks are designed to simulate production workloads (
  <a class="xref" data-cms-ai="0" href="https://mlperf.org/" shape="rect">
   MLperf
  </a>
  ,
  <a class="xref" data-cms-ai="0" href="http://www.tpc.org" rel="noopener" shape="rect" target="_blank">
   TPC
  </a>
  ,
  <a class="xref" data-cms-ai="0" href="http://www.spec.org/" rel="noopener" shape="rect" target="_blank">
   SPEC
  </a>
  , and so on), utilize the entire system, and require an audit process before a vendor can make the benchmark results publicly available. Some benchmarks target measuring the performance of a specific subsystem; CPUs, GPUs, storage, etc. Such benchmarks are sometimes referred to as microbenchmarks, and are often publicly available software intended to generate load and report performance results specific to the components under test, e.g.
  <a class="xref" data-cms-ai="0" href="https://iperf.fr/" shape="rect">
   iperf
  </a>
  for network performance and
  <a class="xref" data-cms-ai="0" href="https://linux.die.net/man/1/fio" shape="rect">
   fio
  </a>
  for disk/storage performance. These subsystem-specific benchmarks and tools are extremely useful as they enable a “building-block” approach to overall system capability and performance. They also are good tools for verifying various compute subsystems before moving to a full system workload.
  <a class="Link" data-cms-ai="0" id="the-language-of-performance" name="the-language-of-performance" shape="rect">
  </a>
 </p>
 <h3 id="the-language-of-performance">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#the-language-of-performance">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#the-language-of-performance" id="the-language-of-performance" name="the-language-of-performance" shape="rect">
    A.1. The Language of Performance
   </a>
  </a>
 </h3>
 <p>
  System performance is typically described either in application-specific terms (for example, images-per-second, transactions-per-second, Deep Learning (DL) training time-per-epoch, Inference throughput, and so on) or more generic terms:
 </p>
 Bandwidth -
 how much
 Bandwidth is the theoretical maximum attainable data rate, expressed as bytes-per-second, sometimes bits-per-second.
 Throughput -
 how fast
 Throughput can reflect a data rate, e.g. 12GB/sec over a network link, 3.4GB/sec sequential reads from an NVMe SSD. We also express some workload-specific metrics in terms of throughput, for example, words-per-second, images-per-second, and so on.
 IOPS -
 how many
 Input/Output operations per second, typically used in the context of disk IO (reads-per-second, writes-per-second) and network IO (packets-per-second, messages-per-second).
 Latency -
 how long
 The time required to complete an operation, e.g. 5 milliseconds (ms) to do a disk read on an HDD, 40 microseconds (us) to write to an SSD, 70 nanoseconds (ns) for a CPU read data from system memory.
 <p>
  Specific aspects of GPU and CPU performance may also be expressed in one or more of the above terms, for example, throughput as the number of floating point or integer operations per second, memory bandwidth and latency, interconnect (UPI/QPI, NVswitch, etc.) bandwidth and latency, and so on.
 </p>
 <p>
  Monitoring performance and capturing metrics that fall into one or more of the above categories is typically done using base operating system utilities. There are a great many tools and utilities available for Linux intended to provide observability into system utilization and performance. A discussion of these tools is beyond the scope of this paper, but we will refer to specific examples as applicable. Also, various benchmark tools generate these detailed metrics, for example, the
  <span>
   gdsio
  </span>
  utility, which gets installed as part of GDS, generates detailed data points of throughput, latency, and IOPS.
 </p>
 <h3 id="benchmarking-storage-performance">
  <a data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-storage-performance">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-storage-performance" id="benchmarking-storage-performance" name="benchmarking-storage-performance" shape="rect">
    A.2. Benchmarking Storage Performance
   </a>
  </a>
 </h3>
 <p>
  Determining expected storage performance (and also network performance) has the advantage of doing the math based on the specifications of the underlying hardware being measured. For example, if the target storage is Direct Attached Storage (DAS) in the form of NVMe SSDs on the system’s PCIe bus, the device specifications provide throughput, IOPS and latency values, and that information, coupled with known PCIe bandwidth (Gen3, Gen4) and number of PCIe lanes (typically 16 lanes) configured for the devices, the maximum theoretical performance can be calculated. This is discussed further in the
  <a class="xref" data-cms-ai="0" href="https://developer.nvidia.com/blog/storage-performance-basics-for-deep-learning/" shape="rect">
   Storage Performance Basics for Deep Learning
  </a>
  blog post.
 </p>
 <p>
  For example, NVIDIA® DGX-2® systems include NVMe SSDs for local storage. For such devices configured as a RAID0 volume, the expectation for large reads and writes will be four times the specified performance for a single device. The same logic applies to workloads that are more IOPS intensive (small reads and writes). Putting some numbers on this example, the device specification for large reads indicates 3.5GB/sec, so (4 x 3.5GB/sec) 14GB/sec expected throughput on large reads from the RAID0 volume. As these are PCIe devices, depending on PCIe topology, achieving 14GB/sec pushes the theoretical limit of PCI3 Gen3 16 lanes. The key point here is a solid understanding of the configuration details is necessary to establish performance expectations.
 </p>
 <p>
  The same logic and methodology applies to Network Attached Storage (NAS), for example, four NVMe SSDs doing large reads or writes will potentially saturate a 100Gb NIC, so it is important to understand the entire storage data path to correctly assess the delivered performance. An important consideration when benchmarking storage is the presence of a file system. There are two components to file systems that will impact performance:
 </p>
 <ul>
  <li>
   The operating system’s page cache
   <p>
    The page cache, which is implemented transparently and used by default when reading and writing data to/from a file system, caches data and metadata in the system’s main memory. The performance benefit of the page cache can be substantial, as a typical two CPU socket system can read/write system memory at significantly higher throughput and lower latency than storage.
   </p>
  </li>
  <li>
   The file system-specific settings and tuneable parameters
   <p>
    File system-specific settings/parameters can happen at both creation time (
    <span>
     mkfs
    </span>
    ) and at mount time in the form of mount options. Some mount options are file system independent (for example,
    <span>
     atime
    </span>
    ,
    <span>
     async
    </span>
    , and so on), while other options are specific to the file system. Any performance evaluation of the storage technology requires bypassing the page cache to ensure that actual storage performance is being measured, and not the page cache read/write performance.
   </p>
  </li>
 </ul>
 <p>
  GDS currently supports the ext4 file system on Linux, and requires the use of the
  <span>
   O_DIRECT
  </span>
  flag on files that are targeted for read/write by the GDS IO APIs. With
  <span>
   O_DIRECT
  </span>
  set, you can bypass the system’s page cache. Another factor in assessing storage performance results are the IO characteristics, which are historically categorized as:
 </p>
 <ul>
  <li>
   Random IO (small IO size)
  </li>
  <li>
   Sequential IO (large IO size)
  </li>
 </ul>
 <p>
  The terms
  random
  and
  sequential
  refer to the on-disk block layout of the target files and are relevant when assessing storage that uses hard disk drives (HDDs). HDD technology implements spinning disks with read/write heads that perform seek operations to locate the target blocks where the data is to be read or written. This
  seek time
  induces IO latency due to electro-mechanical delays, (moving the read/write heads around the platters). With Solid State Disks (SSDs), no such electro-mechanical exist, so sequential versus random IO on SSDs is not a consideration in terms of seek times and latency.
 </p>
 <p>
  However, in terms of setting expectations for the results, it is still important to factor in IO sizes. For IO loads that tend to be small (&lt; 500kB), the performance metric of relevance is IO operations per second (IOPS) and latency, versus throughput, because loads with smaller IO sizes will not necessarily maximize available throughput. If assessing maximum throughput is the goal, larger IO sizes should be used when benchmarking.
 </p>
 <h2 class="StepModuleHeader-title">
  <a class="StepModuleHeader-anchorLink" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#notices-header">
   Notices
  </a>
 </h2>
 <h3>
  Notice
 </h3>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-1" name="notice__notice-para-1" shape="rect">
  </a>
  This document is provided for information purposes only and shall not be regarded as a warranty of a certain functionality, condition, or quality of a product. NVIDIA Corporation (“NVIDIA”) makes no representations or warranties, expressed or implied, as to the accuracy or completeness of the information contained in this document and assumes no responsibility for any errors contained herein. NVIDIA shall have no liability for the consequences or use of such information or for any infringement of patents or other rights of third parties that may result from its use. This document is not a commitment to develop, release, or deliver any Material (defined below), code, or functionality.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-2" name="notice__notice-para-2" shape="rect">
  </a>
  NVIDIA reserves the right to make corrections, modifications, enhancements, improvements, and any other changes to this document, at any time without notice.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-3" name="notice__notice-para-3" shape="rect">
  </a>
  Customer should obtain the latest relevant information before placing orders and should verify that such information is current and complete.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-4" name="notice__notice-para-4" shape="rect">
  </a>
  NVIDIA products are sold subject to the NVIDIA standard terms and conditions of sale supplied at the time of order acknowledgement, unless otherwise agreed in an individual sales agreement signed by authorized representatives of NVIDIA and customer (“Terms of Sale”). NVIDIA hereby expressly objects to applying any customer general terms and conditions with regards to the purchase of the NVIDIA product referenced in this document. No contractual obligations are formed either directly or indirectly by this document.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-5" name="notice__notice-para-5" shape="rect">
  </a>
  NVIDIA products are not designed, authorized, or warranted to be suitable for use in medical, military, aircraft, space, or life support equipment, nor in applications where failure or malfunction of the NVIDIA product can reasonably be expected to result in personal injury, death, or property or environmental damage. NVIDIA accepts no liability for inclusion and/or use of NVIDIA products in such equipment or applications and therefore such inclusion and/or use is at customer’s own risk.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-6" name="notice__notice-para-6" shape="rect">
  </a>
  NVIDIA makes no representation or warranty that products based on this document will be suitable for any specified use. Testing of all parameters of each product is not necessarily performed by NVIDIA. It is customer’s sole responsibility to evaluate and determine the applicability of any information contained in this document, ensure the product is suitable and fit for the application planned by customer, and perform the necessary testing for the application in order to avoid a default of the application or the product. Weaknesses in customer’s product designs may affect the quality and reliability of the NVIDIA product and may result in additional or different conditions and/or requirements beyond those contained in this document. NVIDIA accepts no liability related to any default, damage, costs, or problem which may be based on or attributable to: (i) the use of the NVIDIA product in any manner that is contrary to this document or (ii) customer product designs.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-7" name="notice__notice-para-7" shape="rect">
  </a>
  No license, either expressed or implied, is granted under any NVIDIA patent right, copyright, or other NVIDIA intellectual property right under this document. Information published by NVIDIA regarding third-party products or services does not constitute a license from NVIDIA to use such products or services or a warranty or endorsement thereof. Use of such information may require a license from a third party under the patents or other intellectual property rights of the third party, or a license from NVIDIA under the patents or other intellectual property rights of NVIDIA.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-8" name="notice__notice-para-8" shape="rect">
  </a>
  Reproduction of information in this document is permissible only if approved in advance by NVIDIA in writing, reproduced without alteration and in full compliance with all applicable export laws and regulations, and accompanied by all associated conditions, limitations, and notices.
 </p>
 <p>
  <a class="Link" data-cms-ai="0" id="notice__notice-para-9" name="notice__notice-para-9" shape="rect">
  </a>
  THIS DOCUMENT AND ALL NVIDIA DESIGN SPECIFICATIONS, REFERENCE BOARDS, FILES, DRAWINGS, DIAGNOSTICS, LISTS, AND OTHER DOCUMENTS (TOGETHER AND SEPARATELY, “MATERIALS”) ARE BEING PROVIDED “AS IS.” NVIDIA MAKES NO WARRANTIES, EXPRESSED, IMPLIED, STATUTORY, OR OTHERWISE WITH RESPECT TO THE MATERIALS, AND EXPRESSLY DISCLAIMS ALL IMPLIED WARRANTIES OF NONINFRINGEMENT, MERCHANTABILITY, AND FITNESS FOR A PARTICULAR PURPOSE. TO THE EXTENT NOT PROHIBITED BY LAW, IN NO EVENT WILL NVIDIA BE LIABLE FOR ANY DAMAGES, INCLUDING WITHOUT LIMITATION ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES, HOWEVER CAUSED AND REGARDLESS OF THE THEORY OF LIABILITY, ARISING OUT OF ANY USE OF THIS DOCUMENT, EVEN IF NVIDIA HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. Notwithstanding any damages that customer might incur for any reason whatsoever, NVIDIA’s aggregate and cumulative liability towards customer for the products described herein shall be limited in accordance with the Terms of Sale for the product.
 </p>
 <h3>
  OpenCL
 </h3>
 <p>
  OpenCL is a trademark of Apple Inc. used under license to the Khronos Group Inc.
 </p>
 <h3>
  Trademarks
 </h3>
 <p>
  NVIDIA, the NVIDIA logo, DGX, DGX-1, DGX-2, DGX-A100, Tesla, and Quadro are trademarks and/or registered trademarks of NVIDIA Corporation in the United States and other countries. Other company and product names may be trademarks of the respective companies with which they are associated.
 </p>
 <span class="Page-copyright-text">
  © 2020-2024 NVIDIA Corporation and affiliates. All rights reserved.
 </span>
 <span class="Page-copyright-update">
  Last updated on Jun 14, 2024.
 </span>
 Topics
 <ul class="Book-items">
  <li class="Book-items-item">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/index.html">
    NVIDIA GPUDirect Storage
   </a>
  </li>
  <li class="Book-items-item">
   <span class="Link">
    NVIDIA GPUDirect Storage
   </span>
   <ul class="Chapter-chapters">
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html">
      NVIDIA GPUDirect Storage Design Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#design-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#data-transfer-gpu-storage">
        2. Data Transfer Issues for GPU and Storage
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#gds-storage-benefits">
        3. GPUDirect Storage Benefits
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#app-sustain">
        4. Application Suitability
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#transfers-to-from-gpu">
          4.1. Transfers To and From the GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#io-bottleneck">
          4.2. Understanding IO Bottlenecks
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#explicit">
          4.3. Explicit GDS APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#pinned">
          4.4. Pinned Memory for DMA Transfers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#cufile-apis">
          4.5. cuFile APIs
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#plat-perf-stable">
        5. Platform Performance Suitability
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#bandw-from-storage">
          5.1. Bandwidth from Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#path-storage-gpu">
          5.2. Paths from Storage to GPUs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#gpu-bar1-size">
          5.3. GPU BAR1 Size
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/design-guide/index.html#call-to-action">
        6. Call to Action
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html">
      NVIDIA GPUDirect Storage Overview Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#overview-intro">
        1. Introduction
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#related-docs">
          1.1. Related Documents
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dev-benefits">
          1.2. Benefits for a Developer
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#intended-uses">
          1.3. Intended Uses
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#func-overview">
        2. Functional Overview
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#explicit-and-direct">
          2.1. Explicit and Direct
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#perf-optimize">
          2.2. Performance Optimizations
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#imp-perf-enhance">
            2.2.1. Implementation Performance Enhancements
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#concurrency-across-threads">
            2.2.2. Concurrency Across Threads
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#asychrony">
            2.2.3. Asynchrony
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#batching">
            2.2.4. Batching
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#streams">
            2.2.5. Use of CUDA Streams in cuFile
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#comp-and-gen">
          2.3. Compatibility and Generality
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#monitoring">
          2.4. Monitoring
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#solution-scope">
          2.5. Scope of the Solutions in GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dynamic-routing-overview">
          2.6. Dynamic Routing
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#cufile-config-dynamic-routing">
            2.6.1. cuFile Configuration for Dynamic Routing
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#cufile-config-for-dfs-mount">
            2.6.2. cuFile Configuration for DFS Mount
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#dynamic-routing-cufile-config-validation">
            2.6.3. cuFile Configuration Validation for Dynamic Routing
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#software-arch">
        3. Software Architecture
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#software-comp">
          3.1. Software Components
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#prim-comp">
          3.2. Primary Components
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#gds-workflows">
            3.2.1. Workflows for GDS Functionality
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-1">
            3.2.2. Workflow 1
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#workflow-2">
            3.2.3. Workflow 2
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#align-linux-initiatives">
          3.3. Aligning with Other Linux Initiatives
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#deployment">
        4. Deployment
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#deploy-software-comp">
          4.1. Software Components for Deployment
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/overview-guide/index.html#using-gds-containers">
          4.2. Using GPUDirect Storage in Containers
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html">
      cuFile API Reference Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#introduction">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#usage">
        2. Usage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#dynamic-interactions">
          2.1. Dynamic Interactions
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#driver-file-buffer">
          2.2. Driver, File, and Buffer Management
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-compatibility-mode">
          2.3. cuFile Compatibility Mode
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-api-specification">
        3. cuFile API Specification
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#data-types">
          3.1. Data Types
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#declarations-and-definitions">
            3.1.1. Declarations and Definitions
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#typedefs">
            3.1.2. Typedefs
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#enumerations">
            3.1.3. Enumerations
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-driver-api">
          3.2. cuFile Driver APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-io-api">
          3.3. cuFile Synchronous IO APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-file-handle-api">
          3.4. cuFile File Handle APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-buffer-api">
          3.5. cuFile Buffer APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-stream-api">
          3.6. cuFile Stream APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-batch-api">
          3.7. cuFile Batch APIs
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-api-functional-specification">
        4. cuFile API Functional Specification
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriver-api-functional-specification">
          4.1. cuFileDriver API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriveropen">
            4.1.1. cuFileDriverOpen
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriverclose">
            4.1.2. cuFileDriverClose
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledrivergetproperties">
            4.1.3. cuFileDriverGetProperties
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetpollmode">
            4.1.4. cuFileDriverSetPollMode(bool poll, size_t poll_threshold_size)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetmaxdirectiosize">
            4.1.5. cuFileDriverSetMaxDirectIOSize(size_t max_direct_io_size)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetmaxcachesize">
            4.1.6. cuFileDriverSetMaxCacheSize(size_t max_cache_size)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufiledriversetmaxpinnedmemsize">
            4.1.7. cuFileDriverSetMaxPinnedMemSize(size_t max_pinned_memory_size)
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-io-api-functional-specification">
          4.2. cuFile IO API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilehandleregister">
            4.2.1. cuFileHandleRegister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#unique_1423451733">
            4.2.2. cuFileHandleDeregister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufileread">
            4.2.3. cuFileRead
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilewrite">
            4.2.4. cuFileWrite
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-memory-mgmt-functional-specification">
          4.3. cuFile Memory Management Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebufregister">
            4.3.1. cuFileBufRegister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebufderegister">
            4.3.2. cuFileBufDeregister
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-stream-api-functional-specification">
          4.4. cuFile Stream API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilestreamregister">
            4.4.1. cuFileStreamRegister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilestreamderegister">
            4.4.2. cuFileStreamDeregister
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilereadasync">
            4.4.3. cuFileReadAsync
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilewriteasync">
            4.4.4. cuFileWriteAsync
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufile-batch-api-functional-specification">
          4.5. cuFile Batch API Functional Specification
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiosetup">
            4.5.1. cuFileBatchIOSetUp
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiosubmit">
            4.5.2. cuFileBatchIOSubmit
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiogetstatus">
            4.5.3. cuFileBatchIOGetStatus
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiocancel">
            4.5.4. cuFileBatchIOCancel
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#cufilebatchiodestroy">
            4.5.5. cuFileBatchIODestroy
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#sample-program">
        5. Sample Program with cuFile APIs
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/api-reference-guide/index.html#batch-api-known-limitations">
        6. Known Limitations of cuFile Batch APIs
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html">
      NVIDIA GPUDirect Storage Release Notes
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#rn-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#new-features">
        2. New Features and Changes
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#mofed-fs-req">
        3. MLNX_OFED and Filesystem Requirements
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#support-matrix">
        4. Support Matrix
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#gds-enabled-libraries">
        5. GDS Enabled Libraries/Frameworks
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#included-packages">
        6. Included Packages
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#updates-bug-fixes">
        7. Minor Updates and Bug Fixes
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#known-issues">
        8. Known Issues
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/release-notes/index.html#known-limitations">
        9. Known Limitations
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html">
      Getting Started with NVIDIA GPUDirect Storage
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#gs-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#sys-admin">
        2. If you are a system administrator or a performance engineer
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#developer">
        3. If you are a developer
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#oem-odm-csp">
        4. If you are OEM, ODM, CSP
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/getting-started/index.html#troubleshoting-issues">
        5. Troubleshooting GDS issues
       </a>
      </li>
     </ul>
    </li>
   </ul>
  </li>
  <li class="Book-items-item">
   <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/index.html#understanding-gpudirect-storage">
    Understanding GPUDirect Storage
   </a>
   <ul class="Chapter-chapters">
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html">
      NVIDIA GPUDirect Storage Best Practices Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#gds-bp-intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#settings">
        2. Software Settings
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#system-settings">
          2.1. System Settings
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cuda-context-in-gpu-kernels">
          2.2. Use of CUDA Context in GPU Kernels and Storage IO
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-config">
          2.3. cuFile Configuration Settings
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#api-usage">
        3. API Usage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-driveropen">
          3.1. cuFileDriverOpen
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-handle-register">
          3.2. cuFileHandleRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-bufregister-fileread-filewrite">
          3.3. cuFileBufRegister, cuFileRead, cuFileWrite, cuFileBatchIOSubmit, cuFileBatchIOGetStatus, cuFileReadAsync, cuFileWriteAsync, and cuFileStreamRegister
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-1">
            3.3.1. IO Pattern 1
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-2">
            3.3.2. IO Pattern 2
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-3">
            3.3.3. IO Pattern 3
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-4">
            3.3.4. IO Pattern 4
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-5">
            3.3.5. IO Pattern 5
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-6">
            3.3.6. IO Pattern 6
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#io-pattern-7">
            3.3.7. IO Pattern 7
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-handle-deregister">
          3.4. cuFileHandleDeregister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-buf-deregister">
          3.5. cuFileBufDeregister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-stream-register">
          3.6. cuFileStreamRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#unique_880314822">
          3.7. cuFileStreamDeregister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/best-practices-guide/index.html#cufile-driver-close">
          3.8. cuFileDriverClose
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html">
      NVIDIA GPUDirect Storage Benchmarking and Configuration Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#introduction">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#about-this-guide">
        2. About this Guide
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-gds">
        3. Benchmarking GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#determining-pcie-device-affinity">
          3.1. Determining PCIe Device Affinity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-configuration-parameters">
          3.2. GPUDirect Storage Configuration Parameters
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#system-parameters">
            3.2.1. System Parameters
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-parameters">
            3.2.2. GPUDirect Storage Parameters
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-tools">
          3.3. GPUDirect Storage Benchmarking Tools
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gdsio">
            3.3.1. gdsio Utility
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-stats">
            3.3.2. gds-stats Tool
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-direct-attached-storage-das">
        4. GPUDirect Storage Benchmarking on Direct Attached Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-perf-dgx2">
          4.1. GPUDirect Storage Performance on DGX-2 System
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-perf-dgx-a100">
          4.2. GPUDirect Storage Performance on a DGX A100 System
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-nas">
        5. GPUDirect Storage Benchmarking on Network Attached Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#gds-benchmarking-on-nfs">
          5.1. GPUDirect Storage Benchmarking on NFS
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#summary">
        6. Summary
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-and-performance">
        A. Benchmarking and Performance
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#the-language-of-performance">
          A.1. The Language of Performance
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html#benchmarking-storage-performance">
          A.2. Benchmarking Storage Performance
         </a>
        </li>
       </ul>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html">
      NVIDIA GPUDirect Storage Installation and Troubleshooting Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#intro">
        1. Introduction
       </a>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-install">
        2. Installing GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-prereqs">
          2.1. Before You Install GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-installing">
          2.2. Installing GDS
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#config-file-system-settings">
            2.2.1. Configuring File System Settings for GDS
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-suc-install">
            2.2.2. Verifying a Successful GDS Installation
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-lib-tools">
          2.3. Installed GDS Libraries and Tools
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#uninstall-gds">
          2.4. Uninstalling GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#environment-variables">
          2.5. Environment Variables Used by GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#json-config-params">
          2.6. JSON Config Parameters Used by GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-config">
          2.7. GDS Configuration File Changes to Support Dynamic Routing
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-installed-version">
          2.8. Determining Which Version of GDS is Installed
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#experimental-repos-dgx">
          2.9. Experimental Repos for Network Install of GDS Packages for DGX Systems
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-errors">
        3. API Errors
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-1">
          3.1. CU_FILE_DRIVER_NOT_INITIALIZED
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-2">
          3.2. CU_FILE_DEVICE_NOT_SUPPORTED
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-3">
          3.3. CU_FILE_IO_NOT_SUPPORTED
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-4">
          3.4. CU_FILE_CUDA_MEMORY_TYPE_INVALID
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-troubleshooting">
        4. Basic Troubleshooting
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#log-files-gds-lib">
          4.1. Log Files for the GDS Library
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-diff-log-file-app">
          4.2. Enabling a Different cufile.log File for Each Application
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-trace-lib-calls">
          4.3. Enabling Tracing GDS Library API Calls
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#api-error-5">
          4.4. cuFileHandleRegister Error
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-cufile-errors">
          4.5. Troubleshooting Applications that Return cuFile Errors
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#error-no-activity-gds-stats">
          4.6. cuFile-* Errors with No Activity in GPUDirect Storage Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cuda-error-35">
          4.7. CUDA Runtime and Driver Mismatch with Error Code 35
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cuda-api-errors">
          4.8. CUDA API Errors when Running the cuFile-* APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#find-driver-stats">
          4.9. Finding GDS Driver Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-io-activity-gds-driver">
          4.10. Tracking IO Activity that Goes Through the GDS Driver
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#read-write-bandwidth-latency-nos">
          4.11. Read/Write Bandwidth and Latency Numbers in GDS Stats
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-regis-deregis-gpu-buffers">
          4.12. Tracking Registration and Deregistration of GPU Buffers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enabling-rdma-logging">
          4.13. Enabling RDMA-specific Logging for Userspace File Systems
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#system-not-ready">
          4.14. CUDA_ERROR_SYSTEM_NOT_READY After Installation
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#adding-udev-rules">
          4.15. Adding udev Rules for RAID Volumes
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#incomplete-write-on-nvme-drives">
          4.16. When You Observe “Incomplete write” on NVME Drives
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-async-io-failing">
          4.17. CUFILE async I/O is failing
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#advanced-troubleshooting">
        5. Advanced Troubleshooting
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#hung-cufile-no-response">
          5.1. Resolving Hung cuFile* APIs with No Response
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#kernel-panic-stack-traces">
          5.2. Sending Relevant Data to Customer Support
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-failure-stack-trace-warning">
          5.3. Resolving an IO Failure with EIO and Stack Trace Warning
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#control-gpu-bar-usage">
          5.4. Controlling GPU BAR Memory Usage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-how-much-cache">
          5.5. Determining the Amount of Cache to Set Aside
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-bar-mem-usage">
          5.6. Monitoring BAR Memory Usage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enomem-error-code">
          5.7. Resolving an ENOMEM Error Code
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-comp-mode">
          5.8. GDS and Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#enable-comp-mode">
          5.9. Enabling Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-enable-comp-mode">
          5.10. Tracking the IO After Enabling Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#bypass-gds">
          5.11. Bypassing GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-not-working-mount">
          5.12. GDS Does Not Work for a Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-posix-io-same-file">
          5.13. Simultaneously Running the GPUDirect Storage IO and POSIX IO on the Same File
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-data-verif-tests">
          5.14. Running Data Verification Tests Using GPUDirect Storage
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html">
          NVIDIA GPUDirect Storage Installation and Troubleshooting Guide
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-perf">
        6. Troubleshooting Performance
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#perf-benchmark-examples">
          6.1. Running Performance Benchmarks with GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-internal-cache">
          6.2. Tracking Whether GPUDirect Storage is Using an Internal Cache
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-cross-pcie-root-complex">
          6.3. Tracking when IO Crosses the PCIe Root Complex and Impacts Performance
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-cpu-usage">
          6.4. Using GPUDirect Statistics to Monitor CPU Activity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monit-perf-tracing">
          6.5. Monitoring Performance and Tracing with cuFile-* APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#ex-linux-tools">
          6.6. Example: Using Linux Tracing Tools
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trace-cufile-apis">
          6.7. Tracing the cuFile-* APIs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-improving-performance">
          6.8. Improving Performance using Dynamic Routing
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trouble-io-activity">
        7. Troubleshooting IO Activity
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#manage-coherency-page-cache">
          7.1. Managing Coherency of Data in the Page Cache and on Disk
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-fs-lnet">
        8. EXAScaler Filesystem LNet Troubleshooting
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-client-mod-version">
          8.1. Determining the EXAScaler Filesystem Client Module Version
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#lnet-network-setup">
          8.2. Checking the LNet Network Setup on a Client
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-peer-health">
          8.3. Checking the Health of the Peers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#multi-rail-support">
          8.4. Checking for Multi-Rail Support
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-peer-affinity">
          8.5. Checking GDS Peer Affinity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-lnet-errors">
          8.6. Checking for LNet-Level Errors
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#lnet-nids-deg-timeouts">
          8.7. Resolving LNet NIDs Health Degradation from Timeouts
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#multi-osts-peer-sel">
          8.8. Configuring LNet Networks with Multiple OSTs for Optimal Peer Selection
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#exascaler-fs-perf">
        9. Understanding EXAScaler Filesystem Performance
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#osc-tune-perf-param">
          9.1. osc Tuning Performance Parameters
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#misc-comms">
          9.2. Miscellaneous Commands for osc, mdc, and stripesize
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#num-config-disks">
          9.3. Getting the Number of Configured Object-Based Disks
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#addl-stats-exascaler-fs">
          9.4. Getting Additional Statistics related to the EXAScaler Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#get-md-stats">
          9.5. Getting Metadata Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-exist-mount-lustre">
          9.6. Checking for an Existing Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-fs-cluster">
          9.7. Unmounting an EXAScaler Filesystem Cluster
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#sum-exacaler-fs-stats">
          9.8. Getting a Summary of EXAScaler Filesystem Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-poll-mode">
          9.9. Using GPUDirect Storage in Poll Mode
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trouble-faq-wekafs">
        10. Troubleshooting and FAQ for the WekaIO Filesystem
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#download-wekaio-cp">
          10.1. Downloading the WekaIO Client Package
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#wekaio-version-gds">
          10.2. Determining Whether the WekaIO Version is Ready for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-wekaio-fs-cluster">
          10.3. Mounting a WekaIO File System Cluster
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#resolve-fail-mount">
          10.4. Resolving a Failing Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#resolve-wekaio-usage">
          10.5. Resolving 100% Usage for WekaIO for Two Cores
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-existing-mount-wekafs">
          10.6. Checking for an Existing Mount in the Weka File System
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-summ-wekaio">
          10.7. Checking for a Summary of the WekaIO Filesystem Status
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#summ-wekaio-stats">
          10.8. Displaying the Summary of the WekaIO Filesystem Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#wekaio-writes-posix">
          10.9. Why WekaIO Writes Go Through POSIX
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-nvdiafsko-support">
          10.10. Checking for nvidia-fs.ko Support for Memory Peer Direct
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-mem-peer-stats">
          10.11. Checking Memory Peer Direct Stats
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#rel-nvidia-fs-stats">
          10.12. Checking for Relevant nvidia-fs Statistics for the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-wekaio-fs-test">
          10.13. Conducting a Basic WekaIO Filesystem Test
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-wekaio-fs-cluster">
          10.14. Unmounting a WekaIO File System Cluster
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-installed-libs-wekaio-fs">
          10.15. Verify the Installed Libraries for the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-config-file-changes">
          10.16. GDS Configuration File Changes to Support the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#relevant-userspace-stats-wekaio-fs">
          10.17. Check for Relevant User-Space Statistics for the WekaIO Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-wekafs-support">
          10.18. Check for WekaFS Support
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#spectrum-scale-intro">
        11. Enabling IBM Spectrum Scale Support with GDS
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#spectrum-scale-limitations-with-gds">
          11.1. IBM Spectrum Scale Limitations with GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-for-peerdirect-support">
          11.2. Checking nvidia-fs.ko Support for Mellanox PeerDirect
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verifying-libraries-for-spectrum-scale">
          11.3. Verifying Installed Libraries for IBM Spectrum Scale
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-peerdirect-stats">
          11.4. Checking PeerDirect Stats
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#checking-for-relevant-stats">
          11.5. Checking for Relevant nvidia-fs Stats with IBM Spectrum Scale
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-user-space-stats">
          11.6. GDS User Space Stats for IBM Spectrum Scale for Each Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-configs-to-support-spectrum-scale">
          11.7. GDS Configuration to Support IBM Spectrum Scale
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#scenarios-compat-mode">
          11.8. Scenarios for Falling Back to Compatibility Mode
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-limitations-with-spectrum-scale">
          11.9. GDS Limitations with IBM Spectrum Scale
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-intro">
        12. NetApp E-series BeeGFS with GDS Solution Deployment
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-package-requirements">
          12.1. Netapp BeeGFS/GPUDirect Storage and Package Requirements
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-client-config-for-gds">
          12.2. BeeGFS Client Configuration for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-gpu-hca-topology">
          12.3. GPU/HCA Topology on the Client - DGX-A100 and OSS servers Client Server
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-setup">
          12.4. Verify the Setup
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-management-node">
            12.4.1. List the Management Node
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-metadata-nodes">
            12.4.2. List the Metadata Nodes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-storage-nodes">
            12.4.3. List the Storage Nodes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-client-nodes">
            12.4.4. List the Client Nodes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-display-client-connections">
            12.4.5. Display Client Connections
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-connectivity-svcs">
            12.4.6. Verify Connectivity to the Different Services
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-list-storage-pools">
            12.4.7. List Storage Pools
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-display-free-space-inodes">
            12.4.8. Display the Free Space and inodes on the Storage and Metadata Targets
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-testing">
          12.5. Testing
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-verify-integration">
            12.5.1. Verifying Integration is Working
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#beegfs-conducting-basic-fs-test">
            12.5.2. Conducting a Basic NetApp BeeGFS Filesystem Test
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#troubleshoot-set-up-vast-data">
        13. Setting Up and Troubleshooting VAST Data (NFSoRDMA+MultiPath)
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-mofed-vast">
          13.1. Installing MLNX_OFED and VAST NFSoRDMA+Multipath Packages
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#client-software-reqs">
            13.1.1. Client Software Requirements
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-vast">
            13.1.2. Install the VAST Multipath Package
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#networking-setup">
          13.2. Set Up the Networking
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#vast-network-config">
            13.2.1. VAST Network Configuration
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cllent-network-config">
            13.2.2. Client Network Configuration
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#verify-network-connect">
            13.2.3. Verify Network Connectivity
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-vast-nfs">
          13.3. Mount VAST NFS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#debug-monitor">
          13.4. Debugging and Monitoring VAST Data
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nvme-nvmeof-support">
        14. Troubleshooting and FAQ for NVMe and NVMeOF Support
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mofed-req-install">
          14.1. MLNX_OFED Requirements and Installation
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-nvme-support-gds">
          14.2. Determining Whether the NVMe device is Supported for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#det-raid-level">
          14.3. RAID Support in GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#mount-local-fs">
          14.4. Mounting a Local Filesystem for GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-exist-mount">
          14.5. Check for an Existing EXT4 Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#check-io-stats-block-devmt">
          14.6. Check for IO Statistics with Block Device Mount
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#raid-group-config-gpu-aff">
          14.7. RAID Group Configuration for GPU Affinity
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#basic-ext4-fs-test">
          14.8. Conduct a Basic EXT4 Filesystem Test
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unmount-ext4-fs">
          14.9. Unmount a EXT4 Filesystem
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#udev-name-conv-block-device">
          14.10. Udev Device Naming for a Block Device
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#batch-io-performance">
          14.11. BATCH I/O Performance
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#display-gds-driver-stats">
        15. Displaying GDS NVIDIA FS Driver Statistics
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nvidia-fs-stats2">
          15.1. nvidia-fs Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#analyze-per-gpu-stats">
          15.2. Analyze Statistics for each GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#reset-nvidia-fs-stats">
          15.3. Resetting the nvidia-fs Statistics
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#review-peer-aff-stats">
          15.4. Checking Peer Affinity Stats for a Kernel Filesystem and Storage Drivers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#peer-affinity-usage-kernel-fs-sd">
          15.5. Checking the Peer Affinity Usage for a Kernel File System and Storage Drivers
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gpu-peer-dist-table">
          15.6. Display the GPU-to-Peer Distance Table
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gdsio-tool">
          15.7. The GDSIO Tool
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tab-fields">
          15.8. Tabulated Fields
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gdscheck">
          15.9. The GDSCHECK Tool
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nfs-suppport-gds">
          15.10. NFS Support with GPUDirect Storage
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-nfs-server-rdma-mofed_5-1">
            15.10.1. Install Linux NFS server with RDMA Support on MLNX_OFED 5.3 or Later
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#install-gds-supp-nfs-client">
            15.10.2. Install GPUDirect Storage Support for the NFS Client
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#nfs-gds-stas-debug">
          15.11. NFS GPUDirect Storage Statistics and Debugging
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-io-behavior">
          15.12. GPUDirect Storage IO Behavior
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#read-write-atomiticity-cons">
            15.12.1. Read/Write Atomicity Consistency with GPUDirect Storage Direct IO
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#write-file-in-o-append-mode">
            15.12.2. Write with File a Opened in O_APPEND Mode (cuFileWrite)
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gpu-nic-peer-aff">
            15.12.3. GPU to NIC Peer Affinity
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#comp-mode-unreg-buffers">
            15.12.4. Compatible Mode with Unregistered Buffers
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#unaligned-writes-non-reg-buff">
            15.12.5. Unaligned writes with Non-Registered Buffers
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#process-hang-nfs">
            15.12.6. Process Hang with NFS
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tool-limit-cuda-9">
            15.12.7. Tools Support Limitations for CUDA 9 and Earlier
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-stats">
          15.13. GDS Statistics for Dynamic Routing
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-peer-affinity">
            15.13.1. Peer Affinity Dynamic Routing
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-cufile-log">
            15.13.2. cuFile Log Related to Dynamic Routing
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-lib-tracing">
        16. GDS Library Tracing
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#display-tracepoints">
          16.1. Example: Display Tracepoints
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#tracepoint-arguments">
            16.1.1. Example: Tracepoint Arguments
           </a>
          </li>
         </ul>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-io-issue-cuapis">
          16.2. Example: Track the IO Activity of a Process that Issues cuFileRead/ cuFileWrite
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#disp-io-patters-through-gds">
          16.3. Example: Display the IO Pattern of all the IOs that Go Through GDS
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-pattern-process">
          16.4. Understand the IO Pattern of a Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#io-pattern-file-desc">
          16.5. IO Pattern of a Process with the File Descriptor on Different GPUs
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#iops-bandwidth-process-gpu">
          16.6. Determine the IOPS and Bandwidth for a Process in a GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#freq-reads-cufileread-api">
          16.7. Display the Frequency of Reads by Processes that Issue cuFileRead
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#freq-reads-cufileread-0.1-ms">
          16.8. Display the Frequency of Reads when cuFileRead Takes More than 0.1 ms
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#disp-latency-cufileread">
          16.9. Displaying the Latency of cuFileRead for Each Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#track-process-cufilebufregister">
          16.10. Example: Tracking the Processes that Issue cuFileBufRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#proc-constant-cufilebufregister">
          16.11. Example: Tracking Whether the Process is Constant when Invoking cuFileBufRegister
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#monitor-io-bb">
          16.12. Example: Monitoring IOs that are Going Through the Bounce Buffer
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#trace-cufileread-cufilewrite-issues">
          16.13. Example: Tracing cuFileRead and cuFileWrite Failures, Print, Error Codes, and Time of Failure
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#per-process-user-stats">
          16.14. Example: User-Space Statistics for Each GDS Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-stats-tool">
          16.15. Example: Viewing GDS User-Level Statistics for a Process
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#sample-ul-stats-per-process">
          16.16. Example: Displaying Sample User-Level Statistics for each GDS Process
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#gds-space-counters">
        17. User-Space Counters in GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dist-io-util-gpu">
          17.1. Distribution of IO Usage in Each GPU
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#dynamic-routing-user-space-stats">
          17.2. User-space Statistics for Dynamic Routing
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#user-space-rdma-counters">
        18. User-Space RDMA Counters in GPUDirect Storage
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-io-counters">
          18.1. cuFile RDMA IO Counters (PER_GPU RDMA STATS)
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cufile-rdma-mem-reg-counters">
          18.2. cuFile RDMA Memory Registration Counters (RDMA MRSTATS)
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/troubleshooting-guide/index.html#cheat-sheet">
        19. Cheat Sheet for Diagnosing Problems
       </a>
      </li>
     </ul>
    </li>
    <li>
     <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html">
      NVIDIA GPUDirect Storage O_DIRECT Requirements Guide
     </a>
     <ul class="Chapter-chapters">
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#intro">
        1. Introduction
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#rel-docs">
          1.1. Related Documents
         </a>
        </li>
       </ul>
      </li>
      <li>
       <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#gds-req">
        2. GPUDirect Storage Requirements
       </a>
       <ul class="Chapter-chapters">
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#summ-basic-req">
          2.1. Summary of Basic Requirements
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#client-server">
          2.2. Client and Server
         </a>
        </li>
        <li>
         <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#odirect-not-a-fit">
          2.3. Cases Where O_DIRECT is Not a Fit
         </a>
         <ul class="Chapter-chapters">
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#buffered-io">
            2.3.1. Buffered IO
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#inline-files">
            2.3.2. Inline Files
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#block-alloc-writes">
            2.3.3. Block Allocation For Writes
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#examine-transform-data">
            2.3.4. Examining or Transforming User Data
           </a>
          </li>
          <li>
           <a class="Link" data-cms-ai="0" href="https://docs.nvidia.com/gpudirect-storage/o-direct-guide/index.html#summary">
            2.3.5. Summary
           </a>
          </li>
         </ul>
        </li>
       </ul>
      </li>
     </ul>
    </li>
   </ul>
  </li>
 </ul>
 <ul class="FooterNavigation-items" data-column-count="3">
  <li class="FooterNavigation-items-item">
   <span>
    Corporate Info
   </span>
   <ul class="FooterNavigationItem-items">
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://www.nvidia.com/en-us/" target="_blank">
      <span class="NavigationLink-text">
       NVIDIA.com Home
      </span>
     </a>
    </li>
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/" target="_blank">
      <span class="NavigationLink-text">
       About NVIDIA
      </span>
     </a>
    </li>
   </ul>
  </li>
  <li class="FooterNavigation-items-item">
   <span>
    ‎NVIDIA Developer
   </span>
   <ul class="FooterNavigationItem-items">
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://developer.nvidia.com/" target="_blank">
      <span class="NavigationLink-text">
       Developer Home
      </span>
     </a>
    </li>
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://blogs.nvidia.com/" target="_blank">
      <span class="NavigationLink-text">
       Blog
      </span>
     </a>
    </li>
   </ul>
  </li>
  <li class="FooterNavigation-items-item">
   <span>
    Resources
   </span>
   <ul class="FooterNavigationItem-items">
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://www.nvidia.com/en-us/contact/" target="_blank">
      <span class="NavigationLink-text">
       Contact Us
      </span>
     </a>
    </li>
    <li class="FooterNavigationItem-items-item">
     <a class="NavigationLink" data-cms-ai="0" href="https://developer.nvidia.com/developer-program" target="_blank">
      <span class="NavigationLink-text">
       Developer Program
      </span>
     </a>
    </li>
   </ul>
  </li>
 </ul>
 <p>
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/privacy-policy/" rel="noopener" target="_blank">
   Privacy Policy
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/privacy-center/" rel="noopener" target="_blank">
   Manage My Privacy
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/preferences/start/" rel="noopener" target="_blank">
   Do Not Sell or Share My Data
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/terms-of-service/" rel="noopener" target="_blank">
   Terms of Service
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/accessibility/" rel="noopener" target="_blank">
   Accessibility
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/about-nvidia/company-policies/" rel="noopener" target="_blank">
   Corporate Policies
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/product-security/" rel="noopener" target="_blank">
   Product Security
  </a>
  |
  <a class="Link" data-cms-ai="0" href="https://www.nvidia.com/en-us/contact/" rel="noopener" target="_blank">
   Contact
  </a>
 </p>
 <p>
  Copyright © 2024 NVIDIA Corporation
 </p>
</body>
</body></html>